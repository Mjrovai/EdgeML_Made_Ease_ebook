<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.3.450">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>EdgeML Made Easy - Counting objects with YOLO</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<link href="../../raspi/llm/llm.html" rel="next">
<link href="../../raspi/object_detection/object_detection.html" rel="prev">
<link href="../../images/cover.jpg" rel="icon" type="image/jpeg">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>


<link rel="stylesheet" href="../../styles.css">
</head>

<body class="nav-sidebar floating nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">EdgeML Made Easy</span>
    </a>
  </div>
        <div class="quarto-navbar-tools ms-auto tools-wide">
    <a href="https://github.com/Mjrovai/TinyML_Made_Easy_XIAO_ESP32S3_eBook" rel="" title="Source Code" class="quarto-navigation-tool px-1" aria-label="Source Code"><i class="bi bi-github"></i></a>
    <div class="dropdown">
      <a href="" title="Download" id="quarto-navigation-tool-dropdown-0" class="quarto-navigation-tool dropdown-toggle px-1" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download"><i class="bi bi-download"></i></a>
      <ul class="dropdown-menu dropdown-menu-end" aria-labelledby="quarto-navigation-tool-dropdown-0">
          <li>
            <a class="dropdown-item quarto-navbar-tools ms-auto-item" href="../../EdgeML-Made-Easy.pdf">
              <i class="bi bi-bi-file-pdf pe-1"></i>
            Download PDF
            </a>
          </li>
          <li>
            <a class="dropdown-item quarto-navbar-tools ms-auto-item" href="../../EdgeML-Made-Easy.epub">
              <i class="bi bi-bi-journal pe-1"></i>
            Download ePub
            </a>
          </li>
      </ul>
    </div>
    <div class="dropdown">
      <a href="" title="Share" id="quarto-navigation-tool-dropdown-1" class="quarto-navigation-tool dropdown-toggle px-1" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Share"><i class="bi bi-share"></i></a>
      <ul class="dropdown-menu dropdown-menu-end" aria-labelledby="quarto-navigation-tool-dropdown-1">
          <li>
            <a class="dropdown-item quarto-navbar-tools ms-auto-item" href="https://twitter.com/intent/tweet?url=|url|">
              <i class="bi bi-bi-twitter pe-1"></i>
            Twitter
            </a>
          </li>
          <li>
            <a class="dropdown-item quarto-navbar-tools ms-auto-item" href="https://www.linkedin.com/sharing/share-offsite/?url=|url|">
              <i class="bi bi-bi-linkedin pe-1"></i>
            LinkedIn
            </a>
          </li>
          <li>
            <a class="dropdown-item quarto-navbar-tools ms-auto-item" href="https://www.facebook.com/sharer/sharer.php?u=|url|">
              <i class="bi bi-bi-facebook pe-1"></i>
            Facebook
            </a>
          </li>
      </ul>
    </div>
  <a href="" class="quarto-reader-toggle quarto-navigation-tool px-1" onclick="window.quartoToggleReader(); return false;" title="Toggle reader mode">
  <div class="quarto-reader-toggle-btn">
  <i class="bi"></i>
  </div>
</a>
</div>
          <div id="quarto-search" class="" title="Search"></div>
      </div> <!-- /container-fluid -->
    </nav>
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
      <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../../raspi/counting_objects_yolo/counting_objects_yolo.html">Counting objects with YOLO</a></li></ol></nav>
      <a class="flex-grow-1" role="button" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
      </a>
      <button type="button" class="btn quarto-search-button" aria-label="" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal sidebar-navigation floating overflow-auto">
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Preface</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../Acknowledgements.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Acknowledgments</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../intro.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Introduction</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../about_book.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">About this Book</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../raspi/setup/setup.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Setup</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../raspi/image_classification/image_classification.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Image Classification</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../raspi/object_detection/object_detection.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Object Detection</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../raspi/counting_objects_yolo/counting_objects_yolo.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text">Counting objects with YOLO</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../raspi/llm/llm.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Small Language Models (SLM)</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../references.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">References</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../about_the_author.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">About the author</span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#introduction" id="toc-introduction" class="nav-link active" data-scroll-target="#introduction">Introduction</a></li>
  <li><a href="#installing-and-using-ultralytics-yolov8" id="toc-installing-and-using-ultralytics-yolov8" class="nav-link" data-scroll-target="#installing-and-using-ultralytics-yolov8">Installing and using Ultralytics YOLOv8</a>
  <ul class="collapse">
  <li><a href="#testing-the-yolo" id="toc-testing-the-yolo" class="nav-link" data-scroll-target="#testing-the-yolo">Testing the YOLO</a></li>
  <li><a href="#export-model-to-ncnn-format" id="toc-export-model-to-ncnn-format" class="nav-link" data-scroll-target="#export-model-to-ncnn-format">Export Model to NCNN format</a></li>
  <li><a href="#talking-about-the-yolo-model" id="toc-talking-about-the-yolo-model" class="nav-link" data-scroll-target="#talking-about-the-yolo-model">Talking about the YOLO Model</a></li>
  <li><a href="#exploring-yolo-with-python" id="toc-exploring-yolo-with-python" class="nav-link" data-scroll-target="#exploring-yolo-with-python">Exploring YOLO with Python</a></li>
  </ul></li>
  <li><a href="#estimating-the-number-of-bees" id="toc-estimating-the-number-of-bees" class="nav-link" data-scroll-target="#estimating-the-number-of-bees">Estimating the number of Bees</a>
  <ul class="collapse">
  <li><a href="#dataset" id="toc-dataset" class="nav-link" data-scroll-target="#dataset">Dataset</a></li>
  <li><a href="#pre-processing" id="toc-pre-processing" class="nav-link" data-scroll-target="#pre-processing">Pre-Processing</a></li>
  <li><a href="#training-yolov8-on-a-customized-dataset" id="toc-training-yolov8-on-a-customized-dataset" class="nav-link" data-scroll-target="#training-yolov8-on-a-customized-dataset">Training YOLOv8 on a Customized Dataset</a></li>
  </ul></li>
  <li><a href="#inference-with-the-trained-model-using-the-rasp-zero" id="toc-inference-with-the-trained-model-using-the-rasp-zero" class="nav-link" data-scroll-target="#inference-with-the-trained-model-using-the-rasp-zero">Inference with the trained model, using the Rasp-Zero</a></li>
  <li><a href="#considerations-about-the-post-processing" id="toc-considerations-about-the-post-processing" class="nav-link" data-scroll-target="#considerations-about-the-post-processing">Considerations about the Post-Processing</a>
  <ul class="collapse">
  <li><a href="#script-for-reading-the-sqlite-database" id="toc-script-for-reading-the-sqlite-database" class="nav-link" data-scroll-target="#script-for-reading-the-sqlite-database">Script For Reading the SQLite Database</a></li>
  <li><a href="#adding-environment-data" id="toc-adding-environment-data" class="nav-link" data-scroll-target="#adding-environment-data">Adding Environment data</a></li>
  </ul></li>
  <li><a href="#conclusion" id="toc-conclusion" class="nav-link" data-scroll-target="#conclusion">Conclusion</a></li>
  <li><a href="#resources" id="toc-resources" class="nav-link" data-scroll-target="#resources">Resources</a></li>
  </ul>
<div class="toc-actions"><div><i class="bi bi-github"></i></div><div class="action-links"><p><a href="https://github.com/Mjrovai/TinyML_Made_Easy_XIAO_ESP32S3_eBook/edit/main/raspi/counting_objects_yolo/counting_objects_yolo.qmd" class="toc-action">Edit this page</a></p><p><a href="https://github.com/Mjrovai/TinyML_Made_Easy_XIAO_ESP32S3_eBook/issues/new" class="toc-action">Report an issue</a></p><p><a href="https://github.com/Mjrovai/TinyML_Made_Easy_XIAO_ESP32S3_eBook/blob/main/raspi/counting_objects_yolo/counting_objects_yolo.qmd" class="toc-action">View source</a></p></div></div></nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<div class="quarto-title-block"><div><h1 class="title">Counting objects with YOLO</h1><button type="button" class="btn code-tools-button" id="quarto-code-tools-source"><i class="bi"></i> Code</button></div></div>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  

</header>

<p><strong>Deploying YOLOv8 on Raspberry Pi Zero 2W for Real-Time Bee Counting at the Hive Entrance.”</strong></p>
<p><img src="images/portada.jpeg" class="img-fluid"></p>
<section id="introduction" class="level2">
<h2 class="anchored" data-anchor-id="introduction">Introduction</h2>
<p>At the <a href="https://unifei.edu.br/">Federal University of Itajuba in Brazil</a>, with the master’s student José Anderson Reis and Professor José Alberto Ferreira Filho, we are exploring a project that delves into the intersection of technology and nature. This tutorial will review our first steps and share our observations on deploying YOLOv8, a cutting-edge machine learning model, on the compact and efficient Raspberry Pi Zero 2W (<em>Raspi-Zero</em>). We aim to estimate the number of bees entering and exiting their hive—a task crucial for beekeeping and ecological studies.</p>
<p>Why is this important? Bee populations are vital indicators of environmental health, and their monitoring can provide essential data for ecological research and conservation efforts. However, manual counting is labor-intensive and prone to errors. By leveraging the power of embedded machine learning, or tinyML, we automate this process, enhancing accuracy and efficiency.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/counting_bees.png" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">img</figcaption>
</figure>
</div>
<p>This tutorial will cover setting up the Raspberry Pi, integrating a camera module, optimizing and deploying YOLOv8 for real-time image processing, and analyzing the data gathered.</p>
</section>
<section id="installing-and-using-ultralytics-yolov8" class="level2">
<h2 class="anchored" data-anchor-id="installing-and-using-ultralytics-yolov8">Installing and using Ultralytics YOLOv8</h2>
<p><a href="https://ultralytics.com/">Ultralytics</a> <a href="https://github.com/ultralytics/ultralytics">YOLOv8</a>, is a version of the acclaimed real-time object detection and image segmentation model, YOLO. YOLOv8 is built on cutting-edge advancements in deep learning and computer vision, offering unparalleled performance in terms of speed and accuracy. Its streamlined design makes it suitable for various applications and easily adaptable to different hardware platforms, from edge devices to cloud APIs.</p>
<p>Let’s start installing the Ultarlytics packages for local inference on the Rasp-Zero:</p>
<ol type="1">
<li>Update the packages list, install pip, and upgrade to the latest:</li>
</ol>
<div class="sourceCode" id="cb1"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="fu">sudo</span> apt update</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="fu">sudo</span> apt install python3-pip <span class="at">-y</span></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="ex">pip</span> install <span class="at">-U</span> pip</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<ol start="2" type="1">
<li>Install the <code>ultralytics</code> pip package with optional dependencies:</li>
</ol>
<div class="sourceCode" id="cb2"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="ex">pip</span> install ultralytics<span class="pp">[</span><span class="ss">export</span><span class="pp">]</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<ol start="3" type="1">
<li>Reboot the device:</li>
</ol>
<div class="sourceCode" id="cb3"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="fu">sudo</span> reboot</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<section id="testing-the-yolo" class="level3">
<h3 class="anchored" data-anchor-id="testing-the-yolo">Testing the YOLO</h3>
<p>After the Rasp-Zero booting, let’s create a directory for working with YOLO and change the current location to it::</p>
<div class="sourceCode" id="cb4"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="fu">mkdir</span> Documents/YOLO</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a><span class="bu">cd</span> Documents/YOLO</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Let’s run inference on an image that will be downloaded from the Ultralytics website, using the YOLOV8n model (the smallest in the family) at the Terminal (CLI):</p>
<div class="sourceCode" id="cb5"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="ex">yolo</span> predict model=<span class="st">'yolov8n'</span> source=<span class="st">'https://ultralytics.com/images/bus.jpg'</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>The inference result will appear in the terminal. In the image (bus.jpg), 4 <code>persons</code>, 1 <code>bus,</code> and 1 <code>stop signal</code> were detected:</p>
<p><img src="images/inferfence.png" class="img-fluid"></p>
<p>Also, we got a message that <code>Results saved to runs/detect/predict4</code>. Inspecting that directory, we can see a new image saved (bus.jpg). Let’s download it from the Rasp-Zero to our desktop for inspection:</p>
<p><img src="images/bus.png" class="img-fluid"></p>
<p>So, the Ultrayitics YOLO is correctly installed on our Rasp-Zero.</p>
</section>
<section id="export-model-to-ncnn-format" class="level3">
<h3 class="anchored" data-anchor-id="export-model-to-ncnn-format">Export Model to NCNN format</h3>
<p>An issue is the high latency for this inference, 7.6 s, even with the smaller model of the family (YOLOv8n). This is a reality of deploying computer vision models on edge devices with limited computational power, such as the Rasp-Zero. One alternative is to use a format optimized for optimal performance. This ensures that even devices with limited processing power can handle advanced computer vision tasks well.</p>
<p>Of all the model export formats supported by Ultralytics, the <a href="https://docs.ultralytics.com/integrations/ncnn">NCNN</a> is a high-performance neural network inference computing framework optimized for mobile platforms. From the beginning of the design, NCNN was deeply considerate about deployment and use on mobile phones and did not have third-party dependencies. It is cross-platform and runs faster than all known open-source frameworks (such as TFLite).</p>
<p>NCNN delivers the best inference performance when working with Raspberry Pi devices. NCNN is highly optimized for mobile embedded platforms (such as ARM architecture).</p>
<p>So, let’s convert our model and rerun the inference:</p>
<ol type="1">
<li>Export a YOLOv8n PyTorch model to NCNN format, creating: ‘/yolov8n_ncnn_model’</li>
</ol>
<div class="sourceCode" id="cb6"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="ex">yolo</span> export model=yolov8n.pt format=ncnn </span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<ol start="2" type="1">
<li>Run inference with the exported model (now the source could be the bus.jpg image that was downloaded from the website to the current directory on the last inference):</li>
</ol>
<div class="sourceCode" id="cb7"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="ex">yolo</span> predict model=<span class="st">'./yolov8n_ncnn_model'</span> source=<span class="st">'bus.jpg'</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Now, we can see that the latency was reduced by half.</p>
<p><img src="images/ncnn.png" class="img-fluid"></p>
</section>
<section id="talking-about-the-yolo-model" class="level3">
<h3 class="anchored" data-anchor-id="talking-about-the-yolo-model">Talking about the YOLO Model</h3>
<p>The YOLO (You Only Look Once) model is a highly efficient and widely used object detection algorithm known for its real-time processing capabilities. Unlike traditional object detection systems that repurpose classifiers or localizers to perform detection, YOLO frames the detection problem as a single regression task. This innovative approach enables YOLO to simultaneously predict multiple bounding boxes and their class probabilities from full images in one evaluation, significantly boosting its speed.</p>
<section id="key-features" class="level4">
<h4 class="anchored" data-anchor-id="key-features">Key Features:</h4>
<ol type="1">
<li><p><strong>Single Network Architecture</strong>:</p>
<ul>
<li>YOLO employs a single neural network to process the entire image. This network divides the image into a grid and, for each grid cell, directly predicts bounding boxes and associated class probabilities. This end-to-end training improves speed and simplifies the model architecture.</li>
</ul></li>
<li><p><strong>Real-Time Processing</strong>:</p>
<ul>
<li>One of YOLO’s standout features is its ability to perform object detection in real time. Depending on the version and hardware, YOLO can process images at high frames per second (FPS). This makes it ideal for applications requiring quick and accurate object detection, such as video surveillance, autonomous driving, and live sports analysis.</li>
</ul></li>
<li><p><strong>Evolution of Versions</strong>:</p>
<ul>
<li>Over the years, YOLO has undergone significant improvements, from YOLOv1 to the latest YOLOv10. Each iteration has introduced enhancements in accuracy, speed, and efficiency. YOLOv8, for instance, incorporates advancements in network architecture, improved training methodologies, and better support for various hardware, ensuring a more robust performance.</li>
<li>Although YOLOv10 is the family’s newest member with an encouraging performance based on its paper, it was just released (May 2024) and is not fully integrated with the Ultralitycs library. Conversely, the precision-recall curve analysis suggests that YOLOv8 generally outperforms YOLOv9, capturing a higher proportion of true positives while minimizing false positives more effectively (for more details, see this <a href="https://encord.com/blog/performanceyolov9-vs-yolov8-custom-dataset/">article</a>). So, this work is based on the YOLOv8n.</li>
</ul>
<p><img src="images/latency.png" class="img-fluid"></p></li>
<li><p><strong>Accuracy and Efficiency</strong>:</p>
<ul>
<li>While early versions of YOLO traded off some accuracy for speed, recent versions have made substantial strides in balancing both. The newer models are faster and more accurate, detecting small objects (such as bees) and performing well on complex datasets.</li>
</ul></li>
<li><p><strong>Wide Range of Applications</strong>:</p>
<ul>
<li>YOLO’s versatility has led to its adoption in numerous fields. It is used in traffic monitoring systems to detect and count vehicles, security applications to identify potential threats, and agricultural technology to monitor crops and livestock. Its application extends to any domain requiring efficient and accurate object detection.</li>
</ul></li>
<li><p><strong>Community and Development</strong>:</p>
<ul>
<li>YOLO continues to evolve and is supported by a strong community of developers and researchers (being the YOLOv8 very strong). Open-source implementations and extensive documentation have made it accessible for customization and integration into various projects. Popular deep learning frameworks like Darknet, TensorFlow, and PyTorch support YOLO, further broadening its applicability.</li>
<li><a href="https://github.com/ultralytics/ultralytics?tab=readme-ov-file">Ultralitics YOLOv8</a> can not only <a href="https://docs.ultralytics.com/tasks/detect">Detect</a> (our case here) but also <a href="https://docs.ultralytics.com/tasks/segment">Segment</a> and <a href="https://docs.ultralytics.com/tasks/pose">Pose</a> models pre-trained on the <a href="https://docs.ultralytics.com/datasets/detect/coco">COCO</a> dataset and YOLOv8 <a href="https://docs.ultralytics.com/tasks/classify">Classify</a> models pre-trained on the <a href="https://docs.ultralytics.com/datasets/classify/imagenet">ImageNet</a> dataset. <a href="https://docs.ultralytics.com/modes/track">Track</a> mode is available for all Detect, Segment, and Pose models.</li>
</ul>
<img src="https://raw.githubusercontent.com/ultralytics/assets/main/im/banner-tasks.png" class="img-fluid" alt="Ultralytics YOLO supported tasks"></li>
</ol>
<p>In this tutorial, we leverage the power of YOLOv8 exported to NCNN format to estimate the number of bees at a beehive entrance using a Raspberry Pi Zero 2W (Rasp-Zero) in real-time. This setup demonstrates the practicality and effectiveness of deploying advanced machine learning models on edge devices for real-time environmental monitoring.</p>
</section>
</section>
<section id="exploring-yolo-with-python" class="level3">
<h3 class="anchored" data-anchor-id="exploring-yolo-with-python">Exploring YOLO with Python</h3>
<p>To start, let’s call the Python Interpreter so we can explore how the YOLO model works, line by line:</p>
<div class="sourceCode" id="cb8"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="ex">python</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p><img src="images/py-inter-1.png" class="img-fluid"></p>
<p>Now, we should call the YOLO library from Ultralitics and load the model:</p>
<div class="sourceCode" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> ultralytics <span class="im">import</span> YOLO</span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> YOLO(<span class="st">'yolov8n_ncnn_model'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Next, run inference over an image (let’s use again <code>bus.jpg</code>):</p>
<div class="sourceCode" id="cb10"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a>img <span class="op">=</span> <span class="st">'bus.jpg'</span></span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a>result <span class="op">=</span> model.predict(img, save<span class="op">=</span><span class="va">True</span>, imgsz<span class="op">=</span><span class="dv">640</span>, conf<span class="op">=</span><span class="fl">0.5</span>, iou<span class="op">=</span><span class="fl">0.3</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p><img src="images/py-inf2.png" class="img-fluid"></p>
<p>We can verify that the result is the same as the one we get running the inference at the terminal level (CLI).</p>
<div class="sourceCode" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a>image <span class="dv">1</span><span class="op">/</span><span class="dv">1</span> <span class="op">/</span>home<span class="op">/</span>mjrovai<span class="op">/</span>Documents<span class="op">/</span>YOLO<span class="op">/</span>bus.jpg: <span class="dv">640</span><span class="er">x640</span> <span class="dv">3</span> persons, <span class="dv">1</span> bus, <span class="fl">4048.5</span><span class="er">ms</span></span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a>Speed: <span class="fl">635.7</span><span class="er">ms</span> preprocess, <span class="fl">4048.5</span><span class="er">ms</span> inference, <span class="fl">33897.6</span><span class="er">ms</span> postprocess per image at shape (<span class="dv">1</span>, <span class="dv">3</span>, <span class="dv">640</span>, <span class="dv">640</span>)</span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a>Results saved to runs<span class="op">/</span>detect<span class="op">/</span>predict7 </span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>But, we are interested in analyzing the “result” content.</p>
<p>For example, we can see <code>result[0].boxes.data</code>, showing us the main inference result, which is a tensor shape (4, 6). Each line is one of the objects detected, being the 4 first columns, the bounding boxes coordinates, the 5th, the confidence, and the 6th, the class (in this case, <code>0: person</code> and <code>5: bus</code>):</p>
<p><img src="images/py-inter-3.png" class="img-fluid"></p>
<p>We can access several inference results separately, as the inference time, and have it printed in a better format:</p>
<div class="sourceCode" id="cb12"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a>inference_time <span class="op">=</span> <span class="bu">int</span>(result[<span class="dv">0</span>].speed[<span class="st">'inference'</span>])</span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Inference Time: </span><span class="sc">{</span>inference_time<span class="sc">}</span><span class="ss"> ms"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Or we can have the total number of objects detected:</p>
<div class="sourceCode" id="cb13"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f'Number of objects: </span><span class="sc">{</span><span class="bu">len</span> (result[<span class="dv">0</span>].boxes.cls)<span class="sc">}</span><span class="ss">'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p><img src="images/py-inter-4.png" class="img-fluid"></p>
<p>With Python, we can create a detailed output that meets our needs. In our final project, we will run a Python script at once rather than manually entering it line by line in the interpreter.</p>
<p>For that, let’s use <code>nano</code> as our text editor. First, we should create an empty Python script named, for example, <code>yolov8_tests.py</code>:</p>
<div class="sourceCode" id="cb14"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a>nano yolov8_tests.py</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Enter with the code lines:</p>
<div class="sourceCode" id="cb15"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> ultralytics <span class="im">import</span> YOLO</span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-3"><a href="#cb15-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Load the YOLOv8 model</span></span>
<span id="cb15-4"><a href="#cb15-4" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> YOLO(<span class="st">'yolov8n_ncnn_model'</span>)</span>
<span id="cb15-5"><a href="#cb15-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-6"><a href="#cb15-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Run inference</span></span>
<span id="cb15-7"><a href="#cb15-7" aria-hidden="true" tabindex="-1"></a>img <span class="op">=</span> <span class="st">'bus.jpg'</span></span>
<span id="cb15-8"><a href="#cb15-8" aria-hidden="true" tabindex="-1"></a>result <span class="op">=</span> model.predict(img, save<span class="op">=</span><span class="va">False</span>, imgsz<span class="op">=</span><span class="dv">640</span>, conf<span class="op">=</span><span class="fl">0.5</span>, iou<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb15-9"><a href="#cb15-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-10"><a href="#cb15-10" aria-hidden="true" tabindex="-1"></a><span class="co"># print the results</span></span>
<span id="cb15-11"><a href="#cb15-11" aria-hidden="true" tabindex="-1"></a>inference_time <span class="op">=</span> <span class="bu">int</span>(result[<span class="dv">0</span>].speed[<span class="st">'inference'</span>])</span>
<span id="cb15-12"><a href="#cb15-12" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Inference Time: </span><span class="sc">{</span>inference_time<span class="sc">}</span><span class="ss"> ms"</span>)</span>
<span id="cb15-13"><a href="#cb15-13" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f'Number of objects: </span><span class="sc">{</span><span class="bu">len</span> (result[<span class="dv">0</span>].boxes.cls)<span class="sc">}</span><span class="ss">'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p><img src="images/python_test.png" class="img-fluid"></p>
<p>And enter with the commands: <code>[CTRL+O]</code> + <code>[ENTER]</code> +<code>[CTRL+X]</code> to save the Python script.</p>
<p>Run the script:</p>
<div class="sourceCode" id="cb16"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a><span class="ex">python</span> yolov8_tests.py</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p><img src="images/py-inf1.png" class="img-fluid"></p>
<p>We can verify again that the result is precisely the same as when we run the inference at the terminal level (CLI) and with the built-in Python interpreter.</p>
<section id="note-about-the-latency" class="level4">
<h4 class="anchored" data-anchor-id="note-about-the-latency">Note about the Latency:</h4>
<p>The process of calling the YOLO library and loading the model for inference for the first time takes a long time, but the inferences after that will be much faster. For example, the first single inference took 3 to 4 seconds, but after that, the inference time is reduced to less than 1 second.</p>
<p><img src="images/latency_test.png" class="img-fluid"></p>
</section>
</section>
</section>
<section id="estimating-the-number-of-bees" class="level2">
<h2 class="anchored" data-anchor-id="estimating-the-number-of-bees">Estimating the number of Bees</h2>
<p>For our project at the university, we are preparing to collect a dataset of bees at the entrance of a beehive using the same camera connected to the Rasp-Zero. The images should be collected every 10 seconds. With the Arducam OV5647, the horizontal Field of View (FoV) is 53.5<sup>o</sup>, which means that a camera positioned at the top of a standard Hive (46 cm) will capture all of its entrance (about 47 cm).</p>
<p><img src="images/camera_pos.png" class="img-fluid"></p>
<section id="dataset" class="level3">
<h3 class="anchored" data-anchor-id="dataset">Dataset</h3>
<p>The dataset collection is the most critical phase of the project and should take several weeks or months. For this tutorial, we will use a public dataset: “Sledevic, Tomyslav (2023), “[Labeled dataset for bee detection and direction estimation on beehive landing boards,” Mendeley Data, V5, doi: 10.17632/8gb9r2yhfc.5”</p>
<p>The original dataset has 6,762 images (1920 x 1080), and around 8% of them (518) have no bees (only background). This is very important with Object Detection, where we should keep around 10% of the dataset with only background (without any objects to be detected).</p>
<p>The images contain from zero to up to 61 bees:</p>
<p><img src="images/bee_distr.jpeg" class="img-fluid"></p>
<p>We downloaded the dataset (images and annotations) and uploaded it to <a href="https://roboflow.com/">Roboflow</a>. There, you should create a free account and start a new project, for example, (“Bees_on_Hive_landing_boards”):</p>
<p><img src="images/roboflow_proj.jpeg" class="img-fluid"></p>
<blockquote class="blockquote">
<p>We will not enter details about the Roboflow process once many tutorials are available.</p>
</blockquote>
<p>Once the project is created and the dataset is uploaded, you should review the annotations using the “Auto-Label” Tool. Note that all images with only a background should be saved w/o any annotations. At this step, you can also add additional images.</p>
<p><img src="images/annotate.jpeg" class="img-fluid"></p>
<p>Once all images are annotated, you should split them into training, validation, and testing.</p>
<p><img src="images/split_data.jpeg" class="img-fluid"></p>
</section>
<section id="pre-processing" class="level3">
<h3 class="anchored" data-anchor-id="pre-processing">Pre-Processing</h3>
<p>The last step with the dataset is preprocessing to generate a final version for training. The Yolov8 model can be trained with 640 x 640 pixels (RGB) images. Let’s resize all images and generate augmented versions of each image (augmentation) to create new training examples from which our model can learn.</p>
<p>For augmentation, we will rotate the images (+/-15<sup>o</sup>) and vary the brightness and exposure.</p>
<p><img src="images/preprocess.jpeg" class="img-fluid"></p>
<p>This will create a final dataset of 16,228 images.</p>
<p><img src="images/final-dataset.jpeg" class="img-fluid"></p>
<p>Now, you should export the model in a YOLOv8 format. You can download a zipped version of the dataset to your desktop or get a downloaded code to be used with a Jupyter Notebook:</p>
<p><img src="images/download_code.jpeg" class="img-fluid"></p>
<p>And that is it! We are prepared to start our training using Google Colab.</p>
<blockquote class="blockquote">
<p>The pre-processed dataset can be found at the <a href="https://universe.roboflow.com/marcelo-rovai-riila/bees_on_hive_landing_boards">Roboflow site</a>.</p>
</blockquote>
</section>
<section id="training-yolov8-on-a-customized-dataset" class="level3">
<h3 class="anchored" data-anchor-id="training-yolov8-on-a-customized-dataset">Training YOLOv8 on a Customized Dataset</h3>
<p>For training, let’s adapt one of the public examples available from Ultralitytics and run it on Google Colab:</p>
<ul>
<li>yolov8_bees_on_hive_landing_board.ipynb <a href="https://colab.research.google.com/github/Mjrovai/Bee-Counting/blob/main/yolov8_bees_on_hive_landing_board.ipynb">[Open In Colab]</a></li>
</ul>
<section id="critical-points-on-the-notebook" class="level4">
<h4 class="anchored" data-anchor-id="critical-points-on-the-notebook">Critical points on the Notebook:</h4>
<ol type="1">
<li><p>Run it with GPU (the NVidia T4 is free)</p></li>
<li><p>Install Ultralytics using PIP.</p>
<p><img src="images/inst_Ultralytics.jpeg" class="img-fluid"></p></li>
<li><p>Now, you can import the YOLO and upload your dataset to the CoLab, pasting the Download code that you get from Roboflow. Note that your dataset will be mounted under <code>/content/datasets/</code>:</p></li>
</ol>
<p><img src="images/dataset_colab.jpeg" class="img-fluid"></p>
<ol start="4" type="1">
<li>It is important to verify and change, if needed, the file <code>data.yaml</code> with the correct path for the images:</li>
</ol>
<div class="sourceCode" id="cb17"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a><span class="ex">names:</span></span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a><span class="ex">-</span> bee</span>
<span id="cb17-3"><a href="#cb17-3" aria-hidden="true" tabindex="-1"></a><span class="ex">nc:</span> 1</span>
<span id="cb17-4"><a href="#cb17-4" aria-hidden="true" tabindex="-1"></a><span class="ex">roboflow:</span></span>
<span id="cb17-5"><a href="#cb17-5" aria-hidden="true" tabindex="-1"></a>  <span class="ex">license:</span> CC BY 4.0</span>
<span id="cb17-6"><a href="#cb17-6" aria-hidden="true" tabindex="-1"></a>  <span class="ex">project:</span> bees_on_hive_landing_boards</span>
<span id="cb17-7"><a href="#cb17-7" aria-hidden="true" tabindex="-1"></a>  <span class="ex">url:</span> https://universe.roboflow.com/marcelo-rovai-riila/bees_on_hive_landing_boards/dataset/1</span>
<span id="cb17-8"><a href="#cb17-8" aria-hidden="true" tabindex="-1"></a>  <span class="ex">version:</span> 1</span>
<span id="cb17-9"><a href="#cb17-9" aria-hidden="true" tabindex="-1"></a>  <span class="ex">workspace:</span> marcelo-rovai-riila</span>
<span id="cb17-10"><a href="#cb17-10" aria-hidden="true" tabindex="-1"></a><span class="ex">test:</span> /content/datasets/Bees_on_Hive_landing_boards-1test/images</span>
<span id="cb17-11"><a href="#cb17-11" aria-hidden="true" tabindex="-1"></a><span class="ex">train:</span> /content/datasets/Bees_on_Hive_landing_boards-1/train/images</span>
<span id="cb17-12"><a href="#cb17-12" aria-hidden="true" tabindex="-1"></a><span class="ex">val:</span> /content/datasets/Bees_on_Hive_landing_boards-1/valid/images</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<ol start="5" type="1">
<li><p>Define the main hyperparameters that you want to change from default, for example:</p>
<div class="sourceCode" id="cb18"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a><span class="ex">MODEL</span> = <span class="st">'yolov8n.pt'</span></span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a><span class="ex">IMG_SIZE</span> = 640</span>
<span id="cb18-3"><a href="#cb18-3" aria-hidden="true" tabindex="-1"></a><span class="ex">EPOCHS</span> = 25 <span class="co"># For a final project, you should consider at least 100 epochs </span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div></li>
<li><p>Run the training (using CLI):</p>
<div class="sourceCode" id="cb19"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a><span class="ex">!yolo</span> task=detect mode=train model={MODEL} data={dataset.location}/data.yaml epochs={EPOCHS} imgsz={IMG_SIZE} plots=True </span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p><img src="images/train_result.jpeg" class="img-fluid"></p></li>
</ol>
<p>​ The model took 2.7 hours to train and has an excellent result (mAP50 of 0.984). At the end of the training, all results are saved in the folder listed, for example: <code>/runs/detect/train3/</code>. There, you can find, for example, the confusion matrix and the metrics curves per epoch.</p>
<p><img src="images/train_perf.jpeg" class="img-fluid"></p>
<ol start="7" type="1">
<li>Note that the trained model (<code>best.pt</code>) is saved in the folder <code>/runs/detect/train3/weights/</code>. Now, you should validade the trained model with the <code>valid/images</code>.</li>
</ol>
<div class="sourceCode" id="cb20"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a><span class="ex">!yolo</span> task=detect mode=val model={HOME}/runs/detect/train3/weights/best.pt data={dataset.location}/data.yaml</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>​ The results were similar to training.</p>
<ol start="8" type="1">
<li>Now, we should perform inference on the images left aside for testing</li>
</ol>
<div class="sourceCode" id="cb21"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a><span class="ex">!yolo</span> task=detect mode=predict model={HOME}/runs/detect/train3/weights/best.pt conf=0.25 source={dataset.location}/test/images save=True</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>The inference results are saved in the folder <code>runs/detect/predict</code>. Let’s see some of them:</p>
<p><img src="images/inf_colab.jpeg" class="img-fluid"></p>
<p>We can also perform inference with a completely new and complex image from another beehive with a different background (the beehive of Professor Maurilio of our University). The results were great (but not perfect and with a lower confidence score). The model found 41 bees.</p>
<p><img src="images/maurilio_bees_tEXbFzkpEf.jpg" class="img-fluid"></p>
<ol start="9" type="1">
<li><p>The last thing to do is export the train, validation, and test results for your Drive at Google. To do so, you should mount your drive.</p>
<div class="sourceCode" id="cb22"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> google.colab <span class="im">import</span> drive</span>
<span id="cb22-2"><a href="#cb22-2" aria-hidden="true" tabindex="-1"></a>drive.mount(<span class="st">'/content/gdrive'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>and copy the content of <code>/runs</code> folder to a folder that you should create in your Drive, for example:</p>
<div class="sourceCode" id="cb23"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"></a><span class="ex">!scp</span> <span class="at">-r</span> /content/runs <span class="st">'/content/gdrive/MyDrive/10_UNIFEI/Bee_Project/YOLO/bees_on_hive_landing'</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div></li>
</ol>
</section>
</section>
</section>
<section id="inference-with-the-trained-model-using-the-rasp-zero" class="level2">
<h2 class="anchored" data-anchor-id="inference-with-the-trained-model-using-the-rasp-zero">Inference with the trained model, using the Rasp-Zero</h2>
<p>Using the FileZilla FTP, let’s transfer the <code>best.pt</code> to our Rasp-Zero (before the transfer, you may change the model name, for example, <code>bee_landing_640_best.pt</code>).</p>
<p>The first thing to do is convert the model to an NCNN format:</p>
<div class="sourceCode" id="cb24"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a><span class="ex">yolo</span> export model=bee_landing_640_best.pt format=ncnn </span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>As a result, a new converted model, <code>bee_landing_640_best_ncnn_model</code> is created in the same directory.</p>
<p>Let’s create a folder to receive some test images (under <code>Documents/YOLO/</code>:</p>
<div class="sourceCode" id="cb25"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb25-1"><a href="#cb25-1" aria-hidden="true" tabindex="-1"></a><span class="fu">mkdir</span> test_images</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Using the FileZilla FTP, let’s transfer a few images from the test dataset to our Rasp-Zero:</p>
<p><img src="images/image_test_.png" class="img-fluid"></p>
<p>Let’s use the Python Interpreter:</p>
<div class="sourceCode" id="cb26"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb26-1"><a href="#cb26-1" aria-hidden="true" tabindex="-1"></a><span class="ex">python</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>As before, we will import the YOLO library and define our converted model to detect bees:</p>
<div class="sourceCode" id="cb27"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb27-1"><a href="#cb27-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> ultralytics <span class="im">import</span> YOLO</span>
<span id="cb27-2"><a href="#cb27-2" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> YOLO(<span class="st">'bee_landing_640_best_ncnn_model'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Now, let’s define an image and call the inference (we will save the image result this time to external verification):</p>
<div class="sourceCode" id="cb28"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb28-1"><a href="#cb28-1" aria-hidden="true" tabindex="-1"></a>img <span class="op">=</span> <span class="st">'test_images/15_bees.jpg'</span></span>
<span id="cb28-2"><a href="#cb28-2" aria-hidden="true" tabindex="-1"></a>result <span class="op">=</span> model.predict(img, save<span class="op">=</span><span class="va">True</span>, imgsz<span class="op">=</span><span class="dv">640</span>, conf<span class="op">=</span><span class="fl">0.2</span>, iou<span class="op">=</span><span class="fl">0.3</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>The inference result is saved on the variable <code>result,</code> and the processed image on <code>runs/detect/predict9</code></p>
<p><img src="images/inf_bees_15.png" class="img-fluid"></p>
<p>Using FileZilla FTP, we can send the inference result to our Desktop for verification:</p>
<p><img src="images/inf_img_15_bees.png" class="img-fluid"></p>
<p>let’s go over the other images, analyzing the number of objects (bees) found:</p>
<p><img src="images/multiple_infer.png" class="img-fluid"></p>
<p>Depending on the confidence, we can have some false positives or negatives. But in general, with a model trained based on the smaller base model of the YOLOv8 family (YOLOv8n) and also converted to NCNN, the result is pretty good, running on an Edge device such as the Rasp-Zero. Also, note that the inference latency is around 730ms.</p>
<p>For example, by running the inference on <code>Maurilio-bee.jpeg</code>, we can find <code>40 bees</code>. During the test phase on Colab, 41 bees were found (we only missed one here.)</p>
<p><img src="images/maurilio-bee-rasp.png" class="img-fluid"></p>
</section>
<section id="considerations-about-the-post-processing" class="level2">
<h2 class="anchored" data-anchor-id="considerations-about-the-post-processing">Considerations about the Post-Processing</h2>
<p>Our final project should be very simple in terms of code. We will use the camera to capture an image every 10 seconds. As we did in the previous section, the captured image should be the input for the trained and converted model. We should get the number of bees for each image and save it in a database (for example, timestamp: number of bees).</p>
<p>We can do it with a single Python script or use a Linux system timer, like <code>cron</code>, to periodically capture images every 10 seconds and have a separate Python script to process these images as they are saved. This method can be particularly efficient in managing system resources and can be more robust against potential delays in image processing.</p>
<section id="setting-up-the-image-capture-with-cron" class="level4">
<h4 class="anchored" data-anchor-id="setting-up-the-image-capture-with-cron">Setting Up the Image Capture with <code>cron</code></h4>
<p>First, we should set up a <code>cron</code> job to use the <code>rpicam-jpeg</code> command to capture an image every 10 seconds.</p>
<ol type="1">
<li><p><strong>Edit the <code>crontab</code></strong>:</p>
<ul>
<li>Open the terminal and type <code>crontab -e</code> to edit the cron jobs.</li>
<li><code>cron</code> normally doesn’t support sub-minute intervals directly, so we should use a workaround like a loop or watch for file changes.</li>
</ul></li>
<li><p><strong>Create a Bash Script (<code>capture.sh</code>)</strong>:</p>
<ul>
<li><strong>Image Capture</strong>: This bash script captures images every 10 seconds using <code>rpicam-jpeg</code>, a command that is part of the <code>raspijpeg</code> tool. This command lets us control the camera and capture JPEG images directly from the command line. This is especially useful because we are looking for a lightweight and straightforward method to capture images without the need for additional libraries like <code>Picamera</code> or external software. The script also saves the captured image with a timestamp.</li>
</ul>
<div class="sourceCode" id="cb29"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb29-1"><a href="#cb29-1" aria-hidden="true" tabindex="-1"></a><span class="co">#!/bin/bash</span></span>
<span id="cb29-2"><a href="#cb29-2" aria-hidden="true" tabindex="-1"></a><span class="co"># Script to capture an image every 10 seconds</span></span>
<span id="cb29-3"><a href="#cb29-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-4"><a href="#cb29-4" aria-hidden="true" tabindex="-1"></a><span class="cf">while</span> true</span>
<span id="cb29-5"><a href="#cb29-5" aria-hidden="true" tabindex="-1"></a>do</span>
<span id="cb29-6"><a href="#cb29-6" aria-hidden="true" tabindex="-1"></a>  DATE<span class="op">=</span>$(date <span class="op">+</span><span class="st">"%Y-%m-</span><span class="sc">%d</span><span class="st">_%H%M%S"</span>)</span>
<span id="cb29-7"><a href="#cb29-7" aria-hidden="true" tabindex="-1"></a>  rpicam<span class="op">-</span>jpeg <span class="op">--</span>output test_images<span class="op">/</span>$DATE.jpg <span class="op">--</span>width <span class="dv">640</span> <span class="op">--</span>height <span class="dv">640</span></span>
<span id="cb29-8"><a href="#cb29-8" aria-hidden="true" tabindex="-1"></a>  sleep <span class="dv">10</span></span>
<span id="cb29-9"><a href="#cb29-9" aria-hidden="true" tabindex="-1"></a>done</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<ul>
<li>We should make the script executable with <code>chmod +x capture.sh</code>.</li>
<li>The script must start at boot or use a <code>@reboot</code> entry in <code>cron</code> to start it automatically.</li>
</ul></li>
</ol>
</section>
<section id="setting-up-the-python-script-for-inference" class="level4">
<h4 class="anchored" data-anchor-id="setting-up-the-python-script-for-inference">Setting Up the Python Script for Inference</h4>
<p><strong>Image Processing</strong>: The Python script continuously monitors the designated directory for new images, processes each new image using the YOLOv8 model, updates the database with the count of detected bees, and optionally deletes the image to conserve disk space.</p>
<p><strong>Database Updates</strong>: The results, along with the timestamps, are saved in an SQLite database. For that, a simple option is to use <a href="https://docs.python.org/3/library/sqlite3.html">sqlite3</a>.</p>
<p>In short, we need to write a script that continuously monitors the directory for new images, processes them using a YOLO model, and then saves the results to a SQLite database. Here’s how we can create and make the script executable:</p>
<div class="sourceCode" id="cb30"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb30-1"><a href="#cb30-1" aria-hidden="true" tabindex="-1"></a><span class="co">#!/usr/bin/env python3</span></span>
<span id="cb30-2"><a href="#cb30-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> os</span>
<span id="cb30-3"><a href="#cb30-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> time</span>
<span id="cb30-4"><a href="#cb30-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> sqlite3</span>
<span id="cb30-5"><a href="#cb30-5" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> datetime <span class="im">import</span> datetime</span>
<span id="cb30-6"><a href="#cb30-6" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> ultralytics <span class="im">import</span> YOLO</span>
<span id="cb30-7"><a href="#cb30-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-8"><a href="#cb30-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Constants and paths</span></span>
<span id="cb30-9"><a href="#cb30-9" aria-hidden="true" tabindex="-1"></a>IMAGES_DIR <span class="op">=</span> <span class="st">'test_images/'</span></span>
<span id="cb30-10"><a href="#cb30-10" aria-hidden="true" tabindex="-1"></a>MODEL_PATH <span class="op">=</span> <span class="st">'bee_landing_640_best_ncnn_model'</span></span>
<span id="cb30-11"><a href="#cb30-11" aria-hidden="true" tabindex="-1"></a>DB_PATH <span class="op">=</span> <span class="st">'bee_count.db'</span></span>
<span id="cb30-12"><a href="#cb30-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-13"><a href="#cb30-13" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> setup_database():</span>
<span id="cb30-14"><a href="#cb30-14" aria-hidden="true" tabindex="-1"></a>    <span class="co">""" Establishes a database connection and creates the table if it doesn't exist. """</span></span>
<span id="cb30-15"><a href="#cb30-15" aria-hidden="true" tabindex="-1"></a>    conn <span class="op">=</span> sqlite3.<span class="ex">connect</span>(DB_PATH)</span>
<span id="cb30-16"><a href="#cb30-16" aria-hidden="true" tabindex="-1"></a>    cursor <span class="op">=</span> conn.cursor()</span>
<span id="cb30-17"><a href="#cb30-17" aria-hidden="true" tabindex="-1"></a>    cursor.execute(<span class="st">'''</span></span>
<span id="cb30-18"><a href="#cb30-18" aria-hidden="true" tabindex="-1"></a><span class="st">        CREATE TABLE IF NOT EXISTS bee_counts</span></span>
<span id="cb30-19"><a href="#cb30-19" aria-hidden="true" tabindex="-1"></a><span class="st">        (timestamp TEXT, count INTEGER)</span></span>
<span id="cb30-20"><a href="#cb30-20" aria-hidden="true" tabindex="-1"></a><span class="st">    '''</span>)</span>
<span id="cb30-21"><a href="#cb30-21" aria-hidden="true" tabindex="-1"></a>    conn.commit()</span>
<span id="cb30-22"><a href="#cb30-22" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> conn</span>
<span id="cb30-23"><a href="#cb30-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-24"><a href="#cb30-24" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> process_image(image_path, model, conn):</span>
<span id="cb30-25"><a href="#cb30-25" aria-hidden="true" tabindex="-1"></a>    <span class="co">""" Processes an image to detect objects and logs the count to the database. """</span></span>
<span id="cb30-26"><a href="#cb30-26" aria-hidden="true" tabindex="-1"></a>    result <span class="op">=</span> model.predict(image_path, save<span class="op">=</span><span class="va">False</span>, imgsz<span class="op">=</span><span class="dv">640</span>, conf<span class="op">=</span><span class="fl">0.2</span>, iou<span class="op">=</span><span class="fl">0.3</span>, verbose<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb30-27"><a href="#cb30-27" aria-hidden="true" tabindex="-1"></a>    num_bees <span class="op">=</span> <span class="bu">len</span>(result[<span class="dv">0</span>].boxes.cls) </span>
<span id="cb30-28"><a href="#cb30-28" aria-hidden="true" tabindex="-1"></a>    timestamp <span class="op">=</span> datetime.now().strftime(<span class="st">'%Y-%m-</span><span class="sc">%d</span><span class="st"> %H:%M:%S'</span>)</span>
<span id="cb30-29"><a href="#cb30-29" aria-hidden="true" tabindex="-1"></a>    cursor <span class="op">=</span> conn.cursor()</span>
<span id="cb30-30"><a href="#cb30-30" aria-hidden="true" tabindex="-1"></a>    cursor.execute(<span class="st">"INSERT INTO bee_counts (timestamp, count) VALUES (?, ?)"</span>, (timestamp, num_bees))</span>
<span id="cb30-31"><a href="#cb30-31" aria-hidden="true" tabindex="-1"></a>    conn.commit()</span>
<span id="cb30-32"><a href="#cb30-32" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f'Processed </span><span class="sc">{</span>image_path<span class="sc">}</span><span class="ss">: Number of bees detected = </span><span class="sc">{</span>num_bees<span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb30-33"><a href="#cb30-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-34"><a href="#cb30-34" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> monitor_directory(model, conn):</span>
<span id="cb30-35"><a href="#cb30-35" aria-hidden="true" tabindex="-1"></a>    <span class="co">""" Monitors the directory for new images and processes them as they appear. """</span></span>
<span id="cb30-36"><a href="#cb30-36" aria-hidden="true" tabindex="-1"></a>    processed_files <span class="op">=</span> <span class="bu">set</span>()</span>
<span id="cb30-37"><a href="#cb30-37" aria-hidden="true" tabindex="-1"></a>    <span class="cf">while</span> <span class="va">True</span>:</span>
<span id="cb30-38"><a href="#cb30-38" aria-hidden="true" tabindex="-1"></a>        <span class="cf">try</span>:</span>
<span id="cb30-39"><a href="#cb30-39" aria-hidden="true" tabindex="-1"></a>            files <span class="op">=</span> <span class="bu">set</span>(os.listdir(IMAGES_DIR))</span>
<span id="cb30-40"><a href="#cb30-40" aria-hidden="true" tabindex="-1"></a>            new_files <span class="op">=</span> files <span class="op">-</span> processed_files</span>
<span id="cb30-41"><a href="#cb30-41" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> <span class="bu">file</span> <span class="kw">in</span> new_files:</span>
<span id="cb30-42"><a href="#cb30-42" aria-hidden="true" tabindex="-1"></a>                <span class="cf">if</span> <span class="bu">file</span>.endswith(<span class="st">'.jpg'</span>):</span>
<span id="cb30-43"><a href="#cb30-43" aria-hidden="true" tabindex="-1"></a>                    full_path <span class="op">=</span> os.path.join(IMAGES_DIR, <span class="bu">file</span>)</span>
<span id="cb30-44"><a href="#cb30-44" aria-hidden="true" tabindex="-1"></a>                    process_image(full_path, model, conn)</span>
<span id="cb30-45"><a href="#cb30-45" aria-hidden="true" tabindex="-1"></a>                    processed_files.add(<span class="bu">file</span>)</span>
<span id="cb30-46"><a href="#cb30-46" aria-hidden="true" tabindex="-1"></a>            time.sleep(<span class="dv">1</span>)  <span class="co"># Check every second</span></span>
<span id="cb30-47"><a href="#cb30-47" aria-hidden="true" tabindex="-1"></a>        <span class="cf">except</span> <span class="pp">KeyboardInterrupt</span>:</span>
<span id="cb30-48"><a href="#cb30-48" aria-hidden="true" tabindex="-1"></a>            <span class="bu">print</span>(<span class="st">"Stopping..."</span>)</span>
<span id="cb30-49"><a href="#cb30-49" aria-hidden="true" tabindex="-1"></a>            <span class="cf">break</span></span>
<span id="cb30-50"><a href="#cb30-50" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-51"><a href="#cb30-51" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> main():</span>
<span id="cb30-52"><a href="#cb30-52" aria-hidden="true" tabindex="-1"></a>    conn <span class="op">=</span> setup_database()</span>
<span id="cb30-53"><a href="#cb30-53" aria-hidden="true" tabindex="-1"></a>    model <span class="op">=</span> YOLO(MODEL_PATH)</span>
<span id="cb30-54"><a href="#cb30-54" aria-hidden="true" tabindex="-1"></a>    monitor_directory(model, conn)</span>
<span id="cb30-55"><a href="#cb30-55" aria-hidden="true" tabindex="-1"></a>    conn.close()</span>
<span id="cb30-56"><a href="#cb30-56" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-57"><a href="#cb30-57" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> <span class="va">__name__</span> <span class="op">==</span> <span class="st">"__main__"</span>:</span>
<span id="cb30-58"><a href="#cb30-58" aria-hidden="true" tabindex="-1"></a>    main()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>The python script must be executable, for that:</p>
<ol type="1">
<li><p><strong>Save the script</strong>: For example, as <code>process_images.py</code>.</p></li>
<li><p><strong>Change file permissions</strong> to make it executable:</p>
<div class="sourceCode" id="cb31"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb31-1"><a href="#cb31-1" aria-hidden="true" tabindex="-1"></a><span class="fu">chmod</span> +x process_images.py</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div></li>
<li><p><strong>Run the script</strong> directly from the command line:</p>
<div class="sourceCode" id="cb32"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb32-1"><a href="#cb32-1" aria-hidden="true" tabindex="-1"></a><span class="ex">./process_images.py</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div></li>
</ol>
<p>We should consider keeping the script running even after closing the terminal; for that, we can use <code>nohup</code> or <code>screen</code>:</p>
<div class="sourceCode" id="cb33"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb33-1"><a href="#cb33-1" aria-hidden="true" tabindex="-1"></a><span class="fu">nohup</span> ./process_images.py <span class="kw">&amp;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>or</p>
<div class="sourceCode" id="cb34"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb34-1"><a href="#cb34-1" aria-hidden="true" tabindex="-1"></a><span class="ex">screen</span> <span class="at">-S</span> bee_monitor</span>
<span id="cb34-2"><a href="#cb34-2" aria-hidden="true" tabindex="-1"></a><span class="ex">./process_images.py</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Note that we are capturing images with their own timestamp and then log a separate timestamp for when the inference results are saved to the database. This approach can be beneficial for the following reasons:</p>
<ol type="1">
<li><strong>Accuracy in Data Logging</strong>:
<ul>
<li><strong>Capture Timestamp</strong>: The timestamp associated with each image capture represents the exact moment the image was taken. This is crucial for applications where precise timing of events (like bee activity) is important for analysis.</li>
<li><strong>Inference Timestamp</strong>: This timestamp indicates when the image was processed and the results were recorded in the database. This can differ from the capture time due to processing delays or if the image processing is batched or queued.</li>
</ul></li>
<li><strong>Performance Monitoring</strong>:
<ul>
<li>Having separate timestamps allows us to monitor the performance and efficiency of your image processing pipeline. We can measure the delay between image capture and result logging, which helps optimize the system for real-time processing needs.</li>
</ul></li>
<li><strong>Troubleshooting and Audit</strong>:
<ul>
<li>Separate timestamps provide a better audit trail and troubleshooting data. If there are issues with the image processing or data recording, having distinct timestamps can help isolate whether delays or problems occurred during capture, processing, or logging.</li>
</ul></li>
</ol>
</section>
<section id="script-for-reading-the-sqlite-database" class="level3">
<h3 class="anchored" data-anchor-id="script-for-reading-the-sqlite-database">Script For Reading the SQLite Database</h3>
<p>Here is an example of a code to retrieve the data from the database:</p>
<div class="sourceCode" id="cb35"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb35-1"><a href="#cb35-1" aria-hidden="true" tabindex="-1"></a><span class="co">#!/usr/bin/env python3</span></span>
<span id="cb35-2"><a href="#cb35-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> sqlite3</span>
<span id="cb35-3"><a href="#cb35-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-4"><a href="#cb35-4" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> main():</span>
<span id="cb35-5"><a href="#cb35-5" aria-hidden="true" tabindex="-1"></a>    db_path <span class="op">=</span> <span class="st">'bee_count.db'</span></span>
<span id="cb35-6"><a href="#cb35-6" aria-hidden="true" tabindex="-1"></a>    conn <span class="op">=</span> sqlite3.<span class="ex">connect</span>(db_path)</span>
<span id="cb35-7"><a href="#cb35-7" aria-hidden="true" tabindex="-1"></a>    cursor <span class="op">=</span> conn.cursor()</span>
<span id="cb35-8"><a href="#cb35-8" aria-hidden="true" tabindex="-1"></a>    query <span class="op">=</span> <span class="st">"SELECT * FROM bee_counts"</span></span>
<span id="cb35-9"><a href="#cb35-9" aria-hidden="true" tabindex="-1"></a>    cursor.execute(query)</span>
<span id="cb35-10"><a href="#cb35-10" aria-hidden="true" tabindex="-1"></a>    data <span class="op">=</span> cursor.fetchall()</span>
<span id="cb35-11"><a href="#cb35-11" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> row <span class="kw">in</span> data:</span>
<span id="cb35-12"><a href="#cb35-12" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"Timestamp: </span><span class="sc">{</span>row[<span class="dv">0</span>]<span class="sc">}</span><span class="ss">, Number of bees: </span><span class="sc">{</span>row[<span class="dv">1</span>]<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb35-13"><a href="#cb35-13" aria-hidden="true" tabindex="-1"></a>    conn.close()</span>
<span id="cb35-14"><a href="#cb35-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-15"><a href="#cb35-15" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> <span class="va">__name__</span> <span class="op">==</span> <span class="st">"__main__"</span>:</span>
<span id="cb35-16"><a href="#cb35-16" aria-hidden="true" tabindex="-1"></a>    main()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="adding-environment-data" class="level3">
<h3 class="anchored" data-anchor-id="adding-environment-data">Adding Environment data</h3>
<p>Besides bee counting, environmental data, such as temperature and humidity, are essential for monitoring the bee-have health. Using a Rasp-Zero, it is straightforward to add a digital sensor such as the DHT-22 to get this data.</p>
<p><img src="images/dat_env.jpg" class="img-fluid"></p>
<p>Environmental data will be part of our final project. If you want to know more about connecting sensors to a Raspberry Pi and, even more, how to save the data to a local database and send it to the web, follow this tutorial: <a href="https://www.hackster.io/mjrobot/from-data-to-graph-a-web-journey-with-flask-and-sqlite-4dba35">From Data to Graph: A Web Journey With Flask and SQLite</a>.</p>
<p><img src="images/tutorial_flask.jpg" class="img-fluid"></p>
</section>
</section>
<section id="conclusion" class="level2">
<h2 class="anchored" data-anchor-id="conclusion">Conclusion</h2>
<p>In this tutorial, we have thoroughly explored integrating the YOLOv8 model with a Raspberry Pi Zero 2W to address the practical and pressing task of counting (or better, “estimating”) bees at a beehive entrance. Our project underscores the robust capability of embedding advanced machine learning technologies within compact edge computing devices, highlighting their potential impact on environmental monitoring and ecological studies.</p>
<p>This tutorial provides a step-by-step guide to the practical deployment of the YOLOv8 model. We demonstrate a tangible example of a real-world application by optimizing it for edge computing in terms of efficiency and processing speed (using NCNN format). This not only serves as a functional solution but also as an instructional tool for similar projects.</p>
<p>The technical insights and methodologies shared in this tutorial are the basis for the complete work to be developed at our university in the future. We envision further development, such as integrating additional environmental sensing capabilities and refining the model’s accuracy and processing efficiency. Implementing alternative energy solutions like the proposed solar power setup will expand the project’s sustainability and applicability in remote or underserved locations.</p>
</section>
<section id="resources" class="level2">
<h2 class="anchored" data-anchor-id="resources">Resources</h2>
<blockquote class="blockquote">
<p>The Dataset paper, Notebooks, and PDF version are in the <a href="https://github.com/Mjrovai/Bee-Counting">Project repository</a>.</p>
</blockquote>


<!-- -->

</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  const viewSource = window.document.getElementById('quarto-view-source') ||
                     window.document.getElementById('quarto-code-tools-source');
  if (viewSource) {
    const sourceUrl = viewSource.getAttribute("data-quarto-source-url");
    viewSource.addEventListener("click", function(e) {
      if (sourceUrl) {
        // rstudio viewer pane
        if (/\bcapabilities=\b/.test(window.location)) {
          window.open(sourceUrl);
        } else {
          window.location.href = sourceUrl;
        }
      } else {
        const modal = new bootstrap.Modal(document.getElementById('quarto-embedded-source-code-modal'));
        modal.show();
      }
      return false;
    });
  }
  function toggleCodeHandler(show) {
    return function(e) {
      const detailsSrc = window.document.querySelectorAll(".cell > details > .sourceCode");
      for (let i=0; i<detailsSrc.length; i++) {
        const details = detailsSrc[i].parentElement;
        if (show) {
          details.open = true;
        } else {
          details.removeAttribute("open");
        }
      }
      const cellCodeDivs = window.document.querySelectorAll(".cell > .sourceCode");
      const fromCls = show ? "hidden" : "unhidden";
      const toCls = show ? "unhidden" : "hidden";
      for (let i=0; i<cellCodeDivs.length; i++) {
        const codeDiv = cellCodeDivs[i];
        if (codeDiv.classList.contains(fromCls)) {
          codeDiv.classList.remove(fromCls);
          codeDiv.classList.add(toCls);
        } 
      }
      return false;
    }
  }
  const hideAllCode = window.document.getElementById("quarto-hide-all-code");
  if (hideAllCode) {
    hideAllCode.addEventListener("click", toggleCodeHandler(false));
  }
  const showAllCode = window.document.getElementById("quarto-show-all-code");
  if (showAllCode) {
    showAllCode.addEventListener("click", toggleCodeHandler(true));
  }
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
          // target, if specified
          link.setAttribute("target", "_blank");
          // default icon
          link.classList.add("external");
      }
    }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="../../raspi/object_detection/object_detection.html" class="pagination-link">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text">Object Detection</span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="../../raspi/llm/llm.html" class="pagination-link">
        <span class="nav-page-text">Small Language Models (SLM)</span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav><div class="modal fade" id="quarto-embedded-source-code-modal" tabindex="-1" aria-labelledby="quarto-embedded-source-code-modal-label" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable"><div class="modal-content"><div class="modal-header"><h5 class="modal-title" id="quarto-embedded-source-code-modal-label">Source Code</h5><button class="btn-close" data-bs-dismiss="modal"></button></div><div class="modal-body"><div class="">
<div class="sourceCode" id="cb36" data-shortcodes="false"><pre class="sourceCode markdown code-with-copy"><code class="sourceCode markdown"><span id="cb36-1"><a href="#cb36-1" aria-hidden="true" tabindex="-1"></a><span class="fu"># Counting objects with YOLO {.unnumbered}</span></span>
<span id="cb36-2"><a href="#cb36-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-3"><a href="#cb36-3" aria-hidden="true" tabindex="-1"></a>**Deploying YOLOv8 on Raspberry Pi Zero 2W for Real-Time Bee Counting at the Hive Entrance."**</span>
<span id="cb36-4"><a href="#cb36-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-5"><a href="#cb36-5" aria-hidden="true" tabindex="-1"></a><span class="al">![](images/portada.jpeg)</span></span>
<span id="cb36-6"><a href="#cb36-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-7"><a href="#cb36-7" aria-hidden="true" tabindex="-1"></a><span class="fu">## Introduction</span></span>
<span id="cb36-8"><a href="#cb36-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-9"><a href="#cb36-9" aria-hidden="true" tabindex="-1"></a>At the <span class="co">[</span><span class="ot">Federal University of Itajuba in Brazil</span><span class="co">](https://unifei.edu.br/)</span>, with the master's student José Anderson Reis and Professor  José Alberto Ferreira Filho, we are exploring a project that delves into the intersection of technology and nature. This tutorial will review our first steps and share our observations on deploying YOLOv8, a cutting-edge machine learning model, on the compact and efficient Raspberry Pi Zero 2W (*Raspi-Zero*). We aim to estimate the number of bees entering and exiting their hive—a task crucial for beekeeping and ecological studies.</span>
<span id="cb36-10"><a href="#cb36-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-11"><a href="#cb36-11" aria-hidden="true" tabindex="-1"></a>Why is this important? Bee populations are vital indicators of environmental health, and their monitoring can provide essential data for ecological research and conservation efforts. However, manual counting is labor-intensive and prone to errors. By leveraging the power of embedded machine learning, or tinyML, we automate this process, enhancing accuracy and efficiency.</span>
<span id="cb36-12"><a href="#cb36-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-13"><a href="#cb36-13" aria-hidden="true" tabindex="-1"></a><span class="al">![img](images/counting_bees.png)</span></span>
<span id="cb36-14"><a href="#cb36-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-15"><a href="#cb36-15" aria-hidden="true" tabindex="-1"></a>This tutorial will cover setting up the Raspberry Pi, integrating a camera module, optimizing and deploying YOLOv8 for real-time image processing, and analyzing the data gathered. </span>
<span id="cb36-16"><a href="#cb36-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-17"><a href="#cb36-17" aria-hidden="true" tabindex="-1"></a><span class="fu">## Installing  and using Ultralytics YOLOv8</span></span>
<span id="cb36-18"><a href="#cb36-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-19"><a href="#cb36-19" aria-hidden="true" tabindex="-1"></a><span class="co">[</span><span class="ot">Ultralytics</span><span class="co">](https://ultralytics.com/)</span> <span class="co">[</span><span class="ot">YOLOv8</span><span class="co">](https://github.com/ultralytics/ultralytics)</span>, is a version of the acclaimed real-time object detection and image segmentation model, YOLO. YOLOv8 is built on cutting-edge advancements in deep learning and computer vision, offering unparalleled performance in terms of speed and accuracy. Its streamlined design makes it suitable for various applications and easily adaptable to different hardware platforms, from edge devices to cloud APIs.</span>
<span id="cb36-20"><a href="#cb36-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-21"><a href="#cb36-21" aria-hidden="true" tabindex="-1"></a>Let's start installing the Ultarlytics packages for local inference on the Rasp-Zero:</span>
<span id="cb36-22"><a href="#cb36-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-23"><a href="#cb36-23" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>Update the packages list, install pip, and upgrade to the latest:</span>
<span id="cb36-24"><a href="#cb36-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-25"><a href="#cb36-25" aria-hidden="true" tabindex="-1"></a><span class="in">```bash</span></span>
<span id="cb36-26"><a href="#cb36-26" aria-hidden="true" tabindex="-1"></a><span class="fu">sudo</span> apt update</span>
<span id="cb36-27"><a href="#cb36-27" aria-hidden="true" tabindex="-1"></a><span class="fu">sudo</span> apt install python3-pip <span class="at">-y</span></span>
<span id="cb36-28"><a href="#cb36-28" aria-hidden="true" tabindex="-1"></a><span class="ex">pip</span> install <span class="at">-U</span> pip</span>
<span id="cb36-29"><a href="#cb36-29" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb36-30"><a href="#cb36-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-31"><a href="#cb36-31" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>Install the <span class="in">`ultralytics`</span> pip package with optional dependencies:</span>
<span id="cb36-32"><a href="#cb36-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-33"><a href="#cb36-33" aria-hidden="true" tabindex="-1"></a><span class="in">```bash</span></span>
<span id="cb36-34"><a href="#cb36-34" aria-hidden="true" tabindex="-1"></a><span class="ex">pip</span> install ultralytics<span class="pp">[</span><span class="ss">export</span><span class="pp">]</span></span>
<span id="cb36-35"><a href="#cb36-35" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb36-36"><a href="#cb36-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-37"><a href="#cb36-37" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>Reboot the device:</span>
<span id="cb36-38"><a href="#cb36-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-39"><a href="#cb36-39" aria-hidden="true" tabindex="-1"></a><span class="in">```bash</span></span>
<span id="cb36-40"><a href="#cb36-40" aria-hidden="true" tabindex="-1"></a><span class="fu">sudo</span> reboot</span>
<span id="cb36-41"><a href="#cb36-41" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb36-42"><a href="#cb36-42" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-43"><a href="#cb36-43" aria-hidden="true" tabindex="-1"></a><span class="fu">### Testing the YOLO</span></span>
<span id="cb36-44"><a href="#cb36-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-45"><a href="#cb36-45" aria-hidden="true" tabindex="-1"></a>After the Rasp-Zero booting, let's create a directory for working with YOLO and change the current location to it::</span>
<span id="cb36-46"><a href="#cb36-46" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-47"><a href="#cb36-47" aria-hidden="true" tabindex="-1"></a><span class="in">```bash</span></span>
<span id="cb36-48"><a href="#cb36-48" aria-hidden="true" tabindex="-1"></a><span class="fu">mkdir</span> Documents/YOLO</span>
<span id="cb36-49"><a href="#cb36-49" aria-hidden="true" tabindex="-1"></a><span class="bu">cd</span> Documents/YOLO</span>
<span id="cb36-50"><a href="#cb36-50" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb36-51"><a href="#cb36-51" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-52"><a href="#cb36-52" aria-hidden="true" tabindex="-1"></a>Let's run inference on an image that will be downloaded from the Ultralytics website, using the YOLOV8n model (the smallest in the family) at the Terminal (CLI):</span>
<span id="cb36-53"><a href="#cb36-53" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-54"><a href="#cb36-54" aria-hidden="true" tabindex="-1"></a><span class="in">```bash</span></span>
<span id="cb36-55"><a href="#cb36-55" aria-hidden="true" tabindex="-1"></a><span class="ex">yolo</span> predict model=<span class="st">'yolov8n'</span> source=<span class="st">'https://ultralytics.com/images/bus.jpg'</span></span>
<span id="cb36-56"><a href="#cb36-56" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb36-57"><a href="#cb36-57" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-58"><a href="#cb36-58" aria-hidden="true" tabindex="-1"></a>The inference result will appear in the terminal. In the image (bus.jpg), 4 <span class="in">`persons`</span>, 1 <span class="in">`bus,`</span> and 1 <span class="in">`stop signal`</span> were detected: </span>
<span id="cb36-59"><a href="#cb36-59" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-60"><a href="#cb36-60" aria-hidden="true" tabindex="-1"></a><span class="al">![](images/inferfence.png)</span></span>
<span id="cb36-61"><a href="#cb36-61" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-62"><a href="#cb36-62" aria-hidden="true" tabindex="-1"></a>Also, we got a message that <span class="in">`Results saved to runs/detect/predict4`</span>. Inspecting that directory, we can see a new image saved (bus.jpg). Let's download it from the Rasp-Zero to our desktop for inspection: </span>
<span id="cb36-63"><a href="#cb36-63" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-64"><a href="#cb36-64" aria-hidden="true" tabindex="-1"></a><span class="al">![](images/bus.png)</span></span>
<span id="cb36-65"><a href="#cb36-65" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-66"><a href="#cb36-66" aria-hidden="true" tabindex="-1"></a>So, the Ultrayitics YOLO is correctly installed on our Rasp-Zero. </span>
<span id="cb36-67"><a href="#cb36-67" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-68"><a href="#cb36-68" aria-hidden="true" tabindex="-1"></a><span class="fu">### Export Model to NCNN format</span></span>
<span id="cb36-69"><a href="#cb36-69" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-70"><a href="#cb36-70" aria-hidden="true" tabindex="-1"></a>An issue is the high latency for this inference,  7.6 s, even with the smaller model of the family (YOLOv8n). This is a reality of deploying computer vision models on edge devices with limited computational power, such as the Rasp-Zero. One alternative is to use a format optimized for optimal performance. This ensures that even devices with limited processing power can handle advanced computer vision tasks well.</span>
<span id="cb36-71"><a href="#cb36-71" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-72"><a href="#cb36-72" aria-hidden="true" tabindex="-1"></a>Of all the model export formats supported by Ultralytics, the <span class="co">[</span><span class="ot">NCNN</span><span class="co">](https://docs.ultralytics.com/integrations/ncnn)</span> is a high-performance neural network inference computing framework optimized for mobile platforms. From the beginning of the design, NCNN was deeply considerate about deployment and use on mobile phones and did not have third-party dependencies. It is cross-platform and runs faster than all known open-source frameworks (such as TFLite). </span>
<span id="cb36-73"><a href="#cb36-73" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-74"><a href="#cb36-74" aria-hidden="true" tabindex="-1"></a>NCNN delivers the best inference performance when working with Raspberry Pi devices. NCNN is highly optimized for mobile embedded platforms (such as ARM architecture). </span>
<span id="cb36-75"><a href="#cb36-75" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-76"><a href="#cb36-76" aria-hidden="true" tabindex="-1"></a>So, let's convert our model and rerun the inference:</span>
<span id="cb36-77"><a href="#cb36-77" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-78"><a href="#cb36-78" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>Export a YOLOv8n PyTorch model to NCNN format, creating:  '/yolov8n_ncnn_model'</span>
<span id="cb36-79"><a href="#cb36-79" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-80"><a href="#cb36-80" aria-hidden="true" tabindex="-1"></a><span class="in">```bash</span></span>
<span id="cb36-81"><a href="#cb36-81" aria-hidden="true" tabindex="-1"></a><span class="ex">yolo</span> export model=yolov8n.pt format=ncnn </span>
<span id="cb36-82"><a href="#cb36-82" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb36-83"><a href="#cb36-83" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-84"><a href="#cb36-84" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>Run inference with the exported model (now the source could be the bus.jpg image that was downloaded from the website to the current directory on the last inference):</span>
<span id="cb36-85"><a href="#cb36-85" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-86"><a href="#cb36-86" aria-hidden="true" tabindex="-1"></a><span class="in">```bash</span></span>
<span id="cb36-87"><a href="#cb36-87" aria-hidden="true" tabindex="-1"></a><span class="ex">yolo</span> predict model=<span class="st">'./yolov8n_ncnn_model'</span> source=<span class="st">'bus.jpg'</span></span>
<span id="cb36-88"><a href="#cb36-88" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb36-89"><a href="#cb36-89" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-90"><a href="#cb36-90" aria-hidden="true" tabindex="-1"></a>Now, we can see that the latency was reduced by half. </span>
<span id="cb36-91"><a href="#cb36-91" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-92"><a href="#cb36-92" aria-hidden="true" tabindex="-1"></a><span class="al">![](images/ncnn.png)</span></span>
<span id="cb36-93"><a href="#cb36-93" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-94"><a href="#cb36-94" aria-hidden="true" tabindex="-1"></a><span class="fu">### Talking about the YOLO Model</span></span>
<span id="cb36-95"><a href="#cb36-95" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-96"><a href="#cb36-96" aria-hidden="true" tabindex="-1"></a>The YOLO (You Only Look Once) model is a highly efficient and widely used object detection algorithm known for its real-time processing capabilities. Unlike traditional object detection systems that repurpose classifiers or localizers to perform detection, YOLO frames the detection problem as a single regression task. This innovative approach enables YOLO to simultaneously predict multiple bounding boxes and their class probabilities from full images in one evaluation, significantly boosting its speed.</span>
<span id="cb36-97"><a href="#cb36-97" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-98"><a href="#cb36-98" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Key Features:</span></span>
<span id="cb36-99"><a href="#cb36-99" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-100"><a href="#cb36-100" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>**Single Network Architecture**:</span>
<span id="cb36-101"><a href="#cb36-101" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>YOLO employs a single neural network to process the entire image. This network divides the image into a grid and, for each grid cell, directly predicts bounding boxes and associated class probabilities. This end-to-end training improves speed and simplifies the model architecture.</span>
<span id="cb36-102"><a href="#cb36-102" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-103"><a href="#cb36-103" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>**Real-Time Processing**:</span>
<span id="cb36-104"><a href="#cb36-104" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>One of YOLO’s standout features is its ability to perform object detection in real time. Depending on the version and hardware, YOLO can process images at high frames per second (FPS). This makes it ideal for applications requiring quick and accurate object detection, such as video surveillance, autonomous driving, and live sports analysis.</span>
<span id="cb36-105"><a href="#cb36-105" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-106"><a href="#cb36-106" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>**Evolution of Versions**:</span>
<span id="cb36-107"><a href="#cb36-107" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>Over the years, YOLO has undergone significant improvements, from YOLOv1 to the latest YOLOv10. Each iteration has introduced enhancements in accuracy, speed, and efficiency. YOLOv8, for instance, incorporates advancements in network architecture, improved training methodologies, and better support for various hardware, ensuring a more robust performance.</span>
<span id="cb36-108"><a href="#cb36-108" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>Although YOLOv10 is the family's newest member with an encouraging performance based on its paper, it was just released (May 2024) and is not fully integrated with the Ultralitycs library. Conversely, the precision-recall curve analysis suggests that YOLOv8 generally outperforms YOLOv9, capturing a higher proportion of true positives while minimizing false positives more effectively (for more details, see this <span class="co">[</span><span class="ot">article</span><span class="co">](https://encord.com/blog/performanceyolov9-vs-yolov8-custom-dataset/)</span>). So, this work is based on the YOLOv8n.</span>
<span id="cb36-109"><a href="#cb36-109" aria-hidden="true" tabindex="-1"></a>   </span>
<span id="cb36-110"><a href="#cb36-110" aria-hidden="true" tabindex="-1"></a>   <span class="al">![](images/latency.png)</span></span>
<span id="cb36-111"><a href="#cb36-111" aria-hidden="true" tabindex="-1"></a>   </span>
<span id="cb36-112"><a href="#cb36-112" aria-hidden="true" tabindex="-1"></a><span class="ss">4. </span>**Accuracy and Efficiency**:</span>
<span id="cb36-113"><a href="#cb36-113" aria-hidden="true" tabindex="-1"></a>   </span>
<span id="cb36-114"><a href="#cb36-114" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>While early versions of YOLO traded off some accuracy for speed, recent versions have made substantial strides in balancing both. The newer models are faster and more accurate, detecting small objects (such as bees) and performing well on complex datasets.</span>
<span id="cb36-115"><a href="#cb36-115" aria-hidden="true" tabindex="-1"></a>   </span>
<span id="cb36-116"><a href="#cb36-116" aria-hidden="true" tabindex="-1"></a><span class="ss">5. </span>**Wide Range of Applications**:</span>
<span id="cb36-117"><a href="#cb36-117" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>YOLO’s versatility has led to its adoption in numerous fields. It is used in traffic monitoring systems to detect and count vehicles, security applications to identify potential threats, and agricultural technology to monitor crops and livestock. Its application extends to any domain requiring efficient and accurate object detection.</span>
<span id="cb36-118"><a href="#cb36-118" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-119"><a href="#cb36-119" aria-hidden="true" tabindex="-1"></a><span class="ss">6. </span>**Community and Development**:</span>
<span id="cb36-120"><a href="#cb36-120" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>YOLO continues to evolve and is supported by a strong community of developers and researchers (being the YOLOv8 very strong). Open-source implementations and extensive documentation have made it accessible for customization and integration into various projects. Popular deep learning frameworks like Darknet, TensorFlow, and PyTorch support YOLO, further broadening its applicability.</span>
<span id="cb36-121"><a href="#cb36-121" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span><span class="co">[</span><span class="ot">Ultralitics YOLOv8</span><span class="co">](https://github.com/ultralytics/ultralytics?tab=readme-ov-file)</span> can not only <span class="co">[</span><span class="ot">Detect</span><span class="co">](https://docs.ultralytics.com/tasks/detect)</span> (our case here) but also <span class="co">[</span><span class="ot">Segment</span><span class="co">](https://docs.ultralytics.com/tasks/segment)</span> and <span class="co">[</span><span class="ot">Pose</span><span class="co">](https://docs.ultralytics.com/tasks/pose)</span> models pre-trained on the <span class="co">[</span><span class="ot">COCO</span><span class="co">](https://docs.ultralytics.com/datasets/detect/coco)</span> dataset and YOLOv8 <span class="co">[</span><span class="ot">Classify</span><span class="co">](https://docs.ultralytics.com/tasks/classify)</span> models pre-trained on the <span class="co">[</span><span class="ot">ImageNet</span><span class="co">](https://docs.ultralytics.com/datasets/classify/imagenet)</span> dataset. <span class="co">[</span><span class="ot">Track</span><span class="co">](https://docs.ultralytics.com/modes/track)</span> mode is available for all Detect, Segment, and Pose models.</span>
<span id="cb36-122"><a href="#cb36-122" aria-hidden="true" tabindex="-1"></a>   </span>
<span id="cb36-123"><a href="#cb36-123" aria-hidden="true" tabindex="-1"></a>   <span class="al">![Ultralytics YOLO supported tasks](https://raw.githubusercontent.com/ultralytics/assets/main/im/banner-tasks.png)</span></span>
<span id="cb36-124"><a href="#cb36-124" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-125"><a href="#cb36-125" aria-hidden="true" tabindex="-1"></a>In this tutorial, we leverage the power of YOLOv8 exported to NCNN format to estimate the number of bees at a beehive entrance using a Raspberry Pi Zero 2W (Rasp-Zero) in real-time. This setup demonstrates the practicality and effectiveness of deploying advanced machine learning models on edge devices for real-time environmental monitoring. </span>
<span id="cb36-126"><a href="#cb36-126" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-127"><a href="#cb36-127" aria-hidden="true" tabindex="-1"></a><span class="fu">### Exploring YOLO with Python</span></span>
<span id="cb36-128"><a href="#cb36-128" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-129"><a href="#cb36-129" aria-hidden="true" tabindex="-1"></a>To start, let's call the Python Interpreter so we can explore how the YOLO model works, line by line:</span>
<span id="cb36-130"><a href="#cb36-130" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-131"><a href="#cb36-131" aria-hidden="true" tabindex="-1"></a><span class="in">```bash</span></span>
<span id="cb36-132"><a href="#cb36-132" aria-hidden="true" tabindex="-1"></a><span class="ex">python</span></span>
<span id="cb36-133"><a href="#cb36-133" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb36-134"><a href="#cb36-134" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-135"><a href="#cb36-135" aria-hidden="true" tabindex="-1"></a><span class="al">![](images/py-inter-1.png)</span></span>
<span id="cb36-136"><a href="#cb36-136" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-137"><a href="#cb36-137" aria-hidden="true" tabindex="-1"></a>Now, we should call the YOLO library from Ultralitics and load the model:</span>
<span id="cb36-138"><a href="#cb36-138" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-139"><a href="#cb36-139" aria-hidden="true" tabindex="-1"></a><span class="in">```python</span></span>
<span id="cb36-140"><a href="#cb36-140" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> ultralytics <span class="im">import</span> YOLO</span>
<span id="cb36-141"><a href="#cb36-141" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> YOLO(<span class="st">'yolov8n_ncnn_model'</span>)</span>
<span id="cb36-142"><a href="#cb36-142" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb36-143"><a href="#cb36-143" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-144"><a href="#cb36-144" aria-hidden="true" tabindex="-1"></a>Next, run inference over an image (let's use again <span class="in">`bus.jpg`</span>):</span>
<span id="cb36-145"><a href="#cb36-145" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-146"><a href="#cb36-146" aria-hidden="true" tabindex="-1"></a><span class="in">```python</span></span>
<span id="cb36-147"><a href="#cb36-147" aria-hidden="true" tabindex="-1"></a>img <span class="op">=</span> <span class="st">'bus.jpg'</span></span>
<span id="cb36-148"><a href="#cb36-148" aria-hidden="true" tabindex="-1"></a>result <span class="op">=</span> model.predict(img, save<span class="op">=</span><span class="va">True</span>, imgsz<span class="op">=</span><span class="dv">640</span>, conf<span class="op">=</span><span class="fl">0.5</span>, iou<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb36-149"><a href="#cb36-149" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb36-150"><a href="#cb36-150" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-151"><a href="#cb36-151" aria-hidden="true" tabindex="-1"></a><span class="al">![](images/py-inf2.png)</span></span>
<span id="cb36-152"><a href="#cb36-152" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-153"><a href="#cb36-153" aria-hidden="true" tabindex="-1"></a>We can verify that the result is the same as the one we get running the inference at the terminal level (CLI).</span>
<span id="cb36-154"><a href="#cb36-154" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-155"><a href="#cb36-155" aria-hidden="true" tabindex="-1"></a><span class="in">```python</span></span>
<span id="cb36-156"><a href="#cb36-156" aria-hidden="true" tabindex="-1"></a>image <span class="dv">1</span><span class="op">/</span><span class="dv">1</span> <span class="op">/</span>home<span class="op">/</span>mjrovai<span class="op">/</span>Documents<span class="op">/</span>YOLO<span class="op">/</span>bus.jpg: <span class="dv">640</span><span class="er">x640</span> <span class="dv">3</span> persons, <span class="dv">1</span> bus, <span class="fl">4048.5</span><span class="er">ms</span></span>
<span id="cb36-157"><a href="#cb36-157" aria-hidden="true" tabindex="-1"></a>Speed: <span class="fl">635.7</span><span class="er">ms</span> preprocess, <span class="fl">4048.5</span><span class="er">ms</span> inference, <span class="fl">33897.6</span><span class="er">ms</span> postprocess per image at shape (<span class="dv">1</span>, <span class="dv">3</span>, <span class="dv">640</span>, <span class="dv">640</span>)</span>
<span id="cb36-158"><a href="#cb36-158" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-159"><a href="#cb36-159" aria-hidden="true" tabindex="-1"></a>Results saved to runs<span class="op">/</span>detect<span class="op">/</span>predict7 </span>
<span id="cb36-160"><a href="#cb36-160" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb36-161"><a href="#cb36-161" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-162"><a href="#cb36-162" aria-hidden="true" tabindex="-1"></a>But, we are interested in analyzing the "result" content.</span>
<span id="cb36-163"><a href="#cb36-163" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-164"><a href="#cb36-164" aria-hidden="true" tabindex="-1"></a>For example, we can see <span class="in">`result[0].boxes.data`</span>, showing us the main inference result, which is a tensor shape (4, 6). Each line is one of the objects detected, being the 4 first columns, the bounding boxes coordinates, the 5th, the confidence, and the 6th, the class (in this case, <span class="in">`0: person`</span> and <span class="in">`5: bus`</span>):</span>
<span id="cb36-165"><a href="#cb36-165" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-166"><a href="#cb36-166" aria-hidden="true" tabindex="-1"></a><span class="al">![](images/py-inter-3.png)</span></span>
<span id="cb36-167"><a href="#cb36-167" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-168"><a href="#cb36-168" aria-hidden="true" tabindex="-1"></a>We can access several inference results separately, as the inference time, and have it printed in a better format:</span>
<span id="cb36-169"><a href="#cb36-169" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-170"><a href="#cb36-170" aria-hidden="true" tabindex="-1"></a><span class="in">```python</span></span>
<span id="cb36-171"><a href="#cb36-171" aria-hidden="true" tabindex="-1"></a>inference_time <span class="op">=</span> <span class="bu">int</span>(result[<span class="dv">0</span>].speed[<span class="st">'inference'</span>])</span>
<span id="cb36-172"><a href="#cb36-172" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Inference Time: </span><span class="sc">{</span>inference_time<span class="sc">}</span><span class="ss"> ms"</span>)</span>
<span id="cb36-173"><a href="#cb36-173" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb36-174"><a href="#cb36-174" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-175"><a href="#cb36-175" aria-hidden="true" tabindex="-1"></a>Or we can have the total number of objects detected:</span>
<span id="cb36-176"><a href="#cb36-176" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-177"><a href="#cb36-177" aria-hidden="true" tabindex="-1"></a><span class="in">```python</span></span>
<span id="cb36-178"><a href="#cb36-178" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f'Number of objects: </span><span class="sc">{</span><span class="bu">len</span> (result[<span class="dv">0</span>].boxes.cls)<span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb36-179"><a href="#cb36-179" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb36-180"><a href="#cb36-180" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-181"><a href="#cb36-181" aria-hidden="true" tabindex="-1"></a><span class="al">![](images/py-inter-4.png)</span></span>
<span id="cb36-182"><a href="#cb36-182" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-183"><a href="#cb36-183" aria-hidden="true" tabindex="-1"></a>With Python, we can create a detailed output that meets our needs. In our final project, we will run a Python script at once rather than manually entering it line by line in the interpreter. </span>
<span id="cb36-184"><a href="#cb36-184" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-185"><a href="#cb36-185" aria-hidden="true" tabindex="-1"></a>For that, let's use <span class="in">`nano`</span> as our text editor. First, we should create an empty Python script named, for example, <span class="in">`yolov8_tests.py`</span>:</span>
<span id="cb36-186"><a href="#cb36-186" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-187"><a href="#cb36-187" aria-hidden="true" tabindex="-1"></a><span class="in">```python</span></span>
<span id="cb36-188"><a href="#cb36-188" aria-hidden="true" tabindex="-1"></a>nano yolov8_tests.py</span>
<span id="cb36-189"><a href="#cb36-189" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb36-190"><a href="#cb36-190" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-191"><a href="#cb36-191" aria-hidden="true" tabindex="-1"></a>Enter with the code lines:</span>
<span id="cb36-192"><a href="#cb36-192" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-193"><a href="#cb36-193" aria-hidden="true" tabindex="-1"></a><span class="in">```python</span></span>
<span id="cb36-194"><a href="#cb36-194" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> ultralytics <span class="im">import</span> YOLO</span>
<span id="cb36-195"><a href="#cb36-195" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-196"><a href="#cb36-196" aria-hidden="true" tabindex="-1"></a><span class="co"># Load the YOLOv8 model</span></span>
<span id="cb36-197"><a href="#cb36-197" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> YOLO(<span class="st">'yolov8n_ncnn_model'</span>)</span>
<span id="cb36-198"><a href="#cb36-198" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-199"><a href="#cb36-199" aria-hidden="true" tabindex="-1"></a><span class="co"># Run inference</span></span>
<span id="cb36-200"><a href="#cb36-200" aria-hidden="true" tabindex="-1"></a>img <span class="op">=</span> <span class="st">'bus.jpg'</span></span>
<span id="cb36-201"><a href="#cb36-201" aria-hidden="true" tabindex="-1"></a>result <span class="op">=</span> model.predict(img, save<span class="op">=</span><span class="va">False</span>, imgsz<span class="op">=</span><span class="dv">640</span>, conf<span class="op">=</span><span class="fl">0.5</span>, iou<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb36-202"><a href="#cb36-202" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-203"><a href="#cb36-203" aria-hidden="true" tabindex="-1"></a><span class="co"># print the results</span></span>
<span id="cb36-204"><a href="#cb36-204" aria-hidden="true" tabindex="-1"></a>inference_time <span class="op">=</span> <span class="bu">int</span>(result[<span class="dv">0</span>].speed[<span class="st">'inference'</span>])</span>
<span id="cb36-205"><a href="#cb36-205" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Inference Time: </span><span class="sc">{</span>inference_time<span class="sc">}</span><span class="ss"> ms"</span>)</span>
<span id="cb36-206"><a href="#cb36-206" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f'Number of objects: </span><span class="sc">{</span><span class="bu">len</span> (result[<span class="dv">0</span>].boxes.cls)<span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb36-207"><a href="#cb36-207" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb36-208"><a href="#cb36-208" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-209"><a href="#cb36-209" aria-hidden="true" tabindex="-1"></a><span class="al">![](images/python_test.png)</span></span>
<span id="cb36-210"><a href="#cb36-210" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-211"><a href="#cb36-211" aria-hidden="true" tabindex="-1"></a>And enter with the commands: <span class="in">`[CTRL+O]`</span>  + <span class="in">`[ENTER]`</span> +<span class="in">` [CTRL+X]`</span> to save the Python script. </span>
<span id="cb36-212"><a href="#cb36-212" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-213"><a href="#cb36-213" aria-hidden="true" tabindex="-1"></a>Run the script:</span>
<span id="cb36-214"><a href="#cb36-214" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-215"><a href="#cb36-215" aria-hidden="true" tabindex="-1"></a><span class="in">```bash</span></span>
<span id="cb36-216"><a href="#cb36-216" aria-hidden="true" tabindex="-1"></a><span class="ex">python</span> yolov8_tests.py</span>
<span id="cb36-217"><a href="#cb36-217" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb36-218"><a href="#cb36-218" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-219"><a href="#cb36-219" aria-hidden="true" tabindex="-1"></a><span class="al">![](images/py-inf1.png)</span></span>
<span id="cb36-220"><a href="#cb36-220" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-221"><a href="#cb36-221" aria-hidden="true" tabindex="-1"></a>We can verify again that the result is precisely the same as when we run the inference at the terminal level (CLI) and with the built-in Python interpreter.</span>
<span id="cb36-222"><a href="#cb36-222" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-223"><a href="#cb36-223" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Note about the Latency:</span></span>
<span id="cb36-224"><a href="#cb36-224" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-225"><a href="#cb36-225" aria-hidden="true" tabindex="-1"></a>The process of calling the YOLO library and loading the model for inference for the first time takes a long time, but the inferences after that will be much faster. For example, the first single inference took 3 to 4 seconds, but after that, the inference time is reduced to less than 1 second.</span>
<span id="cb36-226"><a href="#cb36-226" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-227"><a href="#cb36-227" aria-hidden="true" tabindex="-1"></a><span class="al">![](images/latency_test.png)</span></span>
<span id="cb36-228"><a href="#cb36-228" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-229"><a href="#cb36-229" aria-hidden="true" tabindex="-1"></a><span class="fu">## Estimating the number of Bees </span></span>
<span id="cb36-230"><a href="#cb36-230" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-231"><a href="#cb36-231" aria-hidden="true" tabindex="-1"></a>For our project at the university, we are preparing to collect a dataset of bees at the entrance of a beehive using the same camera connected to the Rasp-Zero. The images should be collected every 10 seconds. With the Arducam OV5647, the horizontal Field of View (FoV)  is 53.5^o^, which means that a camera positioned at the top of a standard Hive (46 cm) will capture all of its entrance (about 47 cm).</span>
<span id="cb36-232"><a href="#cb36-232" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-233"><a href="#cb36-233" aria-hidden="true" tabindex="-1"></a><span class="al">![](images/camera_pos.png)</span></span>
<span id="cb36-234"><a href="#cb36-234" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-235"><a href="#cb36-235" aria-hidden="true" tabindex="-1"></a><span class="fu">### Dataset</span></span>
<span id="cb36-236"><a href="#cb36-236" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-237"><a href="#cb36-237" aria-hidden="true" tabindex="-1"></a>The dataset collection is the most critical phase of the project and should take several weeks or months. For this tutorial, we will use a public dataset: "Sledevic, Tomyslav (2023), “[Labeled dataset for bee detection and direction estimation on beehive landing boards,” Mendeley Data, V5, doi: 10.17632/8gb9r2yhfc.5"</span>
<span id="cb36-238"><a href="#cb36-238" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-239"><a href="#cb36-239" aria-hidden="true" tabindex="-1"></a>The original dataset has 6,762 images (1920 x 1080), and around 8% of them (518) have no bees (only background). This is very important with Object Detection, where we should keep around 10% of the dataset with only background (without any objects to be detected). </span>
<span id="cb36-240"><a href="#cb36-240" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-241"><a href="#cb36-241" aria-hidden="true" tabindex="-1"></a>The images contain from zero to up to 61 bees:</span>
<span id="cb36-242"><a href="#cb36-242" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-243"><a href="#cb36-243" aria-hidden="true" tabindex="-1"></a><span class="al">![](images/bee_distr.jpeg)</span></span>
<span id="cb36-244"><a href="#cb36-244" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-245"><a href="#cb36-245" aria-hidden="true" tabindex="-1"></a>We downloaded the dataset (images and annotations) and uploaded it to <span class="co">[</span><span class="ot">Roboflow</span><span class="co">](https://roboflow.com/)</span>. There, you should create a free account and start a new project, for example, ("Bees_on_Hive_landing_boards"):</span>
<span id="cb36-246"><a href="#cb36-246" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-247"><a href="#cb36-247" aria-hidden="true" tabindex="-1"></a><span class="al">![](images/roboflow_proj.jpeg)</span></span>
<span id="cb36-248"><a href="#cb36-248" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-249"><a href="#cb36-249" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; We will not enter details about the Roboflow process once many tutorials are available. </span></span>
<span id="cb36-250"><a href="#cb36-250" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-251"><a href="#cb36-251" aria-hidden="true" tabindex="-1"></a>Once the project is created and the dataset is uploaded, you should review the annotations using the "Auto-Label" Tool. Note that all images with only a background should be saved w/o any annotations. At this step, you can also add additional images. </span>
<span id="cb36-252"><a href="#cb36-252" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-253"><a href="#cb36-253" aria-hidden="true" tabindex="-1"></a><span class="al">![](images/annotate.jpeg)</span></span>
<span id="cb36-254"><a href="#cb36-254" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-255"><a href="#cb36-255" aria-hidden="true" tabindex="-1"></a>Once all images are annotated, you should split them into training, validation, and testing. </span>
<span id="cb36-256"><a href="#cb36-256" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-257"><a href="#cb36-257" aria-hidden="true" tabindex="-1"></a><span class="al">![](images/split_data.jpeg)</span></span>
<span id="cb36-258"><a href="#cb36-258" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-259"><a href="#cb36-259" aria-hidden="true" tabindex="-1"></a><span class="fu">### Pre-Processing</span></span>
<span id="cb36-260"><a href="#cb36-260" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-261"><a href="#cb36-261" aria-hidden="true" tabindex="-1"></a>The last step with the dataset is preprocessing to generate a final version for training. The Yolov8 model can be trained with 640 x 640 pixels (RGB) images. Let's resize all images and generate augmented versions of each image (augmentation) to create new training examples from which our model can learn. </span>
<span id="cb36-262"><a href="#cb36-262" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-263"><a href="#cb36-263" aria-hidden="true" tabindex="-1"></a>For augmentation, we will rotate the images (+/-15^o^) and vary the brightness and exposure.</span>
<span id="cb36-264"><a href="#cb36-264" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-265"><a href="#cb36-265" aria-hidden="true" tabindex="-1"></a><span class="al">![](images/preprocess.jpeg)</span></span>
<span id="cb36-266"><a href="#cb36-266" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-267"><a href="#cb36-267" aria-hidden="true" tabindex="-1"></a>This will create a final dataset of 16,228 images. </span>
<span id="cb36-268"><a href="#cb36-268" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-269"><a href="#cb36-269" aria-hidden="true" tabindex="-1"></a><span class="al">![](images/final-dataset.jpeg)</span></span>
<span id="cb36-270"><a href="#cb36-270" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-271"><a href="#cb36-271" aria-hidden="true" tabindex="-1"></a>Now, you should export the model in a YOLOv8 format. You can download a zipped version of the dataset to your desktop or  get a downloaded code to be used with a Jupyter Notebook:</span>
<span id="cb36-272"><a href="#cb36-272" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-273"><a href="#cb36-273" aria-hidden="true" tabindex="-1"></a><span class="al">![](images/download_code.jpeg)</span></span>
<span id="cb36-274"><a href="#cb36-274" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-275"><a href="#cb36-275" aria-hidden="true" tabindex="-1"></a>And that is it! We are prepared to start our training using Google Colab.</span>
<span id="cb36-276"><a href="#cb36-276" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-277"><a href="#cb36-277" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; The pre-processed dataset can be found at the </span><span class="co">[</span><span class="ot">Roboflow site</span><span class="co">](https://universe.roboflow.com/marcelo-rovai-riila/bees_on_hive_landing_boards)</span><span class="at">.</span></span>
<span id="cb36-278"><a href="#cb36-278" aria-hidden="true" tabindex="-1"></a><span class="at">&gt;</span></span>
<span id="cb36-279"><a href="#cb36-279" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-280"><a href="#cb36-280" aria-hidden="true" tabindex="-1"></a><span class="fu">### Training YOLOv8 on a Customized Dataset</span></span>
<span id="cb36-281"><a href="#cb36-281" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-282"><a href="#cb36-282" aria-hidden="true" tabindex="-1"></a>For training, let's adapt one of the public examples available from Ultralitytics and run it on Google Colab:</span>
<span id="cb36-283"><a href="#cb36-283" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-284"><a href="#cb36-284" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>yolov8_bees_on_hive_landing_board.ipynb [<span class="co">[</span><span class="ot">Open In Colab</span><span class="co">]</span>](https://colab.research.google.com/github/Mjrovai/Bee-Counting/blob/main/yolov8_bees_on_hive_landing_board.ipynb)</span>
<span id="cb36-285"><a href="#cb36-285" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-286"><a href="#cb36-286" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Critical points on the Notebook:</span></span>
<span id="cb36-287"><a href="#cb36-287" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-288"><a href="#cb36-288" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>Run it with GPU (the NVidia T4 is free)</span>
<span id="cb36-289"><a href="#cb36-289" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-290"><a href="#cb36-290" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>Install Ultralytics using PIP.</span>
<span id="cb36-291"><a href="#cb36-291" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-292"><a href="#cb36-292" aria-hidden="true" tabindex="-1"></a>    <span class="al">![](images/inst_Ultralytics.jpeg)</span></span>
<span id="cb36-293"><a href="#cb36-293" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-294"><a href="#cb36-294" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>Now, you can import the YOLO and upload your dataset to the CoLab, pasting the Download code that you get from Roboflow. Note that your dataset will be mounted under <span class="in">`/content/datasets/`</span>:</span>
<span id="cb36-295"><a href="#cb36-295" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-296"><a href="#cb36-296" aria-hidden="true" tabindex="-1"></a><span class="al">![](images/dataset_colab.jpeg)</span></span>
<span id="cb36-297"><a href="#cb36-297" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-298"><a href="#cb36-298" aria-hidden="true" tabindex="-1"></a><span class="ss">4. </span>It is important to verify and change, if needed, the file <span class="in">`data.yaml`</span> with the correct path for the images:</span>
<span id="cb36-299"><a href="#cb36-299" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-300"><a href="#cb36-300" aria-hidden="true" tabindex="-1"></a><span class="in">```bash</span></span>
<span id="cb36-301"><a href="#cb36-301" aria-hidden="true" tabindex="-1"></a><span class="ex">names:</span></span>
<span id="cb36-302"><a href="#cb36-302" aria-hidden="true" tabindex="-1"></a><span class="ex">-</span> bee</span>
<span id="cb36-303"><a href="#cb36-303" aria-hidden="true" tabindex="-1"></a><span class="ex">nc:</span> 1</span>
<span id="cb36-304"><a href="#cb36-304" aria-hidden="true" tabindex="-1"></a><span class="ex">roboflow:</span></span>
<span id="cb36-305"><a href="#cb36-305" aria-hidden="true" tabindex="-1"></a>  <span class="ex">license:</span> CC BY 4.0</span>
<span id="cb36-306"><a href="#cb36-306" aria-hidden="true" tabindex="-1"></a>  <span class="ex">project:</span> bees_on_hive_landing_boards</span>
<span id="cb36-307"><a href="#cb36-307" aria-hidden="true" tabindex="-1"></a>  <span class="ex">url:</span> https://universe.roboflow.com/marcelo-rovai-riila/bees_on_hive_landing_boards/dataset/1</span>
<span id="cb36-308"><a href="#cb36-308" aria-hidden="true" tabindex="-1"></a>  <span class="ex">version:</span> 1</span>
<span id="cb36-309"><a href="#cb36-309" aria-hidden="true" tabindex="-1"></a>  <span class="ex">workspace:</span> marcelo-rovai-riila</span>
<span id="cb36-310"><a href="#cb36-310" aria-hidden="true" tabindex="-1"></a><span class="ex">test:</span> /content/datasets/Bees_on_Hive_landing_boards-1test/images</span>
<span id="cb36-311"><a href="#cb36-311" aria-hidden="true" tabindex="-1"></a><span class="ex">train:</span> /content/datasets/Bees_on_Hive_landing_boards-1/train/images</span>
<span id="cb36-312"><a href="#cb36-312" aria-hidden="true" tabindex="-1"></a><span class="ex">val:</span> /content/datasets/Bees_on_Hive_landing_boards-1/valid/images</span>
<span id="cb36-313"><a href="#cb36-313" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb36-314"><a href="#cb36-314" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-315"><a href="#cb36-315" aria-hidden="true" tabindex="-1"></a><span class="ss">5. </span>Define the main hyperparameters that you want to change from default, for example:</span>
<span id="cb36-316"><a href="#cb36-316" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-317"><a href="#cb36-317" aria-hidden="true" tabindex="-1"></a>    <span class="in">```bash</span></span>
<span id="cb36-318"><a href="#cb36-318" aria-hidden="true" tabindex="-1"></a>    <span class="ex">MODEL</span> = <span class="st">'yolov8n.pt'</span></span>
<span id="cb36-319"><a href="#cb36-319" aria-hidden="true" tabindex="-1"></a>    <span class="ex">IMG_SIZE</span> = 640</span>
<span id="cb36-320"><a href="#cb36-320" aria-hidden="true" tabindex="-1"></a>    <span class="ex">EPOCHS</span> = 25 <span class="co"># For a final project, you should consider at least 100 epochs </span></span>
<span id="cb36-321"><a href="#cb36-321" aria-hidden="true" tabindex="-1"></a>    <span class="in">```</span></span>
<span id="cb36-322"><a href="#cb36-322" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-323"><a href="#cb36-323" aria-hidden="true" tabindex="-1"></a><span class="ss">6. </span>Run the training (using CLI):</span>
<span id="cb36-324"><a href="#cb36-324" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-325"><a href="#cb36-325" aria-hidden="true" tabindex="-1"></a>    <span class="in">```bash</span></span>
<span id="cb36-326"><a href="#cb36-326" aria-hidden="true" tabindex="-1"></a>    <span class="ex">!yolo</span> task=detect mode=train model={MODEL} data={dataset.location}/data.yaml epochs={EPOCHS} imgsz={IMG_SIZE} plots=True </span>
<span id="cb36-327"><a href="#cb36-327" aria-hidden="true" tabindex="-1"></a>    <span class="in">```</span></span>
<span id="cb36-328"><a href="#cb36-328" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-329"><a href="#cb36-329" aria-hidden="true" tabindex="-1"></a>    <span class="al">![](images/train_result.jpeg)</span></span>
<span id="cb36-330"><a href="#cb36-330" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-331"><a href="#cb36-331" aria-hidden="true" tabindex="-1"></a>​   The model took 2.7 hours to train and has an excellent result (mAP50 of 0.984). At the end of the training, all results are saved in the folder listed, for example: <span class="in">`/runs/detect/train3/`</span>. There, you can find, for example, the confusion matrix and the metrics curves per epoch.</span>
<span id="cb36-332"><a href="#cb36-332" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-333"><a href="#cb36-333" aria-hidden="true" tabindex="-1"></a><span class="al">![](images/train_perf.jpeg)</span></span>
<span id="cb36-334"><a href="#cb36-334" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-335"><a href="#cb36-335" aria-hidden="true" tabindex="-1"></a><span class="ss">7. </span>Note that the trained model (<span class="in">`best.pt`</span>) is saved in the folder  <span class="in">`/runs/detect/train3/weights/`</span>. Now, you should validade the trained model with the <span class="in">`valid/images`</span>. </span>
<span id="cb36-336"><a href="#cb36-336" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-337"><a href="#cb36-337" aria-hidden="true" tabindex="-1"></a><span class="in">```bash</span></span>
<span id="cb36-338"><a href="#cb36-338" aria-hidden="true" tabindex="-1"></a><span class="ex">!yolo</span> task=detect mode=val model={HOME}/runs/detect/train3/weights/best.pt data={dataset.location}/data.yaml</span>
<span id="cb36-339"><a href="#cb36-339" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb36-340"><a href="#cb36-340" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-341"><a href="#cb36-341" aria-hidden="true" tabindex="-1"></a>​   The results were similar to training. </span>
<span id="cb36-342"><a href="#cb36-342" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-343"><a href="#cb36-343" aria-hidden="true" tabindex="-1"></a><span class="ss">8. </span>Now, we should perform inference on the images left aside for testing</span>
<span id="cb36-344"><a href="#cb36-344" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-345"><a href="#cb36-345" aria-hidden="true" tabindex="-1"></a><span class="in">```bash</span></span>
<span id="cb36-346"><a href="#cb36-346" aria-hidden="true" tabindex="-1"></a><span class="ex">!yolo</span> task=detect mode=predict model={HOME}/runs/detect/train3/weights/best.pt conf=0.25 source={dataset.location}/test/images save=True</span>
<span id="cb36-347"><a href="#cb36-347" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb36-348"><a href="#cb36-348" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-349"><a href="#cb36-349" aria-hidden="true" tabindex="-1"></a>The inference results are saved in the folder <span class="in">`runs/detect/predict`</span>. Let's see some of them:</span>
<span id="cb36-350"><a href="#cb36-350" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-351"><a href="#cb36-351" aria-hidden="true" tabindex="-1"></a><span class="al">![](images/inf_colab.jpeg)</span></span>
<span id="cb36-352"><a href="#cb36-352" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-353"><a href="#cb36-353" aria-hidden="true" tabindex="-1"></a>We can also perform inference with a completely new and complex image from another beehive with a different background (the beehive of Professor Maurilio of our University). The results were great (but not perfect and with a lower confidence score). The model found 41 bees. </span>
<span id="cb36-354"><a href="#cb36-354" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-355"><a href="#cb36-355" aria-hidden="true" tabindex="-1"></a><span class="al">![](images/maurilio_bees_tEXbFzkpEf.jpg)</span></span>
<span id="cb36-356"><a href="#cb36-356" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-357"><a href="#cb36-357" aria-hidden="true" tabindex="-1"></a><span class="ss">9. </span>The last thing to do is export the train, validation, and test results for your Drive at Google. To do so, you should mount your drive. </span>
<span id="cb36-358"><a href="#cb36-358" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-359"><a href="#cb36-359" aria-hidden="true" tabindex="-1"></a>    <span class="in">```python</span></span>
<span id="cb36-360"><a href="#cb36-360" aria-hidden="true" tabindex="-1"></a>    <span class="im">from</span> google.colab <span class="im">import</span> drive</span>
<span id="cb36-361"><a href="#cb36-361" aria-hidden="true" tabindex="-1"></a>    drive.mount(<span class="st">'/content/gdrive'</span>)</span>
<span id="cb36-362"><a href="#cb36-362" aria-hidden="true" tabindex="-1"></a>    <span class="in">```</span></span>
<span id="cb36-363"><a href="#cb36-363" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-364"><a href="#cb36-364" aria-hidden="true" tabindex="-1"></a>    and copy the content of <span class="in">`/runs`</span> folder to a folder that you should create in your Drive, for example: </span>
<span id="cb36-365"><a href="#cb36-365" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-366"><a href="#cb36-366" aria-hidden="true" tabindex="-1"></a>    <span class="in">```bash</span></span>
<span id="cb36-367"><a href="#cb36-367" aria-hidden="true" tabindex="-1"></a>    <span class="ex">!scp</span> <span class="at">-r</span> /content/runs <span class="st">'/content/gdrive/MyDrive/10_UNIFEI/Bee_Project/YOLO/bees_on_hive_landing'</span></span>
<span id="cb36-368"><a href="#cb36-368" aria-hidden="true" tabindex="-1"></a>    <span class="in">```</span></span>
<span id="cb36-369"><a href="#cb36-369" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-370"><a href="#cb36-370" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb36-371"><a href="#cb36-371" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-372"><a href="#cb36-372" aria-hidden="true" tabindex="-1"></a><span class="fu">## Inference with the trained model, using the Rasp-Zero</span></span>
<span id="cb36-373"><a href="#cb36-373" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-374"><a href="#cb36-374" aria-hidden="true" tabindex="-1"></a>Using the FileZilla FTP, let's transfer the <span class="in">`best.pt`</span> to our Rasp-Zero (before the transfer, you may change the model name, for example, <span class="in">`bee_landing_640_best.pt`</span>). </span>
<span id="cb36-375"><a href="#cb36-375" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-376"><a href="#cb36-376" aria-hidden="true" tabindex="-1"></a>The first thing to do is convert the model to an NCNN format:</span>
<span id="cb36-377"><a href="#cb36-377" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-378"><a href="#cb36-378" aria-hidden="true" tabindex="-1"></a><span class="in">```bash</span></span>
<span id="cb36-379"><a href="#cb36-379" aria-hidden="true" tabindex="-1"></a><span class="ex">yolo</span> export model=bee_landing_640_best.pt format=ncnn </span>
<span id="cb36-380"><a href="#cb36-380" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb36-381"><a href="#cb36-381" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-382"><a href="#cb36-382" aria-hidden="true" tabindex="-1"></a>As a result, a new converted model, <span class="in">`bee_landing_640_best_ncnn_model`</span> is created in the same directory.</span>
<span id="cb36-383"><a href="#cb36-383" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-384"><a href="#cb36-384" aria-hidden="true" tabindex="-1"></a>Let's create a folder to receive some test images (under <span class="in">`Documents/YOLO/`</span>:</span>
<span id="cb36-385"><a href="#cb36-385" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-386"><a href="#cb36-386" aria-hidden="true" tabindex="-1"></a><span class="in">```bash</span></span>
<span id="cb36-387"><a href="#cb36-387" aria-hidden="true" tabindex="-1"></a><span class="fu">mkdir</span> test_images</span>
<span id="cb36-388"><a href="#cb36-388" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb36-389"><a href="#cb36-389" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-390"><a href="#cb36-390" aria-hidden="true" tabindex="-1"></a>Using the FileZilla FTP, let's transfer a few images from the test dataset to our Rasp-Zero:</span>
<span id="cb36-391"><a href="#cb36-391" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-392"><a href="#cb36-392" aria-hidden="true" tabindex="-1"></a><span class="al">![](images/image_test_.png)</span></span>
<span id="cb36-393"><a href="#cb36-393" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-394"><a href="#cb36-394" aria-hidden="true" tabindex="-1"></a>Let's use the Python Interpreter:</span>
<span id="cb36-395"><a href="#cb36-395" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-396"><a href="#cb36-396" aria-hidden="true" tabindex="-1"></a><span class="in">```bash</span></span>
<span id="cb36-397"><a href="#cb36-397" aria-hidden="true" tabindex="-1"></a><span class="ex">python</span></span>
<span id="cb36-398"><a href="#cb36-398" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb36-399"><a href="#cb36-399" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-400"><a href="#cb36-400" aria-hidden="true" tabindex="-1"></a>As before, we will import the YOLO library and define our converted model to detect bees:</span>
<span id="cb36-401"><a href="#cb36-401" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-402"><a href="#cb36-402" aria-hidden="true" tabindex="-1"></a><span class="in">```python</span></span>
<span id="cb36-403"><a href="#cb36-403" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> ultralytics <span class="im">import</span> YOLO</span>
<span id="cb36-404"><a href="#cb36-404" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> YOLO(<span class="st">'bee_landing_640_best_ncnn_model'</span>)</span>
<span id="cb36-405"><a href="#cb36-405" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb36-406"><a href="#cb36-406" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-407"><a href="#cb36-407" aria-hidden="true" tabindex="-1"></a>Now, let's define an image and call the inference (we will save the image result this time to external verification):</span>
<span id="cb36-408"><a href="#cb36-408" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-409"><a href="#cb36-409" aria-hidden="true" tabindex="-1"></a><span class="in">```python</span></span>
<span id="cb36-410"><a href="#cb36-410" aria-hidden="true" tabindex="-1"></a>img <span class="op">=</span> <span class="st">'test_images/15_bees.jpg'</span></span>
<span id="cb36-411"><a href="#cb36-411" aria-hidden="true" tabindex="-1"></a>result <span class="op">=</span> model.predict(img, save<span class="op">=</span><span class="va">True</span>, imgsz<span class="op">=</span><span class="dv">640</span>, conf<span class="op">=</span><span class="fl">0.2</span>, iou<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb36-412"><a href="#cb36-412" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb36-413"><a href="#cb36-413" aria-hidden="true" tabindex="-1"></a>The inference result is saved on the variable <span class="in">`result,`</span> and the processed image on <span class="in">`runs/detect/predict9`</span></span>
<span id="cb36-414"><a href="#cb36-414" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-415"><a href="#cb36-415" aria-hidden="true" tabindex="-1"></a><span class="al">![](images/inf_bees_15.png)</span></span>
<span id="cb36-416"><a href="#cb36-416" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-417"><a href="#cb36-417" aria-hidden="true" tabindex="-1"></a>Using FileZilla FTP, we can send the inference result to our Desktop for verification: </span>
<span id="cb36-418"><a href="#cb36-418" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-419"><a href="#cb36-419" aria-hidden="true" tabindex="-1"></a><span class="al">![](images/inf_img_15_bees.png)</span></span>
<span id="cb36-420"><a href="#cb36-420" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-421"><a href="#cb36-421" aria-hidden="true" tabindex="-1"></a>let's go over the other images, analyzing the number of objects (bees) found:</span>
<span id="cb36-422"><a href="#cb36-422" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-423"><a href="#cb36-423" aria-hidden="true" tabindex="-1"></a><span class="al">![](images/multiple_infer.png)</span></span>
<span id="cb36-424"><a href="#cb36-424" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-425"><a href="#cb36-425" aria-hidden="true" tabindex="-1"></a>Depending on the confidence, we can have some false positives or negatives. But in general, with a model trained based on the smaller base model of the YOLOv8 family (YOLOv8n) and also converted to NCNN, the result is pretty good, running on an Edge device such as the Rasp-Zero. Also, note that the inference latency is around 730ms. </span>
<span id="cb36-426"><a href="#cb36-426" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-427"><a href="#cb36-427" aria-hidden="true" tabindex="-1"></a>For example, by running the inference on <span class="in">`Maurilio-bee.jpeg`</span>, we can find <span class="in">`40 bees`</span>. During the test phase on Colab,  41 bees were found (we only missed one here.)</span>
<span id="cb36-428"><a href="#cb36-428" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-429"><a href="#cb36-429" aria-hidden="true" tabindex="-1"></a><span class="al">![](images/maurilio-bee-rasp.png)</span></span>
<span id="cb36-430"><a href="#cb36-430" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-431"><a href="#cb36-431" aria-hidden="true" tabindex="-1"></a><span class="fu">## Considerations about the Post-Processing</span></span>
<span id="cb36-432"><a href="#cb36-432" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-433"><a href="#cb36-433" aria-hidden="true" tabindex="-1"></a>Our final project should be very simple in terms of code. We will use the camera to capture an image every 10 seconds. As we did in the previous section, the captured image should be the input for the trained and converted model. We should get the number of bees for each image and save it in a database (for example, timestamp: number of bees).</span>
<span id="cb36-434"><a href="#cb36-434" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-435"><a href="#cb36-435" aria-hidden="true" tabindex="-1"></a>We can do it with a single Python script or use a Linux system timer, like <span class="in">`cron`</span>, to periodically capture images every 10 seconds and have a separate Python script to process these images as they are saved. This method can be particularly efficient in managing system resources and can be more robust against potential delays in image processing.</span>
<span id="cb36-436"><a href="#cb36-436" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-437"><a href="#cb36-437" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Setting Up the Image Capture with `cron`</span></span>
<span id="cb36-438"><a href="#cb36-438" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-439"><a href="#cb36-439" aria-hidden="true" tabindex="-1"></a>First, we should set up a <span class="in">`cron`</span> job to use the <span class="in">`rpicam-jpeg`</span> command to capture an image every 10 seconds. </span>
<span id="cb36-440"><a href="#cb36-440" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-441"><a href="#cb36-441" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>**Edit the `crontab`**:</span>
<span id="cb36-442"><a href="#cb36-442" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>Open the terminal and type <span class="in">`crontab -e`</span> to edit the cron jobs.</span>
<span id="cb36-443"><a href="#cb36-443" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span><span class="in">`cron`</span> normally doesn't support sub-minute intervals directly, so we should use a workaround like a loop or watch for file changes. </span>
<span id="cb36-444"><a href="#cb36-444" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-445"><a href="#cb36-445" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>**Create a Bash Script (`capture.sh`)**:</span>
<span id="cb36-446"><a href="#cb36-446" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-447"><a href="#cb36-447" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>**Image Capture**: This bash script captures images every 10 seconds using <span class="in">`rpicam-jpeg`</span>, a command that is part of the <span class="in">`raspijpeg`</span> tool. This command lets us control the camera and capture JPEG images directly from the command line. This is especially useful because we are looking for a lightweight and straightforward method to capture images without the need for additional libraries like <span class="in">`Picamera`</span> or external software. The script also saves the captured image with a timestamp.</span>
<span id="cb36-448"><a href="#cb36-448" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-449"><a href="#cb36-449" aria-hidden="true" tabindex="-1"></a>   <span class="in">```python</span></span>
<span id="cb36-450"><a href="#cb36-450" aria-hidden="true" tabindex="-1"></a>   <span class="co">#!/bin/bash</span></span>
<span id="cb36-451"><a href="#cb36-451" aria-hidden="true" tabindex="-1"></a>   <span class="co"># Script to capture an image every 10 seconds</span></span>
<span id="cb36-452"><a href="#cb36-452" aria-hidden="true" tabindex="-1"></a>   </span>
<span id="cb36-453"><a href="#cb36-453" aria-hidden="true" tabindex="-1"></a>   <span class="cf">while</span> true</span>
<span id="cb36-454"><a href="#cb36-454" aria-hidden="true" tabindex="-1"></a>   do</span>
<span id="cb36-455"><a href="#cb36-455" aria-hidden="true" tabindex="-1"></a>     DATE<span class="op">=</span>$(date <span class="op">+</span><span class="st">"%Y-%m-</span><span class="sc">%d</span><span class="st">_%H%M%S"</span>)</span>
<span id="cb36-456"><a href="#cb36-456" aria-hidden="true" tabindex="-1"></a>     rpicam<span class="op">-</span>jpeg <span class="op">--</span>output test_images<span class="op">/</span>$DATE.jpg <span class="op">--</span>width <span class="dv">640</span> <span class="op">--</span>height <span class="dv">640</span></span>
<span id="cb36-457"><a href="#cb36-457" aria-hidden="true" tabindex="-1"></a>     sleep <span class="dv">10</span></span>
<span id="cb36-458"><a href="#cb36-458" aria-hidden="true" tabindex="-1"></a>   done</span>
<span id="cb36-459"><a href="#cb36-459" aria-hidden="true" tabindex="-1"></a>   <span class="in">```</span></span>
<span id="cb36-460"><a href="#cb36-460" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>We should make the script executable with <span class="in">`chmod +x capture.sh`</span>.</span>
<span id="cb36-461"><a href="#cb36-461" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>The script must start at boot or use a <span class="in">`@reboot`</span> entry in <span class="in">`cron`</span> to start it automatically.</span>
<span id="cb36-462"><a href="#cb36-462" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-463"><a href="#cb36-463" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Setting Up the Python Script for Inference</span></span>
<span id="cb36-464"><a href="#cb36-464" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-465"><a href="#cb36-465" aria-hidden="true" tabindex="-1"></a>**Image Processing**: The Python script continuously monitors the designated directory for new images, processes each new image using the YOLOv8 model, updates the database with the count of detected bees, and optionally deletes the image to conserve disk space.</span>
<span id="cb36-466"><a href="#cb36-466" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-467"><a href="#cb36-467" aria-hidden="true" tabindex="-1"></a>**Database Updates**: The results, along with the timestamps, are saved in an SQLite database. For that, a simple option is to use <span class="co">[</span><span class="ot">sqlite3</span><span class="co">](https://docs.python.org/3/library/sqlite3.html)</span>. </span>
<span id="cb36-468"><a href="#cb36-468" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-469"><a href="#cb36-469" aria-hidden="true" tabindex="-1"></a>In short, we need to write a script that continuously monitors the directory for new images, processes them using a YOLO model, and then saves the results to a SQLite database. Here’s how we can create and make the script executable:</span>
<span id="cb36-470"><a href="#cb36-470" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-471"><a href="#cb36-471" aria-hidden="true" tabindex="-1"></a><span class="in">```python</span></span>
<span id="cb36-472"><a href="#cb36-472" aria-hidden="true" tabindex="-1"></a><span class="co">#!/usr/bin/env python3</span></span>
<span id="cb36-473"><a href="#cb36-473" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> os</span>
<span id="cb36-474"><a href="#cb36-474" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> time</span>
<span id="cb36-475"><a href="#cb36-475" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> sqlite3</span>
<span id="cb36-476"><a href="#cb36-476" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> datetime <span class="im">import</span> datetime</span>
<span id="cb36-477"><a href="#cb36-477" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> ultralytics <span class="im">import</span> YOLO</span>
<span id="cb36-478"><a href="#cb36-478" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-479"><a href="#cb36-479" aria-hidden="true" tabindex="-1"></a><span class="co"># Constants and paths</span></span>
<span id="cb36-480"><a href="#cb36-480" aria-hidden="true" tabindex="-1"></a>IMAGES_DIR <span class="op">=</span> <span class="st">'test_images/'</span></span>
<span id="cb36-481"><a href="#cb36-481" aria-hidden="true" tabindex="-1"></a>MODEL_PATH <span class="op">=</span> <span class="st">'bee_landing_640_best_ncnn_model'</span></span>
<span id="cb36-482"><a href="#cb36-482" aria-hidden="true" tabindex="-1"></a>DB_PATH <span class="op">=</span> <span class="st">'bee_count.db'</span></span>
<span id="cb36-483"><a href="#cb36-483" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-484"><a href="#cb36-484" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> setup_database():</span>
<span id="cb36-485"><a href="#cb36-485" aria-hidden="true" tabindex="-1"></a>    <span class="co">""" Establishes a database connection and creates the table if it doesn't exist. """</span></span>
<span id="cb36-486"><a href="#cb36-486" aria-hidden="true" tabindex="-1"></a>    conn <span class="op">=</span> sqlite3.<span class="ex">connect</span>(DB_PATH)</span>
<span id="cb36-487"><a href="#cb36-487" aria-hidden="true" tabindex="-1"></a>    cursor <span class="op">=</span> conn.cursor()</span>
<span id="cb36-488"><a href="#cb36-488" aria-hidden="true" tabindex="-1"></a>    cursor.execute(<span class="st">'''</span></span>
<span id="cb36-489"><a href="#cb36-489" aria-hidden="true" tabindex="-1"></a><span class="st">        CREATE TABLE IF NOT EXISTS bee_counts</span></span>
<span id="cb36-490"><a href="#cb36-490" aria-hidden="true" tabindex="-1"></a><span class="st">        (timestamp TEXT, count INTEGER)</span></span>
<span id="cb36-491"><a href="#cb36-491" aria-hidden="true" tabindex="-1"></a><span class="st">    '''</span>)</span>
<span id="cb36-492"><a href="#cb36-492" aria-hidden="true" tabindex="-1"></a>    conn.commit()</span>
<span id="cb36-493"><a href="#cb36-493" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> conn</span>
<span id="cb36-494"><a href="#cb36-494" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-495"><a href="#cb36-495" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> process_image(image_path, model, conn):</span>
<span id="cb36-496"><a href="#cb36-496" aria-hidden="true" tabindex="-1"></a>    <span class="co">""" Processes an image to detect objects and logs the count to the database. """</span></span>
<span id="cb36-497"><a href="#cb36-497" aria-hidden="true" tabindex="-1"></a>    result <span class="op">=</span> model.predict(image_path, save<span class="op">=</span><span class="va">False</span>, imgsz<span class="op">=</span><span class="dv">640</span>, conf<span class="op">=</span><span class="fl">0.2</span>, iou<span class="op">=</span><span class="fl">0.3</span>, verbose<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb36-498"><a href="#cb36-498" aria-hidden="true" tabindex="-1"></a>    num_bees <span class="op">=</span> <span class="bu">len</span>(result[<span class="dv">0</span>].boxes.cls) </span>
<span id="cb36-499"><a href="#cb36-499" aria-hidden="true" tabindex="-1"></a>    timestamp <span class="op">=</span> datetime.now().strftime(<span class="st">'%Y-%m-</span><span class="sc">%d</span><span class="st"> %H:%M:%S'</span>)</span>
<span id="cb36-500"><a href="#cb36-500" aria-hidden="true" tabindex="-1"></a>    cursor <span class="op">=</span> conn.cursor()</span>
<span id="cb36-501"><a href="#cb36-501" aria-hidden="true" tabindex="-1"></a>    cursor.execute(<span class="st">"INSERT INTO bee_counts (timestamp, count) VALUES (?, ?)"</span>, (timestamp, num_bees))</span>
<span id="cb36-502"><a href="#cb36-502" aria-hidden="true" tabindex="-1"></a>    conn.commit()</span>
<span id="cb36-503"><a href="#cb36-503" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f'Processed </span><span class="sc">{</span>image_path<span class="sc">}</span><span class="ss">: Number of bees detected = </span><span class="sc">{</span>num_bees<span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb36-504"><a href="#cb36-504" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-505"><a href="#cb36-505" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> monitor_directory(model, conn):</span>
<span id="cb36-506"><a href="#cb36-506" aria-hidden="true" tabindex="-1"></a>    <span class="co">""" Monitors the directory for new images and processes them as they appear. """</span></span>
<span id="cb36-507"><a href="#cb36-507" aria-hidden="true" tabindex="-1"></a>    processed_files <span class="op">=</span> <span class="bu">set</span>()</span>
<span id="cb36-508"><a href="#cb36-508" aria-hidden="true" tabindex="-1"></a>    <span class="cf">while</span> <span class="va">True</span>:</span>
<span id="cb36-509"><a href="#cb36-509" aria-hidden="true" tabindex="-1"></a>        <span class="cf">try</span>:</span>
<span id="cb36-510"><a href="#cb36-510" aria-hidden="true" tabindex="-1"></a>            files <span class="op">=</span> <span class="bu">set</span>(os.listdir(IMAGES_DIR))</span>
<span id="cb36-511"><a href="#cb36-511" aria-hidden="true" tabindex="-1"></a>            new_files <span class="op">=</span> files <span class="op">-</span> processed_files</span>
<span id="cb36-512"><a href="#cb36-512" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> <span class="bu">file</span> <span class="kw">in</span> new_files:</span>
<span id="cb36-513"><a href="#cb36-513" aria-hidden="true" tabindex="-1"></a>                <span class="cf">if</span> <span class="bu">file</span>.endswith(<span class="st">'.jpg'</span>):</span>
<span id="cb36-514"><a href="#cb36-514" aria-hidden="true" tabindex="-1"></a>                    full_path <span class="op">=</span> os.path.join(IMAGES_DIR, <span class="bu">file</span>)</span>
<span id="cb36-515"><a href="#cb36-515" aria-hidden="true" tabindex="-1"></a>                    process_image(full_path, model, conn)</span>
<span id="cb36-516"><a href="#cb36-516" aria-hidden="true" tabindex="-1"></a>                    processed_files.add(<span class="bu">file</span>)</span>
<span id="cb36-517"><a href="#cb36-517" aria-hidden="true" tabindex="-1"></a>            time.sleep(<span class="dv">1</span>)  <span class="co"># Check every second</span></span>
<span id="cb36-518"><a href="#cb36-518" aria-hidden="true" tabindex="-1"></a>        <span class="cf">except</span> <span class="pp">KeyboardInterrupt</span>:</span>
<span id="cb36-519"><a href="#cb36-519" aria-hidden="true" tabindex="-1"></a>            <span class="bu">print</span>(<span class="st">"Stopping..."</span>)</span>
<span id="cb36-520"><a href="#cb36-520" aria-hidden="true" tabindex="-1"></a>            <span class="cf">break</span></span>
<span id="cb36-521"><a href="#cb36-521" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-522"><a href="#cb36-522" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> main():</span>
<span id="cb36-523"><a href="#cb36-523" aria-hidden="true" tabindex="-1"></a>    conn <span class="op">=</span> setup_database()</span>
<span id="cb36-524"><a href="#cb36-524" aria-hidden="true" tabindex="-1"></a>    model <span class="op">=</span> YOLO(MODEL_PATH)</span>
<span id="cb36-525"><a href="#cb36-525" aria-hidden="true" tabindex="-1"></a>    monitor_directory(model, conn)</span>
<span id="cb36-526"><a href="#cb36-526" aria-hidden="true" tabindex="-1"></a>    conn.close()</span>
<span id="cb36-527"><a href="#cb36-527" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-528"><a href="#cb36-528" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> <span class="va">__name__</span> <span class="op">==</span> <span class="st">"__main__"</span>:</span>
<span id="cb36-529"><a href="#cb36-529" aria-hidden="true" tabindex="-1"></a>    main()</span>
<span id="cb36-530"><a href="#cb36-530" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-531"><a href="#cb36-531" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb36-532"><a href="#cb36-532" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-533"><a href="#cb36-533" aria-hidden="true" tabindex="-1"></a>The python script must be executable, for that:</span>
<span id="cb36-534"><a href="#cb36-534" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-535"><a href="#cb36-535" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>**Save the script**: For example, as <span class="in">`process_images.py`</span>.</span>
<span id="cb36-536"><a href="#cb36-536" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-537"><a href="#cb36-537" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>**Change file permissions** to make it executable:</span>
<span id="cb36-538"><a href="#cb36-538" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-539"><a href="#cb36-539" aria-hidden="true" tabindex="-1"></a>    <span class="in">```bash</span></span>
<span id="cb36-540"><a href="#cb36-540" aria-hidden="true" tabindex="-1"></a>    <span class="fu">chmod</span> +x process_images.py</span>
<span id="cb36-541"><a href="#cb36-541" aria-hidden="true" tabindex="-1"></a>    <span class="in">```</span></span>
<span id="cb36-542"><a href="#cb36-542" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-543"><a href="#cb36-543" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>**Run the script** directly from the command line:</span>
<span id="cb36-544"><a href="#cb36-544" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-545"><a href="#cb36-545" aria-hidden="true" tabindex="-1"></a>    <span class="in">```bash</span></span>
<span id="cb36-546"><a href="#cb36-546" aria-hidden="true" tabindex="-1"></a>    <span class="ex">./process_images.py</span></span>
<span id="cb36-547"><a href="#cb36-547" aria-hidden="true" tabindex="-1"></a>    <span class="in">```</span></span>
<span id="cb36-548"><a href="#cb36-548" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-549"><a href="#cb36-549" aria-hidden="true" tabindex="-1"></a>We should consider keeping the script running even after closing the terminal; for that, we can use <span class="in">`nohup`</span> or <span class="in">`screen`</span>:</span>
<span id="cb36-550"><a href="#cb36-550" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-551"><a href="#cb36-551" aria-hidden="true" tabindex="-1"></a><span class="in">```bash</span></span>
<span id="cb36-552"><a href="#cb36-552" aria-hidden="true" tabindex="-1"></a><span class="fu">nohup</span> ./process_images.py <span class="kw">&amp;</span></span>
<span id="cb36-553"><a href="#cb36-553" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb36-554"><a href="#cb36-554" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-555"><a href="#cb36-555" aria-hidden="true" tabindex="-1"></a>or</span>
<span id="cb36-556"><a href="#cb36-556" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-557"><a href="#cb36-557" aria-hidden="true" tabindex="-1"></a><span class="in">```bash</span></span>
<span id="cb36-558"><a href="#cb36-558" aria-hidden="true" tabindex="-1"></a><span class="ex">screen</span> <span class="at">-S</span> bee_monitor</span>
<span id="cb36-559"><a href="#cb36-559" aria-hidden="true" tabindex="-1"></a><span class="ex">./process_images.py</span></span>
<span id="cb36-560"><a href="#cb36-560" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb36-561"><a href="#cb36-561" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-562"><a href="#cb36-562" aria-hidden="true" tabindex="-1"></a>Note that we are capturing images with their own timestamp and then log a separate timestamp for when the inference results are saved to the database. This approach can be beneficial for the following reasons:</span>
<span id="cb36-563"><a href="#cb36-563" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-564"><a href="#cb36-564" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>**Accuracy in Data Logging**:</span>
<span id="cb36-565"><a href="#cb36-565" aria-hidden="true" tabindex="-1"></a><span class="ss">    - </span>**Capture Timestamp**: The timestamp associated with each image capture represents the exact moment the image was taken. This is crucial for applications where precise timing of events (like bee activity) is important for analysis.</span>
<span id="cb36-566"><a href="#cb36-566" aria-hidden="true" tabindex="-1"></a><span class="ss">    - </span>**Inference Timestamp**: This timestamp indicates when the image was processed and the results were recorded in the database. This can differ from the capture time due to processing delays or if the image processing is batched or queued.</span>
<span id="cb36-567"><a href="#cb36-567" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>**Performance Monitoring**:</span>
<span id="cb36-568"><a href="#cb36-568" aria-hidden="true" tabindex="-1"></a><span class="ss">    - </span>Having separate timestamps allows us to monitor the performance and efficiency of your image processing pipeline. We can measure the delay between image capture and result logging, which helps optimize the system for real-time processing needs.</span>
<span id="cb36-569"><a href="#cb36-569" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>**Troubleshooting and Audit**:</span>
<span id="cb36-570"><a href="#cb36-570" aria-hidden="true" tabindex="-1"></a><span class="ss">    - </span>Separate timestamps provide a better audit trail and troubleshooting data. If there are issues with the image processing or data recording, having distinct timestamps can help isolate whether delays or problems occurred during capture, processing, or logging.</span>
<span id="cb36-571"><a href="#cb36-571" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-572"><a href="#cb36-572" aria-hidden="true" tabindex="-1"></a><span class="fu">### Script For Reading the SQLite Database</span></span>
<span id="cb36-573"><a href="#cb36-573" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-574"><a href="#cb36-574" aria-hidden="true" tabindex="-1"></a>Here is an example of a code to retrieve the data from the database: </span>
<span id="cb36-575"><a href="#cb36-575" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-576"><a href="#cb36-576" aria-hidden="true" tabindex="-1"></a><span class="in">```python</span></span>
<span id="cb36-577"><a href="#cb36-577" aria-hidden="true" tabindex="-1"></a><span class="co">#!/usr/bin/env python3</span></span>
<span id="cb36-578"><a href="#cb36-578" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> sqlite3</span>
<span id="cb36-579"><a href="#cb36-579" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-580"><a href="#cb36-580" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> main():</span>
<span id="cb36-581"><a href="#cb36-581" aria-hidden="true" tabindex="-1"></a>    db_path <span class="op">=</span> <span class="st">'bee_count.db'</span></span>
<span id="cb36-582"><a href="#cb36-582" aria-hidden="true" tabindex="-1"></a>    conn <span class="op">=</span> sqlite3.<span class="ex">connect</span>(db_path)</span>
<span id="cb36-583"><a href="#cb36-583" aria-hidden="true" tabindex="-1"></a>    cursor <span class="op">=</span> conn.cursor()</span>
<span id="cb36-584"><a href="#cb36-584" aria-hidden="true" tabindex="-1"></a>    query <span class="op">=</span> <span class="st">"SELECT * FROM bee_counts"</span></span>
<span id="cb36-585"><a href="#cb36-585" aria-hidden="true" tabindex="-1"></a>    cursor.execute(query)</span>
<span id="cb36-586"><a href="#cb36-586" aria-hidden="true" tabindex="-1"></a>    data <span class="op">=</span> cursor.fetchall()</span>
<span id="cb36-587"><a href="#cb36-587" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> row <span class="kw">in</span> data:</span>
<span id="cb36-588"><a href="#cb36-588" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"Timestamp: </span><span class="sc">{</span>row[<span class="dv">0</span>]<span class="sc">}</span><span class="ss">, Number of bees: </span><span class="sc">{</span>row[<span class="dv">1</span>]<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb36-589"><a href="#cb36-589" aria-hidden="true" tabindex="-1"></a>    conn.close()</span>
<span id="cb36-590"><a href="#cb36-590" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-591"><a href="#cb36-591" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> <span class="va">__name__</span> <span class="op">==</span> <span class="st">"__main__"</span>:</span>
<span id="cb36-592"><a href="#cb36-592" aria-hidden="true" tabindex="-1"></a>    main()</span>
<span id="cb36-593"><a href="#cb36-593" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb36-594"><a href="#cb36-594" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-595"><a href="#cb36-595" aria-hidden="true" tabindex="-1"></a><span class="fu">### Adding Environment data</span></span>
<span id="cb36-596"><a href="#cb36-596" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-597"><a href="#cb36-597" aria-hidden="true" tabindex="-1"></a>Besides bee counting, environmental data, such as temperature and humidity, are essential for monitoring the bee-have health. Using a Rasp-Zero, it is straightforward to add a digital sensor such as the DHT-22 to get this data. </span>
<span id="cb36-598"><a href="#cb36-598" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-599"><a href="#cb36-599" aria-hidden="true" tabindex="-1"></a><span class="al">![](images/dat_env.jpg)</span></span>
<span id="cb36-600"><a href="#cb36-600" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-601"><a href="#cb36-601" aria-hidden="true" tabindex="-1"></a>Environmental data will be part of our final project. If you want to know more about connecting sensors to a Raspberry Pi and, even more, how to save the data to a local database and send it to the web, follow this tutorial: <span class="co">[</span><span class="ot">From Data to Graph: A Web Journey With Flask and SQLite</span><span class="co">](https://www.hackster.io/mjrobot/from-data-to-graph-a-web-journey-with-flask-and-sqlite-4dba35)</span>. </span>
<span id="cb36-602"><a href="#cb36-602" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-603"><a href="#cb36-603" aria-hidden="true" tabindex="-1"></a><span class="al">![](images/tutorial_flask.jpg)</span></span>
<span id="cb36-604"><a href="#cb36-604" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-605"><a href="#cb36-605" aria-hidden="true" tabindex="-1"></a><span class="fu">## Conclusion</span></span>
<span id="cb36-606"><a href="#cb36-606" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-607"><a href="#cb36-607" aria-hidden="true" tabindex="-1"></a>In this tutorial, we have thoroughly explored integrating the YOLOv8 model with a Raspberry Pi Zero 2W to address the practical and pressing task of counting (or better, "estimating") bees at a beehive entrance. Our project underscores the robust capability of embedding advanced machine learning technologies within compact edge computing devices, highlighting their potential impact on environmental monitoring and ecological studies.</span>
<span id="cb36-608"><a href="#cb36-608" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-609"><a href="#cb36-609" aria-hidden="true" tabindex="-1"></a>This tutorial provides a step-by-step guide to the practical deployment of the YOLOv8 model. We demonstrate a tangible example of a real-world application by optimizing it for edge computing in terms of efficiency and processing speed (using NCNN format). This not only serves as a functional solution but also as an instructional tool for similar projects.</span>
<span id="cb36-610"><a href="#cb36-610" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-611"><a href="#cb36-611" aria-hidden="true" tabindex="-1"></a>The technical insights and methodologies shared in this tutorial are the basis for the complete work to be developed at our university in the future. We envision further development, such as integrating additional environmental sensing capabilities and refining the model's accuracy and processing efficiency. Implementing alternative energy solutions like the proposed solar power setup will expand the project's sustainability and applicability in remote or underserved locations.</span>
<span id="cb36-612"><a href="#cb36-612" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-613"><a href="#cb36-613" aria-hidden="true" tabindex="-1"></a><span class="fu">## Resources</span></span>
<span id="cb36-614"><a href="#cb36-614" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-615"><a href="#cb36-615" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; The Dataset paper, Notebooks, and PDF version are in the </span><span class="co">[</span><span class="ot">Project repository</span><span class="co">](https://github.com/Mjrovai/Bee-Counting)</span><span class="at">.</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div></div></div></div></div>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">Edited by Prof.&nbsp;Marcelo Rovai (UNIFEI University)</div>   
    <div class="nav-footer-center">
      &nbsp;
    </div>
    <div class="nav-footer-right">This book was built with <a href="https://quarto.org/">Quarto</a>.</div>
  </div>
</footer>



</body></html>