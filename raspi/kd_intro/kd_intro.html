<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.3.450">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Edge AI Engineering - Knowledge Distillation in Practice</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<link href="../../raspi/llm/llm.html" rel="next">
<link href="../../raspi/rnn-verne/rnn-verne.html" rel="prev">
<link href="../../images/unifei-logo.png" rel="icon" type="image/png">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>


<link rel="stylesheet" href="../../styles.css">
</head>

<body class="nav-sidebar floating nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">Edge AI Engineering</span>
    </a>
  </div>
        <div class="quarto-navbar-tools ms-auto tools-wide">
    <a href="https://github.com/Mjrovai/EdgeML-with-Raspberry-Pi" rel="" title="Source Code" class="quarto-navigation-tool px-1" aria-label="Source Code"><i class="bi bi-github"></i></a>
    <a href="../../Edge-AI-Engineering.pdf" rel="" title="Download PDF" class="quarto-navigation-tool px-1" aria-label="Download PDF"><i class="bi bi-file-pdf"></i></a>
    <div class="dropdown">
      <a href="" title="Share" id="quarto-navigation-tool-dropdown-0" class="quarto-navigation-tool dropdown-toggle px-1" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Share"><i class="bi bi-share"></i></a>
      <ul class="dropdown-menu dropdown-menu-end" aria-labelledby="quarto-navigation-tool-dropdown-0">
          <li>
            <a class="dropdown-item quarto-navbar-tools ms-auto-item" href="https://twitter.com/intent/tweet?url=|url|">
              <i class="bi bi-bi-twitter pe-1"></i>
            Twitter
            </a>
          </li>
          <li>
            <a class="dropdown-item quarto-navbar-tools ms-auto-item" href="https://www.linkedin.com/sharing/share-offsite/?url=|url|">
              <i class="bi bi-bi-linkedin pe-1"></i>
            LinkedIn
            </a>
          </li>
          <li>
            <a class="dropdown-item quarto-navbar-tools ms-auto-item" href="https://www.facebook.com/sharer/sharer.php?u=|url|">
              <i class="bi bi-bi-facebook pe-1"></i>
            Facebook
            </a>
          </li>
      </ul>
    </div>
  <a href="" class="quarto-reader-toggle quarto-navigation-tool px-1" onclick="window.quartoToggleReader(); return false;" title="Toggle reader mode">
  <div class="quarto-reader-toggle-btn">
  <i class="bi"></i>
  </div>
</a>
</div>
          <div id="quarto-search" class="" title="Search"></div>
      </div> <!-- /container-fluid -->
    </nav>
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
      <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../../raspi/kd_intro/kd_intro.html">Knowledge Distillation in Practice</a></li></ol></nav>
      <a class="flex-grow-1" role="button" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
      </a>
      <button type="button" class="btn quarto-search-button" aria-label="" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal sidebar-navigation floating overflow-auto">
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Preface</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../Acknowledgements.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Acknowledgments</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../intro.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Introduction</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../about_book.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">About this Book</span></a>
  </div>
</li>
        <li class="px-0"><hr class="sidebar-divider hi "></li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../Classification_of_AI_Applications.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Classification of AI Applications</span></a>
  </div>
</li>
        <li class="px-0"><hr class="sidebar-divider hi "></li>
        <li class="sidebar-item sidebar-item-section">
      <span class="sidebar-item-text sidebar-link text-start">
 <span class="menu-text"><div class="part">Fixed Function AI (Reactive)</div></span></span>
  </li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../raspi/setup/setup.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Setup</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../raspi/image_classification/image_classification_fund.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Image Classification Fundamentals</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../raspi/image_classification/custom_image_classification_project.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Custom Image Classification Project</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../raspi/object_detection/object_detection_fundamentals.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Object Detection: Fundamentals</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../raspi/object_detection/custom_object_detection.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Custom Object Detection Project</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../raspi/object_detection/cv_yolo.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Computer Vision Applications with YOLO</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../raspi/counting_objects_yolo/counting_objects_yolo.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Counting objects with YOLO</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <span class="sidebar-item-text sidebar-link text-start">
 <span class="menu-text"><div class="part">Generative AI (Proactive)</div></span></span>
  </li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../raspi/rnn-verne/rnn-verne.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Text Generation with RNNs</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../raspi/kd_intro/kd_intro.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text">Knowledge Distillation in Practice</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../raspi/llm/llm.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Small Language Models (SLM)</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../raspi/vlm/vlm.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Vision-Language Models at the Edge</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../raspi/physical_comp/RPi_Physical_Computing.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Physical Computing with Raspberry Pi</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../raspi/iot/slm_iot.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Experimenting with SLMs for IoT Control</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../raspi/advancing_adgeai/adv_edgeai.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Advancing EdgeAI: Beyond Basic SLMs</span></a>
  </div>
</li>
        <li class="px-0"><hr class="sidebar-divider hi "></li>
        <li class="sidebar-item sidebar-item-section">
      <span class="sidebar-item-text sidebar-link text-start">
 <span class="menu-text"><div class="part">Weekly Labs</div></span></span>
  </li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../weekly_labs.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Edge AI Engineering - Weekly Labs</span></a>
  </div>
</li>
        <li class="px-0"><hr class="sidebar-divider hi "></li>
        <li class="sidebar-item sidebar-item-section">
      <span class="sidebar-item-text sidebar-link text-start">
 <span class="menu-text"><div class="part">References &amp; Author</div></span></span>
  </li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../references.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">References</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../about_the_author.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">About the author</span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#introduction" id="toc-introduction" class="nav-link active" data-scroll-target="#introduction">Introduction to Knowledge Distillation</a>
  <ul>
  <li><a href="#why-knowledge-distillation-matters" id="toc-why-knowledge-distillation-matters" class="nav-link" data-scroll-target="#why-knowledge-distillation-matters">Why Knowledge Distillation Matters</a></li>
  <li><a href="#the-teacher-student-paradigm" id="toc-the-teacher-student-paradigm" class="nav-link" data-scroll-target="#the-teacher-student-paradigm">The Teacher-Student Paradigm</a></li>
  </ul></li>
  <li><a href="#theoretical-foundations" id="toc-theoretical-foundations" class="nav-link" data-scroll-target="#theoretical-foundations">Theoretical Foundations</a>
  <ul>
  <li><a href="#soft-targets-vs.-hard-targets" id="toc-soft-targets-vs.-hard-targets" class="nav-link" data-scroll-target="#soft-targets-vs.-hard-targets">Soft Targets vs.&nbsp;Hard Targets</a></li>
  <li><a href="#the-temperature-parameter" id="toc-the-temperature-parameter" class="nav-link" data-scroll-target="#the-temperature-parameter">The Temperature Parameter</a></li>
  <li><a href="#distillation-loss-function" id="toc-distillation-loss-function" class="nav-link" data-scroll-target="#distillation-loss-function">Distillation Loss Function</a></li>
  </ul></li>
  <li><a href="#implementation-with-tensorflow-and-mnist" id="toc-implementation-with-tensorflow-and-mnist" class="nav-link" data-scroll-target="#implementation-with-tensorflow-and-mnist">Implementation with TensorFlow and MNIST</a>
  <ul>
  <li><a href="#dataset-overview" id="toc-dataset-overview" class="nav-link" data-scroll-target="#dataset-overview">Dataset Overview</a></li>
  <li><a href="#teacher-model-architecture" id="toc-teacher-model-architecture" class="nav-link" data-scroll-target="#teacher-model-architecture">Teacher Model Architecture</a></li>
  <li><a href="#student-model-architecture" id="toc-student-model-architecture" class="nav-link" data-scroll-target="#student-model-architecture">Student Model Architecture</a></li>
  <li><a href="#knowledge-distillation-implementation" id="toc-knowledge-distillation-implementation" class="nav-link" data-scroll-target="#knowledge-distillation-implementation">Knowledge Distillation Implementation</a></li>
  <li><a href="#training-process" id="toc-training-process" class="nav-link" data-scroll-target="#training-process">Training Process</a></li>
  <li><a href="#results-and-analysis" id="toc-results-and-analysis" class="nav-link" data-scroll-target="#results-and-analysis">Results and Analysis</a></li>
  <li><a href="#visualizing-distillation-benefits" id="toc-visualizing-distillation-benefits" class="nav-link" data-scroll-target="#visualizing-distillation-benefits">Visualizing Distillation Benefits</a></li>
  </ul></li>
  <li><a href="#advanced-techniques" id="toc-advanced-techniques" class="nav-link" data-scroll-target="#advanced-techniques">Advanced Techniques</a>
  <ul>
  <li><a href="#feature-based-distillation" id="toc-feature-based-distillation" class="nav-link" data-scroll-target="#feature-based-distillation">Feature-Based Distillation</a></li>
  <li><a href="#attention-based-distillation" id="toc-attention-based-distillation" class="nav-link" data-scroll-target="#attention-based-distillation">Attention-Based Distillation</a></li>
  <li><a href="#practical-tips-for-advanced-distillation" id="toc-practical-tips-for-advanced-distillation" class="nav-link" data-scroll-target="#practical-tips-for-advanced-distillation">Practical Tips for Advanced Distillation</a></li>
  </ul></li>
  <li><a href="#scaling-to-llms" id="toc-scaling-to-llms" class="nav-link" data-scroll-target="#scaling-to-llms">Scaling to Large Language Models</a>
  <ul>
  <li><a href="#challenges-in-llm-distillation" id="toc-challenges-in-llm-distillation" class="nav-link" data-scroll-target="#challenges-in-llm-distillation">Challenges in LLM Distillation</a></li>
  <li><a href="#llm-distillation-techniques" id="toc-llm-distillation-techniques" class="nav-link" data-scroll-target="#llm-distillation-techniques">LLM Distillation Techniques</a>
  <ul class="collapse">
  <li><a href="#sequence-level-distillation" id="toc-sequence-level-distillation" class="nav-link" data-scroll-target="#sequence-level-distillation">1. Sequence-Level Distillation</a></li>
  <li><a href="#recent-llm-distillation-examples" id="toc-recent-llm-distillation-examples" class="nav-link" data-scroll-target="#recent-llm-distillation-examples">2. Recent LLM Distillation Examples</a></li>
  <li><a href="#distilling-reasoning-abilities" id="toc-distilling-reasoning-abilities" class="nav-link" data-scroll-target="#distilling-reasoning-abilities">3. Distilling Reasoning Abilities</a></li>
  </ul></li>
  <li><a href="#from-mnist-to-real-world-llms-llama-3.2-case-study" id="toc-from-mnist-to-real-world-llms-llama-3.2-case-study" class="nav-link" data-scroll-target="#from-mnist-to-real-world-llms-llama-3.2-case-study">From MNIST to Real-World LLMs: Llama 3.2 Case Study</a>
  <ul class="collapse">
  <li><a href="#key-points" id="toc-key-points" class="nav-link" data-scroll-target="#key-points">Key Points</a></li>
  </ul></li>
  <li><a href="#applications-in-modern-llm-development" id="toc-applications-in-modern-llm-development" class="nav-link" data-scroll-target="#applications-in-modern-llm-development">Applications in Modern LLM Development</a>
  <ul class="collapse">
  <li><a href="#from-chatgpt-to-mobile-assistants" id="toc-from-chatgpt-to-mobile-assistants" class="nav-link" data-scroll-target="#from-chatgpt-to-mobile-assistants">1. From ChatGPT to Mobile Assistants</a></li>
  <li><a href="#domain-specific-distillation" id="toc-domain-specific-distillation" class="nav-link" data-scroll-target="#domain-specific-distillation">2. Domain-Specific Distillation</a></li>
  <li><a href="#from-research-to-production" id="toc-from-research-to-production" class="nav-link" data-scroll-target="#from-research-to-production">3. From Research to Production</a></li>
  </ul></li>
  </ul></li>
  <li><a href="#best-practices" id="toc-best-practices" class="nav-link" data-scroll-target="#best-practices">Practical Guidelines and Best Practices</a>
  <ul>
  <li><a href="#choosing-the-right-architecture" id="toc-choosing-the-right-architecture" class="nav-link" data-scroll-target="#choosing-the-right-architecture">Choosing the Right Architecture</a></li>
  <li><a href="#hyperparameter-selection" id="toc-hyperparameter-selection" class="nav-link" data-scroll-target="#hyperparameter-selection">Hyperparameter Selection</a></li>
  <li><a href="#common-pitfalls-and-solutions" id="toc-common-pitfalls-and-solutions" class="nav-link" data-scroll-target="#common-pitfalls-and-solutions">Common Pitfalls and Solutions</a></li>
  <li><a href="#performance-evaluation" id="toc-performance-evaluation" class="nav-link" data-scroll-target="#performance-evaluation">Performance Evaluation</a></li>
  </ul></li>
  <li><a href="#conclusion" id="toc-conclusion" class="nav-link" data-scroll-target="#conclusion">Conclusion</a>
  <ul>
  <li><a href="#key-takeaways" id="toc-key-takeaways" class="nav-link" data-scroll-target="#key-takeaways">Key Takeaways</a></li>
  <li><a href="#from-mnist-to-llms" id="toc-from-mnist-to-llms" class="nav-link" data-scroll-target="#from-mnist-to-llms">From MNIST to LLMs</a></li>
  <li><a href="#final-thoughts" id="toc-final-thoughts" class="nav-link" data-scroll-target="#final-thoughts">Final Thoughts</a></li>
  </ul></li>
  <li><a href="#resources" id="toc-resources" class="nav-link" data-scroll-target="#resources">Resources</a></li>
  </ul>
<div class="toc-actions"><div><i class="bi bi-github"></i></div><div class="action-links"><p><a href="https://github.com/Mjrovai/EdgeML-with-Raspberry-Pi/edit/main/raspi/kd_intro/kd_intro.qmd" class="toc-action">Edit this page</a></p><p><a href="https://github.com/Mjrovai/EdgeML-with-Raspberry-Pi/issues/new" class="toc-action">Report an issue</a></p><p><a href="https://github.com/Mjrovai/EdgeML-with-Raspberry-Pi/blob/main/raspi/kd_intro/kd_intro.qmd" class="toc-action">View source</a></p></div></div></nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Knowledge Distillation in Practice</h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  

</header>

<p><strong>From MNIST to LLMs</strong></p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="./images/kd-ini.png" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">Image created by DALLE-3</figcaption>
</figure>
</div>
<section id="introduction" class="level2">
<h2 class="anchored" data-anchor-id="introduction">Introduction to Knowledge Distillation</h2>
<p>Knowledge distillation is a powerful technique in machine learning that enables the transfer of knowledge from a large, complex model (the “teacher”) to a smaller, more efficient model (the “student”). This process allows us to create compact models that maintain much of the performance of their larger counterparts while being significantly faster and requiring fewer computational resources.</p>
<section id="why-knowledge-distillation-matters" class="level3">
<h3 class="anchored" data-anchor-id="why-knowledge-distillation-matters">Why Knowledge Distillation Matters</h3>
<p>In today’s AI landscape, models are becoming increasingly large and complex. While these models achieve remarkable performance, they often require substantial computational resources, making deployment challenging in resource-constrained environments such as mobile devices, edge computing systems, or real-time applications. Knowledge distillation addresses this challenge by:</p>
<ol type="1">
<li><strong>Reducing Model Size</strong>: Creating smaller models with fewer parameters</li>
<li><strong>Improving Inference Speed</strong>: Enabling faster predictions in production</li>
<li><strong>Lowering Computational Costs</strong>: Reducing memory and processing requirements</li>
<li><strong>Maintaining Performance</strong>: Preserving much of the original model’s accuracy</li>
<li><strong>Enabling Edge Deployment</strong>: Making AI accessible on resource-limited devices</li>
</ol>
</section>
<section id="the-teacher-student-paradigm" class="level3">
<h3 class="anchored" data-anchor-id="the-teacher-student-paradigm">The Teacher-Student Paradigm</h3>
<p>The core concept of knowledge distillation revolves around the teacher-student relationship:</p>
<ul>
<li><strong>Teacher Model</strong>: A large, well-trained model with high capacity and performance</li>
<li><strong>Student Model</strong>: A smaller, more efficient model trained to mimic the teacher’s behavior</li>
<li><strong>Knowledge Transfer</strong>: The process of transferring the teacher’s “dark knowledge” to the student</li>
</ul>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="./images/knowledge_distilation.jpg" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">Figure from “Knowledge Distillation: A Survey, Jianping Gou Baosheng Yu Stephen J. Maybank Dacheng Tao, 2021</figcaption>
</figure>
</div>
<blockquote class="blockquote">
<p>See paper: “<a href="https://arxiv.org/pdf/2006.05525">Knowledge Distillation: A Survey, Jianping Gou, Baosheng Yu, Stephen J. Maybank, Dacheng Tao, 2021</a></p>
</blockquote>
</section>
</section>
<section id="theoretical-foundations" class="level2">
<h2 class="anchored" data-anchor-id="theoretical-foundations">Theoretical Foundations</h2>
<section id="soft-targets-vs.-hard-targets" class="level3">
<h3 class="anchored" data-anchor-id="soft-targets-vs.-hard-targets">Soft Targets vs.&nbsp;Hard Targets</h3>
<p>Traditional supervised learning uses “hard targets” - one-hot encoded labels that provide limited information. For example, in MNIST digit classification, the label for the digit “5” would be represented as <code>[0, 0, 0, 0, 0, 1, 0, 0, 0, 0]</code>.</p>
<p>Knowledge distillation leverages “soft targets” - the probability distributions produced by the teacher model. These soft targets contain richer information about the relationships between classes. For instance, the teacher might output <code>[0.01, 0.02, 0.01, 0.05, 0.1, 0.78, 0.02, 0.01, 0.0, 0.0]</code> for a “5”, indicating that it’s most confident about “5” but also considers “4” somewhat similar.</p>
</section>
<section id="the-temperature-parameter" class="level3">
<h3 class="anchored" data-anchor-id="the-temperature-parameter">The Temperature Parameter</h3>
<p>The temperature parameter (τ) is crucial in knowledge distillation. It controls the “softness” of the probability distribution by modifying the softmax function:</p>
<pre class="math"><code>P(class_i) = exp(z_i / τ) / Σ_j exp(z_j / τ)</code></pre>
<p>Where:</p>
<ul>
<li>z_i are the logits (pre-softmax outputs)</li>
<li>τ is the temperature parameter</li>
</ul>
<p><strong>Effects of Temperature:</strong></p>
<ul>
<li>τ = 1: Standard softmax (normal sharpness)</li>
<li>τ &gt; 1: Softer distribution (more uniform, reveals class relationships)</li>
<li>τ &lt; 1: Sharper distribution (more peaked, less informative)</li>
</ul>
<blockquote class="blockquote">
<p>In our implementation, we use a temperature of 5.0, which creates softer distributions that effectively transfer the teacher’s “dark knowledge” to the student.</p>
</blockquote>
</section>
<section id="distillation-loss-function" class="level3">
<h3 class="anchored" data-anchor-id="distillation-loss-function">Distillation Loss Function</h3>
<p>The distillation loss combines two components:</p>
<ol type="1">
<li><strong>Distillation Loss</strong> (L_KD): Measures how well the student mimics the teacher’s soft targets</li>
<li><strong>Student Loss</strong> (L_CE): Traditional cross-entropy loss with hard targets</li>
</ol>
<pre class="math"><code>L_total = α * L_CE + (1 - α) * L_KD</code></pre>
<p>Where α is a weighting parameter that balances the two objectives, in our implementation, we use α = 0.3, giving more weight to the soft targets from the teacher.</p>
<p>The distillation loss is typically computed using the KL divergence:</p>
<pre><code>L_KD = τ² * KL_divergence(Teacher_soft_targets, Student_soft_targets)</code></pre>
<p>The τ² factor compensates for the gradient scaling effect of temperature. This is crucial for stable training.</p>
</section>
</section>
<section id="implementation-with-tensorflow-and-mnist" class="level2">
<h2 class="anchored" data-anchor-id="implementation-with-tensorflow-and-mnist">Implementation with TensorFlow and MNIST</h2>
<section id="dataset-overview" class="level3">
<h3 class="anchored" data-anchor-id="dataset-overview">Dataset Overview</h3>
<p>MNIST is an ideal dataset for learning knowledge distillation:</p>
<ul>
<li><strong>60,000 training images</strong> of handwritten digits (0-9)</li>
<li><strong>10,000 test images</strong></li>
<li><strong>28x28 grayscale images</strong></li>
<li><strong>10 classes</strong> (digits 0-9)</li>
<li><strong>Well-established baseline performances</strong></li>
</ul>
</section>
<section id="teacher-model-architecture" class="level3">
<h3 class="anchored" data-anchor-id="teacher-model-architecture">Teacher Model Architecture</h3>
<p>Our teacher model has substantial capacity with multiple convolutional layers, batch normalization, and dropout for regularization:</p>
<div class="sourceCode" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> build_teacher_model():</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>    model <span class="op">=</span> models.Sequential([</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a>        layers.Conv2D(<span class="dv">64</span>, <span class="dv">3</span>, activation<span class="op">=</span><span class="st">'relu'</span>, padding<span class="op">=</span><span class="st">'same'</span>, input_shape<span class="op">=</span>(<span class="dv">28</span>, <span class="dv">28</span>, <span class="dv">1</span>)),</span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a>        layers.BatchNormalization(),</span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a>        layers.Conv2D(<span class="dv">64</span>, <span class="dv">3</span>, activation<span class="op">=</span><span class="st">'relu'</span>, padding<span class="op">=</span><span class="st">'same'</span>),</span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a>        layers.MaxPooling2D(<span class="dv">2</span>, <span class="dv">2</span>),</span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a>        layers.BatchNormalization(),</span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a>        layers.Conv2D(<span class="dv">128</span>, <span class="dv">3</span>, activation<span class="op">=</span><span class="st">'relu'</span>, padding<span class="op">=</span><span class="st">'same'</span>),</span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a>        layers.BatchNormalization(),</span>
<span id="cb4-11"><a href="#cb4-11" aria-hidden="true" tabindex="-1"></a>        layers.Conv2D(<span class="dv">128</span>, <span class="dv">3</span>, activation<span class="op">=</span><span class="st">'relu'</span>, padding<span class="op">=</span><span class="st">'same'</span>),</span>
<span id="cb4-12"><a href="#cb4-12" aria-hidden="true" tabindex="-1"></a>        layers.MaxPooling2D(<span class="dv">2</span>, <span class="dv">2</span>),</span>
<span id="cb4-13"><a href="#cb4-13" aria-hidden="true" tabindex="-1"></a>        layers.BatchNormalization(),</span>
<span id="cb4-14"><a href="#cb4-14" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb4-15"><a href="#cb4-15" aria-hidden="true" tabindex="-1"></a>        layers.Conv2D(<span class="dv">256</span>, <span class="dv">3</span>, activation<span class="op">=</span><span class="st">'relu'</span>, padding<span class="op">=</span><span class="st">'same'</span>),</span>
<span id="cb4-16"><a href="#cb4-16" aria-hidden="true" tabindex="-1"></a>        layers.BatchNormalization(),</span>
<span id="cb4-17"><a href="#cb4-17" aria-hidden="true" tabindex="-1"></a>        layers.GlobalAveragePooling2D(),</span>
<span id="cb4-18"><a href="#cb4-18" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb4-19"><a href="#cb4-19" aria-hidden="true" tabindex="-1"></a>        layers.Dense(<span class="dv">256</span>, activation<span class="op">=</span><span class="st">'relu'</span>),</span>
<span id="cb4-20"><a href="#cb4-20" aria-hidden="true" tabindex="-1"></a>        layers.BatchNormalization(),</span>
<span id="cb4-21"><a href="#cb4-21" aria-hidden="true" tabindex="-1"></a>        layers.Dropout(<span class="fl">0.5</span>),</span>
<span id="cb4-22"><a href="#cb4-22" aria-hidden="true" tabindex="-1"></a>        layers.Dense(<span class="dv">128</span>, activation<span class="op">=</span><span class="st">'relu'</span>),</span>
<span id="cb4-23"><a href="#cb4-23" aria-hidden="true" tabindex="-1"></a>        layers.BatchNormalization(),</span>
<span id="cb4-24"><a href="#cb4-24" aria-hidden="true" tabindex="-1"></a>        layers.Dropout(<span class="fl">0.5</span>),</span>
<span id="cb4-25"><a href="#cb4-25" aria-hidden="true" tabindex="-1"></a>        layers.Dense(<span class="dv">10</span>, activation<span class="op">=</span><span class="st">'softmax'</span>)</span>
<span id="cb4-26"><a href="#cb4-26" aria-hidden="true" tabindex="-1"></a>    ])</span>
<span id="cb4-27"><a href="#cb4-27" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> model</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p><img src="./images/teacher_summary.png" class="img-fluid"></p>
<blockquote class="blockquote">
<p>This architecture achieved <strong>99.44 % accuracy</strong> on MNIST.</p>
</blockquote>
</section>
<section id="student-model-architecture" class="level3">
<h3 class="anchored" data-anchor-id="student-model-architecture">Student Model Architecture</h3>
<p>Our student model is intentionally much simpler, with fewer layers and significantly fewer parameters:</p>
<div class="sourceCode" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> build_student_model():</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>    model <span class="op">=</span> models.Sequential([</span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a>        layers.Conv2D(<span class="dv">16</span>, <span class="dv">3</span>, activation<span class="op">=</span><span class="st">'relu'</span>, padding<span class="op">=</span><span class="st">'same'</span>, input_shape<span class="op">=</span>(<span class="dv">28</span>, <span class="dv">28</span>, <span class="dv">1</span>)),</span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a>        layers.MaxPooling2D(<span class="dv">2</span>, <span class="dv">2</span>),</span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a>        layers.Conv2D(<span class="dv">32</span>, <span class="dv">3</span>, activation<span class="op">=</span><span class="st">'relu'</span>, padding<span class="op">=</span><span class="st">'same'</span>),</span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a>        layers.MaxPooling2D(<span class="dv">2</span>, <span class="dv">2</span>),</span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a>        layers.Flatten(),</span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a>        layers.Dense(<span class="dv">64</span>, activation<span class="op">=</span><span class="st">'relu'</span>),</span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a>        layers.Dense(<span class="dv">10</span>, activation<span class="op">=</span><span class="st">'softmax'</span>)</span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a>    ])</span>
<span id="cb5-11"><a href="#cb5-11" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> model</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p><img src="./images/student_summary.png" class="img-fluid"></p>
<blockquote class="blockquote">
<p>The student model has fewer parameters than the teacher (105K versus 658K), or 6.2 times smaller.</p>
</blockquote>
</section>
<section id="knowledge-distillation-implementation" class="level3">
<h3 class="anchored" data-anchor-id="knowledge-distillation-implementation">Knowledge Distillation Implementation</h3>
<p>In our implementation, we use a custom training loop that explicitly calculates both hard and soft losses:</p>
<div class="sourceCode" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Forward pass</span></span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> tf.GradientTape() <span class="im">as</span> tape:</span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a>    predictions <span class="op">=</span> kd_student(x_batch, training<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Hard target loss (cross-entropy with true labels)</span></span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a>    hard_loss <span class="op">=</span> tf.keras.losses.categorical_crossentropy(y_hard_batch, predictions)</span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Soft target loss (KL divergence with teacher predictions)</span></span>
<span id="cb6-9"><a href="#cb6-9" aria-hidden="true" tabindex="-1"></a>    soft_loss <span class="op">=</span> tf.keras.losses.kullback_leibler_divergence(y_soft_batch, predictions)</span>
<span id="cb6-10"><a href="#cb6-10" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb6-11"><a href="#cb6-11" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Combined loss with temperature scaling factor for soft loss</span></span>
<span id="cb6-12"><a href="#cb6-12" aria-hidden="true" tabindex="-1"></a>    loss <span class="op">=</span> alpha <span class="op">*</span> hard_loss <span class="op">+</span> (<span class="dv">1</span> <span class="op">-</span> alpha) <span class="op">*</span> soft_loss <span class="op">*</span> (temperature <span class="op">**</span> <span class="dv">2</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Key components of our implementation:</p>
<ol type="1">
<li><strong>Temperature Scaling</strong>: We apply a temperature of 5.0 to soften the teacher’s outputs</li>
<li><strong>Alpha Weighting</strong>: We use α = 0.3 to prioritize learning from the teacher’s soft targets</li>
<li><strong>KL Divergence</strong>: This loss function helps the student match the teacher’s probability distributions</li>
<li><strong>Temperature Correction</strong>: We multiply the soft loss by τ² to correct for gradient scaling</li>
</ol>
</section>
<section id="training-process" class="level3">
<h3 class="anchored" data-anchor-id="training-process">Training Process</h3>
<p>Our implementation follows these steps:</p>
<ol type="1">
<li><strong>Train the Teacher</strong>: First, we train the complex teacher model using early stopping and a reduced learning rate.</li>
<li><strong>Train a Vanilla Student</strong>: We train a student model normally on the hard labels for comparison.</li>
<li><strong>Generate Soft Targets</strong>: We use the teacher to create softened probability distributions.</li>
<li><strong>Train the Distilled Student</strong>: We train another student using our custom distillation training loop.</li>
<li><strong>Evaluate and Compare</strong>: We compare the performance, size, and speed of all three models.</li>
</ol>
</section>
<section id="results-and-analysis" class="level3">
<h3 class="anchored" data-anchor-id="results-and-analysis">Results and Analysis</h3>
<p>Our implementation typically shows these results:</p>
<ul>
<li><strong>Teacher Model</strong>: 99.44% accuracy, largest size, slowest inference (0.8632 seconds)</li>
<li><strong>Vanilla Student</strong>: 98.77% accuracy, smaller size, faster inference (0.3579 seconds)</li>
<li><strong>Distilled Student</strong>: 99.32% accuracy, same size as vanilla student, but better performance (+0.55%) and faster inference than the Teacher (0.5467 seconds)</li>
</ul>
<p><img src="./images/kd_comparation.png" class="img-fluid"></p>
<p>These results demonstrate the key benefit of knowledge distillation: the distilled student achieves performance closer to the teacher while maintaining the efficiency benefits of the smaller architecture.</p>
</section>
<section id="visualizing-distillation-benefits" class="level3">
<h3 class="anchored" data-anchor-id="visualizing-distillation-benefits">Visualizing Distillation Benefits</h3>
<section id="we-analyze-the-models-on-challenging-examples-where-the-teacher-succeeds-but-the-vanilla-student-fails.-this-reveals-how-knowledge-distillation-enables-students-to-handle-complex-cases-by-learning-the-teachers-dark-knowledge." class="level6">
<h6 class="anchored" data-anchor-id="we-analyze-the-models-on-challenging-examples-where-the-teacher-succeeds-but-the-vanilla-student-fails.-this-reveals-how-knowledge-distillation-enables-students-to-handle-complex-cases-by-learning-the-teachers-dark-knowledge.">We analyze the models on challenging examples where the teacher succeeds but the vanilla student fails. This reveals how knowledge distillation enables students to handle complex cases by learning the teacher’s “dark knowledge.”</h6>
<p><img src="./images/dif_cases.png" class="img-fluid"></p>
</section>
</section>
</section>
<section id="advanced-techniques" class="level2">
<h2 class="anchored" data-anchor-id="advanced-techniques">Advanced Techniques</h2>
<section id="feature-based-distillation" class="level3">
<h3 class="anchored" data-anchor-id="feature-based-distillation">Feature-Based Distillation</h3>
<p>Beyond output-level distillation, we can transfer knowledge from intermediate layers:</p>
<div class="sourceCode" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> feature_distillation_loss(teacher_features, student_features):</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Distill knowledge from intermediate feature maps"""</span></span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a>    loss <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> t_feat, s_feat <span class="kw">in</span> <span class="bu">zip</span>(teacher_features, student_features):</span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Align dimensions if necessary</span></span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a>        s_feat_aligned <span class="op">=</span> align_feature_dimensions(s_feat, t_feat.shape)</span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a>        loss <span class="op">+=</span> tf.keras.losses.MSE(t_feat, s_feat_aligned)</span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> loss</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="attention-based-distillation" class="level3">
<h3 class="anchored" data-anchor-id="attention-based-distillation">Attention-Based Distillation</h3>
<p>Transfer attention patterns from teacher to student:</p>
<div class="sourceCode" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> attention_distillation_loss(teacher_attention, student_attention):</span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Distill attention mechanisms"""</span></span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> tf.keras.losses.MSE(teacher_attention, student_attention)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="practical-tips-for-advanced-distillation" class="level3">
<h3 class="anchored" data-anchor-id="practical-tips-for-advanced-distillation">Practical Tips for Advanced Distillation</h3>
<ol type="1">
<li><strong>Layer Alignment</strong>: When using feature distillation, carefully align the feature dimensions</li>
<li><strong>Feature Selection</strong>: Not all features are equally important; focus on the most informative ones</li>
<li><strong>Multi-Teacher Distillation</strong>: Combine knowledge from multiple teachers for better results</li>
<li><strong>Online Distillation</strong>: Train teacher and student simultaneously for mutual improvement</li>
</ol>
</section>
</section>
<section id="scaling-to-llms" class="level2">
<h2 class="anchored" data-anchor-id="scaling-to-llms">Scaling to Large Language Models</h2>
<section id="challenges-in-llm-distillation" class="level3">
<h3 class="anchored" data-anchor-id="challenges-in-llm-distillation">Challenges in LLM Distillation</h3>
<p>Scaling knowledge distillation to Large Language Models presents unique challenges:</p>
<ol type="1">
<li><strong>Model Size</strong>: LLMs have billions of parameters, making distillation computationally expensive</li>
<li><strong>Sequence Generation</strong>: Unlike classification, LLMs generate sequences, requiring sequence-level distillation</li>
<li><strong>Vocabulary Differences</strong>: Teacher and student may have different vocabularies</li>
<li><strong>Context Length</strong>: Handling variable-length sequences and attention patterns</li>
<li><strong>Multi-task Learning</strong>: LLMs perform multiple tasks simultaneously</li>
</ol>
</section>
<section id="llm-distillation-techniques" class="level3">
<h3 class="anchored" data-anchor-id="llm-distillation-techniques">LLM Distillation Techniques</h3>
<section id="sequence-level-distillation" class="level4">
<h4 class="anchored" data-anchor-id="sequence-level-distillation">1. Sequence-Level Distillation</h4>
<p>Instead of token-level predictions, match entire sequence probabilities:</p>
<div class="sourceCode" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> sequence_level_distillation(teacher_logits, student_logits, sequence_lengths):</span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Distill at the sequence level for better coherence"""</span></span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a>    teacher_probs <span class="op">=</span> tf.nn.softmax(teacher_logits <span class="op">/</span> temperature)</span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a>    student_probs <span class="op">=</span> tf.nn.softmax(student_logits <span class="op">/</span> temperature)</span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Mask out padding tokens</span></span>
<span id="cb9-7"><a href="#cb9-7" aria-hidden="true" tabindex="-1"></a>    mask <span class="op">=</span> create_sequence_mask(sequence_lengths)</span>
<span id="cb9-8"><a href="#cb9-8" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb9-9"><a href="#cb9-9" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> masked_kl_divergence(teacher_probs, student_probs, mask)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="recent-llm-distillation-examples" class="level4">
<h4 class="anchored" data-anchor-id="recent-llm-distillation-examples">2. Recent LLM Distillation Examples</h4>
<ul>
<li><strong>DistilBERT</strong>: 40% smaller than BERT, retains 97% of performance.</li>
<li><strong>TinyBERT</strong>: 7.5x smaller and 9.4x faster than BERT-base</li>
<li><strong>MiniLM</strong>: Uses deep self-attention distillation for efficient transfer.</li>
<li><strong>Distil-GPT2</strong>: Compressed version of GPT-2 with minimal performance loss</li>
<li><strong>Llama 3.2 3B and 1B</strong>: Distilled from Llama 3.2 models (70B/8B parameters)</li>
</ul>
</section>
<section id="distilling-reasoning-abilities" class="level4">
<h4 class="anchored" data-anchor-id="distilling-reasoning-abilities">3. Distilling Reasoning Abilities</h4>
<p>Modern approaches focus on transferring reasoning abilities, not just predictions:</p>
<ul>
<li><strong>Chain-of-Thought Distillation</strong>: Transfer step-by-step reasoning process</li>
<li><strong>Explanation-Based Distillation</strong>: Use teacher’s explanations to guide student learning</li>
<li><strong>Rationale Extraction</strong>: Identify key reasoning patterns for targeted transfer</li>
</ul>
</section>
</section>
<section id="from-mnist-to-real-world-llms-llama-3.2-case-study" class="level3">
<h3 class="anchored" data-anchor-id="from-mnist-to-real-world-llms-llama-3.2-case-study">From MNIST to Real-World LLMs: Llama 3.2 Case Study</h3>
<ol type="1">
<li><strong>Teacher Model</strong>: The larger Llama 3.2 models (70B/8B parameters) serve as the teachers</li>
<li><strong>Student Model</strong>: Llama 3.2 3B and 1B are the distilled student models. They were created using pruning techniques, which systematically remove less meaningful connections (weights) in the neural network.</li>
</ol>
<p><img src="./images/llama-kd.png" class="img-fluid"></p>
<section id="key-points" class="level4">
<h4 class="anchored" data-anchor-id="key-points">Key Points</h4>
<ol type="1">
<li><strong>Scale Difference</strong>: The parameter reduction from 70B to 3B (~23x) or 1B (~70x) demonstrates industrial-scale distillation</li>
<li><strong>Performance Preservation</strong>: Despite massive size reduction, the smaller models maintain impressive capabilities:
<ul>
<li>The 3B model preserves most of the reasoning abilities of larger models.</li>
<li>The 1B model remains highly functional for many tasks.</li>
</ul></li>
<li><strong>Practical Benefits</strong>:
<ul>
<li>The 1B model can run on consumer laptops and even some mobile devices.</li>
<li>The 3B model offers a balance of performance and accessibility.</li>
</ul></li>
<li><strong>Distillation Techniques Used</strong>:
<ul>
<li>Meta likely used a combination of response-based and feature-based distillation.</li>
<li>They may have employed temperature scaling and specialized loss functions similar to what we demonstrated in our MNIST example.</li>
<li>The principles we covered (soft targets, temperature, loss weighting) all apply at this larger scale.</li>
</ul></li>
</ol>
<p>While our MNIST example demonstrates the application of knowledge distillation principles using a simple dataset, these same principles can be applied directly to state-of-the-art language models.</p>
<p>Meta’s Llama 3.2 family provides a perfect real-world example:</p>
<table class="table">
<colgroup>
<col style="width: 14%">
<col style="width: 11%">
<col style="width: 15%">
<col style="width: 57%">
</colgroup>
<thead>
<tr class="header">
<th>Model</th>
<th>Parameters</th>
<th>Size Reduction</th>
<th>Use Case</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Llama 3.2 70B</td>
<td>70 billion</td>
<td>(Teacher)</td>
<td>Data centers, high-performance applications</td>
</tr>
<tr class="even">
<td>Llama 3.2 8B</td>
<td>8 billion</td>
<td>~9x</td>
<td>Server deployment, high-end workstations</td>
</tr>
<tr class="odd">
<td>Llama 3.2 3B</td>
<td>3 billion</td>
<td>~23x</td>
<td>Consumer laptops, desktop applications</td>
</tr>
<tr class="even">
<td>Llama 3.2 1B</td>
<td>1 billion</td>
<td>~70x</td>
<td>Edge devices, mobile applications, embedded systems</td>
</tr>
</tbody>
</table>
<p>The 3B and 1B models represent successful distillations of the larger models, preserving core capabilities while dramatically reducing computational requirements. This demonstrates the industrial importance of knowledge distillation techniques we’ve explored.</p>
<p>It is possible to note that while the underlying principles remain the same as our MNIST example, industrial LLM distillation includes additional techniques:</p>
<ol type="1">
<li><strong>Distillation-Specific Data</strong>: Carefully curated datasets designed to transfer specific capabilities</li>
<li><strong>Multi-Stage Distillation</strong>: Gradual compression through intermediate models (70B → 8B → 3B → 1B)</li>
<li><strong>Task-Specific Fine-Tuning</strong>: Optimizing smaller models for specific use cases after distillation</li>
<li><strong>Custom Loss Functions</strong>: Specialized loss terms to preserve reasoning patterns and generation quality</li>
</ol>
<p>Despite these additional complexities, the core concept remains the same: using a larger, more capable model to guide the training of a smaller, more efficient one.</p>
</section>
</section>
<section id="applications-in-modern-llm-development" class="level3">
<h3 class="anchored" data-anchor-id="applications-in-modern-llm-development">Applications in Modern LLM Development</h3>
<section id="from-chatgpt-to-mobile-assistants" class="level4">
<h4 class="anchored" data-anchor-id="from-chatgpt-to-mobile-assistants">1. From ChatGPT to Mobile Assistants</h4>
<p>Large models like ChatGPT (175B+ parameters) can be distilled to create mobile-friendly assistants (1-2B parameters) that maintain core capabilities while running locally on smartphones.</p>
</section>
<section id="domain-specific-distillation" class="level4">
<h4 class="anchored" data-anchor-id="domain-specific-distillation">2. Domain-Specific Distillation</h4>
<p>Instead of distilling general capabilities, focus on specific domains:</p>
<ul>
<li><strong>Medical LLMs</strong>: Distill medical knowledge from large models to smaller, specialized ones</li>
<li><strong>Legal Assistants</strong>: Create compact models focused on legal reasoning and terminology</li>
<li><strong>Educational Tools</strong>: Develop small models optimized for teaching specific subjects</li>
</ul>
</section>
<section id="from-research-to-production" class="level4">
<h4 class="anchored" data-anchor-id="from-research-to-production">3. From Research to Production</h4>
<p>The journey from research models to production deployment often involves distillation:</p>
<ol type="1">
<li><strong>Research Phase</strong>: Develop large, state-of-the-art models (100B+ parameters)</li>
<li><strong>Distillation Phase</strong>: Compress knowledge into deployment-ready models (1-10B parameters)</li>
<li><strong>Deployment Phase</strong>: Further optimize for specific hardware and latency requirements</li>
</ol>
</section>
</section>
</section>
<section id="best-practices" class="level2">
<h2 class="anchored" data-anchor-id="best-practices">Practical Guidelines and Best Practices</h2>
<section id="choosing-the-right-architecture" class="level3">
<h3 class="anchored" data-anchor-id="choosing-the-right-architecture">Choosing the Right Architecture</h3>
<p>Based on our experiments, we recommend:</p>
<ol type="1">
<li><strong>Teacher-Student Size Ratio</strong>: Aim for a 10-20x parameter reduction for significant efficiency gains</li>
<li><strong>Architectural Similarity</strong>: Maintain similar architectural patterns between teacher and student</li>
<li><strong>Bottleneck Identification</strong>: Ensure the student has adequate capacity at critical layers</li>
</ol>
</section>
<section id="hyperparameter-selection" class="level3">
<h3 class="anchored" data-anchor-id="hyperparameter-selection">Hyperparameter Selection</h3>
<p>Our experiments suggest these optimal settings:</p>
<ol type="1">
<li><strong>Temperature (τ)</strong>:
<ul>
<li>For MNIST: 3-5 works well</li>
<li>For complex tasks: 5-10 may be better</li>
<li>If outputs are already soft: Lower temperatures (2-3) may suffice</li>
</ul></li>
<li><strong>Alpha Weighting (α)</strong>:
<ul>
<li>For simpler tasks: 0.3-0.5 (balanced approach)</li>
<li>For complex reasoning: 0.1-0.3 (more emphasis on teacher’s knowledge)</li>
<li>When teacher is extremely accurate: Lower α values work better</li>
</ul></li>
<li><strong>Training Duration</strong>:
<ul>
<li>Distilled students often benefit from longer training (1.5-2x the epochs)</li>
<li>Use early stopping with patience to avoid overfitting</li>
</ul></li>
</ol>
</section>
<section id="common-pitfalls-and-solutions" class="level3">
<h3 class="anchored" data-anchor-id="common-pitfalls-and-solutions">Common Pitfalls and Solutions</h3>
<ol type="1">
<li><strong>Teacher Performance</strong>: Ensure the teacher actually outperforms the student (we fixed this in our implementation)</li>
<li><strong>Temperature Selection</strong>: If knowledge transfer is poor, experiment with different temperatures</li>
<li><strong>Loss Weighting</strong>: If the student ignores soft targets, reduce α to emphasize distillation loss</li>
<li><strong>Gradient Scaling</strong>: Always apply the τ² correction factor to the soft loss</li>
</ol>
</section>
<section id="performance-evaluation" class="level3">
<h3 class="anchored" data-anchor-id="performance-evaluation">Performance Evaluation</h3>
<p>Always measure multiple dimensions of performance:</p>
<ol type="1">
<li><strong>Accuracy</strong>: Primary performance metric (should be closer to teacher than vanilla student)</li>
<li><strong>Model Size</strong>: Parameter count and memory footprint (should match vanilla student)</li>
<li><strong>Inference Speed</strong>: Time per prediction (should be significantly faster than teacher)</li>
<li><strong>Challenging Cases</strong>: Performance on difficult examples (should be better than vanilla student)</li>
</ol>
</section>
</section>
<section id="conclusion" class="level2">
<h2 class="anchored" data-anchor-id="conclusion">Conclusion</h2>
<p>Knowledge distillation provides a powerful approach to create efficient, deployable models without sacrificing performance. Our implementation demonstrates that even with a 15-20x reduction in model size, we can maintain performance close to the larger teacher model.</p>
<section id="key-takeaways" class="level3">
<h3 class="anchored" data-anchor-id="key-takeaways">Key Takeaways</h3>
<ol type="1">
<li><strong>Efficiency Without Sacrifice</strong>: Knowledge distillation enables smaller models to achieve performance similar to larger ones</li>
<li><strong>Dark Knowledge Matters</strong>: The soft probability distributions contain valuable information beyond just the predicted class</li>
<li><strong>Temperature and Alpha</strong>: These hyperparameters are crucial for effective knowledge transfer</li>
<li><strong>Practical Benefits</strong>: Smaller size, faster inference, and lower resource requirements make AI more accessible</li>
</ol>
</section>
<section id="from-mnist-to-llms" class="level3">
<h3 class="anchored" data-anchor-id="from-mnist-to-llms">From MNIST to LLMs</h3>
<p>The principles we’ve demonstrated with MNIST directly scale to Large Language Models:</p>
<ol type="1">
<li><strong>Same Core Concept</strong>: Transfer knowledge from larger to smaller models</li>
<li><strong>Same Hyperparameters</strong>: Temperature and alpha weighting remain critical</li>
<li><strong>Same Benefits</strong>: Size reduction, speed improvement, and accessibility</li>
<li><strong>Same Challenges</strong>: Ensuring adequate student capacity and effective knowledge transfer</li>
</ol>
</section>
<section id="final-thoughts" class="level3">
<h3 class="anchored" data-anchor-id="final-thoughts">Final Thoughts</h3>
<p>Knowledge distillation isn’t just an academic technique—it’s essential for practical AI deployment. The same principles that helped us compress our MNIST classifier can be scaled to compress models with hundreds of billions of parameters. This universality makes knowledge distillation an indispensable skill for engineering students entering the field of AI.</p>
</section>
</section>
<section id="resources" class="level2">
<h2 class="anchored" data-anchor-id="resources">Resources</h2>
<p><a href="https://github.com/Mjrovai/EdgeML-with-Raspberry-Pi/blob/main/KD-Knowledge_Destilation/KD_Knowledge_Destilation_with_MNIST.ipynb">Notebook</a></p>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
          // target, if specified
          link.setAttribute("target", "_blank");
          // default icon
          link.classList.add("external");
      }
    }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="../../raspi/rnn-verne/rnn-verne.html" class="pagination-link">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text">Text Generation with RNNs</span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="../../raspi/llm/llm.html" class="pagination-link">
        <span class="nav-page-text">Small Language Models (SLM)</span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">Written and edited by Prof.&nbsp;Marcelo Rovai (UNIFEI University)</div>   
    <div class="nav-footer-center">
      &nbsp;
    </div>
    <div class="nav-footer-right">This book was built with <a href="https://quarto.org/">Quarto</a>.</div>
  </div>
</footer>



</body></html>