<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.3.450">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Edge AI Engineering - Image Classification with EXECUTORCH</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<link href="../../raspi/hw_acceleration/hw_acceleration.html" rel="next">
<link href="../../raspi/counting_objects_yolo/counting_objects_yolo.html" rel="prev">
<link href="../../images/unifei-logo.png" rel="icon" type="image/png">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>


<link rel="stylesheet" href="../../styles.css">
</head>

<body class="nav-sidebar floating nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">Edge AI Engineering</span>
    </a>
  </div>
        <div class="quarto-navbar-tools ms-auto tools-wide">
    <a href="https://github.com/Mjrovai/EdgeML-with-Raspberry-Pi" rel="" title="Source Code" class="quarto-navigation-tool px-1" aria-label="Source Code"><i class="bi bi-github"></i></a>
    <a href="../../Edge-AI-Engineering.pdf" rel="" title="Download PDF" class="quarto-navigation-tool px-1" aria-label="Download PDF"><i class="bi bi-file-pdf"></i></a>
    <div class="dropdown">
      <a href="" title="Share" id="quarto-navigation-tool-dropdown-0" class="quarto-navigation-tool dropdown-toggle px-1" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Share"><i class="bi bi-share"></i></a>
      <ul class="dropdown-menu dropdown-menu-end" aria-labelledby="quarto-navigation-tool-dropdown-0">
          <li>
            <a class="dropdown-item quarto-navbar-tools ms-auto-item" href="https://twitter.com/intent/tweet?url=|url|">
              <i class="bi bi-bi-twitter pe-1"></i>
            Twitter
            </a>
          </li>
          <li>
            <a class="dropdown-item quarto-navbar-tools ms-auto-item" href="https://www.linkedin.com/sharing/share-offsite/?url=|url|">
              <i class="bi bi-bi-linkedin pe-1"></i>
            LinkedIn
            </a>
          </li>
          <li>
            <a class="dropdown-item quarto-navbar-tools ms-auto-item" href="https://www.facebook.com/sharer/sharer.php?u=|url|">
              <i class="bi bi-bi-facebook pe-1"></i>
            Facebook
            </a>
          </li>
      </ul>
    </div>
  <a href="" class="quarto-reader-toggle quarto-navigation-tool px-1" onclick="window.quartoToggleReader(); return false;" title="Toggle reader mode">
  <div class="quarto-reader-toggle-btn">
  <i class="bi"></i>
  </div>
</a>
</div>
          <div id="quarto-search" class="" title="Search"></div>
      </div> <!-- /container-fluid -->
    </nav>
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
      <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../../raspi/executorch/executorch.html">Image Classification with EXECUTORCH</a></li></ol></nav>
      <a class="flex-grow-1" role="button" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
      </a>
      <button type="button" class="btn quarto-search-button" aria-label="" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal sidebar-navigation floating overflow-auto">
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Preface</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../Acknowledgements.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Acknowledgments</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../intro.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Introduction</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../about_book.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">About this Book</span></a>
  </div>
</li>
        <li class="px-0"><hr class="sidebar-divider hi "></li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../Classification_of_AI_Applications.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Classification of AI Applications</span></a>
  </div>
</li>
        <li class="px-0"><hr class="sidebar-divider hi "></li>
        <li class="sidebar-item sidebar-item-section">
      <span class="sidebar-item-text sidebar-link text-start">
 <span class="menu-text"><div class="part">Fixed Function AI (Reactive)</div></span></span>
  </li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../raspi/setup/setup.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Setup</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../raspi/image_classification/image_classification_fund.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Image Classification Fundamentals</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../raspi/image_classification/custom_image_classification_project.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Custom Image Classification Project</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../raspi/object_detection/object_detection_fundamentals.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Object Detection: Fundamentals</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../raspi/object_detection/custom_object_detection.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Custom Object Detection Project</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../raspi/object_detection/cv_yolo.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Computer Vision Applications with YOLO</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../raspi/counting_objects_yolo/counting_objects_yolo.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Counting objects with YOLO</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../raspi/executorch/executorch.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text">Image Classification with EXECUTORCH</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../raspi/hw_acceleration/hw_acceleration.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Beyond CPU - Hardware Acceleration for Edge AI</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <span class="sidebar-item-text sidebar-link text-start">
 <span class="menu-text"><div class="part">Generative AI (Proactive)</div></span></span>
  </li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../raspi/rnn-verne/rnn-verne.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Text Generation with RNNs</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../raspi/kd_intro/kd_intro.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Knowledge Distillation in Practice</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../raspi/llm/slm_intro.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Small Language Models (SLM)</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../raspi/llm/slm_opt_tech.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">SLM: Basic Optimization Techniques</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../raspi/vlm/vlm.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Vision-Language Models at the Edge</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../raspi/audio_pipeline/audio_pipeline.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Audio and Vision AI Pipeline</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../raspi/physical_comp/RPi_Physical_Computing.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Physical Computing with Raspberry Pi</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../raspi/iot/slm_iot.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Experimenting with SLMs for IoT Control</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../raspi/advancing_adgeai/adv_edgeai.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Advancing EdgeAI: Beyond Basic SLMs</span></a>
  </div>
</li>
        <li class="px-0"><hr class="sidebar-divider hi "></li>
        <li class="sidebar-item sidebar-item-section">
      <span class="sidebar-item-text sidebar-link text-start">
 <span class="menu-text"><div class="part">Weekly Labs</div></span></span>
  </li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../weekly_labs.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Edge AI Engineering - Weekly Labs</span></a>
  </div>
</li>
        <li class="px-0"><hr class="sidebar-divider hi "></li>
        <li class="sidebar-item sidebar-item-section">
      <span class="sidebar-item-text sidebar-link text-start">
 <span class="menu-text"><div class="part">References &amp; Author</div></span></span>
  </li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../references.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">References</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../about_the_author.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">About the author</span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#introduction" id="toc-introduction" class="nav-link active" data-scroll-target="#introduction">Introduction</a>
  <ul>
  <li><a href="#what-is-executorch" id="toc-what-is-executorch" class="nav-link" data-scroll-target="#what-is-executorch">What is EXECUTORCH?</a></li>
  <li><a href="#why-executorch-for-edge-ai" id="toc-why-executorch-for-edge-ai" class="nav-link" data-scroll-target="#why-executorch-for-edge-ai">Why EXECUTORCH for Edge AI?</a></li>
  <li><a href="#framework-comparison-executorch-vs-tensorflow-lite" id="toc-framework-comparison-executorch-vs-tensorflow-lite" class="nav-link" data-scroll-target="#framework-comparison-executorch-vs-tensorflow-lite">Framework Comparison: EXECUTORCH vs TensorFlow Lite</a></li>
  </ul></li>
  <li><a href="#setting-up-the-environment" id="toc-setting-up-the-environment" class="nav-link" data-scroll-target="#setting-up-the-environment">Setting Up the Environment</a>
  <ul>
  <li><a href="#updating-the-raspberry-pi" id="toc-updating-the-raspberry-pi" class="nav-link" data-scroll-target="#updating-the-raspberry-pi">Updating the Raspberry Pi</a></li>
  <li><a href="#installing-required-system-level-libraries" id="toc-installing-required-system-level-libraries" class="nav-link" data-scroll-target="#installing-required-system-level-libraries">Installing Required System-Level Libraries</a></li>
  <li><a href="#setting-up-a-virtual-environment" id="toc-setting-up-a-virtual-environment" class="nav-link" data-scroll-target="#setting-up-a-virtual-environment">Setting up a Virtual Environment</a>
  <ul class="collapse">
  <li><a href="#install-pyenv-dependencies" id="toc-install-pyenv-dependencies" class="nav-link" data-scroll-target="#install-pyenv-dependencies">Install pyenv Dependencies</a></li>
  <li><a href="#install-pyenv" id="toc-install-pyenv" class="nav-link" data-scroll-target="#install-pyenv">Install pyenv</a></li>
  <li><a href="#configure-shell" id="toc-configure-shell" class="nav-link" data-scroll-target="#configure-shell">Configure Shell</a></li>
  <li><a href="#install-python-3.11-or-3.12" id="toc-install-python-3.11-or-3.12" class="nav-link" data-scroll-target="#install-python-3.11-or-3.12">Install Python 3.11 (or 3.12)</a></li>
  <li><a href="#create-executorch-workspace" id="toc-create-executorch-workspace" class="nav-link" data-scroll-target="#create-executorch-workspace">Create ExecuTorch Workspace</a></li>
  <li><a href="#create-virtual-environment" id="toc-create-virtual-environment" class="nav-link" data-scroll-target="#create-virtual-environment">Create Virtual Environment</a></li>
  </ul></li>
  <li><a href="#install-python-packages" id="toc-install-python-packages" class="nav-link" data-scroll-target="#install-python-packages">Install Python Packages</a></li>
  </ul></li>
  <li><a href="#pytorch-and-executorch-installation" id="toc-pytorch-and-executorch-installation" class="nav-link" data-scroll-target="#pytorch-and-executorch-installation">PyTorch and EXECUTORCH Installation</a>
  <ul>
  <li><a href="#installing-pytorch-for-raspberry-pi" id="toc-installing-pytorch-for-raspberry-pi" class="nav-link" data-scroll-target="#installing-pytorch-for-raspberry-pi">Installing PyTorch for Raspberry Pi</a></li>
  <li><a href="#installing-executorch-runtime" id="toc-installing-executorch-runtime" class="nav-link" data-scroll-target="#installing-executorch-runtime">Installing EXECUTORCH Runtime</a></li>
  <li><a href="#verifying-the-setup" id="toc-verifying-the-setup" class="nav-link" data-scroll-target="#verifying-the-setup">Verifying the Setup</a></li>
  </ul></li>
  <li><a href="#image-classification-using-mobilenet-v2" id="toc-image-classification-using-mobilenet-v2" class="nav-link" data-scroll-target="#image-classification-using-mobilenet-v2">Image Classification using MobileNet V2</a>
  <ul>
  <li><a href="#working-directory" id="toc-working-directory" class="nav-link" data-scroll-target="#working-directory">Working directory:</a></li>
  <li><a href="#making-inference-with-torch" id="toc-making-inference-with-torch" class="nav-link" data-scroll-target="#making-inference-with-torch">Making inference with Torch</a></li>
  </ul></li>
  <li><a href="#exporting-models-to-executorch-format" id="toc-exporting-models-to-executorch-format" class="nav-link" data-scroll-target="#exporting-models-to-executorch-format">Exporting Models to EXECUTORCH Format</a>
  <ul>
  <li><a href="#understanding-the-export-process" id="toc-understanding-the-export-process" class="nav-link" data-scroll-target="#understanding-the-export-process">Understanding the Export Process</a></li>
  <li><a href="#exporting-mobilenet-v2-to-executorch" id="toc-exporting-mobilenet-v2-to-executorch" class="nav-link" data-scroll-target="#exporting-mobilenet-v2-to-executorch">Exporting MobileNet V2 to ExecuTorch</a></li>
  <li><a href="#model-quantization" id="toc-model-quantization" class="nav-link" data-scroll-target="#model-quantization">Model Quantization</a></li>
  <li><a href="#model-sizeperformance-comparison" id="toc-model-sizeperformance-comparison" class="nav-link" data-scroll-target="#model-sizeperformance-comparison">Model Size/Performance Comparison</a></li>
  </ul></li>
  <li><a href="#making-inferences-with-executorch" id="toc-making-inferences-with-executorch" class="nav-link" data-scroll-target="#making-inferences-with-executorch">Making Inferences with EXECUTORCH</a>
  <ul>
  <li><a href="#setting-up-jupyter-notebook" id="toc-setting-up-jupyter-notebook" class="nav-link" data-scroll-target="#setting-up-jupyter-notebook">Setting up Jupyter Notebook</a></li>
  <li><a href="#the-project-folder" id="toc-the-project-folder" class="nav-link" data-scroll-target="#the-project-folder">The Project folder</a></li>
  <li><a href="#loading-and-running-a-model" id="toc-loading-and-running-a-model" class="nav-link" data-scroll-target="#loading-and-running-a-model">Loading and Running a Model</a></li>
  <li><a href="#setup-and-verification" id="toc-setup-and-verification" class="nav-link" data-scroll-target="#setup-and-verification">Setup and Verification</a></li>
  <li><a href="#download-test-image" id="toc-download-test-image" class="nav-link" data-scroll-target="#download-test-image">Download Test Image</a></li>
  </ul></li>
  <li><a href="#load-executorch-model" id="toc-load-executorch-model" class="nav-link" data-scroll-target="#load-executorch-model">Load EXECUTORCH Model</a></li>
  <li><a href="#download-imagenet-labels" id="toc-download-imagenet-labels" class="nav-link" data-scroll-target="#download-imagenet-labels">Download ImageNet Labels</a></li>
  <li><a href="#image-preprocessing" id="toc-image-preprocessing" class="nav-link" data-scroll-target="#image-preprocessing">Image Preprocessing</a>
  <ul>
  <li><a href="#define-preprocessing-pipeline" id="toc-define-preprocessing-pipeline" class="nav-link" data-scroll-target="#define-preprocessing-pipeline">Define preprocessing pipeline</a></li>
  <li><a href="#apply-preprocessing" id="toc-apply-preprocessing" class="nav-link" data-scroll-target="#apply-preprocessing">Apply preprocessing</a></li>
  <li><a href="#add-batch-dimension-1-3-224-224" id="toc-add-batch-dimension-1-3-224-224" class="nav-link" data-scroll-target="#add-batch-dimension-1-3-224-224">Add batch dimension: [1, 3, 224, 224]</a></li>
  </ul></li>
  <li><a href="#run-inference" id="toc-run-inference" class="nav-link" data-scroll-target="#run-inference">Run Inference</a></li>
  <li><a href="#process-and-display-results" id="toc-process-and-display-results" class="nav-link" data-scroll-target="#process-and-display-results">Process and Display Results</a></li>
  <li><a href="#create-reusable-classification-function" id="toc-create-reusable-classification-function" class="nav-link" data-scroll-target="#create-reusable-classification-function">Create Reusable Classification Function</a></li>
  <li><a href="#classification-function-test" id="toc-classification-function-test" class="nav-link" data-scroll-target="#classification-function-test">Classification Function Test</a></li>
  <li><a href="#using-the-xnnpack-accelerated-backend" id="toc-using-the-xnnpack-accelerated-backend" class="nav-link" data-scroll-target="#using-the-xnnpack-accelerated-backend">Using the XNNPACK accelerated backend</a></li>
  <li><a href="#quantized-model---xnnpack-accelerated-backend" id="toc-quantized-model---xnnpack-accelerated-backend" class="nav-link" data-scroll-target="#quantized-model---xnnpack-accelerated-backend">Quantized model - XNNPACK accelerated backend</a></li>
  <li><a href="#camera-integration" id="toc-camera-integration" class="nav-link" data-scroll-target="#camera-integration">Camera Integration</a>
  <ul>
  <li><a href="#image-capture" id="toc-image-capture" class="nav-link" data-scroll-target="#image-capture">Image Capture</a></li>
  </ul></li>
  <li><a href="#performance-benchmarking" id="toc-performance-benchmarking" class="nav-link" data-scroll-target="#performance-benchmarking">Performance Benchmarking</a>
  <ul>
  <li><a href="#basic-float32-mobilenet_v2.pte" id="toc-basic-float32-mobilenet_v2.pte" class="nav-link" data-scroll-target="#basic-float32-mobilenet_v2.pte">Basic (Float32): mobilenet_v2.pte</a></li>
  <li><a href="#xnnpack-backend-flot32-mobilenet_v2_xnnpack.pte" id="toc-xnnpack-backend-flot32-mobilenet_v2_xnnpack.pte" class="nav-link" data-scroll-target="#xnnpack-backend-flot32-mobilenet_v2_xnnpack.pte">XNNPACK Backend (Flot32): mobilenet_v2_xnnpack.pte</a></li>
  <li><a href="#quantization-int8-mobilenet_v2_quantized_xnnpack.pte" id="toc-quantization-int8-mobilenet_v2_quantized_xnnpack.pte" class="nav-link" data-scroll-target="#quantization-int8-mobilenet_v2_quantized_xnnpack.pte">Quantization (INT8): mobilenet_v2_quantized_xnnpack.pte</a></li>
  <li><a href="#performance-comparison-table" id="toc-performance-comparison-table" class="nav-link" data-scroll-target="#performance-comparison-table">Performance Comparison Table</a></li>
  </ul></li>
  <li><a href="#exploring-custom-models" id="toc-exploring-custom-models" class="nav-link" data-scroll-target="#exploring-custom-models">Exploring Custom Models</a>
  <ul>
  <li><a href="#exporting-a-custom-trained-model" id="toc-exporting-a-custom-trained-model" class="nav-link" data-scroll-target="#exporting-a-custom-trained-model">Exporting a Custom Trained Model</a></li>
  <li><a href="#running-custom-models-on-raspberry-pi" id="toc-running-custom-models-on-raspberry-pi" class="nav-link" data-scroll-target="#running-custom-models-on-raspberry-pi">Running Custom Models on Raspberry Pi</a></li>
  </ul></li>
  <li><a href="#conclusion" id="toc-conclusion" class="nav-link" data-scroll-target="#conclusion">Conclusion</a>
  <ul>
  <li><a href="#key-takeaways" id="toc-key-takeaways" class="nav-link" data-scroll-target="#key-takeaways">Key Takeaways</a></li>
  <li><a href="#performance-considerations" id="toc-performance-considerations" class="nav-link" data-scroll-target="#performance-considerations">Performance Considerations</a></li>
  </ul></li>
  <li><a href="#resources" id="toc-resources" class="nav-link" data-scroll-target="#resources">Resources</a>
  <ul>
  <li><a href="#code-repository" id="toc-code-repository" class="nav-link" data-scroll-target="#code-repository">Code Repository</a></li>
  <li><a href="#official-documentation" id="toc-official-documentation" class="nav-link" data-scroll-target="#official-documentation">Official Documentation</a></li>
  <li><a href="#books" id="toc-books" class="nav-link" data-scroll-target="#books">Books</a></li>
  </ul></li>
  </ul>
<div class="toc-actions"><div><i class="bi bi-github"></i></div><div class="action-links"><p><a href="https://github.com/Mjrovai/EdgeML-with-Raspberry-Pi/edit/main/raspi/executorch/executorch.qmd" class="toc-action">Edit this page</a></p><p><a href="https://github.com/Mjrovai/EdgeML-with-Raspberry-Pi/issues/new" class="toc-action">Report an issue</a></p><p><a href="https://github.com/Mjrovai/EdgeML-with-Raspberry-Pi/blob/main/raspi/executorch/executorch.qmd" class="toc-action">View source</a></p></div></div></nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Image Classification with EXECUTORCH</h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  

</header>

<p><img src="./images/png/cover.png" class="img-fluid"></p>
<p><em>Implementing efficient image classification using PyTorch EXECUTORCH on edge devices</em></p>
<hr>
<section id="introduction" class="level2">
<h2 class="anchored" data-anchor-id="introduction">Introduction</h2>
<p>Image classification is a fundamental computer vision task that powers countless real-world applications—from quality control in manufacturing to wildlife monitoring, medical diagnostics, and smart home devices. In the edge AI landscape, the ability to run these models efficiently on resource-constrained devices has become increasingly critical for privacy-preserving, low-latency applications.</p>
<p>In the chapter <a href="https://mjrovai.github.io/EdgeML_Made_Ease_ebook/raspi/image_classification/image_classification_fund.html">Image Classification Fundamentals</a>, we explored image classification with <strong>TensorFlow Lite</strong> and demonstrated how to deploy efficient neural networks on the Raspberry Pi. That tutorial covered the complete workflow from model conversion to real-time camera inference, achieving excellent results with the <strong>MobileNet V2</strong> architecture and a real dataset (CIFAR-10).</p>
<p>This chapter takes a parallel approach using <a href="https://docs.pytorch.org/executorch/stable/index.html"><strong>PyTorch EXECUTORCH</strong></a>—Meta’s modern solution for edge deployment. Rather than replacing our TFLite knowledge, this chapter expands your edge AI toolkit, giving us the flexibility to choose the right framework for our specific needs.</p>
<section id="what-is-executorch" class="level3">
<h3 class="anchored" data-anchor-id="what-is-executorch">What is EXECUTORCH?</h3>
<p>EXECUTORCH is PyTorch’s official solution for deploying machine learning models on edge devices, from smartphones and embedded systems to microcontrollers and IoT devices. Released in 2023, it represents Meta’s commitment to bringing the entire PyTorch ecosystem to edge computing.</p>
<p><strong>Core Capabilities:</strong></p>
<ul>
<li><strong>Native PyTorch Integration</strong>: Seamless workflow from model training to edge deployment without switching frameworks</li>
<li><strong>Efficient Execution</strong>: Optimized runtime designed specifically for resource-constrained devices</li>
<li><strong>Broad Portability</strong>: Runs on diverse hardware platforms (ARM, x86, specialized accelerators)</li>
<li><strong>Flexible Backend System</strong>: Extensible delegate architecture for hardware-specific optimizations</li>
<li><strong>Quantization Support</strong>: Built-in integration with PyTorch’s quantization tools for model compression</li>
</ul>
</section>
<section id="why-executorch-for-edge-ai" class="level3">
<h3 class="anchored" data-anchor-id="why-executorch-for-edge-ai">Why EXECUTORCH for Edge AI?</h3>
<p>EXECUTORCH offers compelling advantages for edge deployment:</p>
<p><strong>1. Unified Workflow</strong> If we are training models in PyTorch, EXECUTORCH provides a natural deployment path without framework switching. This eliminates conversion errors and maintains model fidelity from training to deployment.</p>
<p><strong>2. Modern Architecture</strong> Built from the ground up for edge computing with contemporary best practices, EXECUTORCH incorporates lessons learned from previous mobile deployment frameworks.</p>
<p><strong>3. Comprehensive Quantization</strong> Native support for various quantization techniques (dynamic, static, quantization-aware training) enables significant model size reduction with minimal accuracy loss.</p>
<p><strong>4. Extensible Backend System</strong> The delegate system allows seamless integration with hardware accelerators (XNNPACK for CPU optimization, QNN for Qualcomm chips, CoreML for Apple devices, and more).</p>
<p><strong>5. Active Development</strong> Backed by Meta with rapid iteration and strong community support, ensuring the framework evolves with edge AI needs.</p>
<p><strong>6. Growing Model Zoo</strong> Access to pretrained models specifically optimized for edge deployment, with consistent performance across devices.</p>
</section>
<section id="framework-comparison-executorch-vs-tensorflow-lite" class="level3">
<h3 class="anchored" data-anchor-id="framework-comparison-executorch-vs-tensorflow-lite">Framework Comparison: EXECUTORCH vs TensorFlow Lite</h3>
<p>Understanding when to choose each framework is crucial for effective edge deployment:</p>
<table class="table">
<colgroup>
<col style="width: 25%">
<col style="width: 40%">
<col style="width: 33%">
</colgroup>
<thead>
<tr class="header">
<th>Feature</th>
<th>EXECUTORCH</th>
<th>TensorFlow Lite</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>Training Framework</strong></td>
<td>PyTorch</td>
<td>TensorFlow/Keras</td>
</tr>
<tr class="even">
<td><strong>Maturity</strong></td>
<td>Newer (2023+)</td>
<td>Mature (2017+)</td>
</tr>
<tr class="odd">
<td><strong>Model Format</strong></td>
<td><code>.pte</code></td>
<td><code>.tflite (.lite)</code></td>
</tr>
<tr class="even">
<td><strong>Quantization</strong></td>
<td>PyTorch native quantization</td>
<td>TF quantization-aware training</td>
</tr>
<tr class="odd">
<td><strong>Backend Acceleration</strong></td>
<td>Delegate system (XNNPACK, QNN, CoreML)</td>
<td>Delegates (GPU, NNAPI, Hexagon)</td>
</tr>
<tr class="even">
<td><strong>Community</strong></td>
<td>Rapidly growing</td>
<td>Large, established</td>
</tr>
<tr class="odd">
<td><strong>Hardware Support</strong></td>
<td>Expanding quickly</td>
<td>Extensive, mature</td>
</tr>
<tr class="even">
<td><strong>Learning Curve</strong></td>
<td>Easier for PyTorch users</td>
<td>Easier for TF/Keras users</td>
</tr>
<tr class="odd">
<td><strong>Documentation</strong></td>
<td>Growing, modern</td>
<td>Comprehensive, mature</td>
</tr>
<tr class="even">
<td><strong>Industry Adoption</strong></td>
<td>Increasing in research</td>
<td>Widespread in production</td>
</tr>
</tbody>
</table>
<p><strong>The Reality: Both Are Excellent Choices</strong></p>
<p>In practice, both frameworks achieve similar goals with different philosophies. Our choice often comes down to:</p>
<ol type="1">
<li>Our training framework preference</li>
<li>Team expertise and existing infrastructure</li>
<li>Specific hardware requirements</li>
<li>Project timeline and maturity needs</li>
</ol>
<p>This chapter demonstrates that transitioning between frameworks is straightforward, allowing us to make informed decisions based on project needs rather than framework limitations.</p>
<hr>
</section>
</section>
<section id="setting-up-the-environment" class="level2">
<h2 class="anchored" data-anchor-id="setting-up-the-environment">Setting Up the Environment</h2>
<section id="updating-the-raspberry-pi" class="level3">
<h3 class="anchored" data-anchor-id="updating-the-raspberry-pi">Updating the Raspberry Pi</h3>
<p>First, ensure that the Raspberry Pi is up to date:</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="fu">sudo</span> apt update</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="fu">sudo</span> apt upgrade <span class="at">-y</span></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="fu">sudo</span> reboot  <span class="co"># Reboot to ensure all updates take effect</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="installing-required-system-level-libraries" class="level3">
<h3 class="anchored" data-anchor-id="installing-required-system-level-libraries">Installing Required System-Level Libraries</h3>
<p>Install Python tools, camera libraries, and build dependencies for PyTorch:</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="fu">sudo</span> apt install <span class="at">-y</span> python3-pip python3-venv python3-picamera2</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a><span class="fu">sudo</span> apt install <span class="at">-y</span> libcamera-dev libcamera-tools libcamera-apps</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a><span class="fu">sudo</span> apt install <span class="at">-y</span> libopenblas-dev libjpeg-dev zlib1g-dev libpng-dev</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p><strong>Picamera2 Installation Test</strong></p>
<p>We can test the camera with:</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="ex">rpicam-hello</span> <span class="at">--list-cameras</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p><img src="./images/png/cam-setup.png" class="img-fluid"></p>
<blockquote class="blockquote">
<p>We should see that the OV5647 cam is installed.</p>
</blockquote>
<p>Now, let’s create a test script to verify everything works:</p>
<p><a href="https://github.com/Mjrovai/EdgeML-with-Raspberry-Pi/blob/main/EXECUTORCH/scripts/camera_capture.py">camera_capture.py</a></p>
<div class="sourceCode" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> picamera2 <span class="im">import</span> Picamera2</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> time</span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"NumPy version: </span><span class="sc">{</span>np<span class="sc">.</span>__version__<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Initialize camera</span></span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a>picam2 <span class="op">=</span> Picamera2()</span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a>config <span class="op">=</span> picam2.create_preview_configuration(main<span class="op">=</span>{<span class="st">"size"</span>:(<span class="dv">640</span>,<span class="dv">480</span>)}) </span>
<span id="cb4-11"><a href="#cb4-11" aria-hidden="true" tabindex="-1"></a>picam2.configure(config)</span>
<span id="cb4-12"><a href="#cb4-12" aria-hidden="true" tabindex="-1"></a>picam2.start()</span>
<span id="cb4-13"><a href="#cb4-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-14"><a href="#cb4-14" aria-hidden="true" tabindex="-1"></a><span class="co"># Wait for camera to warm up</span></span>
<span id="cb4-15"><a href="#cb4-15" aria-hidden="true" tabindex="-1"></a>time.sleep(<span class="dv">2</span>)</span>
<span id="cb4-16"><a href="#cb4-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-17"><a href="#cb4-17" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Camera working in the system!"</span>)</span>
<span id="cb4-18"><a href="#cb4-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-19"><a href="#cb4-19" aria-hidden="true" tabindex="-1"></a><span class="co"># Capture image</span></span>
<span id="cb4-20"><a href="#cb4-20" aria-hidden="true" tabindex="-1"></a>picam2.capture_file(<span class="st">"camera_capture.jpg"</span>)</span>
<span id="cb4-21"><a href="#cb4-21" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Image captured: cam_test.jpg"</span>)</span>
<span id="cb4-22"><a href="#cb4-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-23"><a href="#cb4-23" aria-hidden="true" tabindex="-1"></a><span class="co"># Stop camera</span></span>
<span id="cb4-24"><a href="#cb4-24" aria-hidden="true" tabindex="-1"></a>picam2.stop()</span>
<span id="cb4-25"><a href="#cb4-25" aria-hidden="true" tabindex="-1"></a>picam2.close()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<blockquote class="blockquote">
<p>A test image should be created in the current directory</p>
</blockquote>
</section>
<section id="setting-up-a-virtual-environment" class="level3">
<h3 class="anchored" data-anchor-id="setting-up-a-virtual-environment">Setting up a Virtual Environment</h3>
<p>First, let’s confirm the System Python version:</p>
<div class="sourceCode" id="cb5"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="ex">python</span> <span class="at">--version</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>If we use the latest Raspberry Pi OS (based on Debian Trixie), it should be:</p>
<p><code>3.13.5</code></p>
<p>As of today (January 2026), ExecuTorch officially supports only <strong>Python 3.10 to 3.12</strong>; Python 3.13.5 is too new and will likely cause compatibility issues. Since <strong>Debian Trixie ships with Python 3.13</strong> by default, we’ll need to install a compatible Python version alongside it.</p>
<p>One solution is to install <a href="https://github.com/pyenv/pyenv">Pyenv</a>, so that we can easily manage multiple Python versions for different projects without affecting the system Python.</p>
<blockquote class="blockquote">
<p>If the Raspberry Pi OS is the legacy, the Python version should be 3.11, and it is not necessary to install Pyenv.</p>
</blockquote>
<section id="install-pyenv-dependencies" class="level4">
<h4 class="anchored" data-anchor-id="install-pyenv-dependencies">Install pyenv Dependencies</h4>
<div class="sourceCode" id="cb6"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="fu">sudo</span> apt update</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a><span class="fu">sudo</span> apt install <span class="at">-y</span> build-essential libssl-dev zlib1g-dev <span class="dt">\</span></span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a>    libbz2-dev libreadline-dev libsqlite3-dev curl git <span class="dt">\</span></span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a>    libncursesw5-dev xz-utils tk-dev libxml2-dev <span class="dt">\</span></span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a>    libxmlsec1-dev libffi-dev liblzma-dev <span class="dt">\</span></span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a>    libopenblas-dev libjpeg-dev libpng-dev cmake</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="install-pyenv" class="level4">
<h4 class="anchored" data-anchor-id="install-pyenv">Install pyenv</h4>
<div class="sourceCode" id="cb7"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Download and install pyenv</span></span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a><span class="ex">curl</span> https://pyenv.run <span class="kw">|</span> <span class="fu">bash</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="configure-shell" class="level4">
<h4 class="anchored" data-anchor-id="configure-shell">Configure Shell</h4>
<p>Add pyenv to <code>~/.bashrc</code>:</p>
<div class="sourceCode" id="cb8"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="fu">cat</span> <span class="op">&gt;&gt;</span> ~/.bashrc <span class="op">&lt;&lt; 'EOF'</span></span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a><span class="st"># pyenv configuration</span></span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a><span class="st">export PYENV_ROOT="$HOME/.pyenv"</span></span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a><span class="st">[[ -d $PYENV_ROOT/bin ]] &amp;&amp; export PATH="$PYENV_ROOT/bin:$PATH"</span></span>
<span id="cb8-6"><a href="#cb8-6" aria-hidden="true" tabindex="-1"></a><span class="st">eval "$(pyenv init -)"</span></span>
<span id="cb8-7"><a href="#cb8-7" aria-hidden="true" tabindex="-1"></a><span class="op">EOF</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Reload the shell:</p>
<div class="sourceCode" id="cb9"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="bu">source</span> ~/.bashrc</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Verify if pyenv is installed:</p>
<div class="sourceCode" id="cb10"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="ex">pyenv</span> <span class="at">--version</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="install-python-3.11-or-3.12" class="level4">
<h4 class="anchored" data-anchor-id="install-python-3.11-or-3.12">Install Python 3.11 (or 3.12)</h4>
<div class="sourceCode" id="cb11"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="co"># See available versions</span></span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a><span class="ex">pyenv</span> install <span class="at">--list</span> <span class="kw">|</span> <span class="fu">grep</span> <span class="st">" 3.11"</span></span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Install Python 3.11.14 (latest 3.11 stable)</span></span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true" tabindex="-1"></a><span class="ex">pyenv</span> install 3.11.14</span>
<span id="cb11-6"><a href="#cb11-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-7"><a href="#cb11-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Or install Python 3.12.3 if you prefer</span></span>
<span id="cb11-8"><a href="#cb11-8" aria-hidden="true" tabindex="-1"></a><span class="co"># pyenv install 3.12.12</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<blockquote class="blockquote">
<p>This will take a few minutes to compile.</p>
</blockquote>
</section>
<section id="create-executorch-workspace" class="level4">
<h4 class="anchored" data-anchor-id="create-executorch-workspace">Create ExecuTorch Workspace</h4>
<div class="sourceCode" id="cb12"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="bu">cd</span> Documents    </span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a><span class="fu">mkdir</span> EXECUTORCH</span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a><span class="bu">cd</span> EXECUTORCH</span>
<span id="cb12-4"><a href="#cb12-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-5"><a href="#cb12-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Set Python 3.11.14 for this directory</span></span>
<span id="cb12-6"><a href="#cb12-6" aria-hidden="true" tabindex="-1"></a><span class="ex">pyenv</span> local 3.11.14</span>
<span id="cb12-7"><a href="#cb12-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-8"><a href="#cb12-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Verify</span></span>
<span id="cb12-9"><a href="#cb12-9" aria-hidden="true" tabindex="-1"></a><span class="ex">python</span> <span class="at">--version</span>  <span class="co"># Should show Python 3.11.14</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="create-virtual-environment" class="level4">
<h4 class="anchored" data-anchor-id="create-virtual-environment">Create Virtual Environment</h4>
<div class="sourceCode" id="cb13"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="ex">python</span> <span class="at">-m</span> venv executorch-venv</span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a><span class="bu">source</span> executorch-venv/bin/activate</span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Verify if we're using the correct Python</span></span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true" tabindex="-1"></a><span class="fu">which</span> python</span>
<span id="cb13-6"><a href="#cb13-6" aria-hidden="true" tabindex="-1"></a><span class="ex">python</span> <span class="at">--version</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>To exit the virtual environment later:</p>
<div class="sourceCode" id="cb14"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a><span class="ex">deactivate</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
</section>
<section id="install-python-packages" class="level3">
<h3 class="anchored" data-anchor-id="install-python-packages">Install Python Packages</h3>
<blockquote class="blockquote">
<p>Ensure we’re in the virtual environment (venv)</p>
</blockquote>
<div class="sourceCode" id="cb15"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a><span class="ex">pip</span> install <span class="at">--upgrade</span> pip</span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a><span class="ex">pip</span> install numpy pillow matplotlib opencv-python</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Verify installation:</p>
<div class="sourceCode" id="cb16"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a><span class="ex">pip</span> list <span class="kw">|</span> <span class="fu">grep</span> <span class="at">-E</span> <span class="st">"(numpy|pillow|opencv)"</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p><img src="./images/png/versions.png" class="img-fluid"></p>
</section>
</section>
<section id="pytorch-and-executorch-installation" class="level2">
<h2 class="anchored" data-anchor-id="pytorch-and-executorch-installation">PyTorch and EXECUTORCH Installation</h2>
<section id="installing-pytorch-for-raspberry-pi" class="level3">
<h3 class="anchored" data-anchor-id="installing-pytorch-for-raspberry-pi">Installing PyTorch for Raspberry Pi</h3>
<p>PyTorch provides pre-built wheels for ARM64 architecture (Raspberry Pi 3/4/5).</p>
<p>For <strong>Raspberry Pi 4/5 (aarch64)</strong>:</p>
<div class="sourceCode" id="cb17"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Install PyTorch (CPU version for ARM64)</span></span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a><span class="ex">pip</span> install torch torchvision <span class="at">--index-url</span> <span class="dt">\</span></span>
<span id="cb17-3"><a href="#cb17-3" aria-hidden="true" tabindex="-1"></a>https://download.pytorch.org/whl/cpu</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<blockquote class="blockquote">
<p>For the <strong>Raspberry Pi Zero 2 W</strong> (32-bit ARM), we may need to build from source or use lighter alternatives, which are not covered here.</p>
</blockquote>
<p>Verify PyTorch installation:</p>
<div class="sourceCode" id="cb18"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a><span class="ex">python</span> <span class="at">-c</span> <span class="st">"import torch; print(f'PyTorch version: </span><span class="dt">\</span></span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a><span class="st">{torch.__version__}')"</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>We will get, for example, <code>PyTorch version: 2.9.1+cpu</code></p>
</section>
<section id="installing-executorch-runtime" class="level3">
<h3 class="anchored" data-anchor-id="installing-executorch-runtime">Installing EXECUTORCH Runtime</h3>
<p>EXECUTORCH can be installed via pip:</p>
<div class="sourceCode" id="cb19"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a><span class="ex">pip</span> install executorch</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p><strong>Building from Source (Optional - for latest features):</strong></p>
<p>If we want the absolute latest features or need to customize:</p>
<div class="sourceCode" id="cb20"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Clone the repository</span></span>
<span id="cb20-2"><a href="#cb20-2" aria-hidden="true" tabindex="-1"></a><span class="fu">git</span> clone https://github.com/pytorch/executorch.git</span>
<span id="cb20-3"><a href="#cb20-3" aria-hidden="true" tabindex="-1"></a><span class="bu">cd</span> executorch</span>
<span id="cb20-4"><a href="#cb20-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-5"><a href="#cb20-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Install dependencies</span></span>
<span id="cb20-6"><a href="#cb20-6" aria-hidden="true" tabindex="-1"></a><span class="ex">./install_requirements.sh</span></span>
<span id="cb20-7"><a href="#cb20-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-8"><a href="#cb20-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Install EXECUTORCH in development mode</span></span>
<span id="cb20-9"><a href="#cb20-9" aria-hidden="true" tabindex="-1"></a><span class="ex">pip</span> install <span class="at">-e</span> .</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="verifying-the-setup" class="level3">
<h3 class="anchored" data-anchor-id="verifying-the-setup">Verifying the Setup</h3>
<p>Let’s verify our setup with a test script. Create <code>setup_test.py</code> (for example, using nano):</p>
<div class="sourceCode" id="cb21"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb21-2"><a href="#cb21-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb21-3"><a href="#cb21-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> PIL <span class="im">import</span> Image</span>
<span id="cb21-4"><a href="#cb21-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> executorch</span>
<span id="cb21-5"><a href="#cb21-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-6"><a href="#cb21-6" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"="</span> <span class="op">*</span> <span class="dv">50</span>)</span>
<span id="cb21-7"><a href="#cb21-7" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"SETUP VERIFICATION"</span>)</span>
<span id="cb21-8"><a href="#cb21-8" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"="</span> <span class="op">*</span> <span class="dv">50</span>)</span>
<span id="cb21-9"><a href="#cb21-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-10"><a href="#cb21-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Check versions</span></span>
<span id="cb21-11"><a href="#cb21-11" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"PyTorch version: </span><span class="sc">{</span>torch<span class="sc">.</span>__version__<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb21-12"><a href="#cb21-12" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"NumPy version: </span><span class="sc">{</span>np<span class="sc">.</span>__version__<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb21-13"><a href="#cb21-13" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"PIL version: </span><span class="sc">{</span>Image<span class="sc">.</span>__version__<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb21-14"><a href="#cb21-14" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"EXECUTORCH available: </span><span class="sc">{</span>executorch <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span><span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb21-15"><a href="#cb21-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-16"><a href="#cb21-16" aria-hidden="true" tabindex="-1"></a><span class="co"># Test basic PyTorch functionality</span></span>
<span id="cb21-17"><a href="#cb21-17" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> torch.randn(<span class="dv">3</span>, <span class="dv">224</span>, <span class="dv">224</span>)</span>
<span id="cb21-18"><a href="#cb21-18" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">Created test tensor with shape: </span><span class="sc">{</span>x<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb21-19"><a href="#cb21-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-20"><a href="#cb21-20" aria-hidden="true" tabindex="-1"></a><span class="co"># Test PIL</span></span>
<span id="cb21-21"><a href="#cb21-21" aria-hidden="true" tabindex="-1"></a>test_img <span class="op">=</span> Image.new(<span class="st">'RGB'</span>, (<span class="dv">224</span>, <span class="dv">224</span>), color<span class="op">=</span><span class="st">'red'</span>)</span>
<span id="cb21-22"><a href="#cb21-22" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Created test PIL image: </span><span class="sc">{</span>test_img<span class="sc">.</span>size<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb21-23"><a href="#cb21-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-24"><a href="#cb21-24" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">✓ Setup verification complete!"</span>)</span>
<span id="cb21-25"><a href="#cb21-25" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"="</span> <span class="op">*</span> <span class="dv">50</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Run it:</p>
<div class="sourceCode" id="cb22"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a><span class="ex">python</span> setup_test.py</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Expected output (the versions can be different):</p>
<pre><code>==================================================
SETUP VERIFICATION
==================================================
PyTorch version: 2.9.1+cpu
NumPy version: 2.2.6
PIL version: 12.1.0
EXECUTORCH available: True

Created test tensor with shape: torch.Size([3, 224, 224])
Created test PIL image: (224, 224)

✓ Setup verification complete!
==================================================</code></pre>
<hr>
</section>
</section>
<section id="image-classification-using-mobilenet-v2" class="level2">
<h2 class="anchored" data-anchor-id="image-classification-using-mobilenet-v2">Image Classification using MobileNet V2</h2>
<section id="working-directory" class="level3">
<h3 class="anchored" data-anchor-id="working-directory">Working directory:</h3>
<div class="sourceCode" id="cb24"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a><span class="bu">cd</span> Documents</span>
<span id="cb24-2"><a href="#cb24-2" aria-hidden="true" tabindex="-1"></a><span class="bu">cd</span> EXECUTORCH</span>
<span id="cb24-3"><a href="#cb24-3" aria-hidden="true" tabindex="-1"></a><span class="fu">mkdir</span> IMG_CLASS</span>
<span id="cb24-4"><a href="#cb24-4" aria-hidden="true" tabindex="-1"></a><span class="bu">cd</span> IMG_CLASS</span>
<span id="cb24-5"><a href="#cb24-5" aria-hidden="true" tabindex="-1"></a><span class="fu">mkdir</span> MOBILENET</span>
<span id="cb24-6"><a href="#cb24-6" aria-hidden="true" tabindex="-1"></a><span class="bu">cd</span> MOBILENET</span>
<span id="cb24-7"><a href="#cb24-7" aria-hidden="true" tabindex="-1"></a><span class="fu">mkdir</span> models images notebooks</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="making-inference-with-torch" class="level3">
<h3 class="anchored" data-anchor-id="making-inference-with-torch">Making inference with Torch</h3>
<p>Load an image from the internet, for example, a cat: <code>"https://upload.wikimedia.org/wikipedia/commons/3/3a/Cat03.jpg"</code></p>
<p>And save it in the images folder as “cat.jpg”:</p>
<div class="sourceCode" id="cb25"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb25-1"><a href="#cb25-1" aria-hidden="true" tabindex="-1"></a><span class="fu">wget</span> <span class="st">"https://upload.wikimedia.org/wikipedia/commons/3/3a/Cat03.jpg"</span> <span class="dt">\</span></span>
<span id="cb25-2"><a href="#cb25-2" aria-hidden="true" tabindex="-1"></a>     <span class="at">-O</span> ./images/cat.jpg</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Now, let’s create a test program where we should take into consideration:</p>
<ol type="1">
<li><strong>First run</strong> - Downloads model &amp; labels (and saves them)</li>
<li><strong>Preprocessing</strong> - MobileNetV2 expects 224x224 images with ImageNet normalization</li>
<li><strong>torch.no_grad()</strong> -Disables gradient calculation for faster inference</li>
<li><strong>Timing</strong> - Measures only inference time, not preprocessing</li>
<li><strong>Softmax</strong> - Converts raw outputs to probabilities</li>
<li><strong>Top-5</strong> - Shows the 5 most likely classes</li>
</ol>
<p>and save it as <a href="https://github.com/Mjrovai/EdgeML-with-Raspberry-Pi/blob/main/EXECUTORCH/scripts/img_class_test_torch.py">img_class_test_torch.py</a>:</p>
<div class="sourceCode" id="cb26"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb26-1"><a href="#cb26-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb26-2"><a href="#cb26-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torchvision.transforms <span class="im">as</span> transforms</span>
<span id="cb26-3"><a href="#cb26-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torchvision <span class="im">import</span> models</span>
<span id="cb26-4"><a href="#cb26-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> PIL <span class="im">import</span> Image</span>
<span id="cb26-5"><a href="#cb26-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> time</span>
<span id="cb26-6"><a href="#cb26-6" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> json</span>
<span id="cb26-7"><a href="#cb26-7" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> urllib.request</span>
<span id="cb26-8"><a href="#cb26-8" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> os</span>
<span id="cb26-9"><a href="#cb26-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-10"><a href="#cb26-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Paths</span></span>
<span id="cb26-11"><a href="#cb26-11" aria-hidden="true" tabindex="-1"></a>MODEL_PATH <span class="op">=</span> <span class="st">"models/mobilenet_v2.pth"</span></span>
<span id="cb26-12"><a href="#cb26-12" aria-hidden="true" tabindex="-1"></a>LABELS_PATH <span class="op">=</span> <span class="st">"models/imagenet_labels.json"</span></span>
<span id="cb26-13"><a href="#cb26-13" aria-hidden="true" tabindex="-1"></a>IMAGE_PATH <span class="op">=</span> <span class="st">"images/cat.jpg"</span></span>
<span id="cb26-14"><a href="#cb26-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-15"><a href="#cb26-15" aria-hidden="true" tabindex="-1"></a><span class="co"># Download and save ImageNet labels (only first time)</span></span>
<span id="cb26-16"><a href="#cb26-16" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> <span class="kw">not</span> os.path.exists(LABELS_PATH):</span>
<span id="cb26-17"><a href="#cb26-17" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"Downloading ImageNet labels..."</span>)</span>
<span id="cb26-18"><a href="#cb26-18" aria-hidden="true" tabindex="-1"></a>    LABELS_URL <span class="op">=</span> <span class="st">"https://raw.githubusercontent.com/anishathalye/</span><span class="ch">\</span></span>
<span id="cb26-19"><a href="#cb26-19" aria-hidden="true" tabindex="-1"></a><span class="st">    imagenet-simple-labels/master/imagenet-simple-labels.json"</span></span>
<span id="cb26-20"><a href="#cb26-20" aria-hidden="true" tabindex="-1"></a>    <span class="cf">with</span> urllib.request.urlopen(LABELS_URL) <span class="im">as</span> url:</span>
<span id="cb26-21"><a href="#cb26-21" aria-hidden="true" tabindex="-1"></a>        labels <span class="op">=</span> json.load(url)</span>
<span id="cb26-22"><a href="#cb26-22" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb26-23"><a href="#cb26-23" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Save labels locally</span></span>
<span id="cb26-24"><a href="#cb26-24" aria-hidden="true" tabindex="-1"></a>    <span class="cf">with</span> <span class="bu">open</span>(LABELS_PATH, <span class="st">'w'</span>) <span class="im">as</span> f:</span>
<span id="cb26-25"><a href="#cb26-25" aria-hidden="true" tabindex="-1"></a>        json.dump(labels, f)</span>
<span id="cb26-26"><a href="#cb26-26" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Labels saved to </span><span class="sc">{</span>LABELS_PATH<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb26-27"><a href="#cb26-27" aria-hidden="true" tabindex="-1"></a><span class="cf">else</span>:</span>
<span id="cb26-28"><a href="#cb26-28" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"Loading labels from disk..."</span>)</span>
<span id="cb26-29"><a href="#cb26-29" aria-hidden="true" tabindex="-1"></a>    <span class="cf">with</span> <span class="bu">open</span>(LABELS_PATH, <span class="st">'r'</span>) <span class="im">as</span> f:</span>
<span id="cb26-30"><a href="#cb26-30" aria-hidden="true" tabindex="-1"></a>        labels <span class="op">=</span> json.load(f)</span>
<span id="cb26-31"><a href="#cb26-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-32"><a href="#cb26-32" aria-hidden="true" tabindex="-1"></a><span class="co"># Load or download model</span></span>
<span id="cb26-33"><a href="#cb26-33" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> <span class="kw">not</span> os.path.exists(MODEL_PATH):</span>
<span id="cb26-34"><a href="#cb26-34" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"Downloading MobileNetV2 model..."</span>)</span>
<span id="cb26-35"><a href="#cb26-35" aria-hidden="true" tabindex="-1"></a>    model <span class="op">=</span> models.mobilenet_v2(pretrained<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb26-36"><a href="#cb26-36" aria-hidden="true" tabindex="-1"></a>    model.<span class="bu">eval</span>()</span>
<span id="cb26-37"><a href="#cb26-37" aria-hidden="true" tabindex="-1"></a>    torch.save(model.state_dict(), MODEL_PATH)</span>
<span id="cb26-38"><a href="#cb26-38" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Model saved to </span><span class="sc">{</span>MODEL_PATH<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb26-39"><a href="#cb26-39" aria-hidden="true" tabindex="-1"></a><span class="cf">else</span>:</span>
<span id="cb26-40"><a href="#cb26-40" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"Loading model from disk..."</span>)</span>
<span id="cb26-41"><a href="#cb26-41" aria-hidden="true" tabindex="-1"></a>    model <span class="op">=</span> models.mobilenet_v2()</span>
<span id="cb26-42"><a href="#cb26-42" aria-hidden="true" tabindex="-1"></a>    model.load_state_dict(torch.load(MODEL_PATH, map_location<span class="op">=</span><span class="st">'cpu'</span>))</span>
<span id="cb26-43"><a href="#cb26-43" aria-hidden="true" tabindex="-1"></a>    model.<span class="bu">eval</span>()</span>
<span id="cb26-44"><a href="#cb26-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-45"><a href="#cb26-45" aria-hidden="true" tabindex="-1"></a><span class="co"># Define image preprocessing</span></span>
<span id="cb26-46"><a href="#cb26-46" aria-hidden="true" tabindex="-1"></a>preprocess <span class="op">=</span> transforms.Compose([</span>
<span id="cb26-47"><a href="#cb26-47" aria-hidden="true" tabindex="-1"></a>    transforms.Resize(<span class="dv">256</span>),</span>
<span id="cb26-48"><a href="#cb26-48" aria-hidden="true" tabindex="-1"></a>    transforms.CenterCrop(<span class="dv">224</span>),</span>
<span id="cb26-49"><a href="#cb26-49" aria-hidden="true" tabindex="-1"></a>    transforms.ToTensor(),</span>
<span id="cb26-50"><a href="#cb26-50" aria-hidden="true" tabindex="-1"></a>    transforms.Normalize(mean<span class="op">=</span>[<span class="fl">0.485</span>, <span class="fl">0.456</span>, <span class="fl">0.406</span>], </span>
<span id="cb26-51"><a href="#cb26-51" aria-hidden="true" tabindex="-1"></a>                         std<span class="op">=</span>[<span class="fl">0.229</span>, <span class="fl">0.224</span>, <span class="fl">0.225</span>]),</span>
<span id="cb26-52"><a href="#cb26-52" aria-hidden="true" tabindex="-1"></a>])</span>
<span id="cb26-53"><a href="#cb26-53" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-54"><a href="#cb26-54" aria-hidden="true" tabindex="-1"></a><span class="co"># Load and preprocess image</span></span>
<span id="cb26-55"><a href="#cb26-55" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">Loading image from </span><span class="sc">{</span>IMAGE_PATH<span class="sc">}</span><span class="ss">..."</span>)</span>
<span id="cb26-56"><a href="#cb26-56" aria-hidden="true" tabindex="-1"></a>img <span class="op">=</span> Image.<span class="bu">open</span>(IMAGE_PATH)</span>
<span id="cb26-57"><a href="#cb26-57" aria-hidden="true" tabindex="-1"></a>img_tensor <span class="op">=</span> preprocess(img)</span>
<span id="cb26-58"><a href="#cb26-58" aria-hidden="true" tabindex="-1"></a>batch <span class="op">=</span> img_tensor.unsqueeze(<span class="dv">0</span>)</span>
<span id="cb26-59"><a href="#cb26-59" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-60"><a href="#cb26-60" aria-hidden="true" tabindex="-1"></a><span class="co"># Perform inference with timing</span></span>
<span id="cb26-61"><a href="#cb26-61" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Running inference..."</span>)</span>
<span id="cb26-62"><a href="#cb26-62" aria-hidden="true" tabindex="-1"></a>start_time <span class="op">=</span> time.time()</span>
<span id="cb26-63"><a href="#cb26-63" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-64"><a href="#cb26-64" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> torch.no_grad():</span>
<span id="cb26-65"><a href="#cb26-65" aria-hidden="true" tabindex="-1"></a>    output <span class="op">=</span> model(batch)</span>
<span id="cb26-66"><a href="#cb26-66" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb26-67"><a href="#cb26-67" aria-hidden="true" tabindex="-1"></a>inference_time <span class="op">=</span> (time.time() <span class="op">-</span> start_time) <span class="op">*</span> <span class="dv">1000</span></span>
<span id="cb26-68"><a href="#cb26-68" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-69"><a href="#cb26-69" aria-hidden="true" tabindex="-1"></a><span class="co"># Get predictions</span></span>
<span id="cb26-70"><a href="#cb26-70" aria-hidden="true" tabindex="-1"></a>probabilities <span class="op">=</span> torch.nn.functional.softmax(output[<span class="dv">0</span>], dim<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb26-71"><a href="#cb26-71" aria-hidden="true" tabindex="-1"></a>top5_prob, top5_idx <span class="op">=</span> torch.topk(probabilities, <span class="dv">5</span>)</span>
<span id="cb26-72"><a href="#cb26-72" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-73"><a href="#cb26-73" aria-hidden="true" tabindex="-1"></a><span class="co"># Display results</span></span>
<span id="cb26-74"><a href="#cb26-74" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">"</span> <span class="op">+</span> <span class="st">"="</span><span class="op">*</span><span class="dv">50</span>)</span>
<span id="cb26-75"><a href="#cb26-75" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"CLASSIFICATION RESULTS"</span>)</span>
<span id="cb26-76"><a href="#cb26-76" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"="</span><span class="op">*</span><span class="dv">50</span>)</span>
<span id="cb26-77"><a href="#cb26-77" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Inference Time: </span><span class="sc">{</span>inference_time<span class="sc">:.2f}</span><span class="ss"> ms</span><span class="ch">\n</span><span class="ss">"</span>)</span>
<span id="cb26-78"><a href="#cb26-78" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Top 5 Predictions:"</span>)</span>
<span id="cb26-79"><a href="#cb26-79" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"-"</span><span class="op">*</span><span class="dv">50</span>)</span>
<span id="cb26-80"><a href="#cb26-80" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-81"><a href="#cb26-81" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">5</span>):</span>
<span id="cb26-82"><a href="#cb26-82" aria-hidden="true" tabindex="-1"></a>    idx <span class="op">=</span> top5_idx[i].item()</span>
<span id="cb26-83"><a href="#cb26-83" aria-hidden="true" tabindex="-1"></a>    prob <span class="op">=</span> top5_prob[i].item()</span>
<span id="cb26-84"><a href="#cb26-84" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"</span><span class="sc">{</span>i<span class="op">+</span><span class="dv">1</span><span class="sc">}</span><span class="ss">. </span><span class="sc">{</span>labels[idx]<span class="sc">:20s}</span><span class="ss"> - </span><span class="sc">{</span>prob<span class="op">*</span><span class="dv">100</span><span class="sc">:.2f}</span><span class="ss">%"</span>)</span>
<span id="cb26-85"><a href="#cb26-85" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-86"><a href="#cb26-86" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"="</span><span class="op">*</span><span class="dv">50</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>The result:</p>
<div class="sourceCode" id="cb27"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb27-1"><a href="#cb27-1" aria-hidden="true" tabindex="-1"></a><span class="ex">Loading</span> image from images/cat.jpg...</span>
<span id="cb27-2"><a href="#cb27-2" aria-hidden="true" tabindex="-1"></a><span class="ex">Running</span> inference...</span>
<span id="cb27-3"><a href="#cb27-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-4"><a href="#cb27-4" aria-hidden="true" tabindex="-1"></a><span class="ex">==================================================</span></span>
<span id="cb27-5"><a href="#cb27-5" aria-hidden="true" tabindex="-1"></a><span class="ex">CLASSIFICATION</span> RESULTS</span>
<span id="cb27-6"><a href="#cb27-6" aria-hidden="true" tabindex="-1"></a><span class="ex">==================================================</span></span>
<span id="cb27-7"><a href="#cb27-7" aria-hidden="true" tabindex="-1"></a><span class="ex">Inference</span> Time: 86.12 ms</span>
<span id="cb27-8"><a href="#cb27-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-9"><a href="#cb27-9" aria-hidden="true" tabindex="-1"></a><span class="ex">Top</span> 5 Predictions:</span>
<span id="cb27-10"><a href="#cb27-10" aria-hidden="true" tabindex="-1"></a><span class="ex">--------------------------------------------------</span></span>
<span id="cb27-11"><a href="#cb27-11" aria-hidden="true" tabindex="-1"></a><span class="ex">1.</span> tiger cat            <span class="at">-</span> 47.44%</span>
<span id="cb27-12"><a href="#cb27-12" aria-hidden="true" tabindex="-1"></a><span class="ex">2.</span> Egyptian Mau         <span class="at">-</span> 37.61%</span>
<span id="cb27-13"><a href="#cb27-13" aria-hidden="true" tabindex="-1"></a><span class="ex">3.</span> lynx                 <span class="at">-</span> 6.91%</span>
<span id="cb27-14"><a href="#cb27-14" aria-hidden="true" tabindex="-1"></a><span class="ex">4.</span> tabby cat            <span class="at">-</span> 6.22%</span>
<span id="cb27-15"><a href="#cb27-15" aria-hidden="true" tabindex="-1"></a><span class="ex">5.</span> plastic bag          <span class="at">-</span> 0.47%</span>
<span id="cb27-16"><a href="#cb27-16" aria-hidden="true" tabindex="-1"></a><span class="ex">==================================================</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>The inference was OK, taking 86ms (first time). We can also verify the size of the saved Torch model</p>
<div class="sourceCode" id="cb28"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb28-1"><a href="#cb28-1" aria-hidden="true" tabindex="-1"></a><span class="fu">ls</span> <span class="at">-lh</span> ./models/mobilenet_v2.pth</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Which has <code>14Mb</code>.</p>
</section>
</section>
<section id="exporting-models-to-executorch-format" class="level2">
<h2 class="anchored" data-anchor-id="exporting-models-to-executorch-format">Exporting Models to EXECUTORCH Format</h2>
<p>Unlike TensorFlow Lite, where we downloaded pre-converted .tflite models, with EXECUTORCH, we typically export PyTorch models to the <code>.pte</code> (PyTorch EXECUTORCH) format ourselves. This gives us full control over the export process.</p>
<section id="understanding-the-export-process" class="level3">
<h3 class="anchored" data-anchor-id="understanding-the-export-process">Understanding the Export Process</h3>
<p>The EXECUTORCH export process involves several steps:</p>
<ol type="1">
<li><strong>Load a PyTorch model</strong> (pretrained or custom)</li>
<li><strong>Trace/script the model</strong> (convert to TorchScript)</li>
<li><strong>Export to EXECUTORCH format</strong> (.pte file)</li>
</ol>
<p><strong>Optional optimization steps:</strong></p>
<ul>
<li>Quantization (before or during export)</li>
<li>Backend delegation (XNNPACK, QNN, etc.)</li>
<li>Memory planning optimization</li>
</ul>
<p><strong>The complete ExecuTorch pipeline:</strong></p>
<ol type="1">
<li><code>export()</code> → Captures the model graph</li>
<li><code>to_edge()</code> → Converts to Edge dialect</li>
<li><code>to_executorch()</code> → Lowers to ExecuTorch format</li>
<li><code>.buffer</code> → Gets the binary data to save</li>
</ol>
<pre><code>PyTorch Model (.pt/.pth)
          ↓
    torch.export()         # Export to ExportedProgram
          ↓
    to_edge()              # Convert to Edge dialect
          ↓
    to_executorch()        # Generate EXECUTORCH program
          ↓
   .pte file               # Ready for edge deployment</code></pre>
</section>
<section id="exporting-mobilenet-v2-to-executorch" class="level3">
<h3 class="anchored" data-anchor-id="exporting-mobilenet-v2-to-executorch">Exporting MobileNet V2 to ExecuTorch</h3>
<p>Let’s export a MobileNet V2 model to EXECUTORCH basic format. Creating a Python script as <a href="https://github.com/Mjrovai/EdgeML-with-Raspberry-Pi/blob/main/EXECUTORCH/scripts/convert_mobv2_executorch.py">convert_mobv2_executorch.py</a></p>
<div class="sourceCode" id="cb30"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb30-1"><a href="#cb30-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb30-2"><a href="#cb30-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torchvision <span class="im">import</span> models</span>
<span id="cb30-3"><a href="#cb30-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> executorch.exir <span class="im">import</span> to_edge</span>
<span id="cb30-4"><a href="#cb30-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch.export <span class="im">import</span> export</span>
<span id="cb30-5"><a href="#cb30-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-6"><a href="#cb30-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Paths</span></span>
<span id="cb30-7"><a href="#cb30-7" aria-hidden="true" tabindex="-1"></a>PYTORCH_MODEL_PATH <span class="op">=</span> <span class="st">"models/mobilenet_v2.pth"</span></span>
<span id="cb30-8"><a href="#cb30-8" aria-hidden="true" tabindex="-1"></a>EXECUTORCH_MODEL_PATH <span class="op">=</span> <span class="st">"models/mobilenet_v2.pte"</span></span>
<span id="cb30-9"><a href="#cb30-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-10"><a href="#cb30-10" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Loading PyTorch model..."</span>)</span>
<span id="cb30-11"><a href="#cb30-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Load the saved model</span></span>
<span id="cb30-12"><a href="#cb30-12" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> models.mobilenet_v2()</span>
<span id="cb30-13"><a href="#cb30-13" aria-hidden="true" tabindex="-1"></a>model.load_state_dict(torch.load(PYTORCH_MODEL_PATH, map_location<span class="op">=</span><span class="st">'cpu'</span>))</span>
<span id="cb30-14"><a href="#cb30-14" aria-hidden="true" tabindex="-1"></a>model.<span class="bu">eval</span>()</span>
<span id="cb30-15"><a href="#cb30-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-16"><a href="#cb30-16" aria-hidden="true" tabindex="-1"></a><span class="co"># Create example input (batch_size=1, channels=3, height=224, width=224)</span></span>
<span id="cb30-17"><a href="#cb30-17" aria-hidden="true" tabindex="-1"></a>example_input <span class="op">=</span> (torch.randn(<span class="dv">1</span>, <span class="dv">3</span>, <span class="dv">224</span>, <span class="dv">224</span>),)</span>
<span id="cb30-18"><a href="#cb30-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-19"><a href="#cb30-19" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Exporting to ExecuTorch format..."</span>)</span>
<span id="cb30-20"><a href="#cb30-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-21"><a href="#cb30-21" aria-hidden="true" tabindex="-1"></a><span class="co"># Step 1: Export to EXIR (ExecuTorch Intermediate Representation)</span></span>
<span id="cb30-22"><a href="#cb30-22" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"  1. Capturing model with torch.export..."</span>)</span>
<span id="cb30-23"><a href="#cb30-23" aria-hidden="true" tabindex="-1"></a>exported_program <span class="op">=</span> export(model, example_input)</span>
<span id="cb30-24"><a href="#cb30-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-25"><a href="#cb30-25" aria-hidden="true" tabindex="-1"></a><span class="co"># Step 2: Convert to Edge dialect</span></span>
<span id="cb30-26"><a href="#cb30-26" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"  2. Converting to Edge dialect..."</span>)</span>
<span id="cb30-27"><a href="#cb30-27" aria-hidden="true" tabindex="-1"></a>edge_program <span class="op">=</span> to_edge(exported_program)</span>
<span id="cb30-28"><a href="#cb30-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-29"><a href="#cb30-29" aria-hidden="true" tabindex="-1"></a><span class="co"># Step 3: Convert to ExecuTorch program</span></span>
<span id="cb30-30"><a href="#cb30-30" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"  3. Lowering to ExecuTorch..."</span>)</span>
<span id="cb30-31"><a href="#cb30-31" aria-hidden="true" tabindex="-1"></a>executorch_program <span class="op">=</span> edge_program.to_executorch()</span>
<span id="cb30-32"><a href="#cb30-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-33"><a href="#cb30-33" aria-hidden="true" tabindex="-1"></a><span class="co"># Step 4: Save as .pte file</span></span>
<span id="cb30-34"><a href="#cb30-34" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"  4. Saving to .pte file..."</span>)</span>
<span id="cb30-35"><a href="#cb30-35" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> <span class="bu">open</span>(EXECUTORCH_MODEL_PATH, <span class="st">"wb"</span>) <span class="im">as</span> f:</span>
<span id="cb30-36"><a href="#cb30-36" aria-hidden="true" tabindex="-1"></a>    f.write(executorch_program.<span class="bu">buffer</span>)</span>
<span id="cb30-37"><a href="#cb30-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-38"><a href="#cb30-38" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">? Model successfully exported to </span><span class="sc">{</span>EXECUTORCH_MODEL_PATH<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb30-39"><a href="#cb30-39" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-40"><a href="#cb30-40" aria-hidden="true" tabindex="-1"></a><span class="co"># Display file sizes for comparison</span></span>
<span id="cb30-41"><a href="#cb30-41" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> os</span>
<span id="cb30-42"><a href="#cb30-42" aria-hidden="true" tabindex="-1"></a>pytorch_size <span class="op">=</span> os.path.getsize(PYTORCH_MODEL_PATH)<span class="op">/</span>(<span class="dv">1024</span><span class="op">*</span><span class="dv">1024</span>)</span>
<span id="cb30-43"><a href="#cb30-43" aria-hidden="true" tabindex="-1"></a>executorch_size <span class="op">=</span> os.path.getsize(EXECUTORCH_MODEL_PATH)<span class="op">/</span>(<span class="dv">1024</span><span class="op">*</span><span class="dv">1024</span>)</span>
<span id="cb30-44"><a href="#cb30-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-45"><a href="#cb30-45" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">"</span> <span class="op">+</span> <span class="st">"="</span><span class="op">*</span><span class="dv">50</span>)</span>
<span id="cb30-46"><a href="#cb30-46" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"MODEL SIZE COMPARISON"</span>)</span>
<span id="cb30-47"><a href="#cb30-47" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"="</span><span class="op">*</span><span class="dv">50</span>)</span>
<span id="cb30-48"><a href="#cb30-48" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"PyTorch model:    </span><span class="sc">{</span>pytorch_size<span class="sc">:.2f}</span><span class="ss"> MB"</span>)</span>
<span id="cb30-49"><a href="#cb30-49" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"ExecuTorch model: </span><span class="sc">{</span>executorch_size<span class="sc">:.2f}</span><span class="ss"> MB"</span>)</span>
<span id="cb30-50"><a href="#cb30-50" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Reduction:        </span><span class="sc">{</span>((pytorch_size <span class="op">-</span> executorch_size) <span class="op">\</span></span>
<span id="cb30-51"><a href="#cb30-51" aria-hidden="true" tabindex="-1"></a><span class="op">/</span>pytorch_size <span class="op">*</span> <span class="dv">100</span>)<span class="sc">:.1f}</span><span class="ss">%"</span>)</span>
<span id="cb30-52"><a href="#cb30-52" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"="</span><span class="op">*</span><span class="dv">50</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Runing the export script:</p>
<div class="sourceCode" id="cb31"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb31-1"><a href="#cb31-1" aria-hidden="true" tabindex="-1"></a><span class="ex">python</span> export_mobv2_executorch.py</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>We will get:</p>
<div class="sourceCode" id="cb32"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb32-1"><a href="#cb32-1" aria-hidden="true" tabindex="-1"></a><span class="ex">Loading</span> PyTorch model...</span>
<span id="cb32-2"><a href="#cb32-2" aria-hidden="true" tabindex="-1"></a><span class="ex">Exporting</span> to ExecuTorch format...</span>
<span id="cb32-3"><a href="#cb32-3" aria-hidden="true" tabindex="-1"></a>  <span class="ex">1.</span> Capturing model with torch.export...</span>
<span id="cb32-4"><a href="#cb32-4" aria-hidden="true" tabindex="-1"></a>  <span class="ex">2.</span> Converting to Edge dialect...</span>
<span id="cb32-5"><a href="#cb32-5" aria-hidden="true" tabindex="-1"></a>  <span class="ex">3.</span> Lowering to ExecuTorch...</span>
<span id="cb32-6"><a href="#cb32-6" aria-hidden="true" tabindex="-1"></a>  <span class="ex">4.</span> Saving to .pte file...</span>
<span id="cb32-7"><a href="#cb32-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-8"><a href="#cb32-8" aria-hidden="true" tabindex="-1"></a><span class="ex">?</span> Model successfully exported to models/mobilenet_v2.pte</span>
<span id="cb32-9"><a href="#cb32-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-10"><a href="#cb32-10" aria-hidden="true" tabindex="-1"></a><span class="ex">==================================================</span></span>
<span id="cb32-11"><a href="#cb32-11" aria-hidden="true" tabindex="-1"></a><span class="ex">MODEL</span> SIZE COMPARISON</span>
<span id="cb32-12"><a href="#cb32-12" aria-hidden="true" tabindex="-1"></a><span class="ex">==================================================</span></span>
<span id="cb32-13"><a href="#cb32-13" aria-hidden="true" tabindex="-1"></a><span class="ex">PyTorch</span> model:    13.60 MB</span>
<span id="cb32-14"><a href="#cb32-14" aria-hidden="true" tabindex="-1"></a><span class="ex">ExecuTorch</span> model: 13.58 MB</span>
<span id="cb32-15"><a href="#cb32-15" aria-hidden="true" tabindex="-1"></a><span class="ex">Reduction:</span>        0.2%</span>
<span id="cb32-16"><a href="#cb32-16" aria-hidden="true" tabindex="-1"></a><span class="ex">==================================================</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>The basic ExecuTorch conversion doesn’t compress the model much - it’s mainly for runtime efficiency. To get <strong>real size reduction</strong>, we need <strong>quantization</strong>, which we will explore later. But first, let’s do an inference test using the converted model.</p>
<p>Runing the script <a href="https://github.com/Mjrovai/EdgeML-with-Raspberry-Pi/blob/main/EXECUTORCH/scripts/mobv2_executorch.py">mobv2_executorch.py</a>:</p>
<div class="sourceCode" id="cb33"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb33-1"><a href="#cb33-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb33-2"><a href="#cb33-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torchvision.transforms <span class="im">as</span> transforms</span>
<span id="cb33-3"><a href="#cb33-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> PIL <span class="im">import</span> Image</span>
<span id="cb33-4"><a href="#cb33-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> time</span>
<span id="cb33-5"><a href="#cb33-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> json</span>
<span id="cb33-6"><a href="#cb33-6" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> executorch.extension.pybindings.portable_lib <span class="im">import</span> _load_for_executorch</span>
<span id="cb33-7"><a href="#cb33-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-8"><a href="#cb33-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Paths</span></span>
<span id="cb33-9"><a href="#cb33-9" aria-hidden="true" tabindex="-1"></a>EXECUTORCH_MODEL_PATH <span class="op">=</span> <span class="st">"models/mobilenet_v2.pte"</span></span>
<span id="cb33-10"><a href="#cb33-10" aria-hidden="true" tabindex="-1"></a>LABELS_PATH <span class="op">=</span> <span class="st">"models/imagenet_labels.json"</span></span>
<span id="cb33-11"><a href="#cb33-11" aria-hidden="true" tabindex="-1"></a>IMAGE_PATH <span class="op">=</span> <span class="st">"images/cat.jpg"</span></span>
<span id="cb33-12"><a href="#cb33-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-13"><a href="#cb33-13" aria-hidden="true" tabindex="-1"></a><span class="co"># Load labels</span></span>
<span id="cb33-14"><a href="#cb33-14" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Loading labels..."</span>)</span>
<span id="cb33-15"><a href="#cb33-15" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> <span class="bu">open</span>(LABELS_PATH, <span class="st">'r'</span>) <span class="im">as</span> f:</span>
<span id="cb33-16"><a href="#cb33-16" aria-hidden="true" tabindex="-1"></a>    labels <span class="op">=</span> json.load(f)</span>
<span id="cb33-17"><a href="#cb33-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-18"><a href="#cb33-18" aria-hidden="true" tabindex="-1"></a><span class="co"># Load ExecuTorch model</span></span>
<span id="cb33-19"><a href="#cb33-19" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Loading ExecuTorch model from </span><span class="sc">{</span>EXECUTORCH_MODEL_PATH<span class="sc">}</span><span class="ss">..."</span>)</span>
<span id="cb33-20"><a href="#cb33-20" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> _load_for_executorch(EXECUTORCH_MODEL_PATH)</span>
<span id="cb33-21"><a href="#cb33-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-22"><a href="#cb33-22" aria-hidden="true" tabindex="-1"></a><span class="co"># Define image preprocessing (same as PyTorch)</span></span>
<span id="cb33-23"><a href="#cb33-23" aria-hidden="true" tabindex="-1"></a>preprocess <span class="op">=</span> transforms.Compose([</span>
<span id="cb33-24"><a href="#cb33-24" aria-hidden="true" tabindex="-1"></a>    transforms.Resize(<span class="dv">256</span>),</span>
<span id="cb33-25"><a href="#cb33-25" aria-hidden="true" tabindex="-1"></a>    transforms.CenterCrop(<span class="dv">224</span>),</span>
<span id="cb33-26"><a href="#cb33-26" aria-hidden="true" tabindex="-1"></a>    transforms.ToTensor(),</span>
<span id="cb33-27"><a href="#cb33-27" aria-hidden="true" tabindex="-1"></a>    transforms.Normalize(mean<span class="op">=</span>[<span class="fl">0.485</span>, <span class="fl">0.456</span>, <span class="fl">0.406</span>], </span>
<span id="cb33-28"><a href="#cb33-28" aria-hidden="true" tabindex="-1"></a>                         std<span class="op">=</span>[<span class="fl">0.229</span>, <span class="fl">0.224</span>, <span class="fl">0.225</span>]),</span>
<span id="cb33-29"><a href="#cb33-29" aria-hidden="true" tabindex="-1"></a>])</span>
<span id="cb33-30"><a href="#cb33-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-31"><a href="#cb33-31" aria-hidden="true" tabindex="-1"></a><span class="co"># Load and preprocess image</span></span>
<span id="cb33-32"><a href="#cb33-32" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Loading image from </span><span class="sc">{</span>IMAGE_PATH<span class="sc">}</span><span class="ss">..."</span>)</span>
<span id="cb33-33"><a href="#cb33-33" aria-hidden="true" tabindex="-1"></a>img <span class="op">=</span> Image.<span class="bu">open</span>(IMAGE_PATH)</span>
<span id="cb33-34"><a href="#cb33-34" aria-hidden="true" tabindex="-1"></a>img_tensor <span class="op">=</span> preprocess(img)</span>
<span id="cb33-35"><a href="#cb33-35" aria-hidden="true" tabindex="-1"></a>batch <span class="op">=</span> img_tensor.unsqueeze(<span class="dv">0</span>)  <span class="co"># Add batch dimension</span></span>
<span id="cb33-36"><a href="#cb33-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-37"><a href="#cb33-37" aria-hidden="true" tabindex="-1"></a><span class="co"># Perform inference with timing</span></span>
<span id="cb33-38"><a href="#cb33-38" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Running ExecuTorch inference..."</span>)</span>
<span id="cb33-39"><a href="#cb33-39" aria-hidden="true" tabindex="-1"></a>start_time <span class="op">=</span> time.time()</span>
<span id="cb33-40"><a href="#cb33-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-41"><a href="#cb33-41" aria-hidden="true" tabindex="-1"></a><span class="co"># ExecuTorch expects a tuple of inputs</span></span>
<span id="cb33-42"><a href="#cb33-42" aria-hidden="true" tabindex="-1"></a>output <span class="op">=</span> model.forward((batch,))</span>
<span id="cb33-43"><a href="#cb33-43" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-44"><a href="#cb33-44" aria-hidden="true" tabindex="-1"></a>inference_time <span class="op">=</span> (time.time() <span class="op">-</span> start_time) <span class="op">*</span> <span class="dv">1000</span>  <span class="co"># Convert to ms</span></span>
<span id="cb33-45"><a href="#cb33-45" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-46"><a href="#cb33-46" aria-hidden="true" tabindex="-1"></a><span class="co"># Get predictions</span></span>
<span id="cb33-47"><a href="#cb33-47" aria-hidden="true" tabindex="-1"></a>output_tensor <span class="op">=</span> output[<span class="dv">0</span>]  <span class="co"># ExecuTorch returns a list</span></span>
<span id="cb33-48"><a href="#cb33-48" aria-hidden="true" tabindex="-1"></a>probabilities <span class="op">=</span> torch.nn.functional.softmax(output_tensor[<span class="dv">0</span>], dim<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb33-49"><a href="#cb33-49" aria-hidden="true" tabindex="-1"></a>top5_prob, top5_idx <span class="op">=</span> torch.topk(probabilities, <span class="dv">5</span>)</span>
<span id="cb33-50"><a href="#cb33-50" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-51"><a href="#cb33-51" aria-hidden="true" tabindex="-1"></a><span class="co"># Display results</span></span>
<span id="cb33-52"><a href="#cb33-52" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">"</span> <span class="op">+</span> <span class="st">"="</span><span class="op">*</span><span class="dv">50</span>)</span>
<span id="cb33-53"><a href="#cb33-53" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"EXECUTORCH CLASSIFICATION RESULTS"</span>)</span>
<span id="cb33-54"><a href="#cb33-54" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"="</span><span class="op">*</span><span class="dv">50</span>)</span>
<span id="cb33-55"><a href="#cb33-55" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Inference Time: </span><span class="sc">{</span>inference_time<span class="sc">:.2f}</span><span class="ss"> ms</span><span class="ch">\n</span><span class="ss">"</span>)</span>
<span id="cb33-56"><a href="#cb33-56" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Top 5 Predictions:"</span>)</span>
<span id="cb33-57"><a href="#cb33-57" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"-"</span><span class="op">*</span><span class="dv">50</span>)</span>
<span id="cb33-58"><a href="#cb33-58" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-59"><a href="#cb33-59" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">5</span>):</span>
<span id="cb33-60"><a href="#cb33-60" aria-hidden="true" tabindex="-1"></a>    idx <span class="op">=</span> top5_idx[i].item()</span>
<span id="cb33-61"><a href="#cb33-61" aria-hidden="true" tabindex="-1"></a>    prob <span class="op">=</span> top5_prob[i].item()</span>
<span id="cb33-62"><a href="#cb33-62" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"</span><span class="sc">{</span>i<span class="op">+</span><span class="dv">1</span><span class="sc">}</span><span class="ss">. </span><span class="sc">{</span>labels[idx]<span class="sc">:20s}</span><span class="ss"> - </span><span class="sc">{</span>prob<span class="op">*</span><span class="dv">100</span><span class="sc">:.2f}</span><span class="ss">%"</span>)</span>
<span id="cb33-63"><a href="#cb33-63" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-64"><a href="#cb33-64" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"="</span><span class="op">*</span><span class="dv">50</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>As a result, we got a similar inference result, but a much higher latency (almost 2.5 seconds), which was unexpected.</p>
<div class="sourceCode" id="cb34"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb34-1"><a href="#cb34-1" aria-hidden="true" tabindex="-1"></a><span class="ex">Loading</span> labels...</span>
<span id="cb34-2"><a href="#cb34-2" aria-hidden="true" tabindex="-1"></a><span class="ex">Loading</span> ExecuTorch model from models/mobilenet_v2.pte...</span>
<span id="cb34-3"><a href="#cb34-3" aria-hidden="true" tabindex="-1"></a><span class="ex">Loading</span> image from images/cat.jpg...</span>
<span id="cb34-4"><a href="#cb34-4" aria-hidden="true" tabindex="-1"></a><span class="ex">Running</span> ExecuTorch inference...</span>
<span id="cb34-5"><a href="#cb34-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-6"><a href="#cb34-6" aria-hidden="true" tabindex="-1"></a><span class="ex">==================================================</span></span>
<span id="cb34-7"><a href="#cb34-7" aria-hidden="true" tabindex="-1"></a><span class="ex">EXECUTORCH</span> CLASSIFICATION RESULTS</span>
<span id="cb34-8"><a href="#cb34-8" aria-hidden="true" tabindex="-1"></a><span class="ex">==================================================</span></span>
<span id="cb34-9"><a href="#cb34-9" aria-hidden="true" tabindex="-1"></a><span class="ex">Inference</span> Time: 2445.78 ms</span>
<span id="cb34-10"><a href="#cb34-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-11"><a href="#cb34-11" aria-hidden="true" tabindex="-1"></a><span class="ex">Top</span> 5 Predictions:</span>
<span id="cb34-12"><a href="#cb34-12" aria-hidden="true" tabindex="-1"></a><span class="ex">--------------------------------------------------</span></span>
<span id="cb34-13"><a href="#cb34-13" aria-hidden="true" tabindex="-1"></a><span class="ex">1.</span> tiger cat            <span class="at">-</span> 47.44%</span>
<span id="cb34-14"><a href="#cb34-14" aria-hidden="true" tabindex="-1"></a><span class="ex">2.</span> Egyptian Mau         <span class="at">-</span> 37.61%</span>
<span id="cb34-15"><a href="#cb34-15" aria-hidden="true" tabindex="-1"></a><span class="ex">3.</span> lynx                 <span class="at">-</span> 6.91%</span>
<span id="cb34-16"><a href="#cb34-16" aria-hidden="true" tabindex="-1"></a><span class="ex">4.</span> tabby cat            <span class="at">-</span> 6.22%</span>
<span id="cb34-17"><a href="#cb34-17" aria-hidden="true" tabindex="-1"></a><span class="ex">5.</span> plastic bag          <span class="at">-</span> 0.47%</span>
<span id="cb34-18"><a href="#cb34-18" aria-hidden="true" tabindex="-1"></a><span class="ex">==================================================</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>That export path produces a generic ExecuTorch CPU graph with reference kernels and no backend optimizations or fusions, so significantly higher latency than PyTorch is expected for MobileNet_v2 on a Pi 5.</p>
<p>ExecuTorch is designed to shine when delegated to a backend (XNNPACK, OpenVINO, etc.), where large subgraphs are lowered into highly optimized kernels. Without a delegate, most of the graph runs on the generic portable path, which is known to be significantly slower than PyTorch for many models.</p>
<p>So, let’s export the .pth model again with a CPU‑optimized backend (e.g., XNNPACK) and run with that backend enabled; this alone should reduce latency when compared with the naïve interpreter path.</p>
<p>Here’s the corrected conversion script with XNNPACK delegation (<a href="https://github.com/Mjrovai/EdgeML-with-Raspberry-Pi/blob/main/EXECUTORCH/scripts/convert_mobv2_xnnpack.py">convert_mobv2_xnnpack.py</a>):</p>
<div class="sourceCode" id="cb35"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb35-1"><a href="#cb35-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb35-2"><a href="#cb35-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torchvision <span class="im">import</span> models</span>
<span id="cb35-3"><a href="#cb35-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> executorch.exir <span class="im">import</span> to_edge</span>
<span id="cb35-4"><a href="#cb35-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch.export <span class="im">import</span> export</span>
<span id="cb35-5"><a href="#cb35-5" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> executorch.backends.xnnpack.partition.xnnpack_partitioner <span class="op">\</span></span>
<span id="cb35-6"><a href="#cb35-6" aria-hidden="true" tabindex="-1"></a>     <span class="im">import</span> XnnpackPartitioner</span>
<span id="cb35-7"><a href="#cb35-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-8"><a href="#cb35-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Paths</span></span>
<span id="cb35-9"><a href="#cb35-9" aria-hidden="true" tabindex="-1"></a>PYTORCH_MODEL_PATH <span class="op">=</span> <span class="st">"models/mobilenet_v2.pth"</span></span>
<span id="cb35-10"><a href="#cb35-10" aria-hidden="true" tabindex="-1"></a>EXECUTORCH_MODEL_PATH <span class="op">=</span> <span class="st">"models/mobilenet_v2_xnnpack.pte"</span></span>
<span id="cb35-11"><a href="#cb35-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-12"><a href="#cb35-12" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Loading PyTorch model..."</span>)</span>
<span id="cb35-13"><a href="#cb35-13" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> models.mobilenet_v2()</span>
<span id="cb35-14"><a href="#cb35-14" aria-hidden="true" tabindex="-1"></a>model.load_state_dict(torch.load(PYTORCH_MODEL_PATH, map_location<span class="op">=</span><span class="st">'cpu'</span>))</span>
<span id="cb35-15"><a href="#cb35-15" aria-hidden="true" tabindex="-1"></a>model.<span class="bu">eval</span>()</span>
<span id="cb35-16"><a href="#cb35-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-17"><a href="#cb35-17" aria-hidden="true" tabindex="-1"></a><span class="co"># Create example input</span></span>
<span id="cb35-18"><a href="#cb35-18" aria-hidden="true" tabindex="-1"></a>example_input <span class="op">=</span> (torch.randn(<span class="dv">1</span>, <span class="dv">3</span>, <span class="dv">224</span>, <span class="dv">224</span>),)</span>
<span id="cb35-19"><a href="#cb35-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-20"><a href="#cb35-20" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Exporting to ExecuTorch with XNNPACK backend..."</span>)</span>
<span id="cb35-21"><a href="#cb35-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-22"><a href="#cb35-22" aria-hidden="true" tabindex="-1"></a><span class="co"># Step 1: Export to EXIR</span></span>
<span id="cb35-23"><a href="#cb35-23" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"  1. Capturing model with torch.export..."</span>)</span>
<span id="cb35-24"><a href="#cb35-24" aria-hidden="true" tabindex="-1"></a>exported_program <span class="op">=</span> export(model, example_input)</span>
<span id="cb35-25"><a href="#cb35-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-26"><a href="#cb35-26" aria-hidden="true" tabindex="-1"></a><span class="co"># Step 2: Convert to Edge dialect with XNNPACK partitioner</span></span>
<span id="cb35-27"><a href="#cb35-27" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"  2. Converting to Edge dialect with XNNPACK delegation..."</span>)</span>
<span id="cb35-28"><a href="#cb35-28" aria-hidden="true" tabindex="-1"></a>edge_program <span class="op">=</span> to_edge(exported_program)</span>
<span id="cb35-29"><a href="#cb35-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-30"><a href="#cb35-30" aria-hidden="true" tabindex="-1"></a><span class="co"># Step 3: Partition for XNNPACK backend</span></span>
<span id="cb35-31"><a href="#cb35-31" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"  3. Delegating to XNNPACK backend..."</span>)</span>
<span id="cb35-32"><a href="#cb35-32" aria-hidden="true" tabindex="-1"></a>edge_program <span class="op">=</span> edge_program.to_backend(XnnpackPartitioner())</span>
<span id="cb35-33"><a href="#cb35-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-34"><a href="#cb35-34" aria-hidden="true" tabindex="-1"></a><span class="co"># Step 4: Convert to ExecuTorch program</span></span>
<span id="cb35-35"><a href="#cb35-35" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"  4. Lowering to ExecuTorch..."</span>)</span>
<span id="cb35-36"><a href="#cb35-36" aria-hidden="true" tabindex="-1"></a>executorch_program <span class="op">=</span> edge_program.to_executorch()</span>
<span id="cb35-37"><a href="#cb35-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-38"><a href="#cb35-38" aria-hidden="true" tabindex="-1"></a><span class="co"># Step 5: Save as .pte file</span></span>
<span id="cb35-39"><a href="#cb35-39" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"  5. Saving to .pte file..."</span>)</span>
<span id="cb35-40"><a href="#cb35-40" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> <span class="bu">open</span>(EXECUTORCH_MODEL_PATH, <span class="st">"wb"</span>) <span class="im">as</span> f:</span>
<span id="cb35-41"><a href="#cb35-41" aria-hidden="true" tabindex="-1"></a>    f.write(executorch_program.<span class="bu">buffer</span>)</span>
<span id="cb35-42"><a href="#cb35-42" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-43"><a href="#cb35-43" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">? Model successfully exported to </span><span class="sc">{</span>EXECUTORCH_MODEL_PATH<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb35-44"><a href="#cb35-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-45"><a href="#cb35-45" aria-hidden="true" tabindex="-1"></a><span class="co"># Display file size</span></span>
<span id="cb35-46"><a href="#cb35-46" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> os</span>
<span id="cb35-47"><a href="#cb35-47" aria-hidden="true" tabindex="-1"></a>pytorch_size <span class="op">=</span> os.path.getsize(PYTORCH_MODEL_PATH) <span class="op">/</span> (<span class="dv">1024</span> <span class="op">*</span> <span class="dv">1024</span>)</span>
<span id="cb35-48"><a href="#cb35-48" aria-hidden="true" tabindex="-1"></a>executorch_size <span class="op">=</span> os.path.getsize(EXECUTORCH_MODEL_PATH) <span class="op">/</span> (<span class="dv">1024</span> <span class="op">*</span> <span class="dv">1024</span>)</span>
<span id="cb35-49"><a href="#cb35-49" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-50"><a href="#cb35-50" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">"</span> <span class="op">+</span> <span class="st">"="</span><span class="op">*</span><span class="dv">50</span>)</span>
<span id="cb35-51"><a href="#cb35-51" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"MODEL SIZE COMPARISON"</span>)</span>
<span id="cb35-52"><a href="#cb35-52" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"="</span><span class="op">*</span><span class="dv">50</span>)</span>
<span id="cb35-53"><a href="#cb35-53" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"PyTorch model:           </span><span class="sc">{</span>pytorch_size<span class="sc">:.2f}</span><span class="ss"> MB"</span>)</span>
<span id="cb35-54"><a href="#cb35-54" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"ExecuTorch+XNNPACK:      </span><span class="sc">{</span>executorch_size<span class="sc">:.2f}</span><span class="ss"> MB"</span>)</span>
<span id="cb35-55"><a href="#cb35-55" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"="</span><span class="op">*</span><span class="dv">50</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Runing it we get:</p>
<div class="sourceCode" id="cb36"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb36-1"><a href="#cb36-1" aria-hidden="true" tabindex="-1"></a><span class="ex">Loading</span> PyTorch model...</span>
<span id="cb36-2"><a href="#cb36-2" aria-hidden="true" tabindex="-1"></a><span class="ex">Exporting</span> to ExecuTorch with XNNPACK backend...</span>
<span id="cb36-3"><a href="#cb36-3" aria-hidden="true" tabindex="-1"></a>  <span class="ex">1.</span> Capturing model with torch.export...</span>
<span id="cb36-4"><a href="#cb36-4" aria-hidden="true" tabindex="-1"></a>  <span class="ex">2.</span> Lowering to Edge with XNNPACK delegation...</span>
<span id="cb36-5"><a href="#cb36-5" aria-hidden="true" tabindex="-1"></a>  <span class="ex">3.</span> Converting to ExecuTorch...</span>
<span id="cb36-6"><a href="#cb36-6" aria-hidden="true" tabindex="-1"></a>  <span class="ex">4.</span> Saving to .pte file...</span>
<span id="cb36-7"><a href="#cb36-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-8"><a href="#cb36-8" aria-hidden="true" tabindex="-1"></a><span class="ex">?</span> Model successfully exported to models/mobilenet_v2_xnnpack.pte</span>
<span id="cb36-9"><a href="#cb36-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-10"><a href="#cb36-10" aria-hidden="true" tabindex="-1"></a><span class="ex">==================================================</span></span>
<span id="cb36-11"><a href="#cb36-11" aria-hidden="true" tabindex="-1"></a><span class="ex">MODEL</span> SIZE COMPARISON</span>
<span id="cb36-12"><a href="#cb36-12" aria-hidden="true" tabindex="-1"></a><span class="ex">==================================================</span></span>
<span id="cb36-13"><a href="#cb36-13" aria-hidden="true" tabindex="-1"></a><span class="ex">PyTorch</span> model:           13.60 MB</span>
<span id="cb36-14"><a href="#cb36-14" aria-hidden="true" tabindex="-1"></a><span class="ex">ExecuTorch+XNNPACK:</span>      13.35 MB</span>
<span id="cb36-15"><a href="#cb36-15" aria-hidden="true" tabindex="-1"></a><span class="ex">==================================================</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>We did not gain in terms of size, but let’s run the same inference script as before, with this new converted model, to inspect the latency:</p>
<p>the result:</p>
<pre><code>Loading labels...
Loading ExecuTorch model from models/mobilenet_v2_xnnpack.pte...
Loading image from images/cat.jpg...
Running ExecuTorch inference...

==================================================
EXECUTORCH CLASSIFICATION RESULTS
==================================================
Inference Time: 19.95 ms

Top 5 Predictions:
--------------------------------------------------
1. tiger cat            - 47.44%
2. Egyptian Mau         - 37.61%
3. lynx                 - 6.91%
4. tabby cat            - 6.22%
5. plastic bag          - 0.47%
==================================================</code></pre>
<p>Now, the ExecuTorch runtime detects the backend automatically from the .pte file metadata. We have achieved much faster inference: 20ms instead of 2445ms. This latency is, in fact, several times faster than PyTorch.</p>
<p><strong>Why XNNPACK is so fast:</strong></p>
<ul>
<li>✅ ARM NEON SIMD optimizations</li>
<li>✅ Multi-threading on Raspberry Pi’s 4 cores</li>
<li>✅ Operator fusion and memory optimization</li>
<li>✅ Cache-friendly memory access patterns</li>
</ul>
<p><strong>This demonstrates:</strong></p>
<ol type="1">
<li>ExecuTorch (basic) without a backend = <strong>don’t use in production</strong></li>
<li>ExecuTorch + XNNPACK = <strong>production-ready edge AI</strong></li>
<li>Raspberry Pi 5 can do <strong>50+ inferences/second</strong> at this speed!</li>
</ol>
<p>Now we can add quantization to get an even smaller model size while maintaining (or even increasing) this speed!</p>
</section>
<section id="model-quantization" class="level3">
<h3 class="anchored" data-anchor-id="model-quantization">Model Quantization</h3>
<p>Quantization reduces model size and can further improve inference speed. EXECUTORCH supports PyTorch’s native quantization.</p>
<p><strong>Quantization Overview</strong></p>
<p>Quantization is a technique that reduces the precision of numbers used in a model’s computations and stored weights—typically from 32-bit floats to 8-bit integers. This reduces the model’s memory footprint, speeds up inference, and lowers power consumption, often with minimal loss in accuracy.</p>
<p>Quantization is especially important for deploying models on edge devices such as wearables, embedded systems, and microcontrollers, which often have limited compute, memory, and battery capacity. By quantizing models, we can make them significantly more efficient and better suited to these resource-constrained environments.</p>
<p><strong>Quantization in ExecuTorch</strong></p>
<p>ExecuTorch uses <a href="https://github.com/pytorch/ao/tree/main/torchao">torchao</a> as its quantization library. This integration allows ExecuTorch to leverage PyTorch-native tools for preparing, calibrating, and converting quantized models.</p>
<p>Quantization in ExecuTorch is backend-specific. Each backend defines how models should be quantized based on its hardware capabilities. Most ExecuTorch backends use the torchao <a href="https://docs.pytorch.org/ao/main/tutorials_source/pt2e_quant_ptq.html">PT2E quantization</a> flow, which works with models exported with torch.export and enables tailored quantization for each backend.</p>
<p>For a quantized XNNPACK <code>.pte</code> we need a different pipeline: PT2E quantization (with <code>XNNPACKQuantizer</code>), then lowering with <code>XnnpackPartitioner</code> before <code>to_executorch()</code>. Otherwise, we will hit errors or get an undelegated model.</p>
<p>For the conversion, we need: (1) calibrate with real, preprocessed images, and (2) compute the quantized <code>.pte</code> size after you actually write the file.</p>
<p>First, let us create a small <code>calib_images/</code> folder (e.g., 50–100 natural images across a few classes). A simple way is to reuse an existing dataset (e.g., CIFAR‑10) and save 50–100 images into <code>calib_images/</code> with an ImageNet‑style folder layout.</p>
<p>The script <a href="https://github.com/Mjrovai/EdgeML-with-Raspberry-Pi/blob/main/EXECUTORCH/scripts/gen_calibr_images.py">gen_calibr_images.py</a> will: • Download CIFAR‑10. • Pick 10 classes × 10 images each = 100 images. • Save them under <code>calib_images/&lt;class_name&gt;/img_XXX.jpg</code>.</p>
<div class="sourceCode" id="cb38"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb38-1"><a href="#cb38-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> os</span>
<span id="cb38-2"><a href="#cb38-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> pathlib <span class="im">import</span> Path</span>
<span id="cb38-3"><a href="#cb38-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-4"><a href="#cb38-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb38-5"><a href="#cb38-5" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torchvision <span class="im">import</span> datasets, transforms</span>
<span id="cb38-6"><a href="#cb38-6" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torchvision.utils <span class="im">import</span> save_image</span>
<span id="cb38-7"><a href="#cb38-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-8"><a href="#cb38-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Where to store calibration images</span></span>
<span id="cb38-9"><a href="#cb38-9" aria-hidden="true" tabindex="-1"></a>OUT_ROOT <span class="op">=</span> Path(<span class="st">"calib_images"</span>)</span>
<span id="cb38-10"><a href="#cb38-10" aria-hidden="true" tabindex="-1"></a>OUT_ROOT.mkdir(parents<span class="op">=</span><span class="va">True</span>, exist_ok<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb38-11"><a href="#cb38-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-12"><a href="#cb38-12" aria-hidden="true" tabindex="-1"></a><span class="co"># 1) Load a small, natural-image dataset (CIFAR-10)</span></span>
<span id="cb38-13"><a href="#cb38-13" aria-hidden="true" tabindex="-1"></a>transform <span class="op">=</span> transforms.ToTensor()  <span class="co"># we will NOT normalize here</span></span>
<span id="cb38-14"><a href="#cb38-14" aria-hidden="true" tabindex="-1"></a>dataset <span class="op">=</span> datasets.CIFAR10(</span>
<span id="cb38-15"><a href="#cb38-15" aria-hidden="true" tabindex="-1"></a>    root<span class="op">=</span><span class="st">"data"</span>,</span>
<span id="cb38-16"><a href="#cb38-16" aria-hidden="true" tabindex="-1"></a>    train<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb38-17"><a href="#cb38-17" aria-hidden="true" tabindex="-1"></a>    download<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb38-18"><a href="#cb38-18" aria-hidden="true" tabindex="-1"></a>    transform<span class="op">=</span>transform,</span>
<span id="cb38-19"><a href="#cb38-19" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb38-20"><a href="#cb38-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-21"><a href="#cb38-21" aria-hidden="true" tabindex="-1"></a><span class="co"># 2) Map label index -&gt; class name (CIFAR-10 has 10 classes)</span></span>
<span id="cb38-22"><a href="#cb38-22" aria-hidden="true" tabindex="-1"></a>classes <span class="op">=</span> dataset.classes  <span class="co"># ['airplane', 'automobile', ..., 'truck']</span></span>
<span id="cb38-23"><a href="#cb38-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-24"><a href="#cb38-24" aria-hidden="true" tabindex="-1"></a><span class="co"># 3) Choose how many classes and images per class</span></span>
<span id="cb38-25"><a href="#cb38-25" aria-hidden="true" tabindex="-1"></a>num_classes <span class="op">=</span> <span class="dv">10</span></span>
<span id="cb38-26"><a href="#cb38-26" aria-hidden="true" tabindex="-1"></a>images_per_class <span class="op">=</span> <span class="dv">10</span>   <span class="co"># 10 x 10 = 100 images</span></span>
<span id="cb38-27"><a href="#cb38-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-28"><a href="#cb38-28" aria-hidden="true" tabindex="-1"></a><span class="co"># 4) Collect and save images</span></span>
<span id="cb38-29"><a href="#cb38-29" aria-hidden="true" tabindex="-1"></a>counts <span class="op">=</span> {cls: <span class="dv">0</span> <span class="cf">for</span> cls <span class="kw">in</span> classes[:num_classes]}</span>
<span id="cb38-30"><a href="#cb38-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-31"><a href="#cb38-31" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> img, label <span class="kw">in</span> dataset:</span>
<span id="cb38-32"><a href="#cb38-32" aria-hidden="true" tabindex="-1"></a>    cls_name <span class="op">=</span> classes[label]</span>
<span id="cb38-33"><a href="#cb38-33" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> cls_name <span class="kw">not</span> <span class="kw">in</span> counts:</span>
<span id="cb38-34"><a href="#cb38-34" aria-hidden="true" tabindex="-1"></a>        <span class="cf">continue</span></span>
<span id="cb38-35"><a href="#cb38-35" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> counts[cls_name] <span class="op">&gt;=</span> images_per_class:</span>
<span id="cb38-36"><a href="#cb38-36" aria-hidden="true" tabindex="-1"></a>        <span class="cf">continue</span></span>
<span id="cb38-37"><a href="#cb38-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-38"><a href="#cb38-38" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Make class subdir</span></span>
<span id="cb38-39"><a href="#cb38-39" aria-hidden="true" tabindex="-1"></a>    class_dir <span class="op">=</span> OUT_ROOT <span class="op">/</span> cls_name</span>
<span id="cb38-40"><a href="#cb38-40" aria-hidden="true" tabindex="-1"></a>    class_dir.mkdir(parents<span class="op">=</span><span class="va">True</span>, exist_ok<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb38-41"><a href="#cb38-41" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-42"><a href="#cb38-42" aria-hidden="true" tabindex="-1"></a>    idx <span class="op">=</span> counts[cls_name]</span>
<span id="cb38-43"><a href="#cb38-43" aria-hidden="true" tabindex="-1"></a>    out_path <span class="op">=</span> class_dir <span class="op">/</span> <span class="ss">f"img_</span><span class="sc">{</span>idx<span class="sc">:04d}</span><span class="ss">.jpg"</span></span>
<span id="cb38-44"><a href="#cb38-44" aria-hidden="true" tabindex="-1"></a>    save_image(img, out_path)</span>
<span id="cb38-45"><a href="#cb38-45" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-46"><a href="#cb38-46" aria-hidden="true" tabindex="-1"></a>    counts[cls_name] <span class="op">+=</span> <span class="dv">1</span></span>
<span id="cb38-47"><a href="#cb38-47" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-48"><a href="#cb38-48" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Stop when we have enough</span></span>
<span id="cb38-49"><a href="#cb38-49" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> <span class="bu">all</span>(counts[c] <span class="op">&gt;=</span> images_per_class <span class="cf">for</span> c <span class="kw">in</span> counts):</span>
<span id="cb38-50"><a href="#cb38-50" aria-hidden="true" tabindex="-1"></a>        <span class="cf">break</span></span>
<span id="cb38-51"><a href="#cb38-51" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-52"><a href="#cb38-52" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Saved calibration images:"</span>)</span>
<span id="cb38-53"><a href="#cb38-53" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> cls_name, n <span class="kw">in</span> counts.items():</span>
<span id="cb38-54"><a href="#cb38-54" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"  </span><span class="sc">{</span>cls_name<span class="sc">}</span><span class="ss">: </span><span class="sc">{</span>n<span class="sc">}</span><span class="ss"> images"</span>)</span>
<span id="cb38-55"><a href="#cb38-55" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">Root folder: </span><span class="sc">{</span>OUT_ROOT<span class="sc">.</span>resolve()<span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Let’s use the inference script <a href="https://github.com/Mjrovai/EdgeML-with-Raspberry-Pi/blob/main/EXECUTORCH/scripts/convert_mobv2_xnnpack_int8.py">convert_mobv2_xnnpack_int8.py</a>, which is the same inference script as before, with this new int8 converted model to inspect the latency:</p>
<div class="sourceCode" id="cb39"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb39-1"><a href="#cb39-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> os</span>
<span id="cb39-2"><a href="#cb39-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb39-3"><a href="#cb39-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torchvision.models <span class="im">as</span> models</span>
<span id="cb39-4"><a href="#cb39-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torchvision.transforms <span class="im">as</span> transforms</span>
<span id="cb39-5"><a href="#cb39-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torchvision.datasets <span class="im">as</span> datasets</span>
<span id="cb39-6"><a href="#cb39-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-7"><a href="#cb39-7" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch.export <span class="im">import</span> export</span>
<span id="cb39-8"><a href="#cb39-8" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torchao.quantization.pt2e.quantize_pt2e <span class="im">import</span> (</span>
<span id="cb39-9"><a href="#cb39-9" aria-hidden="true" tabindex="-1"></a>    prepare_pt2e,</span>
<span id="cb39-10"><a href="#cb39-10" aria-hidden="true" tabindex="-1"></a>    convert_pt2e,</span>
<span id="cb39-11"><a href="#cb39-11" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb39-12"><a href="#cb39-12" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> executorch.backends.xnnpack.quantizer.xnnpack_quantizer <span class="im">import</span> (</span>
<span id="cb39-13"><a href="#cb39-13" aria-hidden="true" tabindex="-1"></a>    get_symmetric_quantization_config,</span>
<span id="cb39-14"><a href="#cb39-14" aria-hidden="true" tabindex="-1"></a>    XNNPACKQuantizer,</span>
<span id="cb39-15"><a href="#cb39-15" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb39-16"><a href="#cb39-16" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> executorch.backends.xnnpack.partition.xnnpack_partitioner <span class="im">import</span> (</span>
<span id="cb39-17"><a href="#cb39-17" aria-hidden="true" tabindex="-1"></a>    XnnpackPartitioner,</span>
<span id="cb39-18"><a href="#cb39-18" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb39-19"><a href="#cb39-19" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> executorch.exir <span class="im">import</span> to_edge_transform_and_lower</span>
<span id="cb39-20"><a href="#cb39-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-21"><a href="#cb39-21" aria-hidden="true" tabindex="-1"></a>PYTORCH_MODEL_PATH <span class="op">=</span> <span class="st">"models/mobilenet_v2.pth"</span></span>
<span id="cb39-22"><a href="#cb39-22" aria-hidden="true" tabindex="-1"></a>EXECUTORCH_QUANTIZED_PATH <span class="op">=</span> <span class="st">"models/mobilenet_v2_quantized_xnnpack.pte"</span></span>
<span id="cb39-23"><a href="#cb39-23" aria-hidden="true" tabindex="-1"></a>CALIB_IMAGES_DIR <span class="op">=</span> <span class="st">"calib_images"</span>   <span class="co"># &lt;-- put some natural images here</span></span>
<span id="cb39-24"><a href="#cb39-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-25"><a href="#cb39-25" aria-hidden="true" tabindex="-1"></a><span class="co"># 1) Load FP32 model</span></span>
<span id="cb39-26"><a href="#cb39-26" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> models.mobilenet_v2()</span>
<span id="cb39-27"><a href="#cb39-27" aria-hidden="true" tabindex="-1"></a>model.load_state_dict(torch.load(PYTORCH_MODEL_PATH, map_location<span class="op">=</span><span class="st">"cpu"</span>))</span>
<span id="cb39-28"><a href="#cb39-28" aria-hidden="true" tabindex="-1"></a>model.<span class="bu">eval</span>()</span>
<span id="cb39-29"><a href="#cb39-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-30"><a href="#cb39-30" aria-hidden="true" tabindex="-1"></a><span class="co"># Example input only defines shapes for export</span></span>
<span id="cb39-31"><a href="#cb39-31" aria-hidden="true" tabindex="-1"></a>example_inputs <span class="op">=</span> (torch.randn(<span class="dv">1</span>, <span class="dv">3</span>, <span class="dv">224</span>, <span class="dv">224</span>),)</span>
<span id="cb39-32"><a href="#cb39-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-33"><a href="#cb39-33" aria-hidden="true" tabindex="-1"></a><span class="co"># 2) Configure XNNPACK quantizer (global symmetric config)</span></span>
<span id="cb39-34"><a href="#cb39-34" aria-hidden="true" tabindex="-1"></a>qparams <span class="op">=</span> get_symmetric_quantization_config(is_per_channel<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb39-35"><a href="#cb39-35" aria-hidden="true" tabindex="-1"></a>quantizer <span class="op">=</span> XNNPACKQuantizer()</span>
<span id="cb39-36"><a href="#cb39-36" aria-hidden="true" tabindex="-1"></a>quantizer.set_global(qparams)</span>
<span id="cb39-37"><a href="#cb39-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-38"><a href="#cb39-38" aria-hidden="true" tabindex="-1"></a><span class="co"># 3) Export float model for PT2E and prepare for quantization</span></span>
<span id="cb39-39"><a href="#cb39-39" aria-hidden="true" tabindex="-1"></a>exported <span class="op">=</span> torch.export.export(model, example_inputs)</span>
<span id="cb39-40"><a href="#cb39-40" aria-hidden="true" tabindex="-1"></a>training_ep <span class="op">=</span> exported.module()</span>
<span id="cb39-41"><a href="#cb39-41" aria-hidden="true" tabindex="-1"></a>prepared <span class="op">=</span> prepare_pt2e(training_ep, quantizer)</span>
<span id="cb39-42"><a href="#cb39-42" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-43"><a href="#cb39-43" aria-hidden="true" tabindex="-1"></a><span class="co"># 4) Calibration with REAL images using SAME preprocessing as inference</span></span>
<span id="cb39-44"><a href="#cb39-44" aria-hidden="true" tabindex="-1"></a>calib_transform <span class="op">=</span> transforms.Compose([</span>
<span id="cb39-45"><a href="#cb39-45" aria-hidden="true" tabindex="-1"></a>    transforms.Resize(<span class="dv">256</span>),</span>
<span id="cb39-46"><a href="#cb39-46" aria-hidden="true" tabindex="-1"></a>    transforms.CenterCrop(<span class="dv">224</span>),</span>
<span id="cb39-47"><a href="#cb39-47" aria-hidden="true" tabindex="-1"></a>    transforms.ToTensor(),</span>
<span id="cb39-48"><a href="#cb39-48" aria-hidden="true" tabindex="-1"></a>    transforms.Normalize(mean<span class="op">=</span>[<span class="fl">0.485</span>, <span class="fl">0.456</span>, <span class="fl">0.406</span>],</span>
<span id="cb39-49"><a href="#cb39-49" aria-hidden="true" tabindex="-1"></a>                         std<span class="op">=</span>[<span class="fl">0.229</span>, <span class="fl">0.224</span>, <span class="fl">0.225</span>]),</span>
<span id="cb39-50"><a href="#cb39-50" aria-hidden="true" tabindex="-1"></a>])</span>
<span id="cb39-51"><a href="#cb39-51" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-52"><a href="#cb39-52" aria-hidden="true" tabindex="-1"></a>calib_dataset <span class="op">=</span> datasets.ImageFolder(CALIB_IMAGES_DIR, </span>
<span id="cb39-53"><a href="#cb39-53" aria-hidden="true" tabindex="-1"></a>                                     transform<span class="op">=</span>calib_transform)</span>
<span id="cb39-54"><a href="#cb39-54" aria-hidden="true" tabindex="-1"></a>calib_loader <span class="op">=</span> torch.utils.data.DataLoader(</span>
<span id="cb39-55"><a href="#cb39-55" aria-hidden="true" tabindex="-1"></a>    calib_dataset, batch_size<span class="op">=</span><span class="dv">1</span>, shuffle<span class="op">=</span><span class="va">True</span></span>
<span id="cb39-56"><a href="#cb39-56" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb39-57"><a href="#cb39-57" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-58"><a href="#cb39-58" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Calibrating on </span><span class="sc">{</span><span class="bu">len</span>(calib_dataset)<span class="sc">}</span><span class="ss"> images from </span><span class="sc">{</span>CALIB_IMAGES_DIR<span class="sc">}</span><span class="ss">..."</span>)</span>
<span id="cb39-59"><a href="#cb39-59" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-60"><a href="#cb39-60" aria-hidden="true" tabindex="-1"></a>num_calib <span class="op">=</span> <span class="bu">min</span>(<span class="dv">100</span>, <span class="bu">len</span>(calib_dataset))  <span class="co"># or adjust</span></span>
<span id="cb39-61"><a href="#cb39-61" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> torch.no_grad():</span>
<span id="cb39-62"><a href="#cb39-62" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i, (calib_img, _) <span class="kw">in</span> <span class="bu">enumerate</span>(calib_loader):</span>
<span id="cb39-63"><a href="#cb39-63" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> i <span class="op">&gt;=</span> num_calib:</span>
<span id="cb39-64"><a href="#cb39-64" aria-hidden="true" tabindex="-1"></a>            <span class="cf">break</span></span>
<span id="cb39-65"><a href="#cb39-65" aria-hidden="true" tabindex="-1"></a>        prepared(calib_img)</span>
<span id="cb39-66"><a href="#cb39-66" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-67"><a href="#cb39-67" aria-hidden="true" tabindex="-1"></a><span class="co"># 5) Convert calibrated model to quantized model</span></span>
<span id="cb39-68"><a href="#cb39-68" aria-hidden="true" tabindex="-1"></a>quantized_model <span class="op">=</span> convert_pt2e(prepared)</span>
<span id="cb39-69"><a href="#cb39-69" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-70"><a href="#cb39-70" aria-hidden="true" tabindex="-1"></a><span class="co"># 6) Export quantized model and lower to XNNPACK, then to ExecuTorch</span></span>
<span id="cb39-71"><a href="#cb39-71" aria-hidden="true" tabindex="-1"></a>exported_quant <span class="op">=</span> export(quantized_model, example_inputs)</span>
<span id="cb39-72"><a href="#cb39-72" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-73"><a href="#cb39-73" aria-hidden="true" tabindex="-1"></a>et_program <span class="op">=</span> to_edge_transform_and_lower(</span>
<span id="cb39-74"><a href="#cb39-74" aria-hidden="true" tabindex="-1"></a>    exported_quant,</span>
<span id="cb39-75"><a href="#cb39-75" aria-hidden="true" tabindex="-1"></a>    partitioner<span class="op">=</span>[XnnpackPartitioner()],</span>
<span id="cb39-76"><a href="#cb39-76" aria-hidden="true" tabindex="-1"></a>).to_executorch()</span>
<span id="cb39-77"><a href="#cb39-77" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-78"><a href="#cb39-78" aria-hidden="true" tabindex="-1"></a><span class="co"># 7) Save .pte and compute sizes</span></span>
<span id="cb39-79"><a href="#cb39-79" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> <span class="bu">open</span>(EXECUTORCH_QUANTIZED_PATH, <span class="st">"wb"</span>) <span class="im">as</span> f:</span>
<span id="cb39-80"><a href="#cb39-80" aria-hidden="true" tabindex="-1"></a>    et_program.write_to_file(f)</span>
<span id="cb39-81"><a href="#cb39-81" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-82"><a href="#cb39-82" aria-hidden="true" tabindex="-1"></a>pytorch_size <span class="op">=</span> os.path.getsize(PYTORCH_MODEL_PATH)<span class="op">/</span>(<span class="dv">1024</span><span class="op">*</span><span class="dv">1024</span>)</span>
<span id="cb39-83"><a href="#cb39-83" aria-hidden="true" tabindex="-1"></a>quantized_size <span class="op">=</span> os.path.getsize(EXECUTORCH_QUANTIZED_PATH)<span class="op">/</span>(<span class="dv">1024</span><span class="op">*</span><span class="dv">1024</span>)</span>
<span id="cb39-84"><a href="#cb39-84" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-85"><a href="#cb39-85" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">"</span> <span class="op">+</span> <span class="st">"="</span><span class="op">*</span><span class="dv">60</span>)</span>
<span id="cb39-86"><a href="#cb39-86" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"MODEL SIZE COMPARISON"</span>)</span>
<span id="cb39-87"><a href="#cb39-87" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"="</span><span class="op">*</span><span class="dv">60</span>)</span>
<span id="cb39-88"><a href="#cb39-88" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"PyTorch (FP32):                  </span><span class="sc">{</span>pytorch_size<span class="sc">:6.2f}</span><span class="ss"> MB"</span>)</span>
<span id="cb39-89"><a href="#cb39-89" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"ExecuTorch Quantized (INT8):     </span><span class="sc">{</span>quantized_size<span class="sc">:6.2f}</span><span class="ss"> MB"</span>)</span>
<span id="cb39-90"><a href="#cb39-90" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Size reduction:                  </span><span class="sc">{</span>((pytorch_size <span class="op">-</span> quantized_size) <span class="op">\</span></span>
<span id="cb39-91"><a href="#cb39-91" aria-hidden="true" tabindex="-1"></a><span class="op">/</span> pytorch_size <span class="op">*</span> <span class="dv">100</span>)<span class="sc">:5.1f}</span><span class="ss">%"</span>)</span>
<span id="cb39-92"><a href="#cb39-92" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Savings:                         </span><span class="sc">{</span>pytorch_size <span class="op">-</span> quantized_size<span class="sc">:6.2f}</span><span class="ss"> MB"</span>)</span>
<span id="cb39-93"><a href="#cb39-93" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"="</span><span class="op">*</span><span class="dv">60</span>)</span>
<span id="cb39-94"><a href="#cb39-94" aria-hidden="true" tabindex="-1"></a></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Runing the script, we get:</p>
<div class="sourceCode" id="cb40"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb40-1"><a href="#cb40-1" aria-hidden="true" tabindex="-1"></a><span class="ex">Calibrating</span> on 100 images from calib_images...</span>
<span id="cb40-2"><a href="#cb40-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-3"><a href="#cb40-3" aria-hidden="true" tabindex="-1"></a><span class="ex">============================================================</span></span>
<span id="cb40-4"><a href="#cb40-4" aria-hidden="true" tabindex="-1"></a><span class="ex">MODEL</span> SIZE COMPARISON</span>
<span id="cb40-5"><a href="#cb40-5" aria-hidden="true" tabindex="-1"></a><span class="ex">============================================================</span></span>
<span id="cb40-6"><a href="#cb40-6" aria-hidden="true" tabindex="-1"></a><span class="ex">PyTorch</span> <span class="er">(</span><span class="ex">FP32</span><span class="kw">)</span><span class="bu">:</span>                   13.60 MB</span>
<span id="cb40-7"><a href="#cb40-7" aria-hidden="true" tabindex="-1"></a><span class="ex">ExecuTorch</span> Quantized <span class="er">(</span><span class="ex">INT8</span><span class="kw">)</span><span class="bu">:</span>       3.59 MB</span>
<span id="cb40-8"><a href="#cb40-8" aria-hidden="true" tabindex="-1"></a><span class="ex">Size</span> reduction:                   73.6%</span>
<span id="cb40-9"><a href="#cb40-9" aria-hidden="true" tabindex="-1"></a><span class="ex">Savings:</span>                          10.01 MB</span>
<span id="cb40-10"><a href="#cb40-10" aria-hidden="true" tabindex="-1"></a><span class="ex">============================================================</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>The quantized (int8) model achieved 74% size reduction: ~3.5 MB (similar to TFLite). Let’s see about the inference latency, runing <a href="https://github.com/Mjrovai/EdgeML-with-Raspberry-Pi/blob/main/EXECUTORCH/scripts/mobv2_xnnpack_int8.py">mobv2_xnnpack_int8.py</a>.</p>
<div class="sourceCode" id="cb41"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb41-1"><a href="#cb41-1" aria-hidden="true" tabindex="-1"></a><span class="ex">Loading</span> labels...</span>
<span id="cb41-2"><a href="#cb41-2" aria-hidden="true" tabindex="-1"></a><span class="ex">Loading</span> ExecuTorch model from models/mobilenet_v2_quantized_xnnpack.pte...</span>
<span id="cb41-3"><a href="#cb41-3" aria-hidden="true" tabindex="-1"></a><span class="ex">Loading</span> image from images/cat.jpg...</span>
<span id="cb41-4"><a href="#cb41-4" aria-hidden="true" tabindex="-1"></a><span class="ex">Running</span> ExecuTorch inference <span class="er">(</span><span class="ex">Quantized</span> INT8<span class="kw">)</span><span class="ex">...</span></span>
<span id="cb41-5"><a href="#cb41-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb41-6"><a href="#cb41-6" aria-hidden="true" tabindex="-1"></a><span class="ex">==================================================</span></span>
<span id="cb41-7"><a href="#cb41-7" aria-hidden="true" tabindex="-1"></a><span class="ex">EXECUTORCH</span> QUANTIZED INT8 RESULTS</span>
<span id="cb41-8"><a href="#cb41-8" aria-hidden="true" tabindex="-1"></a><span class="ex">==================================================</span></span>
<span id="cb41-9"><a href="#cb41-9" aria-hidden="true" tabindex="-1"></a><span class="ex">Inference</span> Time: 13.56 ms</span>
<span id="cb41-10"><a href="#cb41-10" aria-hidden="true" tabindex="-1"></a><span class="ex">Output</span> dtype:   torch.float32</span>
<span id="cb41-11"><a href="#cb41-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb41-12"><a href="#cb41-12" aria-hidden="true" tabindex="-1"></a><span class="ex">Top</span> 5 Predictions:</span>
<span id="cb41-13"><a href="#cb41-13" aria-hidden="true" tabindex="-1"></a><span class="ex">--------------------------------------------------</span></span>
<span id="cb41-14"><a href="#cb41-14" aria-hidden="true" tabindex="-1"></a><span class="ex">1.</span> tiger cat            <span class="at">-</span> 51.01%</span>
<span id="cb41-15"><a href="#cb41-15" aria-hidden="true" tabindex="-1"></a><span class="ex">2.</span> Egyptian Mau         <span class="at">-</span> 34.11%</span>
<span id="cb41-16"><a href="#cb41-16" aria-hidden="true" tabindex="-1"></a><span class="ex">3.</span> lynx                 <span class="at">-</span> 7.54%</span>
<span id="cb41-17"><a href="#cb41-17" aria-hidden="true" tabindex="-1"></a><span class="ex">4.</span> tabby cat            <span class="at">-</span> 6.17%</span>
<span id="cb41-18"><a href="#cb41-18" aria-hidden="true" tabindex="-1"></a><span class="ex">5.</span> plastic bag          <span class="at">-</span> 0.37%</span>
<span id="cb41-19"><a href="#cb41-19" aria-hidden="true" tabindex="-1"></a><span class="ex">==================================================</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<blockquote class="blockquote">
<p>Slightly higher top‑1 probabilities in the INT8 model are normal and do not indicate a problem by themselves. Quantization slightly changes the logits, and softmax can become a bit “sharper” or “flatter” even when top‑1 remains correct.</p>
</blockquote>
</section>
<section id="model-sizeperformance-comparison" class="level3">
<h3 class="anchored" data-anchor-id="model-sizeperformance-comparison">Model Size/Performance Comparison</h3>
<table class="table">
<thead>
<tr class="header">
<th>Model Configuration</th>
<th>File Size</th>
<th>Size Reduction</th>
<th>Latency</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Float32 (basic export)</td>
<td>13.58 MB</td>
<td>Baseline</td>
<td>2.5 s</td>
</tr>
<tr class="even">
<td>Float32 + XNNPACK</td>
<td>13.35 MB</td>
<td>~0%</td>
<td>20 ms</td>
</tr>
<tr class="odd">
<td>INT8 + XNNPACK</td>
<td>3.59 MB</td>
<td>~75%</td>
<td>14 ms</td>
</tr>
</tbody>
</table>
<p><strong>NOTE</strong></p>
<ul>
<li>Looking at <code>Htop</code>, we can see that only one of the Pi’s cores is at 100%. This indicates that the shipped Python runtime currently runs our ExecuTorch/XNNPACK model effectively single‑threaded on Pi.</li>
<li>To exploit all four cores, the next step would be to move inference into a small C++ wrapper that sets the ExecuTorch threadpool size before executing the graph. With the pure‑Python path, there is no clean public knob to change it yet. We will not explore it here.</li>
</ul>
</section>
</section>
<section id="making-inferences-with-executorch" class="level2">
<h2 class="anchored" data-anchor-id="making-inferences-with-executorch">Making Inferences with EXECUTORCH</h2>
<p>Now that we have our EXECUTORCH models, let’s explore them in more detail for image classification using a Jupyter Notebook!</p>
<section id="setting-up-jupyter-notebook" class="level3">
<h3 class="anchored" data-anchor-id="setting-up-jupyter-notebook">Setting up Jupyter Notebook</h3>
<p>Set up Jupyter Notebook for interactive development:</p>
<div class="sourceCode" id="cb42"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb42-1"><a href="#cb42-1" aria-hidden="true" tabindex="-1"></a><span class="ex">pip</span> install jupyter jupyterlab notebook</span>
<span id="cb42-2"><a href="#cb42-2" aria-hidden="true" tabindex="-1"></a><span class="ex">jupyter</span> notebook <span class="at">--generate-config</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>To run the Jupyter notebook on the Raspberry Pi desktop, run:</p>
<div class="sourceCode" id="cb43"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb43-1"><a href="#cb43-1" aria-hidden="true" tabindex="-1"></a><span class="ex">jupyter</span> notebook</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>and open the URL with the token</p>
<p>To run Jupyter Notebook on your computer (headless), run the command below, replacing with your Raspberry Pi’s IP address:</p>
<p>To get the IP Address, we can use the command: <code>hostname -I</code></p>
<div class="sourceCode" id="cb44"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb44-1"><a href="#cb44-1" aria-hidden="true" tabindex="-1"></a><span class="ex">jupyter</span> notebook <span class="at">--ip</span><span class="op">=</span>192.168.4.42 <span class="at">--no-browser</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<blockquote class="blockquote">
<p>Access it from another device using the provided token in your web browser.</p>
</blockquote>
</section>
<section id="the-project-folder" class="level3">
<h3 class="anchored" data-anchor-id="the-project-folder">The Project folder</h3>
<p>We must be sure that we have this project folder structure:</p>
<pre><code>EXECUTORCH/MOBILENET/
├── convert_mobv2_executorch.py
├── convert_mobv2_xnnpack.py
├── convert_mobv2_xnnpack_int8.py        
├── mobv2_executorch.py
├── mobv2_xnnpack.py     
├── mobv2_xnnpack_int8.py 
├── calib_images/
├── data/         
├── models/
│   ├── mobilenet_v2.pth                        # Float32 pytorch model
│   ├── mobilenet_v2.pte                        # Float32 conv model
│   ├── mobilenet_v2_xnnpack.pte                # Float32 conv model
│   ├── mobilenet_v2_quantized_xnnpack.pte      # Quantized conv model
│   └── imagenet_labels.json                    # Labels
├── images/                                     # Test images
│   ├── cat.jpg
│   └── camera_capture.jpg
└── notebooks/                                   
    └── image_classification_executorch.ipynb</code></pre>
</section>
<section id="loading-and-running-a-model" class="level3">
<h3 class="anchored" data-anchor-id="loading-and-running-a-model">Loading and Running a Model</h3>
<p>Inside the folder ‘notebooks’, on the project space <code>IMAGE_CLASS/MOBILENET</code>, create a new notebook: <a href="https://github.com/Mjrovai/EdgeML-with-Raspberry-Pi/blob/main/EXECUTORCH/notebooks/image_classification_executorch.ipynb">image_classification_executorch.ipynb</a>.</p>
</section>
<section id="setup-and-verification" class="level3">
<h3 class="anchored" data-anchor-id="setup-and-verification">Setup and Verification</h3>
<div class="sourceCode" id="cb46"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb46-1"><a href="#cb46-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Import required libraries</span></span>
<span id="cb46-2"><a href="#cb46-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> os</span>
<span id="cb46-3"><a href="#cb46-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> time</span>
<span id="cb46-4"><a href="#cb46-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> json</span>
<span id="cb46-5"><a href="#cb46-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> urllib.request</span>
<span id="cb46-6"><a href="#cb46-6" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb46-7"><a href="#cb46-7" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb46-8"><a href="#cb46-8" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> PIL <span class="im">import</span> Image</span>
<span id="cb46-9"><a href="#cb46-9" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb46-10"><a href="#cb46-10" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torchvision <span class="im">import</span> transforms</span>
<span id="cb46-11"><a href="#cb46-11" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> executorch</span>
<span id="cb46-12"><a href="#cb46-12" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> executorch.extension.pybindings.portable_lib <span class="im">import</span> _load_for_executorch</span>
<span id="cb46-13"><a href="#cb46-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-14"><a href="#cb46-14" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"="</span> <span class="op">*</span> <span class="dv">50</span>)</span>
<span id="cb46-15"><a href="#cb46-15" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"SETUP VERIFICATION"</span>)</span>
<span id="cb46-16"><a href="#cb46-16" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"="</span> <span class="op">*</span> <span class="dv">50</span>)</span>
<span id="cb46-17"><a href="#cb46-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-18"><a href="#cb46-18" aria-hidden="true" tabindex="-1"></a><span class="co"># Check versions</span></span>
<span id="cb46-19"><a href="#cb46-19" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"PyTorch version: </span><span class="sc">{</span>torch<span class="sc">.</span>__version__<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb46-20"><a href="#cb46-20" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"NumPy version: </span><span class="sc">{</span>np<span class="sc">.</span>__version__<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb46-21"><a href="#cb46-21" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"PIL version: </span><span class="sc">{</span>Image<span class="sc">.</span>__version__<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb46-22"><a href="#cb46-22" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"EXECUTORCH available: </span><span class="sc">{</span>executorch <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span><span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb46-23"><a href="#cb46-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-24"><a href="#cb46-24" aria-hidden="true" tabindex="-1"></a><span class="co"># Test basic PyTorch functionality</span></span>
<span id="cb46-25"><a href="#cb46-25" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> torch.randn(<span class="dv">3</span>, <span class="dv">224</span>, <span class="dv">224</span>)</span>
<span id="cb46-26"><a href="#cb46-26" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">Created test tensor with shape: </span><span class="sc">{</span>x<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb46-27"><a href="#cb46-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-28"><a href="#cb46-28" aria-hidden="true" tabindex="-1"></a><span class="co"># Test PIL</span></span>
<span id="cb46-29"><a href="#cb46-29" aria-hidden="true" tabindex="-1"></a>test_img <span class="op">=</span> Image.new(<span class="st">'RGB'</span>, (<span class="dv">224</span>, <span class="dv">224</span>), color<span class="op">=</span><span class="st">'red'</span>)</span>
<span id="cb46-30"><a href="#cb46-30" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Created test PIL image: </span><span class="sc">{</span>test_img<span class="sc">.</span>size<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb46-31"><a href="#cb46-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-32"><a href="#cb46-32" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">✓ Setup verification complete!"</span>)</span>
<span id="cb46-33"><a href="#cb46-33" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"="</span> <span class="op">*</span> <span class="dv">50</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>We get:</p>
<pre><code>==================================================
SETUP VERIFICATION
==================================================
PyTorch version: 2.9.1+cpu
NumPy version: 2.2.6
PIL version: 12.1.0
EXECUTORCH available: True

Created test tensor with shape: torch.Size([3, 224, 224])
Created test PIL image: (224, 224)

✓ Setup verification complete!
==================================================</code></pre>
</section>
<section id="download-test-image" class="level3">
<h3 class="anchored" data-anchor-id="download-test-image">Download Test Image</h3>
<ul>
<li>Download test image for example from:
<ul>
<li>“https://upload.wikimedia.org/wikipedia/commons/3/3a/Cat03.jpg”</li>
<li>And save it on the ../images folder as “cat.jpg”</li>
</ul></li>
</ul>
<div class="sourceCode" id="cb48"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb48-1"><a href="#cb48-1" aria-hidden="true" tabindex="-1"></a>img_path <span class="op">=</span> <span class="st">"../images/cat.jpg"</span></span>
<span id="cb48-2"><a href="#cb48-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb48-3"><a href="#cb48-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Load and display</span></span>
<span id="cb48-4"><a href="#cb48-4" aria-hidden="true" tabindex="-1"></a>img <span class="op">=</span> Image.<span class="bu">open</span>(img_path)</span>
<span id="cb48-5"><a href="#cb48-5" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">6</span>, <span class="dv">6</span>))</span>
<span id="cb48-6"><a href="#cb48-6" aria-hidden="true" tabindex="-1"></a>plt.imshow(img)</span>
<span id="cb48-7"><a href="#cb48-7" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">"Original Image"</span>)</span>
<span id="cb48-8"><a href="#cb48-8" aria-hidden="true" tabindex="-1"></a><span class="co">#plt.axis('off')</span></span>
<span id="cb48-9"><a href="#cb48-9" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb48-10"><a href="#cb48-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb48-11"><a href="#cb48-11" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Image size: </span><span class="sc">{</span>img<span class="sc">.</span>size<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb48-12"><a href="#cb48-12" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Image mode: </span><span class="sc">{</span>img<span class="sc">.</span>mode<span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="./images/png/output_5_0.png" class="img-fluid figure-img" style="width:65.0%"></p>
</figure>
</div>
<pre><code>Image size: (1600, 1598)
Image mode: RGB</code></pre>
</section>
</section>
<section id="load-executorch-model" class="level2">
<h2 class="anchored" data-anchor-id="load-executorch-model">Load EXECUTORCH Model</h2>
<blockquote class="blockquote">
<p><strong>Note:</strong> You need to export a model first using the <code>export_mobv2_executorch.py</code> script.</p>
<p>If you don’t have a model yet, run the export script first:</p>
<ul>
<li><code>python export_mobv2_executorch.py</code></li>
</ul>
</blockquote>
<p>Let’s verify what the models in the folder <code>../models</code>:</p>
<pre><code>imagenet_labels.json  mobilenet_v2_quantized_xnnpack.pte
mobilenet_v2.pte      mobilenet_v2_xnnpack.pte
mobilenet_v2.pth</code></pre>
<blockquote class="blockquote">
<p>The conversions were performed using the Python scripts in the previous sections.</p>
</blockquote>
<div class="sourceCode" id="cb51"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb51-1"><a href="#cb51-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Load the EXECUTORCH model</span></span>
<span id="cb51-2"><a href="#cb51-2" aria-hidden="true" tabindex="-1"></a>model_path <span class="op">=</span> <span class="st">"../models/mobilenet_v2.pte"</span></span>
<span id="cb51-3"><a href="#cb51-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-4"><a href="#cb51-4" aria-hidden="true" tabindex="-1"></a><span class="cf">try</span>:</span>
<span id="cb51-5"><a href="#cb51-5" aria-hidden="true" tabindex="-1"></a>    model <span class="op">=</span> _load_for_executorch(model_path)</span>
<span id="cb51-6"><a href="#cb51-6" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Model loaded successfully from: </span><span class="sc">{</span>model_path<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb51-7"><a href="#cb51-7" aria-hidden="true" tabindex="-1"></a>    <span class="co">#print(f"  Available methods: {model.method_names}")</span></span>
<span id="cb51-8"><a href="#cb51-8" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb51-9"><a href="#cb51-9" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Check file size</span></span>
<span id="cb51-10"><a href="#cb51-10" aria-hidden="true" tabindex="-1"></a>    file_size <span class="op">=</span> os.path.getsize(model_path) <span class="op">/</span> (<span class="dv">1024</span> <span class="op">*</span> <span class="dv">1024</span>)  <span class="co"># MB</span></span>
<span id="cb51-11"><a href="#cb51-11" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Model size: </span><span class="sc">{</span>file_size<span class="sc">:.2f}</span><span class="ss"> MB"</span>)</span>
<span id="cb51-12"><a href="#cb51-12" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb51-13"><a href="#cb51-13" aria-hidden="true" tabindex="-1"></a><span class="cf">except</span> <span class="pp">FileNotFoundError</span>:</span>
<span id="cb51-14"><a href="#cb51-14" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"✗ Model not found: </span><span class="sc">{</span>model_path<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb51-15"><a href="#cb51-15" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">Please run the export script first:"</span>)</span>
<span id="cb51-16"><a href="#cb51-16" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"  python export_mobilenet.py"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre><code>Model loaded successfully from: ../models/mobilenet_v2.pte
Model size: 13.58 MB</code></pre>
</section>
<section id="download-imagenet-labels" class="level2">
<h2 class="anchored" data-anchor-id="download-imagenet-labels">Download ImageNet Labels</h2>
<div class="sourceCode" id="cb53"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb53-1"><a href="#cb53-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Download and save ImageNet labels (if you do not have it)</span></span>
<span id="cb53-2"><a href="#cb53-2" aria-hidden="true" tabindex="-1"></a>LABELS_PATH <span class="op">=</span> <span class="st">"../models/imagenet_labels.json"</span></span>
<span id="cb53-3"><a href="#cb53-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-4"><a href="#cb53-4" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> <span class="kw">not</span> os.path.exists(LABELS_PATH):</span>
<span id="cb53-5"><a href="#cb53-5" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"Downloading ImageNet labels..."</span>)</span>
<span id="cb53-6"><a href="#cb53-6" aria-hidden="true" tabindex="-1"></a>    LABELS_URL <span class="op">=</span> <span class="st">"https://raw.githubusercontent.com/anishathalye/</span><span class="ch">\</span></span>
<span id="cb53-7"><a href="#cb53-7" aria-hidden="true" tabindex="-1"></a><span class="st">    imagenet-simple-labels/master/imagenet-simple-labels.json"</span></span>
<span id="cb53-8"><a href="#cb53-8" aria-hidden="true" tabindex="-1"></a>    <span class="cf">with</span> urllib.request.urlopen(LABELS_URL) <span class="im">as</span> url:</span>
<span id="cb53-9"><a href="#cb53-9" aria-hidden="true" tabindex="-1"></a>        labels <span class="op">=</span> json.load(url)</span>
<span id="cb53-10"><a href="#cb53-10" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb53-11"><a href="#cb53-11" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Save labels locally</span></span>
<span id="cb53-12"><a href="#cb53-12" aria-hidden="true" tabindex="-1"></a>    <span class="cf">with</span> <span class="bu">open</span>(LABELS_PATH, <span class="st">'w'</span>) <span class="im">as</span> f:</span>
<span id="cb53-13"><a href="#cb53-13" aria-hidden="true" tabindex="-1"></a>        json.dump(labels, f)</span>
<span id="cb53-14"><a href="#cb53-14" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Labels saved to </span><span class="sc">{</span>LABELS_PATH<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb53-15"><a href="#cb53-15" aria-hidden="true" tabindex="-1"></a><span class="cf">else</span>:</span>
<span id="cb53-16"><a href="#cb53-16" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"Loading labels from disk..."</span>)</span>
<span id="cb53-17"><a href="#cb53-17" aria-hidden="true" tabindex="-1"></a>    <span class="cf">with</span> <span class="bu">open</span>(LABELS_PATH, <span class="st">'r'</span>) <span class="im">as</span> f:</span>
<span id="cb53-18"><a href="#cb53-18" aria-hidden="true" tabindex="-1"></a>        labels <span class="op">=</span> json.load(f)      </span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Check the labels:</p>
<div class="sourceCode" id="cb54"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb54-1"><a href="#cb54-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">Total classes: </span><span class="sc">{</span><span class="bu">len</span>(labels)<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb54-2"><a href="#cb54-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Sample labels: </span><span class="sc">{</span>labels[<span class="dv">280</span>:<span class="dv">285</span>]<span class="sc">}</span><span class="ss">"</span>)  </span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre><code>Total classes: 1000
Sample labels: ['grey fox', 'tabby cat', 'tiger cat', 'Persian cat', 'Siamese cat']</code></pre>
</section>
<section id="image-preprocessing" class="level2">
<h2 class="anchored" data-anchor-id="image-preprocessing">Image Preprocessing</h2>
<p>A preprocessing pipeline is needed because ExecuTorch only runs the exported core network; it does not include the input normalization logic that MobileNet v2 expects, and the model will give incorrect predictions if the input tensor is not in the exact format it was trained on.</p>
<p>What MobileNet v2 expects For typical PyTorch MobileNet v2 models (ImageNet‑pretrained): • Input shape: 3‑channel RGB tensor of size. • Value range: floating-point values, usually in float32 after dividing by 255. • Normalization: per‑channel mean/std (ImageNet) normalization, e.g., <code>mean=0.485, 0.456, 0.406</code>, <code>std=0.229, 0.224, 0.225</code>.</p>
<p>These steps (resize, convert to tensor, normalize) are not “optional decorations”; they are part of the functional definition of the model’s expected input distribution.</p>
<section id="define-preprocessing-pipeline" class="level3">
<h3 class="anchored" data-anchor-id="define-preprocessing-pipeline">Define preprocessing pipeline</h3>
<div class="sourceCode" id="cb56"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb56-1"><a href="#cb56-1" aria-hidden="true" tabindex="-1"></a>preprocess <span class="op">=</span> transforms.Compose([</span>
<span id="cb56-2"><a href="#cb56-2" aria-hidden="true" tabindex="-1"></a>    transforms.Resize(<span class="dv">256</span>),              <span class="co"># Resize to 256</span></span>
<span id="cb56-3"><a href="#cb56-3" aria-hidden="true" tabindex="-1"></a>    transforms.CenterCrop(<span class="dv">224</span>),          <span class="co"># Center crop to 224x224</span></span>
<span id="cb56-4"><a href="#cb56-4" aria-hidden="true" tabindex="-1"></a>    transforms.ToTensor(),               <span class="co"># Convert to tensor [0, 1]</span></span>
<span id="cb56-5"><a href="#cb56-5" aria-hidden="true" tabindex="-1"></a>    transforms.Normalize(                <span class="co"># Normalize with ImageNet stats</span></span>
<span id="cb56-6"><a href="#cb56-6" aria-hidden="true" tabindex="-1"></a>        mean<span class="op">=</span>[<span class="fl">0.485</span>, <span class="fl">0.456</span>, <span class="fl">0.406</span>],</span>
<span id="cb56-7"><a href="#cb56-7" aria-hidden="true" tabindex="-1"></a>        std<span class="op">=</span>[<span class="fl">0.229</span>, <span class="fl">0.224</span>, <span class="fl">0.225</span>]</span>
<span id="cb56-8"><a href="#cb56-8" aria-hidden="true" tabindex="-1"></a>    ),</span>
<span id="cb56-9"><a href="#cb56-9" aria-hidden="true" tabindex="-1"></a>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="apply-preprocessing" class="level3">
<h3 class="anchored" data-anchor-id="apply-preprocessing">Apply preprocessing</h3>
<div class="sourceCode" id="cb57"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb57-1"><a href="#cb57-1" aria-hidden="true" tabindex="-1"></a>input_tensor <span class="op">=</span> preprocess(img)</span>
<span id="cb57-2"><a href="#cb57-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"  Input shape: </span><span class="sc">{</span>input_tensor<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb57-3"><a href="#cb57-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"  Input dtype: </span><span class="sc">{</span>input_tensor<span class="sc">.</span>dtype<span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre><code>  Input shape: torch.Size([3, 224, 224])
  Input dtype: torch.float32</code></pre>
</section>
<section id="add-batch-dimension-1-3-224-224" class="level3">
<h3 class="anchored" data-anchor-id="add-batch-dimension-1-3-224-224">Add batch dimension: [1, 3, 224, 224]</h3>
<div class="sourceCode" id="cb59"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb59-1"><a href="#cb59-1" aria-hidden="true" tabindex="-1"></a>input_batch <span class="op">=</span> input_tensor.unsqueeze(<span class="dv">0</span>)  </span>
<span id="cb59-2"><a href="#cb59-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb59-3"><a href="#cb59-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"  Input shape: </span><span class="sc">{</span>input_batch<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb59-4"><a href="#cb59-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"  Input dtype: </span><span class="sc">{</span>input_batch<span class="sc">.</span>dtype<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb59-5"><a href="#cb59-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"  Value range: [</span><span class="sc">{</span>input_batch<span class="sc">.</span><span class="bu">min</span>()<span class="sc">:.3f}</span><span class="ss">, </span><span class="sc">{</span>input_batch<span class="sc">.</span><span class="bu">max</span>()<span class="sc">:.3f}</span><span class="ss">]"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre><code>  Input shape: torch.Size([1, 3, 224, 224])
  Input dtype: torch.float32
  Value range: [-2.084, 2.309]</code></pre>
<p>The Preprocessing is complete!</p>
</section>
</section>
<section id="run-inference" class="level2">
<h2 class="anchored" data-anchor-id="run-inference">Run Inference</h2>
<p>For inference, we should run a forward pass of the model in inference mode (<code>torch.no_grad()</code>), measure the time, and print basic information about the outputs.</p>
<p><code>torch.no_grad()</code> is a context manager that disables gradient calculation inside its block. During inference, we do not need gradients, so disabling them:</p>
<ul>
<li>Saves memory (no computation graph is stored).</li>
<li>Can speed up computation slightly.</li>
<li>Everything computed inside this block will have <code>requires_grad=False</code>, so we cannot call <code>.backward()</code> on it.</li>
</ul>
<div class="sourceCode" id="cb61"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb61-1"><a href="#cb61-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Run inference</span></span>
<span id="cb61-2"><a href="#cb61-2" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> torch.no_grad():</span>
<span id="cb61-3"><a href="#cb61-3" aria-hidden="true" tabindex="-1"></a>    start_time <span class="op">=</span> time.time()</span>
<span id="cb61-4"><a href="#cb61-4" aria-hidden="true" tabindex="-1"></a>    outputs <span class="op">=</span> model.forward((input_batch,))</span>
<span id="cb61-5"><a href="#cb61-5" aria-hidden="true" tabindex="-1"></a>    inference_time <span class="op">=</span> time.time() <span class="op">-</span> start_time</span>
<span id="cb61-6"><a href="#cb61-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-7"><a href="#cb61-7" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Inference completed in </span><span class="sc">{</span>inference_time<span class="op">*</span><span class="dv">1000</span><span class="sc">:.2f}</span><span class="ss"> ms"</span>)</span>
<span id="cb61-8"><a href="#cb61-8" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Output type: </span><span class="sc">{</span><span class="bu">type</span>(outputs)<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb61-9"><a href="#cb61-9" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Output shape: </span><span class="sc">{</span>outputs[<span class="dv">0</span>]<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre><code>Inference completed in 2478.74 ms
Output type: &lt;class 'list'&gt;
Output shape: torch.Size([1, 1000])</code></pre>
<p><code>type(outputs)</code> tells us what container the model returned. Often this is a tuple or list when working with exported/ExecuTorch‑style models, e.g., <code>&lt;class 'tuple'&gt;</code>.</p>
<p>That container may hold one or more tensors (e.g., logits, auxiliary outputs).</p>
<ul>
<li><code>outputs[0]</code> accesses the first element of that container (usually the main output tensor), and <code>.shape</code> prints its dimensions (For image classification, this is often <code>batch_size, num_classes</code>).</li>
</ul>
</section>
<section id="process-and-display-results" class="level2">
<h2 class="anchored" data-anchor-id="process-and-display-results">Process and Display Results</h2>
<p>Now we should take the model’s raw scores (logits) for a single image, convert them into probabilities with softmax, select the top‑5 most likely classes, and print them nicely formatted.</p>
<ul>
<li><code>outputs[0][0]</code> selects the first element in the batch, giving a 1D tensor of logits of length <code>num_classes</code>.</li>
<li><code>torch.nn.functional.softmax(..., dim=0)</code> applies the softmax function along that 1D dimension, turning logits into probabilities that sum to 1.</li>
</ul>
<div class="sourceCode" id="cb63"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb63-1"><a href="#cb63-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Apply softmax to get probabilities</span></span>
<span id="cb63-2"><a href="#cb63-2" aria-hidden="true" tabindex="-1"></a>probabilities <span class="op">=</span> torch.nn.functional.softmax(outputs[<span class="dv">0</span>][<span class="dv">0</span>], dim<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb63-3"><a href="#cb63-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb63-4"><a href="#cb63-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Get top 5 predictions</span></span>
<span id="cb63-5"><a href="#cb63-5" aria-hidden="true" tabindex="-1"></a>top5_prob, top5_indices <span class="op">=</span> torch.topk(probabilities, <span class="dv">5</span>)</span>
<span id="cb63-6"><a href="#cb63-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb63-7"><a href="#cb63-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Display results</span></span>
<span id="cb63-8"><a href="#cb63-8" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">"</span> <span class="op">+</span> <span class="st">"="</span><span class="op">*</span><span class="dv">60</span>)</span>
<span id="cb63-9"><a href="#cb63-9" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"TOP 5 PREDICTIONS"</span>)</span>
<span id="cb63-10"><a href="#cb63-10" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"="</span><span class="op">*</span><span class="dv">60</span>)</span>
<span id="cb63-11"><a href="#cb63-11" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="sc">{</span><span class="st">'Class'</span><span class="sc">:&lt;35}</span><span class="ss"> </span><span class="sc">{</span><span class="st">'Probability'</span><span class="sc">:&gt;10}</span><span class="ss">"</span>)</span>
<span id="cb63-12"><a href="#cb63-12" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"-"</span><span class="op">*</span><span class="dv">60</span>)</span>
<span id="cb63-13"><a href="#cb63-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb63-14"><a href="#cb63-14" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">5</span>):</span>
<span id="cb63-15"><a href="#cb63-15" aria-hidden="true" tabindex="-1"></a>    label <span class="op">=</span> labels[top5_indices[i]]</span>
<span id="cb63-16"><a href="#cb63-16" aria-hidden="true" tabindex="-1"></a>    prob <span class="op">=</span> top5_prob[i].item() <span class="op">*</span> <span class="dv">100</span></span>
<span id="cb63-17"><a href="#cb63-17" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"</span><span class="sc">{</span>label<span class="sc">:&lt;35}</span><span class="ss"> </span><span class="sc">{</span>prob<span class="sc">:&gt;9.2f}</span><span class="ss">%"</span>)</span>
<span id="cb63-18"><a href="#cb63-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb63-19"><a href="#cb63-19" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"="</span><span class="op">*</span><span class="dv">60</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre><code>============================================================
TOP 5 PREDICTIONS
============================================================
Class                               Probability
------------------------------------------------------------
tiger cat                               12.85%
Egyptian cat                             9.75%
tabby                                    6.09%
lynx                                     1.70%
carton                                   0.84%
============================================================</code></pre>
</section>
<section id="create-reusable-classification-function" class="level2">
<h2 class="anchored" data-anchor-id="create-reusable-classification-function">Create Reusable Classification Function</h2>
<p>For simplicity and reuse across other tests, let’s create a reusable function that builds on what was done so far.</p>
<div class="sourceCode" id="cb65"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb65-1"><a href="#cb65-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> classify_image_executorch(img_path, model_path, labels_path, </span>
<span id="cb65-2"><a href="#cb65-2" aria-hidden="true" tabindex="-1"></a>                              top_k<span class="op">=</span><span class="dv">5</span>, show_image<span class="op">=</span><span class="va">True</span>):</span>
<span id="cb65-3"><a href="#cb65-3" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb65-4"><a href="#cb65-4" aria-hidden="true" tabindex="-1"></a><span class="co">    Classify an image using EXECUTORCH model</span></span>
<span id="cb65-5"><a href="#cb65-5" aria-hidden="true" tabindex="-1"></a><span class="co">    </span></span>
<span id="cb65-6"><a href="#cb65-6" aria-hidden="true" tabindex="-1"></a><span class="co">    Args:</span></span>
<span id="cb65-7"><a href="#cb65-7" aria-hidden="true" tabindex="-1"></a><span class="co">        img_path: Path to input image</span></span>
<span id="cb65-8"><a href="#cb65-8" aria-hidden="true" tabindex="-1"></a><span class="co">        model_path: Path to .pte model file</span></span>
<span id="cb65-9"><a href="#cb65-9" aria-hidden="true" tabindex="-1"></a><span class="co">        labels_path: Path to labels text file</span></span>
<span id="cb65-10"><a href="#cb65-10" aria-hidden="true" tabindex="-1"></a><span class="co">        top_k: Number of top predictions to return</span></span>
<span id="cb65-11"><a href="#cb65-11" aria-hidden="true" tabindex="-1"></a><span class="co">        show_image: Whether to display the image</span></span>
<span id="cb65-12"><a href="#cb65-12" aria-hidden="true" tabindex="-1"></a><span class="co">    </span></span>
<span id="cb65-13"><a href="#cb65-13" aria-hidden="true" tabindex="-1"></a><span class="co">    Returns:</span></span>
<span id="cb65-14"><a href="#cb65-14" aria-hidden="true" tabindex="-1"></a><span class="co">        inference_time: Inference time in ms</span></span>
<span id="cb65-15"><a href="#cb65-15" aria-hidden="true" tabindex="-1"></a><span class="co">        top_indices: Indices of top k predictions</span></span>
<span id="cb65-16"><a href="#cb65-16" aria-hidden="true" tabindex="-1"></a><span class="co">        top_probs: Probabilities of top k predictions</span></span>
<span id="cb65-17"><a href="#cb65-17" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb65-18"><a href="#cb65-18" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Load image</span></span>
<span id="cb65-19"><a href="#cb65-19" aria-hidden="true" tabindex="-1"></a>    img <span class="op">=</span> Image.<span class="bu">open</span>(img_path).convert(<span class="st">'RGB'</span>)</span>
<span id="cb65-20"><a href="#cb65-20" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb65-21"><a href="#cb65-21" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Display image</span></span>
<span id="cb65-22"><a href="#cb65-22" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> show_image:</span>
<span id="cb65-23"><a href="#cb65-23" aria-hidden="true" tabindex="-1"></a>        plt.figure(figsize<span class="op">=</span>(<span class="dv">4</span>, <span class="dv">4</span>))</span>
<span id="cb65-24"><a href="#cb65-24" aria-hidden="true" tabindex="-1"></a>        plt.imshow(img)</span>
<span id="cb65-25"><a href="#cb65-25" aria-hidden="true" tabindex="-1"></a>        plt.axis(<span class="st">'off'</span>)</span>
<span id="cb65-26"><a href="#cb65-26" aria-hidden="true" tabindex="-1"></a>        plt.title(<span class="st">'Input Image'</span>)</span>
<span id="cb65-27"><a href="#cb65-27" aria-hidden="true" tabindex="-1"></a>        plt.show()</span>
<span id="cb65-28"><a href="#cb65-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb65-29"><a href="#cb65-29" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Image Path: </span><span class="sc">{</span>img_path<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb65-30"><a href="#cb65-30" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb65-31"><a href="#cb65-31" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Load model</span></span>
<span id="cb65-32"><a href="#cb65-32" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Model Path </span><span class="sc">{</span>model_path<span class="sc">}</span><span class="ss">"</span>) </span>
<span id="cb65-33"><a href="#cb65-33" aria-hidden="true" tabindex="-1"></a>    model_size <span class="op">=</span> os.path.getsize(model_path) <span class="op">/</span> (<span class="dv">1024</span> <span class="op">*</span> <span class="dv">1024</span>)</span>
<span id="cb65-34"><a href="#cb65-34" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Model size: </span><span class="sc">{</span>model_size<span class="sc">:6.2f}</span><span class="ss"> MB"</span>)</span>
<span id="cb65-35"><a href="#cb65-35" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb65-36"><a href="#cb65-36" aria-hidden="true" tabindex="-1"></a>    model <span class="op">=</span> _load_for_executorch(model_path)</span>
<span id="cb65-37"><a href="#cb65-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb65-38"><a href="#cb65-38" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Preprocess</span></span>
<span id="cb65-39"><a href="#cb65-39" aria-hidden="true" tabindex="-1"></a>    preprocess <span class="op">=</span> transforms.Compose([</span>
<span id="cb65-40"><a href="#cb65-40" aria-hidden="true" tabindex="-1"></a>        transforms.Resize(<span class="dv">256</span>),</span>
<span id="cb65-41"><a href="#cb65-41" aria-hidden="true" tabindex="-1"></a>        transforms.CenterCrop(<span class="dv">224</span>),</span>
<span id="cb65-42"><a href="#cb65-42" aria-hidden="true" tabindex="-1"></a>        transforms.ToTensor(),</span>
<span id="cb65-43"><a href="#cb65-43" aria-hidden="true" tabindex="-1"></a>        transforms.Normalize(</span>
<span id="cb65-44"><a href="#cb65-44" aria-hidden="true" tabindex="-1"></a>            mean<span class="op">=</span>[<span class="fl">0.485</span>, <span class="fl">0.456</span>, <span class="fl">0.406</span>],</span>
<span id="cb65-45"><a href="#cb65-45" aria-hidden="true" tabindex="-1"></a>            std<span class="op">=</span>[<span class="fl">0.229</span>, <span class="fl">0.224</span>, <span class="fl">0.225</span>]</span>
<span id="cb65-46"><a href="#cb65-46" aria-hidden="true" tabindex="-1"></a>        ),</span>
<span id="cb65-47"><a href="#cb65-47" aria-hidden="true" tabindex="-1"></a>    ])</span>
<span id="cb65-48"><a href="#cb65-48" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb65-49"><a href="#cb65-49" aria-hidden="true" tabindex="-1"></a>    input_tensor <span class="op">=</span> preprocess(img)</span>
<span id="cb65-50"><a href="#cb65-50" aria-hidden="true" tabindex="-1"></a>    input_batch <span class="op">=</span> input_tensor.unsqueeze(<span class="dv">0</span>)</span>
<span id="cb65-51"><a href="#cb65-51" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb65-52"><a href="#cb65-52" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Inference</span></span>
<span id="cb65-53"><a href="#cb65-53" aria-hidden="true" tabindex="-1"></a>    <span class="cf">with</span> torch.no_grad():</span>
<span id="cb65-54"><a href="#cb65-54" aria-hidden="true" tabindex="-1"></a>        start_time <span class="op">=</span> time.time()</span>
<span id="cb65-55"><a href="#cb65-55" aria-hidden="true" tabindex="-1"></a>        outputs <span class="op">=</span> model.forward((input_batch,))</span>
<span id="cb65-56"><a href="#cb65-56" aria-hidden="true" tabindex="-1"></a>        inference_time <span class="op">=</span> (time.time() <span class="op">-</span> start_time)<span class="op">*</span><span class="dv">1000</span></span>
<span id="cb65-57"><a href="#cb65-57" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb65-58"><a href="#cb65-58" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb65-59"><a href="#cb65-59" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Process results</span></span>
<span id="cb65-60"><a href="#cb65-60" aria-hidden="true" tabindex="-1"></a>    probabilities <span class="op">=</span> torch.nn.functional.softmax(outputs[<span class="dv">0</span>][<span class="dv">0</span>], dim<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb65-61"><a href="#cb65-61" aria-hidden="true" tabindex="-1"></a>    top_prob, top_indices <span class="op">=</span> torch.topk(probabilities, top_k)</span>
<span id="cb65-62"><a href="#cb65-62" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb65-63"><a href="#cb65-63" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Load labels</span></span>
<span id="cb65-64"><a href="#cb65-64" aria-hidden="true" tabindex="-1"></a>    <span class="cf">with</span> <span class="bu">open</span>(labels_path, <span class="st">'r'</span>) <span class="im">as</span> f:</span>
<span id="cb65-65"><a href="#cb65-65" aria-hidden="true" tabindex="-1"></a>        labels <span class="op">=</span> json.load(f)</span>
<span id="cb65-66"><a href="#cb65-66" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb65-67"><a href="#cb65-67" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Display results</span></span>
<span id="cb65-68"><a href="#cb65-68" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">Inference time: </span><span class="sc">{</span>inference_time<span class="sc">:.2f}</span><span class="ss"> ms"</span>)</span>
<span id="cb65-69"><a href="#cb65-69" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">"</span> <span class="op">+</span> <span class="st">"="</span><span class="op">*</span><span class="dv">60</span>)</span>
<span id="cb65-70"><a href="#cb65-70" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"</span><span class="sc">{</span><span class="st">'[PREDICTION]'</span><span class="sc">:&lt;35}</span><span class="ss"> </span><span class="sc">{</span><span class="st">'[Probability]'</span><span class="sc">:&gt;15}</span><span class="ss">"</span>)</span>
<span id="cb65-71"><a href="#cb65-71" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"-"</span><span class="op">*</span><span class="dv">60</span>)</span>
<span id="cb65-72"><a href="#cb65-72" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb65-73"><a href="#cb65-73" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(top_k):</span>
<span id="cb65-74"><a href="#cb65-74" aria-hidden="true" tabindex="-1"></a>        label <span class="op">=</span> labels[top_indices[i]]</span>
<span id="cb65-75"><a href="#cb65-75" aria-hidden="true" tabindex="-1"></a>        prob <span class="op">=</span> top_prob[i].item() <span class="op">*</span> <span class="dv">100</span></span>
<span id="cb65-76"><a href="#cb65-76" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"</span><span class="sc">{</span>label<span class="sc">:&lt;35}</span><span class="ss"> </span><span class="sc">{</span>prob<span class="sc">:&gt;14.2f}</span><span class="ss">%"</span>)</span>
<span id="cb65-77"><a href="#cb65-77" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb65-78"><a href="#cb65-78" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"="</span><span class="op">*</span><span class="dv">60</span>)</span>
<span id="cb65-79"><a href="#cb65-79" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb65-80"><a href="#cb65-80" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> inference_time, top_indices, top_prob</span>
<span id="cb65-81"><a href="#cb65-81" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb65-82"><a href="#cb65-82" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"✓ Classification function defined!"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre><code>✓ Classification function defined!</code></pre>
</section>
<section id="classification-function-test" class="level2">
<h2 class="anchored" data-anchor-id="classification-function-test">Classification Function Test</h2>
<div class="sourceCode" id="cb67"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb67-1"><a href="#cb67-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Test with the cat image</span></span>
<span id="cb67-2"><a href="#cb67-2" aria-hidden="true" tabindex="-1"></a>inf_time, indices, probs <span class="op">=</span> classify_image_executorch(</span>
<span id="cb67-3"><a href="#cb67-3" aria-hidden="true" tabindex="-1"></a>    img_path<span class="op">=</span><span class="st">"../images/cat.jpg"</span>,</span>
<span id="cb67-4"><a href="#cb67-4" aria-hidden="true" tabindex="-1"></a>    model_path<span class="op">=</span><span class="st">"../models/mobilenet_v2.pte"</span>,</span>
<span id="cb67-5"><a href="#cb67-5" aria-hidden="true" tabindex="-1"></a>    labels_path<span class="op">=</span><span class="st">"../models/imagenet_labels.json"</span>,</span>
<span id="cb67-6"><a href="#cb67-6" aria-hidden="true" tabindex="-1"></a>    top_k<span class="op">=</span><span class="dv">5</span></span>
<span id="cb67-7"><a href="#cb67-7" aria-hidden="true" tabindex="-1"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="./images/png/infer1.png" class="img-fluid figure-img" style="width:65.0%"></p>
</figure>
</div>
<p>We can also check what is retrurned fron the function</p>
<div class="sourceCode" id="cb68"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb68-1"><a href="#cb68-1" aria-hidden="true" tabindex="-1"></a>inf_time, indices, probs</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="sourceCode" id="cb69"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb69-1"><a href="#cb69-1" aria-hidden="true" tabindex="-1"></a><span class="kw">(</span><span class="ex">2445.200204849243,</span></span>
<span id="cb69-2"><a href="#cb69-2" aria-hidden="true" tabindex="-1"></a> <span class="ex">tensor</span><span class="er">(</span><span class="ex">[282,</span> 285, 287, 281, 728]<span class="kw">)</span><span class="ex">,</span></span>
<span id="cb69-3"><a href="#cb69-3" aria-hidden="true" tabindex="-1"></a> <span class="ex">tensor</span><span class="er">(</span><span class="ex">[0.4744,</span> 0.3761, 0.0691, 0.0622, 0.0047]<span class="kw">))</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="using-the-xnnpack-accelerated-backend" class="level2">
<h2 class="anchored" data-anchor-id="using-the-xnnpack-accelerated-backend">Using the XNNPACK accelerated backend</h2>
<p><strong>Note</strong>: We need to export a model using the <code>convert_mobv2_xnnpack.py</code> script first.</p>
<div class="sourceCode" id="cb70"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb70-1"><a href="#cb70-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Test with the cat image</span></span>
<span id="cb70-2"><a href="#cb70-2" aria-hidden="true" tabindex="-1"></a>inf_time, indices, probs <span class="op">=</span> classify_image_executorch(</span>
<span id="cb70-3"><a href="#cb70-3" aria-hidden="true" tabindex="-1"></a>    img_path<span class="op">=</span><span class="st">"../images/cat.jpg"</span>,</span>
<span id="cb70-4"><a href="#cb70-4" aria-hidden="true" tabindex="-1"></a>    model_path<span class="op">=</span><span class="st">"../models/mobilenet_v2_xnnpack.pte"</span>,</span>
<span id="cb70-5"><a href="#cb70-5" aria-hidden="true" tabindex="-1"></a>    labels_path<span class="op">=</span><span class="st">"../models/imagenet_labels.json"</span>,</span>
<span id="cb70-6"><a href="#cb70-6" aria-hidden="true" tabindex="-1"></a>    top_k<span class="op">=</span><span class="dv">5</span></span>
<span id="cb70-7"><a href="#cb70-7" aria-hidden="true" tabindex="-1"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="./images/png/infer2.png" class="img-fluid figure-img" style="width:65.0%"></p>
</figure>
</div>
<p><code>The inference time was reduced from +2.5s to around -20ms</code></p>
</section>
<section id="quantized-model---xnnpack-accelerated-backend" class="level2">
<h2 class="anchored" data-anchor-id="quantized-model---xnnpack-accelerated-backend">Quantized model - XNNPACK accelerated backend</h2>
<p><strong>Note</strong>: We need to export a model first using the <code>convert_mobv2_xnnpack_int8.py</code> script.</p>
<div class="sourceCode" id="cb71"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb71-1"><a href="#cb71-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Test with the cat image</span></span>
<span id="cb71-2"><a href="#cb71-2" aria-hidden="true" tabindex="-1"></a>inf_time, indices, probs <span class="op">=</span> classify_image_executorch(</span>
<span id="cb71-3"><a href="#cb71-3" aria-hidden="true" tabindex="-1"></a>    img_path<span class="op">=</span><span class="st">"../images/cat.jpg"</span>,</span>
<span id="cb71-4"><a href="#cb71-4" aria-hidden="true" tabindex="-1"></a>    model_path<span class="op">=</span><span class="st">"../models/mobilenet_v2_quantized_xnnpack.pte"</span>,</span>
<span id="cb71-5"><a href="#cb71-5" aria-hidden="true" tabindex="-1"></a>    labels_path<span class="op">=</span><span class="st">"../models/imagenet_labels.json"</span>,</span>
<span id="cb71-6"><a href="#cb71-6" aria-hidden="true" tabindex="-1"></a>    top_k<span class="op">=</span><span class="dv">5</span></span>
<span id="cb71-7"><a href="#cb71-7" aria-hidden="true" tabindex="-1"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="./images/png/infer3.png" class="img-fluid figure-img" style="width:65.0%"></p>
</figure>
</div>
<p><code>==&gt; Even faster inference with a lower model in size</code></p>
<blockquote class="blockquote">
<p>Slightly higher probabilities in the INT8 model are normal and do not indicate a problem by themselves. Quantization slightly changes the logits, and softmax can become a bit “sharper” or “flatter” even when top‑1 remains correct.</p>
</blockquote>
</section>
<section id="camera-integration" class="level2">
<h2 class="anchored" data-anchor-id="camera-integration">Camera Integration</h2>
<p>We essentially have two different Python worlds: system Python 3.13 (where the camera stack is wired up) and our 3.11 virtual env (where ExecuTorch is installed). To run ExecuTorch on live frames from the Pi camera, we need to bridge those worlds.</p>
<blockquote class="blockquote">
<p>Why the camera “only works” in 3.13</p>
<ul>
<li>Recent Raspberry Pi OS uses Picamera2 on top of libcamera as the recommended interface.</li>
<li>The Picamera2/libcamera Python bindings are usually installed into the system Python and are not trivially pip‑installable into arbitrary venvs or other Python versions.</li>
<li>Once we create a separate 3.11 environment, it will not automatically see the Picamera2/libcamera bindings under 3.13, so imports fail or the camera device is not accessible from that environment.</li>
</ul>
</blockquote>
<p>We will use a <code>two‑process solution</code>: capture in 3.13, infer in 3.11. For that, we should run a small capture service under Python 3.13 that:</p>
<ul>
<li>Grabs frames from the Pi camera (Picamera2 / libcamera).</li>
<li>Sends frames to your ExecuTorch process (3.11) over a local channel (e.g., ZeroMQ, TCP/UDP socket, shared memory, filesystem (write JPEG/PNG to a temp directory and signal), or a simple HTTP server.</li>
</ul>
<p>The 3.11 process (under venev) receives the frame, decodes it, runs the preprocessing pipeline (resize, normalize), then calls ExecuTorch for inference..</p>
<section id="image-capture" class="level3">
<h3 class="anchored" data-anchor-id="image-capture">Image Capture</h3>
<p>Outside of the ExecuTorch env and folder, we will create a folder (<code>CAMERA</code>).</p>
<pre><code>Documents/
├── EXECUTORCH/MOBILENET/     # Python 3.11
├── CAMERA/                   # Python 3.13
       ├── camera_capture.py
       ├── camera_capture.jpg</code></pre>
<p>There we will run the script <a href="https://github.com/Mjrovai/EdgeML-with-Raspberry-Pi/blob/main/EXECUTORCH/scripts/camera_capture.py">camera_capture.py</a>):</p>
<div class="sourceCode" id="cb73"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb73-1"><a href="#cb73-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb73-2"><a href="#cb73-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> picamera2 <span class="im">import</span> Picamera2</span>
<span id="cb73-3"><a href="#cb73-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> time</span>
<span id="cb73-4"><a href="#cb73-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb73-5"><a href="#cb73-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"NumPy version: </span><span class="sc">{</span>np<span class="sc">.</span>__version__<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb73-6"><a href="#cb73-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb73-7"><a href="#cb73-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Initialize camera</span></span>
<span id="cb73-8"><a href="#cb73-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb73-9"><a href="#cb73-9" aria-hidden="true" tabindex="-1"></a>picam2 <span class="op">=</span> Picamera2()</span>
<span id="cb73-10"><a href="#cb73-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb73-11"><a href="#cb73-11" aria-hidden="true" tabindex="-1"></a>config <span class="op">=</span> picam2.create_preview_configuration(main<span class="op">=</span>{<span class="st">"size"</span>:(<span class="dv">640</span>,<span class="dv">480</span>)}) </span>
<span id="cb73-12"><a href="#cb73-12" aria-hidden="true" tabindex="-1"></a>picam2.configure(config)</span>
<span id="cb73-13"><a href="#cb73-13" aria-hidden="true" tabindex="-1"></a>picam2.start()</span>
<span id="cb73-14"><a href="#cb73-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb73-15"><a href="#cb73-15" aria-hidden="true" tabindex="-1"></a><span class="co"># Wait for camera to warm up</span></span>
<span id="cb73-16"><a href="#cb73-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb73-17"><a href="#cb73-17" aria-hidden="true" tabindex="-1"></a>time.sleep(<span class="dv">2</span>)</span>
<span id="cb73-18"><a href="#cb73-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb73-19"><a href="#cb73-19" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Camera working in isolated venv!"</span>)</span>
<span id="cb73-20"><a href="#cb73-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb73-21"><a href="#cb73-21" aria-hidden="true" tabindex="-1"></a><span class="co"># Capture image</span></span>
<span id="cb73-22"><a href="#cb73-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb73-23"><a href="#cb73-23" aria-hidden="true" tabindex="-1"></a>picam2.capture_file(<span class="st">"camera_capture.jpg"</span>)</span>
<span id="cb73-24"><a href="#cb73-24" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Image captured: camera_capture.jpg"</span>)</span>
<span id="cb73-25"><a href="#cb73-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb73-26"><a href="#cb73-26" aria-hidden="true" tabindex="-1"></a><span class="co"># Stop camera</span></span>
<span id="cb73-27"><a href="#cb73-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb73-28"><a href="#cb73-28" aria-hidden="true" tabindex="-1"></a>picam2.stop()</span>
<span id="cb73-29"><a href="#cb73-29" aria-hidden="true" tabindex="-1"></a>picam2.close()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Runing the script, we um get an image that will be stored on:</p>
<ul>
<li><code>/Documents/CAMERA/camera_capture.jpg</code></li>
</ul>
<p>Looking from the notebook folder, the image path will be:</p>
<div class="sourceCode" id="cb74"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb74-1"><a href="#cb74-1" aria-hidden="true" tabindex="-1"></a>..<span class="op">/</span>..<span class="op">/</span>..<span class="op">/</span>..<span class="op">/</span>CAMERA<span class="op">/</span>camera_capture.jpg</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Let’s run the same function used with the test image:</p>
<div class="sourceCode" id="cb75"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb75-1"><a href="#cb75-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Test the quantized model with the captured image</span></span>
<span id="cb75-2"><a href="#cb75-2" aria-hidden="true" tabindex="-1"></a>inf_time, indices, probs <span class="op">=</span> classify_image_executorch(</span>
<span id="cb75-3"><a href="#cb75-3" aria-hidden="true" tabindex="-1"></a>    img_path<span class="op">=</span><span class="st">"../../../../CAMERA/camera_capture.jpg"</span>,</span>
<span id="cb75-4"><a href="#cb75-4" aria-hidden="true" tabindex="-1"></a>    model_path<span class="op">=</span><span class="st">"../models/mobilenet_v2_quantized_xnnpack.pte"</span>,</span>
<span id="cb75-5"><a href="#cb75-5" aria-hidden="true" tabindex="-1"></a>    labels_path<span class="op">=</span><span class="st">"../models/imagenet_labels.json"</span>,</span>
<span id="cb75-6"><a href="#cb75-6" aria-hidden="true" tabindex="-1"></a>    top_k<span class="op">=</span><span class="dv">5</span></span>
<span id="cb75-7"><a href="#cb75-7" aria-hidden="true" tabindex="-1"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p><img src="./images/png/infer4.png" class="img-fluid"></p>
</section>
</section>
<section id="performance-benchmarking" class="level2">
<h2 class="anchored" data-anchor-id="performance-benchmarking">Performance Benchmarking</h2>
<p>Let’s now define a function to run inference several times for each model and compare their performance.</p>
<div class="sourceCode" id="cb76"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb76-1"><a href="#cb76-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> benchmark_inference(model_path, num_runs<span class="op">=</span><span class="dv">50</span>):</span>
<span id="cb76-2"><a href="#cb76-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb76-3"><a href="#cb76-3" aria-hidden="true" tabindex="-1"></a><span class="co">    Benchmark model inference speed</span></span>
<span id="cb76-4"><a href="#cb76-4" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb76-5"><a href="#cb76-5" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Benchmarking model: </span><span class="sc">{</span>model_path<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb76-6"><a href="#cb76-6" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Number of runs: </span><span class="sc">{</span>num_runs<span class="sc">}</span><span class="ch">\n</span><span class="ss">"</span>)</span>
<span id="cb76-7"><a href="#cb76-7" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb76-8"><a href="#cb76-8" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Load model</span></span>
<span id="cb76-9"><a href="#cb76-9" aria-hidden="true" tabindex="-1"></a>    model <span class="op">=</span> _load_for_executorch(model_path)</span>
<span id="cb76-10"><a href="#cb76-10" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb76-11"><a href="#cb76-11" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Create dummy input</span></span>
<span id="cb76-12"><a href="#cb76-12" aria-hidden="true" tabindex="-1"></a>    dummy_input <span class="op">=</span> torch.randn(<span class="dv">1</span>, <span class="dv">3</span>, <span class="dv">224</span>, <span class="dv">224</span>)</span>
<span id="cb76-13"><a href="#cb76-13" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb76-14"><a href="#cb76-14" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Warmup (10 runs)</span></span>
<span id="cb76-15"><a href="#cb76-15" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"Warming up..."</span>)</span>
<span id="cb76-16"><a href="#cb76-16" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">10</span>):</span>
<span id="cb76-17"><a href="#cb76-17" aria-hidden="true" tabindex="-1"></a>        <span class="cf">with</span> torch.no_grad():</span>
<span id="cb76-18"><a href="#cb76-18" aria-hidden="true" tabindex="-1"></a>            _ <span class="op">=</span> model.forward((dummy_input,))</span>
<span id="cb76-19"><a href="#cb76-19" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb76-20"><a href="#cb76-20" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Benchmark</span></span>
<span id="cb76-21"><a href="#cb76-21" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Running benchmark..."</span>)</span>
<span id="cb76-22"><a href="#cb76-22" aria-hidden="true" tabindex="-1"></a>    times <span class="op">=</span> []</span>
<span id="cb76-23"><a href="#cb76-23" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(num_runs):</span>
<span id="cb76-24"><a href="#cb76-24" aria-hidden="true" tabindex="-1"></a>        start <span class="op">=</span> time.time()</span>
<span id="cb76-25"><a href="#cb76-25" aria-hidden="true" tabindex="-1"></a>        <span class="cf">with</span> torch.no_grad():</span>
<span id="cb76-26"><a href="#cb76-26" aria-hidden="true" tabindex="-1"></a>            _ <span class="op">=</span> model.forward((dummy_input,))</span>
<span id="cb76-27"><a href="#cb76-27" aria-hidden="true" tabindex="-1"></a>        times.append(time.time() <span class="op">-</span> start)</span>
<span id="cb76-28"><a href="#cb76-28" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb76-29"><a href="#cb76-29" aria-hidden="true" tabindex="-1"></a>    times <span class="op">=</span> np.array(times) <span class="op">*</span> <span class="dv">1000</span>  <span class="co"># Convert to ms</span></span>
<span id="cb76-30"><a href="#cb76-30" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb76-31"><a href="#cb76-31" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Print statistics</span></span>
<span id="cb76-32"><a href="#cb76-32" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">"</span> <span class="op">+</span> <span class="st">"="</span><span class="op">*</span><span class="dv">50</span>)</span>
<span id="cb76-33"><a href="#cb76-33" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"BENCHMARK RESULTS"</span>)</span>
<span id="cb76-34"><a href="#cb76-34" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"="</span><span class="op">*</span><span class="dv">50</span>)</span>
<span id="cb76-35"><a href="#cb76-35" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"  Mean:   </span><span class="sc">{</span>times<span class="sc">.</span>mean()<span class="sc">:.2f}</span><span class="ss"> ms"</span>)</span>
<span id="cb76-36"><a href="#cb76-36" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"  Median: </span><span class="sc">{</span>np<span class="sc">.</span>median(times)<span class="sc">:.2f}</span><span class="ss"> ms"</span>)</span>
<span id="cb76-37"><a href="#cb76-37" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"  Std:    </span><span class="sc">{</span>times<span class="sc">.</span>std()<span class="sc">:.2f}</span><span class="ss"> ms"</span>)</span>
<span id="cb76-38"><a href="#cb76-38" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"  Min:    </span><span class="sc">{</span>times<span class="sc">.</span><span class="bu">min</span>()<span class="sc">:.2f}</span><span class="ss"> ms"</span>)</span>
<span id="cb76-39"><a href="#cb76-39" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"  Max:    </span><span class="sc">{</span>times<span class="sc">.</span><span class="bu">max</span>()<span class="sc">:.2f}</span><span class="ss"> ms"</span>)</span>
<span id="cb76-40"><a href="#cb76-40" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"="</span><span class="op">*</span><span class="dv">50</span>)</span>
<span id="cb76-41"><a href="#cb76-41" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb76-42"><a href="#cb76-42" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Plot distribution</span></span>
<span id="cb76-43"><a href="#cb76-43" aria-hidden="true" tabindex="-1"></a>    plt.figure(figsize<span class="op">=</span>(<span class="dv">12</span>, <span class="dv">4</span>))</span>
<span id="cb76-44"><a href="#cb76-44" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb76-45"><a href="#cb76-45" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Histogram</span></span>
<span id="cb76-46"><a href="#cb76-46" aria-hidden="true" tabindex="-1"></a>    plt.subplot(<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">1</span>)</span>
<span id="cb76-47"><a href="#cb76-47" aria-hidden="true" tabindex="-1"></a>    plt.hist(times, bins<span class="op">=</span><span class="dv">20</span>, edgecolor<span class="op">=</span><span class="st">'black'</span>, alpha<span class="op">=</span><span class="fl">0.7</span>)</span>
<span id="cb76-48"><a href="#cb76-48" aria-hidden="true" tabindex="-1"></a>    plt.axvline(times.mean(), color<span class="op">=</span><span class="st">'red'</span>, linestyle<span class="op">=</span><span class="st">'--'</span>, </span>
<span id="cb76-49"><a href="#cb76-49" aria-hidden="true" tabindex="-1"></a>                label<span class="op">=</span><span class="ss">f'Mean: </span><span class="sc">{</span>times<span class="sc">.</span>mean()<span class="sc">:.2f}</span><span class="ss"> ms'</span>)</span>
<span id="cb76-50"><a href="#cb76-50" aria-hidden="true" tabindex="-1"></a>    plt.xlabel(<span class="st">'Inference Time (ms)'</span>)</span>
<span id="cb76-51"><a href="#cb76-51" aria-hidden="true" tabindex="-1"></a>    plt.ylabel(<span class="st">'Frequency'</span>)</span>
<span id="cb76-52"><a href="#cb76-52" aria-hidden="true" tabindex="-1"></a>    plt.title(<span class="st">'Inference Time Distribution'</span>)</span>
<span id="cb76-53"><a href="#cb76-53" aria-hidden="true" tabindex="-1"></a>    plt.legend()</span>
<span id="cb76-54"><a href="#cb76-54" aria-hidden="true" tabindex="-1"></a>    plt.grid(alpha<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb76-55"><a href="#cb76-55" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb76-56"><a href="#cb76-56" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Time series</span></span>
<span id="cb76-57"><a href="#cb76-57" aria-hidden="true" tabindex="-1"></a>    plt.subplot(<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">2</span>)</span>
<span id="cb76-58"><a href="#cb76-58" aria-hidden="true" tabindex="-1"></a>    plt.plot(times, marker<span class="op">=</span><span class="st">'o'</span>, markersize<span class="op">=</span><span class="dv">3</span>, alpha<span class="op">=</span><span class="fl">0.6</span>)</span>
<span id="cb76-59"><a href="#cb76-59" aria-hidden="true" tabindex="-1"></a>    plt.axhline(times.mean(), color<span class="op">=</span><span class="st">'red'</span>, linestyle<span class="op">=</span><span class="st">'--'</span>, </span>
<span id="cb76-60"><a href="#cb76-60" aria-hidden="true" tabindex="-1"></a>                label<span class="op">=</span><span class="ss">f'Mean: </span><span class="sc">{</span>times<span class="sc">.</span>mean()<span class="sc">:.2f}</span><span class="ss"> ms'</span>)</span>
<span id="cb76-61"><a href="#cb76-61" aria-hidden="true" tabindex="-1"></a>    plt.xlabel(<span class="st">'Run Number'</span>)</span>
<span id="cb76-62"><a href="#cb76-62" aria-hidden="true" tabindex="-1"></a>    plt.ylabel(<span class="st">'Inference Time (ms)'</span>)</span>
<span id="cb76-63"><a href="#cb76-63" aria-hidden="true" tabindex="-1"></a>    plt.title(<span class="st">'Inference Time Over Runs'</span>)</span>
<span id="cb76-64"><a href="#cb76-64" aria-hidden="true" tabindex="-1"></a>    plt.legend()</span>
<span id="cb76-65"><a href="#cb76-65" aria-hidden="true" tabindex="-1"></a>    plt.grid(alpha<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb76-66"><a href="#cb76-66" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb76-67"><a href="#cb76-67" aria-hidden="true" tabindex="-1"></a>    plt.tight_layout()</span>
<span id="cb76-68"><a href="#cb76-68" aria-hidden="true" tabindex="-1"></a>    plt.show()</span>
<span id="cb76-69"><a href="#cb76-69" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb76-70"><a href="#cb76-70" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> times</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>To recal, we have the folowing converted models:</p>
<pre><code>mobilenet_v2.pte     
mobilenet_v2_xnnpack.pte
mobilenet_v2_quantized_xnnpack.pte</code></pre>
<section id="basic-float32-mobilenet_v2.pte" class="level3">
<h3 class="anchored" data-anchor-id="basic-float32-mobilenet_v2.pte">Basic (Float32): mobilenet_v2.pte</h3>
<div class="sourceCode" id="cb78"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb78-1"><a href="#cb78-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Run benchmark</span></span>
<span id="cb78-2"><a href="#cb78-2" aria-hidden="true" tabindex="-1"></a>benchmark_times <span class="op">=</span> benchmark_inference(</span>
<span id="cb78-3"><a href="#cb78-3" aria-hidden="true" tabindex="-1"></a>    model_path<span class="op">=</span><span class="st">"../models/mobilenet_v2.pte"</span>,</span>
<span id="cb78-4"><a href="#cb78-4" aria-hidden="true" tabindex="-1"></a>    num_runs<span class="op">=</span><span class="dv">50</span></span>
<span id="cb78-5"><a href="#cb78-5" aria-hidden="true" tabindex="-1"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p><img src="./images/png/bench1.png" class="img-fluid"></p>
</section>
<section id="xnnpack-backend-flot32-mobilenet_v2_xnnpack.pte" class="level3">
<h3 class="anchored" data-anchor-id="xnnpack-backend-flot32-mobilenet_v2_xnnpack.pte">XNNPACK Backend (Flot32): mobilenet_v2_xnnpack.pte</h3>
<div class="sourceCode" id="cb79"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb79-1"><a href="#cb79-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Run benchmark</span></span>
<span id="cb79-2"><a href="#cb79-2" aria-hidden="true" tabindex="-1"></a>benchmark_times <span class="op">=</span> benchmark_inference(</span>
<span id="cb79-3"><a href="#cb79-3" aria-hidden="true" tabindex="-1"></a>    model_path<span class="op">=</span><span class="st">"../models/mobilenet_v2_xnnpack.pte"</span>,</span>
<span id="cb79-4"><a href="#cb79-4" aria-hidden="true" tabindex="-1"></a>    num_runs<span class="op">=</span><span class="dv">50</span></span>
<span id="cb79-5"><a href="#cb79-5" aria-hidden="true" tabindex="-1"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p><img src="./images/png/bench2.png" class="img-fluid"></p>
</section>
<section id="quantization-int8-mobilenet_v2_quantized_xnnpack.pte" class="level3">
<h3 class="anchored" data-anchor-id="quantization-int8-mobilenet_v2_quantized_xnnpack.pte">Quantization (INT8): mobilenet_v2_quantized_xnnpack.pte</h3>
<div class="sourceCode" id="cb80"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb80-1"><a href="#cb80-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Run benchmark</span></span>
<span id="cb80-2"><a href="#cb80-2" aria-hidden="true" tabindex="-1"></a>benchmark_times <span class="op">=</span> benchmark_inference(</span>
<span id="cb80-3"><a href="#cb80-3" aria-hidden="true" tabindex="-1"></a>    model_path<span class="op">=</span><span class="st">"../models/mobilenet_v2_quantized_xnnpack.pte"</span>,</span>
<span id="cb80-4"><a href="#cb80-4" aria-hidden="true" tabindex="-1"></a>    num_runs<span class="op">=</span><span class="dv">50</span></span>
<span id="cb80-5"><a href="#cb80-5" aria-hidden="true" tabindex="-1"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p><img src="./images/png/bench3.png" class="img-fluid"></p>
</section>
<section id="performance-comparison-table" class="level3">
<h3 class="anchored" data-anchor-id="performance-comparison-table">Performance Comparison Table</h3>
<p>Based on actual benchmarking results on Raspberry Pi 5:</p>
<table class="table">
<colgroup>
<col style="width: 26%">
<col style="width: 12%">
<col style="width: 15%">
<col style="width: 16%">
<col style="width: 19%">
<col style="width: 9%">
</colgroup>
<thead>
<tr class="header">
<th>Model Configuration</th>
<th>Mean (ms)</th>
<th>Median (ms)</th>
<th>Std Dev (ms)</th>
<th>File Size (MB)</th>
<th>Latency</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Float32 (basic)</td>
<td>2440</td>
<td>2440</td>
<td>2.17</td>
<td>13.58</td>
<td>+600×</td>
</tr>
<tr class="even">
<td>Float32 + XNNPACK</td>
<td>11.24</td>
<td>10.84</td>
<td>1.67</td>
<td>13.35</td>
<td>~3×</td>
</tr>
<tr class="odd">
<td>INT8 + XNNPACK</td>
<td>3.91</td>
<td>3.69</td>
<td>0.55</td>
<td>3.59</td>
<td>1×</td>
</tr>
</tbody>
</table>
<p><strong>Key Observations:</strong></p>
<ol type="1">
<li><strong>XNNPACK Impact</strong>: Backend delegation provides an important speedup even without quantization</li>
<li><strong>Quantization Benefit</strong>: INT8 quantization, besides size reduction, adds additional speedup beyond XNNPACK</li>
<li><strong>Variability</strong>: Quantized model shows lower standard deviation, indicating more stable performance</li>
<li><strong>Size-Speed Tradeoff</strong>: 75% size reduction (14MB → 3.5MB) with 3× speed improvement</li>
</ol>
</section>
</section>
<section id="exploring-custom-models" class="level2">
<h2 class="anchored" data-anchor-id="exploring-custom-models">Exploring Custom Models</h2>
<p><strong>CIFAR-10 Dataset:</strong></p>
<ul>
<li>10 classes: airplane, automobile, bird, cat, deer, dog, frog, horse, ship, truck</li>
</ul>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://pytorch.org/tutorials/_static/img/cifar10.png" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">cifar10</figcaption>
</figure>
</div>
<ul>
<li>The images in CIFAR-10 are of size 3x32x32 (3-channel color images of 32x32 pixels in size).</li>
</ul>
<section id="exporting-a-custom-trained-model" class="level3">
<h3 class="anchored" data-anchor-id="exporting-a-custom-trained-model">Exporting a Custom Trained Model</h3>
<p>Let’s create a Project folder structure as below (some files are shown as they will appear later)</p>
<pre><code>EXECUTORCH/CIFAR-10/
├── export_cifar10_xnnpack.py
├── inference_cifar10_xnnpack.py       
├── models/
│   ├── cifar10_model_jit.pt                    # Float32 pytorch model
│   └── cifar10_xnnpack.pte                     # Float32 conv model
├── images/                                     # Test images
│   └── cat.jpg
└── notebooks/                                   
    └── CIFAR-10_Inference_RPI.ipynb</code></pre>
<p>Let’s train a model from scratch on CIFAR-10. For that, we can run the Notebook below on Google Colab:</p>
<p><a href="https://github.com/Mjrovai/EdgeML-with-Raspberry-Pi/blob/main/EXECUTORCH/notebooks/cifar10_colab_training.ipynb">cifar10_colab_training.ipynb</a></p>
<p>From the training, we will have the trained model:</p>
<p><code>cifar10_model_jit.pt</code>, which should be saved on <code>/models</code> folder</p>
<p>Next, as we did before, we should export the PyTorch model to ExecuTorch, and let’s use XNNPACK. Run the script: <a href="https://github.com/Mjrovai/EdgeML-with-Raspberry-Pi/blob/main/EXECUTORCH/notebooks/cifar10_colab_training.ipynb">export_cifar10_xnnpack.py</a>, as a result, we have:</p>
<p><img src="./images/png/cifar-10-export.png" class="img-fluid"></p>
<p>Runing it, a converted model <code>cifar10_xnnpack.pte</code> will be saved in <code>./models/</code> folder.</p>
</section>
<section id="running-custom-models-on-raspberry-pi" class="level3">
<h3 class="anchored" data-anchor-id="running-custom-models-on-raspberry-pi">Running Custom Models on Raspberry Pi</h3>
<p>Runing the script <a href="https://github.com/Mjrovai/EdgeML-with-Raspberry-Pi/blob/main/EXECUTORCH/scripts/inference_cifar10_xnnpack.py">inference_cifar10_xnnpack.py</a>, over the “cat” image, we can see that the converted model is working fine:</p>
<p><code>python inference_cifar10_xnnpack.py ./images/cat.jpg</code></p>
<p><img src="./images/png/cifar-10-test1.png" class="img-fluid"></p>
<p>And runing 20 times….</p>
<p><img src="./images/png/cifar-10-benchmark.png" class="img-fluid"></p>
<p>Despite the exported model being OK, when we make an inference with the original PyTorch model, in this case (a small model), we will find even lower latencies.</p>
<p><img src="./images/png/performance_comparison.png" class="img-fluid"></p>
<p>In short, our export script is conceptually the right pattern for ExecuTorch+XNNPACK on Arm, but for this specific small CIFAR‑10 CNN, the overhead of ExecuTorch and partial XNNPACK delegation on a Pi‑class device can easily make it slower than a well‑optimized plain PyTorch JIT model.</p>
<p>Optionally, it is possible to explore those models with the notebook:</p>
<p><code>CIFAR-10_Inference_RPI_Updated.ipynb</code></p>
<hr>
</section>
</section>
<section id="conclusion" class="level2">
<h2 class="anchored" data-anchor-id="conclusion">Conclusion</h2>
<p>This chapter adapted our image classification workflow from TensorFlow Lite to PyTorch EXECUTORCH, demonstrating that the PyTorch ecosystem provides a powerful and modern alternative for edge AI deployment on Raspberry Pi devices.</p>
<p>EXECUTORCH represents a significant evolution in edge AI deployment, bringing PyTorch’s research-friendly ecosystem to production edge devices. While TensorFlow Lite remains excellent and mature, having EXECUTORCH in your toolkit makes you a more versatile edge AI practitioner.</p>
<p>The future of edge AI is multi-framework, multi-platform, and rapidly evolving. By mastering both EXECUTORCH and TensorFlow Lite, you’re positioned to make informed technical decisions and adapt as the landscape changes.</p>
<blockquote class="blockquote">
<p><strong>Remember:</strong> The best framework is the one that serves your specific needs. This tutorial empowers you to make that choice confidently.</p>
</blockquote>
<section id="key-takeaways" class="level3">
<h3 class="anchored" data-anchor-id="key-takeaways">Key Takeaways</h3>
<p><strong>Technical Achievements:</strong></p>
<ul>
<li>Successfully set up PyTorch and EXECUTORCH on Raspberry Pi (4/5)</li>
<li>Learned the complete model export pipeline from PyTorch to .pte format</li>
<li>Implemented quantization for reduced model size (~3.5MB vs ~14MB)</li>
<li>Created reusable inference functions for both standard and custom models</li>
<li>Integrated camera capture with EXECUTORCH inference</li>
</ul>
<p><strong>EXECUTORCH Advantages:</strong></p>
<ul>
<li><strong>Unified ecosystem</strong>: Training and deployment in the same framework</li>
<li><strong>Modern architecture</strong>: Built for contemporary edge computing needs</li>
<li><strong>Flexibility</strong>: Easy export of any PyTorch model</li>
<li><strong>Quantization</strong>: Native PyTorch quantization support</li>
<li><strong>Active development</strong>: Continuous improvements from Meta and the community</li>
</ul>
<p><strong>Comparison with TFLite:</strong> Both frameworks achieve similar goals with different philosophies:</p>
<ul>
<li><strong>EXECUTORCH</strong>: Better for PyTorch users, newer technology, growing ecosystem</li>
<li><strong>TFLite</strong>: More mature, broader hardware support, larger community</li>
</ul>
<p>The choice between them often comes down to your training framework and specific requirements.</p>
</section>
<section id="performance-considerations" class="level3">
<h3 class="anchored" data-anchor-id="performance-considerations">Performance Considerations</h3>
<p>On Raspberry Pi 4/5, you can expect: - <strong>Float32 models</strong>: 10-20ms per inference (MobileNet V2)</p>
<ul>
<li><p><strong>Quantized models</strong>: 3-5ms per inference</p></li>
<li><p><strong>Memory usage</strong>: 4-15MB, depending on model size</p>
<hr></li>
</ul>
</section>
</section>
<section id="resources" class="level2">
<h2 class="anchored" data-anchor-id="resources">Resources</h2>
<section id="code-repository" class="level3">
<h3 class="anchored" data-anchor-id="code-repository">Code Repository</h3>
<ul>
<li><a href="https://github.com/Mjrovai/EdgeML-with-Raspberry-Pi/tree/main/EXECUTORCH">Tutorial Code Repository</a></li>
<li><a href="https://github.com/Mjrovai/EdgeML-with-Raspberry-Pi/tree/main/EXECUTORCH/scripts">Model Export and Inference Scripts</a></li>
<li><a href="https://github.com/Mjrovai/EdgeML-with-Raspberry-Pi/tree/main/EXECUTORCH/notebooks">Notebooks</a></li>
</ul>
</section>
<section id="official-documentation" class="level3">
<h3 class="anchored" data-anchor-id="official-documentation">Official Documentation</h3>
<p><strong>PyTorch &amp; EXECUTORCH:</strong></p>
<ul>
<li><a href="https://pytorch.org/">PyTorch Official Website</a></li>
<li><a href="https://pytorch.org/executorch/">EXECUTORCH Documentation</a></li>
<li><a href="https://github.com/pytorch/executorch">EXECUTORCH GitHub Repository</a></li>
<li><a href="https://pytorch.org/mobile/">PyTorch Mobile</a></li>
<li><a href="https://docs.pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html">Deep Learning with PyTorch: A 60 Minute Blitz</a></li>
</ul>
<p><strong>Quantization:</strong></p>
<ul>
<li><a href="https://pytorch.org/docs/stable/quantization.html">PyTorch Quantization</a></li>
<li><a href="https://pytorch.org/tutorials/advanced/static_quantization_tutorial.html">Quantization API Tutorial</a></li>
</ul>
<p><strong>Models:</strong></p>
<ul>
<li><a href="https://pytorch.org/vision/stable/models.html">Torchvision Models</a></li>
<li><a href="https://pytorch.org/executorch/stable/tutorials/export-to-executorch-tutorial.html">Pretrained Model Deployment Guide</a></li>
</ul>
<p><strong>Hardware Resources</strong></p>
<ul>
<li><a href="https://www.raspberrypi.com/documentation/">Raspberry Pi Official Documentation</a></li>
<li><a href="https://github.com/raspberrypi/picamera2">Picamera2 Library</a></li>
<li><a href="https://developer.arm.com/">ARM Architecture Optimization</a></li>
</ul>
</section>
<section id="books" class="level3">
<h3 class="anchored" data-anchor-id="books">Books</h3>
<ul>
<li><a href="https://mjrovai.github.io/EdgeML_Made_Ease_ebook/"><strong>Edge AI Engineering e-book</strong></a>- by Prof.&nbsp;Marcelo Rovai, UNIFEI</li>
<li><a href="https://mlsysbook.ai/book/"><strong>Machine Learning Systems</strong></a> - by Prof.&nbsp;Vijay Janapa Reddi, Harvard University</li>
<li><a href="https://learning.oreilly.com/library/view/ai-and-ml/9781098199166/"><strong>AI and ML for Coders in PyTorch</strong></a> by Laurence Moroney</li>
</ul>


</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
          // target, if specified
          link.setAttribute("target", "_blank");
          // default icon
          link.classList.add("external");
      }
    }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="../../raspi/counting_objects_yolo/counting_objects_yolo.html" class="pagination-link">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text">Counting objects with YOLO</span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="../../raspi/hw_acceleration/hw_acceleration.html" class="pagination-link">
        <span class="nav-page-text">Beyond CPU - Hardware Acceleration for Edge AI</span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">Written and edited by Prof.&nbsp;Marcelo Rovai (UNIFEI University)</div>   
    <div class="nav-footer-center">
      &nbsp;
    </div>
    <div class="nav-footer-right">This book was built with <a href="https://quarto.org/">Quarto</a>.</div>
  </div>
</footer>



</body></html>