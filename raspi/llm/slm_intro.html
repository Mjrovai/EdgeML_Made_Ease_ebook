<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.3.450">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Edge AI Engineering - Small Language Models (SLM)</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<link href="../../raspi/llm/slm_opt_tech.html" rel="next">
<link href="../../raspi/kd_intro/kd_intro.html" rel="prev">
<link href="../../images/unifei-logo.png" rel="icon" type="image/png">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>


<link rel="stylesheet" href="../../styles.css">
</head>

<body class="nav-sidebar floating nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">Edge AI Engineering</span>
    </a>
  </div>
        <div class="quarto-navbar-tools ms-auto tools-wide">
    <a href="https://github.com/Mjrovai/EdgeML-with-Raspberry-Pi" rel="" title="Source Code" class="quarto-navigation-tool px-1" aria-label="Source Code"><i class="bi bi-github"></i></a>
    <a href="../../Edge-AI-Engineering.pdf" rel="" title="Download PDF" class="quarto-navigation-tool px-1" aria-label="Download PDF"><i class="bi bi-file-pdf"></i></a>
    <div class="dropdown">
      <a href="" title="Share" id="quarto-navigation-tool-dropdown-0" class="quarto-navigation-tool dropdown-toggle px-1" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Share"><i class="bi bi-share"></i></a>
      <ul class="dropdown-menu dropdown-menu-end" aria-labelledby="quarto-navigation-tool-dropdown-0">
          <li>
            <a class="dropdown-item quarto-navbar-tools ms-auto-item" href="https://twitter.com/intent/tweet?url=|url|">
              <i class="bi bi-bi-twitter pe-1"></i>
            Twitter
            </a>
          </li>
          <li>
            <a class="dropdown-item quarto-navbar-tools ms-auto-item" href="https://www.linkedin.com/sharing/share-offsite/?url=|url|">
              <i class="bi bi-bi-linkedin pe-1"></i>
            LinkedIn
            </a>
          </li>
          <li>
            <a class="dropdown-item quarto-navbar-tools ms-auto-item" href="https://www.facebook.com/sharer/sharer.php?u=|url|">
              <i class="bi bi-bi-facebook pe-1"></i>
            Facebook
            </a>
          </li>
      </ul>
    </div>
  <a href="" class="quarto-reader-toggle quarto-navigation-tool px-1" onclick="window.quartoToggleReader(); return false;" title="Toggle reader mode">
  <div class="quarto-reader-toggle-btn">
  <i class="bi"></i>
  </div>
</a>
</div>
          <div id="quarto-search" class="" title="Search"></div>
      </div> <!-- /container-fluid -->
    </nav>
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
      <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../../raspi/llm/slm_intro.html">Small Language Models (SLM)</a></li></ol></nav>
      <a class="flex-grow-1" role="button" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
      </a>
      <button type="button" class="btn quarto-search-button" aria-label="" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal sidebar-navigation floating overflow-auto">
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Preface</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../Acknowledgements.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Acknowledgments</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../intro.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Introduction</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../about_book.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">About this Book</span></a>
  </div>
</li>
        <li class="px-0"><hr class="sidebar-divider hi "></li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../Classification_of_AI_Applications.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Classification of AI Applications</span></a>
  </div>
</li>
        <li class="px-0"><hr class="sidebar-divider hi "></li>
        <li class="sidebar-item sidebar-item-section">
      <span class="sidebar-item-text sidebar-link text-start">
 <span class="menu-text"><div class="part">Fixed Function AI (Reactive)</div></span></span>
  </li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../raspi/setup/setup.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Setup</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../raspi/image_classification/image_classification_fund.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Image Classification Fundamentals</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../raspi/image_classification/custom_image_classification_project.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Custom Image Classification Project</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../raspi/object_detection/object_detection_fundamentals.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Object Detection: Fundamentals</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../raspi/object_detection/custom_object_detection.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Custom Object Detection Project</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../raspi/object_detection/cv_yolo.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Computer Vision Applications with YOLO</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../raspi/counting_objects_yolo/counting_objects_yolo.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Counting objects with YOLO</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <span class="sidebar-item-text sidebar-link text-start">
 <span class="menu-text"><div class="part">Generative AI (Proactive)</div></span></span>
  </li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../raspi/rnn-verne/rnn-verne.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Text Generation with RNNs</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../raspi/kd_intro/kd_intro.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Knowledge Distillation in Practice</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../raspi/llm/slm_intro.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text">Small Language Models (SLM)</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../raspi/llm/slm_opt_tech.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">SLM: Basic Optimization Techniques</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../raspi/vlm/vlm.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Vision-Language Models at the Edge</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../raspi/audio_pipeline/audio_pipeline.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Audio and Vision AI Pipeline</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../raspi/physical_comp/RPi_Physical_Computing.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Physical Computing with Raspberry Pi</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../raspi/iot/slm_iot.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Experimenting with SLMs for IoT Control</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../raspi/advancing_adgeai/adv_edgeai.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Advancing EdgeAI: Beyond Basic SLMs</span></a>
  </div>
</li>
        <li class="px-0"><hr class="sidebar-divider hi "></li>
        <li class="sidebar-item sidebar-item-section">
      <span class="sidebar-item-text sidebar-link text-start">
 <span class="menu-text"><div class="part">Weekly Labs</div></span></span>
  </li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../weekly_labs.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Edge AI Engineering - Weekly Labs</span></a>
  </div>
</li>
        <li class="px-0"><hr class="sidebar-divider hi "></li>
        <li class="sidebar-item sidebar-item-section">
      <span class="sidebar-item-text sidebar-link text-start">
 <span class="menu-text"><div class="part">References &amp; Author</div></span></span>
  </li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../references.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">References</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../about_the_author.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">About the author</span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#introduction" id="toc-introduction" class="nav-link active" data-scroll-target="#introduction">Introduction</a></li>
  <li><a href="#setup" id="toc-setup" class="nav-link" data-scroll-target="#setup">Setup</a>
  <ul>
  <li><a href="#raspberry-pi-active-cooler" id="toc-raspberry-pi-active-cooler" class="nav-link" data-scroll-target="#raspberry-pi-active-cooler">Raspberry Pi Active Cooler</a></li>
  </ul></li>
  <li><a href="#generative-ai-genai" id="toc-generative-ai-genai" class="nav-link" data-scroll-target="#generative-ai-genai">Generative AI (GenAI)</a>
  <ul>
  <li><a href="#large-language-models-llms" id="toc-large-language-models-llms" class="nav-link" data-scroll-target="#large-language-models-llms">Large Language Models (LLMs)</a></li>
  <li><a href="#closed-vs-open-models" id="toc-closed-vs-open-models" class="nav-link" data-scroll-target="#closed-vs-open-models">Closed vs Open Models:</a></li>
  <li><a href="#small-language-models-slms" id="toc-small-language-models-slms" class="nav-link" data-scroll-target="#small-language-models-slms">Small Language Models (SLMs)</a></li>
  </ul></li>
  <li><a href="#ollama" id="toc-ollama" class="nav-link" data-scroll-target="#ollama">Ollama</a>
  <ul>
  <li><a href="#installing-ollama" id="toc-installing-ollama" class="nav-link" data-scroll-target="#installing-ollama">Installing Ollama</a></li>
  <li><a href="#meta-llama-3.2-1b3b" id="toc-meta-llama-3.2-1b3b" class="nav-link" data-scroll-target="#meta-llama-3.2-1b3b">Meta Llama 3.2 1B/3B</a></li>
  <li><a href="#google-gemma" id="toc-google-gemma" class="nav-link" data-scroll-target="#google-gemma">Google Gemma</a>
  <ul class="collapse">
  <li><a href="#gemma3" id="toc-gemma3" class="nav-link" data-scroll-target="#gemma3">Gemma3</a></li>
  <li><a href="#gemma-3n" id="toc-gemma-3n" class="nav-link" data-scroll-target="#gemma-3n">Gemma 3n</a></li>
  </ul></li>
  <li><a href="#microsoft-phi3.5-3.8b" id="toc-microsoft-phi3.5-3.8b" class="nav-link" data-scroll-target="#microsoft-phi3.5-3.8b">Microsoft Phi3.5 3.8B</a></li>
  <li><a href="#moondream" id="toc-moondream" class="nav-link" data-scroll-target="#moondream">MoonDream</a></li>
  <li><a href="#lava-phi-3" id="toc-lava-phi-3" class="nav-link" data-scroll-target="#lava-phi-3">Lava-Phi-3</a></li>
  <li><a href="#inspecting-local-resources" id="toc-inspecting-local-resources" class="nav-link" data-scroll-target="#inspecting-local-resources">Inspecting local resources</a></li>
  </ul></li>
  <li><a href="#ollama-python-library" id="toc-ollama-python-library" class="nav-link" data-scroll-target="#ollama-python-library">Ollama Python Library</a>
  <ul>
  <li><a href="#ollama-generate" id="toc-ollama-generate" class="nav-link" data-scroll-target="#ollama-generate">Ollama Generate</a>
  <ul class="collapse">
  <li><a href="#streaming-with-ollama.generate" id="toc-streaming-with-ollama.generate" class="nav-link" data-scroll-target="#streaming-with-ollama.generate">Streaming with ollama.generate()</a></li>
  <li><a href="#system-prompt" id="toc-system-prompt" class="nav-link" data-scroll-target="#system-prompt">System Prompt</a></li>
  <li><a href="#temperature-and-sampling" id="toc-temperature-and-sampling" class="nav-link" data-scroll-target="#temperature-and-sampling"><strong>Temperature and Sampling</strong></a></li>
  </ul></li>
  <li><a href="#ollama.chat" id="toc-ollama.chat" class="nav-link" data-scroll-target="#ollama.chat">Ollama.chat()</a></li>
  <li><a href="#image-description" id="toc-image-description" class="nav-link" data-scroll-target="#image-description">Image Description:</a></li>
  </ul></li>
  <li><a href="#calling-direct-api" id="toc-calling-direct-api" class="nav-link" data-scroll-target="#calling-direct-api">Calling Direct API</a>
  <ul>
  <li><a href="#error-handling" id="toc-error-handling" class="nav-link" data-scroll-target="#error-handling">Error Handling</a></li>
  <li><a href="#connection-management" id="toc-connection-management" class="nav-link" data-scroll-target="#connection-management">Connection Management</a></li>
  <li><a href="#features-functionality" id="toc-features-functionality" class="nav-link" data-scroll-target="#features-functionality">Features &amp; Functionality</a></li>
  <li><a href="#dependencies" id="toc-dependencies" class="nav-link" data-scroll-target="#dependencies">Dependencies</a></li>
  <li><a href="#advanced-features" id="toc-advanced-features" class="nav-link" data-scroll-target="#advanced-features">Advanced Features</a></li>
  <li><a href="#when-to-use-each" id="toc-when-to-use-each" class="nav-link" data-scroll-target="#when-to-use-each">When to Use Each?</a>
  <ul class="collapse">
  <li><a href="#use-the-python-library-when" id="toc-use-the-python-library-when" class="nav-link" data-scroll-target="#use-the-python-library-when">Use the Python Library when:</a></li>
  <li><a href="#use-direct-api-calls-when" id="toc-use-direct-api-calls-when" class="nav-link" data-scroll-target="#use-direct-api-calls-when">Use Direct API Calls when:</a></li>
  </ul></li>
  <li><a href="#performance-difference" id="toc-performance-difference" class="nav-link" data-scroll-target="#performance-difference">Performance Difference</a></li>
  </ul></li>
  <li><a href="#going-further" id="toc-going-further" class="nav-link" data-scroll-target="#going-further">Going Further</a></li>
  <li><a href="#conclusion" id="toc-conclusion" class="nav-link" data-scroll-target="#conclusion">Conclusion</a></li>
  <li><a href="#resources" id="toc-resources" class="nav-link" data-scroll-target="#resources">Resources</a></li>
  </ul>
<div class="toc-actions"><div><i class="bi bi-github"></i></div><div class="action-links"><p><a href="https://github.com/Mjrovai/EdgeML-with-Raspberry-Pi/edit/main/raspi/llm/slm_intro.qmd" class="toc-action">Edit this page</a></p><p><a href="https://github.com/Mjrovai/EdgeML-with-Raspberry-Pi/issues/new" class="toc-action">Report an issue</a></p><p><a href="https://github.com/Mjrovai/EdgeML-with-Raspberry-Pi/blob/main/raspi/llm/slm_intro.qmd" class="toc-action">View source</a></p></div></div></nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Small Language Models (SLM)</h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  

</header>

<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/jpeg/cover.jpg" class="img-fluid figure-img"></p>
<figcaption class="figure-caption"><em>DALL·E prompt - A 1950s-style cartoon illustration showing a Raspberry Pi running a small language model at the edge. The Raspberry Pi is stylized in a retro-futuristic way with rounded edges and chrome accents, connected to playful cartoonish sensors and devices. Speech bubbles are floating around, representing language processing, and the background has a whimsical landscape of interconnected devices with wires and small gadgets, all drawn in a vintage cartoon style. The color palette uses soft pastel colors and bold outlines typical of 1950s cartoons, giving a fun and nostalgic vibe to the scene.</em></figcaption>
</figure>
</div>
<section id="introduction" class="level2">
<h2 class="anchored" data-anchor-id="introduction">Introduction</h2>
<p>In the fast-growing area of artificial intelligence, edge computing presents an opportunity to decentralize capabilities traditionally reserved for powerful, centralized servers. This chapter explores the practical integration of small versions of traditional large language models (LLMs) into a Raspberry Pi 5, transforming this edge device into an AI hub capable of real-time, on-site data processing.</p>
<p>As large language models grow in size and complexity, Small Language Models (SLMs) offer a compelling alternative for edge devices, striking a balance between performance and resource efficiency. By running these models directly on Raspberry Pi, we can create responsive, privacy-preserving applications that operate even in environments with limited or no internet connectivity.</p>
<p>This chapter guides us through setting up, optimizing, and leveraging SLMs on Raspberry Pi. We will explore the installation and utilization of <a href="https://ollama.com/">Ollama</a>. This open-source framework allows us to run LLMs locally on our machines (desktops or edge devices such as NVIDIA Jetson or Raspberry Pis).</p>
<p>Ollama is designed to be efficient, scalable, and easy to use, making it a good option for deploying AI models such as Microsoft Phi, Google Gemma, Meta Llama, and more. We will integrate some of those models into projects using Python’s ecosystem, exploring their potential in real-world scenarios (or at least point in this direction).</p>
</section>
<section id="setup" class="level2">
<h2 class="anchored" data-anchor-id="setup">Setup</h2>
<p>We could use any Raspberry Pi model in the previous labs, but here, the choice <strong>must be</strong> the Raspberry Pi 5 (Raspi-5). It is a robust platform that substantially upgrades the last version 4, equipped with the Broadcom BCM2712, a 2.4GHz quad-core 64-bit Arm Cortex-A76 CPU featuring Cryptographic Extension and enhanced caching capabilities. It boasts a VideoCore VII GPU, dual 4Kp60 HDMI® outputs with HDR, and a 4Kp60 HEVC decoder. Memory options include 4GB, 8GB, and 16GB of high-speed LPDDR4X SDRAM, with 8GB as our choice for running SLMs. It also features expandable storage via a microSD card slot and a PCIe 2.0 interface for fast peripherals such as M.2 SSDs (Solid State Drives).</p>
<blockquote class="blockquote">
<p>For real SSL applications, SSDs are a better option than SD cards.</p>
</blockquote>
<p>By the way, as <a href="https://www.hackster.io/aallan">Alasdair Allan</a> discussed, inferencing directly on the Raspberry Pi 5 CPU—with no GPU acceleration—is now on par with the performance of the Coral TPU.</p>
<p><img src="images/png/bench.png" class="img-fluid"></p>
<p>For more info, please see the complete article: <a href="https://www.hackster.io/news/benchmarking-tensorflow-and-tensorflow-lite-on-raspberry-pi-5-b9156d58a6a2?mc_cid=0cab3d08f4&amp;mc_eid=e96256ccba">Benchmarking TensorFlow and TensorFlow Lite on Raspberry Pi 5</a>.</p>
<section id="raspberry-pi-active-cooler" class="level3">
<h3 class="anchored" data-anchor-id="raspberry-pi-active-cooler">Raspberry Pi Active Cooler</h3>
<p>We suggest installing an Active Cooler, a dedicated clip-on cooling solution for Raspberry Pi 5 (Raspi-5), for this lab. It combines an aluminum heatsink with a temperature-controlled blower fan to keep the Raspi-5 operating comfortably under heavy loads, such as running SLMs.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/png/cooler.png" class="img-fluid figure-img" style="width:60.0%"></p>
</figure>
</div>
<p>The Active Cooler has pre-applied thermal pads for heat transfer and is mounted directly to the Raspberry Pi 5 board using spring-loaded push pins. The Raspberry Pi firmware actively manages it: at 60°C, the blower’s fan is turned on; at 67.5°C, the fan speed is increased; and finally, at 75°C, the fan increases to full speed. The blower’s fan will spin down automatically when the temperature drops below these limits.</p>
<p><img src="images/png/temp_comp.png" class="img-fluid"></p>
<blockquote class="blockquote">
<p>To prevent overheating, all Raspberry Pi boards begin to throttle the processor when the temperature reaches 80°Cand throttle even further when it reaches the maximum temperature of 85°C (more detail <a href="https://www.raspberrypi.com/news/heating-and-cooling-raspberry-pi-5/">here</a>).</p>
</blockquote>
</section>
</section>
<section id="generative-ai-genai" class="level2">
<h2 class="anchored" data-anchor-id="generative-ai-genai">Generative AI (GenAI)</h2>
<p>Generative AI is an artificial intelligence system capable of creating new, original content across various media such as <strong>text, images, audio, and video</strong>. These systems learn patterns from existing data and use that knowledge to generate novel outputs that didn’t previously exist. <strong>Large Language Models (LLMs)</strong>, <strong>Small Language Models (SLMs)</strong>, and <strong>multimodal models</strong> can all be considered types of GenAI when used for generative tasks.</p>
<p>GenAI provides the conceptual framework for AI-driven content creation, with LLMs serving as powerful general-purpose text generators. SLMs adapt this technology for edge computing, while multimodal models extend GenAI capabilities across different data types. Together, they represent a spectrum of generative AI technologies, each with its strengths and applications, collectively driving AI-powered content creation and understanding.</p>
<section id="large-language-models-llms" class="level3">
<h3 class="anchored" data-anchor-id="large-language-models-llms">Large Language Models (LLMs)</h3>
<p>Large Language Models (LLMs) are advanced artificial intelligence systems that understand, process, and generate human-like text. These models are characterized by their massive scale in terms of the amount of data they are trained on and the number of parameters they contain. Critical aspects of LLMs include:</p>
<ol type="1">
<li><p><strong>Size</strong>: LLMs typically contain billions of parameters. For example, GPT-3 has 175 billion parameters, while some newer models exceed a trillion parameters.</p></li>
<li><p><strong>Training Data</strong>: They are trained on vast amounts of text data, often including books, websites, and other diverse sources, amounting to hundreds of gigabytes or even terabytes of text.</p></li>
<li><p><strong>Architecture</strong>: Most LLMs use <a href="https://en.wikipedia.org/wiki/Transformer_(deep_learning_architecture)">transformer-based architectures</a>, which allow them to process and generate text by paying attention to different parts of the input simultaneously.</p></li>
<li><p><strong>Capabilities</strong>: LLMs can perform a wide range of language tasks without specific fine-tuning, including:</p>
<ul>
<li>Text generation</li>
<li>Translation</li>
<li>Summarization</li>
<li>Question answering</li>
<li>Code generation</li>
<li>Logical reasoning</li>
</ul></li>
<li><p><strong>Few-shot Learning</strong>: They can often understand and perform new tasks with minimal examples or instructions.</p></li>
<li><p><strong>Resource-Intensive</strong>: Due to their size, LLMs typically require significant computational resources to run, often needing powerful GPUs or TPUs.</p></li>
<li><p><strong>Continual Development</strong>: The field of LLMs is rapidly evolving, with new models and techniques constantly emerging.</p></li>
<li><p><strong>Ethical Considerations</strong>: The use of LLMs raises important questions about bias, misinformation, and the environmental impact of training such large models.</p></li>
<li><p><strong>Applications</strong>: LLMs are used in various fields, including content creation, customer service, research assistance, and software development.</p></li>
<li><p><strong>Limitations</strong>: Despite their power, LLMs can produce incorrect or biased information and lack true understanding or reasoning capabilities.</p></li>
</ol>
<p>We must note that we use large models beyond text, which we call <em>multi-modal models</em>. These models integrate and process information from multiple types of input simultaneously. They are designed to understand and generate content across various data types, such as text, images, audio, and video.</p>
<p>Certainly. Let’s define open and closed models in the context of AI and language models:</p>
</section>
<section id="closed-vs-open-models" class="level3">
<h3 class="anchored" data-anchor-id="closed-vs-open-models">Closed vs Open Models:</h3>
<p><strong>Closed models</strong>, also called proprietary models, are AI models whose internal workings, code, and training data are not publicly disclosed. Examples: GPT-3 and beyond (by OpenAI), Claude (by Anthropic), Gemini (by Google).</p>
<p><strong>Open models</strong>, also known as open-source models, are AI models whose code, architecture, and, often, training data are publicly available. Examples: Gemma (by Google), LLaMA (by Meta), and Phi (by Microsoft).</p>
<p>Open models are particularly relevant for running models on edge devices like Raspberry Pi as they can be more easily adapted, optimized, and deployed in resource-constrained environments. Still, it is crucial to verify their Licenses. Open models come with various open-source licenses that may affect their use in commercial applications, while closed models have clear, albeit restrictive, terms of service.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="./images/png/llms-slm.png" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">Adapted from https://arxiv.org/pdf/2304.13712</figcaption>
</figure>
</div>
</section>
<section id="small-language-models-slms" class="level3">
<h3 class="anchored" data-anchor-id="small-language-models-slms">Small Language Models (SLMs)</h3>
<p>In the context of edge computing on devices like Raspberry Pi, full-scale LLMs are typically too large and resource-intensive to run directly. This limitation has driven the development of smaller, more efficient models, such as the Small Language Models (SLMs).</p>
<p>SLMs are compact versions of LLMs designed to run efficiently on resource-constrained devices such as smartphones, IoT devices, and single-board computers like the Raspberry Pi. These models are significantly smaller in size and computational requirements than their larger counterparts while still retaining impressive language understanding and generation capabilities.</p>
<p>Key characteristics of SLMs include:</p>
<ol type="1">
<li><p><strong>Reduced parameter count</strong>: Typically ranging from a few hundred million to a few billion parameters, compared to two-digit billions in larger models.</p></li>
<li><p><strong>Lower memory footprint</strong>: Requiring, at most, a few gigabytes of memory rather than tens or hundreds of gigabytes.</p></li>
<li><p><strong>Faster inference time</strong>: Can generate responses in milliseconds to seconds on edge devices.</p></li>
<li><p><strong>Energy efficiency</strong>: Consuming less power, making them suitable for battery-powered devices.</p></li>
<li><p><strong>Privacy-preserving</strong>: Enabling on-device processing without sending data to cloud servers.</p></li>
<li><p><strong>Offline functionality</strong>: Operating without an internet connection.</p></li>
</ol>
<p>SLMs achieve their compact size through various techniques such as knowledge distillation, model pruning, and quantization. While they may not match the broad capabilities of larger models, SLMs excel in specific tasks and domains, making them ideal for targeted applications on edge devices.</p>
<blockquote class="blockquote">
<p>We will generally consider SLMs —language models with fewer than 5 to 8 billion parameters — quantized to 4 bits.</p>
</blockquote>
<p><img src="./images/png/llm2slm.png" class="img-fluid"></p>
<p>Examples of SLMs include compressed versions of models like Meta Llama, Microsoft PHI, and Google Gemma. These models enable a wide range of natural language processing tasks directly on edge devices, from text classification and sentiment analysis to question answering and limited text generation.</p>
<p>For more information on SLMs, the paper, <a href="https://arxiv.org/pdf/2408.11796">LLM Pruning and Distillation in Practice: The Minitron Approach</a>, provides an approach applying pruning and distillation to obtain SLMs from LLMs. And, <a href="https://arxiv.org/pdf/2409.15790">SMALL LANGUAGE MODELS: SURVEY, MEASUREMENTS, AND INSIGHTS</a>, presents a comprehensive survey and analysis of Small Language Models (SLMs), which are language models with 100 million to 5 billion parameters designed for resource-constrained devices.</p>
</section>
</section>
<section id="ollama" class="level2">
<h2 class="anchored" data-anchor-id="ollama">Ollama</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://ollama.com/public/ollama.png" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">ollama logo</figcaption>
</figure>
</div>
<p>The primary and most user-friendly tool for running Small Language Models (SLMs) directly on a Raspberry Pi, especially the Pi 5, is <a href="https://ollama.com/">Ollama</a>. It is an open-source framework that allows us to install, manage, and run various SLMs (such as TinyLlama, smollm, Microsoft Phi, Google Gemma, Meta Llama, MoonDream, LLaVa, among others) locally on our Raspberry Pi for tasks such as text generation, image captioning, and translation.</p>
<blockquote class="blockquote">
<p><strong>Alternatives options:</strong></p>
<p><strong><a href="https://en.wikipedia.org/wiki/Llama.cpp">llama.cpp</a><a href="https://en.wikipedia.org/wiki/Llama.cpp"></a></strong>and the Hugging Face <strong><a href="https://huggingface.co/docs/transformers/en/index">Transformers</a></strong> library are well-supported for running SLMs on Raspberry Pi. llama.cpp is particularly efficient for running quantized models natively. At the same time, Hugging Face Transformers offers a broader range of models and tasks, which are best suited to smaller architectures due to hardware limitations.</p>
</blockquote>
<p>Here are some critical points about Ollama:</p>
<ol type="1">
<li><p><strong>Local Model Execution</strong>: Ollama enables running LMs on personal computers or edge devices such as the Raspi-5, eliminating the need for cloud-based API calls.</p></li>
<li><p><strong>Ease of Use</strong>: It provides a simple command-line interface for downloading, running, and managing different language models.</p></li>
<li><p><strong>Model Variety</strong>: Ollama supports various LLMs, including Phi, Gemma, Llama, Mistral, and other open-source models.</p></li>
<li><p><strong>Customization</strong>: Users can create and share custom models tailored to specific needs or domains.</p></li>
<li><p><strong>Lightweight</strong>: Designed to be efficient and run on consumer-grade hardware.</p></li>
<li><p><strong>API Integration</strong>: Offers an API that allows integration with other applications and services.</p></li>
<li><p><strong>Privacy-Focused</strong>: By running models locally, it addresses privacy concerns associated with sending data to external servers.</p></li>
<li><p><strong>Cross-Platform</strong>: Available for macOS, Windows, and Linux systems (our case, here).</p></li>
<li><p><strong>Active Development</strong>: Regularly updated with new features and model support.</p></li>
<li><p><strong>Community-Driven</strong>: Benefits from community contributions and model sharing.</p></li>
</ol>
<p>To learn more about what Ollama is and how it works under the hood, you should see this short video from <a href="https://www.youtube.com/@technovangelist">Matt Williams</a>, one of the founders of Ollama:</p>
<div class="quarto-video ratio ratio-16x9"><iframe data-external="1" src="https://www.youtube.com/embed/90ozfdsQOKo" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
<blockquote class="blockquote">
<p>Matt has an entirely free course about Ollama that we recommend: </p><div class="quarto-video ratio ratio-16x9"><iframe data-external="1" src="https://www.youtube.com/embed/9KEUFe4KQAI?si=D_-q3CMbHiT-twuy" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div><p></p>
</blockquote>
<section id="installing-ollama" class="level3">
<h3 class="anchored" data-anchor-id="installing-ollama">Installing Ollama</h3>
<p>Let’s set up and activate a Virtual Environment for working with Ollama:</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="ex">python3</span> <span class="at">-m</span> venv ~/ollama</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="bu">source</span> ~/ollama/bin/activate</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>And run the command to install Ollama:</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="ex">curl</span> <span class="at">-fsSL</span> https://ollama.com/install.sh <span class="kw">|</span> <span class="fu">sh</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>As a result, an API will run in the background on <code>127.0.0.1:11434</code>. From now on, we can run Ollama via the terminal. For starting, let’s verify the Ollama version, which will also tell us that it is correctly installed:</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="ex">ollama</span> <span class="at">-v</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p><img src="images/png/install-ollama-rpi5.png" class="img-fluid"></p>
<p>On the <a href="https://ollama.com/library">Ollama Library page</a>, we can find the models Ollama supports. For example, by filtering by <code>Most popular</code>, we can see Meta Llama, Google Gemma, Microsoft Phi, LLaVa, etc.</p>
</section>
<section id="meta-llama-3.2-1b3b" class="level3">
<h3 class="anchored" data-anchor-id="meta-llama-3.2-1b3b">Meta Llama 3.2 1B/3B</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/png/small_and_multimodal.png" class="img-fluid figure-img" style="width:60.0%"></p>
</figure>
</div>
<p>Let’s install and run our first small language model, <a href="https://ai.meta.com/blog/llama-3-2-connect-2024-vision-edge-mobile-devices/">Llama 3.2</a> 1B (and 3B). The Meta Llama, 3.2 collections of multilingual large language models (LLMs), is a collection of pre-trained and instruction-tuned generative models in 1B and 3B sizes (text in/text out). The Llama 3.2 instruction-tuned text-only models are optimized for multilingual dialogue use cases, including agentic retrieval and summarization tasks.</p>
<p>The 1B and 3B models were pruned from the Llama 8B, and then logits from the 8B and 70B models were used as token-level targets (token-level distillation). Knowledge distillation was used to recover performance (they were trained with 9 trillion tokens). The 1B model has 1,24B, quantized to integer (Q8_0), and the 3B, 3.12B parameters, with a Q4_0 quantization, which ends with a size of 1.3 GB and 2GB, respectively. Its context window is 131,072 tokens.</p>
<p><img src="images/jpeg/llama3_2.jpg" class="img-fluid"></p>
<p><strong>Install and run the</strong> <strong>Model</strong></p>
<div class="sourceCode" id="cb4"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="ex">ollama</span> run llama3.2:1b</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Running the model with the command before, we should have the Ollama prompt available for us to input a question and start chatting with the LLM model; for example,</p>
<p><code>&gt;&gt;&gt; What is the capital of France?</code></p>
<p>Almost immediately, we get the correct answer:</p>
<p><code>The capital of France is Paris.</code></p>
<p>Using the option <code>--verbose</code> when calling the model will generate several statistics about its performance (The model will be polling only the first time we run the command).</p>
<p><img src="images/png/llama3_2_1b_performance.png" class="img-fluid"></p>
<p>Each metric gives insights into how the model processes inputs and generates outputs. Here’s a breakdown of what each metric means:</p>
<ul>
<li><strong>Total Duration (2.620170326s)</strong>: This is the complete time taken from the start of the command to the completion of the response. It encompasses loading the model, processing the input prompt, and generating the response.</li>
<li><strong>Load Duration (39.947908ms)</strong>: This duration indicates the time to load the model or necessary components into memory. If this value is minimal, it can suggest that the model was preloaded or that only a minimal setup was required.</li>
<li><strong>Prompt Eval Count (32 tokens)</strong>: The number of tokens in the input prompt. In NLP, tokens are typically words or subwords, so this count includes all the tokens that the model evaluated to understand and respond to the query.</li>
<li><strong>Prompt Eval Duration (1.644773s)</strong>: This measures the model’s time to evaluate or process the input prompt. It accounts for the bulk of the total duration, implying that understanding the query and preparing a response is the most time-consuming part of the process.</li>
<li><strong>Prompt Eval Rate (19.46 tokens/s)</strong>: This rate indicates how quickly the model processes tokens from the input prompt. It reflects the model’s speed in terms of natural language comprehension.</li>
<li><strong>Eval Count (8 token(s))</strong>: This is the number of tokens in the model’s response, which in this case was, “The capital of France is Paris.”</li>
<li><strong>Eval Duration (889.941ms)</strong>: This is the time taken to generate the output based on the evaluated input. It’s much shorter than the prompt evaluation, suggesting that generating the response is less complex or computationally intensive than understanding the prompt.</li>
<li><strong>Eval Rate (8.99 tokens/s)</strong>: Similar to the prompt eval rate, this indicates the speed at which the model generates output tokens. It’s a crucial metric for understanding the model’s efficiency in output generation.</li>
</ul>
<p>This detailed breakdown can help understand the computational demands and performance characteristics of running SLMs like Llama on edge devices like the Raspberry Pi 5. It shows that while prompt evaluation is more time-consuming, the actual generation of responses is relatively quicker. This analysis is crucial for optimizing performance and diagnosing potential bottlenecks in real-time applications.</p>
<p>Loading and running the 3B model, we can see the difference in performance for the same prompt;</p>
<p><img src="images/png/llama3_2_3b_performance.png" class="img-fluid"></p>
<p>The eval rate is lower, 5.3 tokens/s versus 9 tokens/s with the smaller model.</p>
<p>When question about</p>
<p><code>&gt;&gt;&gt; What is the distance between Paris and Santiago, Chile?</code></p>
<p>The 1B model answered <code>9,841 kilometers (6,093 miles)</code>, which is inaccurate, and the 3B model answered <code>7,300 miles (11,700 km)</code>, which is close to the correct (11,642 km).</p>
<p>Let’s ask for the Paris’s coordinates:</p>
<p><code>&gt;&gt;&gt; what is the latitude and longitude of Paris?</code></p>
<div class="sourceCode" id="cb5"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="ex">The</span> latitude and longitude of Paris are 48.8567° N <span class="er">(</span><span class="ex">48°55</span><span class="st">' </span></span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a><span class="st">42" N) and 2.3510° E (2°22'</span> 8<span class="st">" E), respectively.</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/png/paris-lat-lon.png" class="img-fluid figure-img" style="width:60.0%"></p>
</figure>
</div>
<p>Both 1B and 3B models gave correct answers.</p>
</section>
<section id="google-gemma" class="level3">
<h3 class="anchored" data-anchor-id="google-gemma">Google Gemma</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="./images/png/google-gemma-ai.png" class="img-fluid figure-img" style="width:60.0%"></p>
</figure>
</div>
<p><a href="https://deepmind.google/models/gemma/">Google Gemma</a>, is a collection of lightweight, state-of-the-art open models built from the same technology that powers our Gemini models. Today, the Gemma family has the <code>Gemma3</code> and <code>Gemma3n</code> models.</p>
<section id="gemma3" class="level4">
<h4 class="anchored" data-anchor-id="gemma3">Gemma3</h4>
<p>We can, for example, install <code>gemma3:latest</code>, using <code>ollama run gemma3:latest</code>. This model has 4.3B parameters, with a context length of 8,192 and an embedding length of 2,560. A typical quantization schema is the Q4_K_M. This model has vision capabilities. Besides the 4B, we can also install the 1B parameter model.</p>
<p><strong>Install and run the</strong> <strong>Model</strong></p>
<div class="sourceCode" id="cb6"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="ex">ollama</span> run gemma3:4b <span class="at">--verbose</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Running the model with the command before, we should have the Ollama prompt available for us to input a question and start chatting with the LLM model; for example,</p>
<p><code>&gt;&gt;&gt; What is the capital of France?</code></p>
<p>Almost immediately, we get the correct answer:</p>
<p><code>The capital of France is **Paris**. It's a global center for art, fashion, gastronomy, and culture. 😊 Do you want to know anything more about Paris?</code></p>
<p>And its statistics.</p>
<p><img src="./images/png/gemma.png" class="img-fluid"></p>
<blockquote class="blockquote">
<p>We can see that Gemma 3:4B has roughly the same performance as Lama 3.2:3B, despite having more parameters.</p>
</blockquote>
<p><strong>Other examples:</strong></p>
<p><img src="./images/png/gemma3_ex1.png" class="img-fluid"></p>
<p>Also, a good response but less accurate than Llama3.2:3B.</p>
<p><img src="./images/png/gemma3_ex2.png" class="img-fluid"></p>
<p>A good and accurate answer (a little more verbose than the Llama answers).</p>
<p>An advantage of the Gemma3 models are their vision capability, for example, we can ask it to caption an image:</p>
<p><img src="./images/png/caption-gemma3.png" class="img-fluid"></p>
<p>This is a very accurate model, but it still has high latency, as we can see in the example above (more than 3 minutes to caption the image).</p>
<blockquote class="blockquote">
<p>The Gemma 3 1B size models are text only and don’t support image input.</p>
</blockquote>
</section>
<section id="gemma-3n" class="level4">
<h4 class="anchored" data-anchor-id="gemma-3n">Gemma 3n</h4>
<p>Gemma3 is a powerful, efficient open-source model that runs locally on phones, tablets, and laptops. The models are listed with parameter counts, such as <strong><code>E2B</code></strong> and <strong><code>E4B</code></strong>, that are <em>lower</em> than the total number of parameters contained in the models. The <strong><code>E</code></strong> prefix indicates these models can operate with a reduced set of Effective parameters. This reduced-parameter operation can be achieved using the flexible parameter technology built into Gemma 3n models, which helps them run efficiently on lower-resource devices.</p>
<p>The parameters in Gemma 3n models are divided into four main groups: text, visual, audio, and per-layer embedding (PLE) parameters. In the standard execution of the E2B model, over 5 billion parameters are loaded. However, by using parameter skipping and PLE caching, this model can be operated with an effective memory load of just under 2 billion (1.91B) parameters, as illustrated below:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="./images/png/gemma_3.png" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">Immage from Google: Gemma 3n diagram of parameter usage</figcaption>
</figure>
</div>
<p>Once installed, (using <code>ollama run gemma3n:e2b</code>), inspecting the model we get:</p>
<ul>
<li>Architecture: gemma3n<br>
</li>
<li>Parameters: 4.5B<br>
</li>
<li>Context length 32768<br>
</li>
<li>Embedding length 2048<br>
</li>
<li>Quantization Q4_K_M</li>
<li>Capabilities: completion only</li>
</ul>
<p>When we run it, we can see that, despite having 4.5B parameters, it is faster than Llama3.3:3B.</p>
<p><img src="./images/png/gemma_2.png" class="img-fluid"></p>
</section>
</section>
<section id="microsoft-phi3.5-3.8b" class="level3">
<h3 class="anchored" data-anchor-id="microsoft-phi3.5-3.8b">Microsoft Phi3.5 3.8B</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="./images/png/ms-phi.jpg" class="img-fluid figure-img" style="width:60.0%"></p>
</figure>
</div>
<p>Let’s pull now the <a href="https://ollama.com/library/phi3.5">PHI3.5,</a> a 3.8B lightweight state-of-the-art open model by Microsoft. The model belongs to the Phi-3 model family and supports <code>128K token</code> context length and the languages: Arabic, Chinese, Czech, Danish, Dutch, English, Finnish, French, German, Hebrew, Hungarian, Italian, Japanese, Korean, Norwegian, Polish, Portuguese, Russian, Spanish, Swedish, Thai, Turkish, and Ukrainian.</p>
<p>The model size, in terms of bytes, will depend on the specific quantization format used. The size can go from 2-bit quantization (<code>q2_k</code>) of 1.4 GB (higher performance/lower quality) to 16-bit quantization (fp-16) of 7.6 GB (lower performance/higher quality).</p>
<p>Let’s run the 4-bit quantization (<code>Q4_0</code>), which will need 2.2 GB of RAM, with an intermediary trade-off regarding output quality and performance.</p>
<div class="sourceCode" id="cb7"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="ex">ollama</span> run phi3.5:3.8b <span class="at">--verbose</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<blockquote class="blockquote">
<p>You can use <code>run</code> or <code>pull</code> to download the model. What happens is that Ollama keeps note of the pulled models, and once the PHI3 does not exist, before running it, Ollama pulls it.</p>
</blockquote>
<p>Let’s enter with the same prompt used before:</p>
<div class="sourceCode" id="cb8"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="op">&gt;&gt;&gt;</span> What <span class="ex">is</span> the capital of France<span class="pp">?</span></span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a><span class="ex">The</span> capital of France is Paris. It<span class="st">' extradites significant</span></span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a><span class="st">historical, cultural, and political importance to the country as</span></span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a><span class="st">well as being a major European city known for its art, fashion,</span></span>
<span id="cb8-6"><a href="#cb8-6" aria-hidden="true" tabindex="-1"></a><span class="st">gastronomy, and culture. Its influence extends beyond national</span></span>
<span id="cb8-7"><a href="#cb8-7" aria-hidden="true" tabindex="-1"></a><span class="st">borders, with millions of tourists visiting each year from around</span></span>
<span id="cb8-8"><a href="#cb8-8" aria-hidden="true" tabindex="-1"></a><span class="st">the globe. The Seine River flows through Paris before it reaches</span></span>
<span id="cb8-9"><a href="#cb8-9" aria-hidden="true" tabindex="-1"></a><span class="st">the broader English Channel at Le Havre. Moreover, France is one</span></span>
<span id="cb8-10"><a href="#cb8-10" aria-hidden="true" tabindex="-1"></a><span class="st">of Europe'</span>s leading economies with its capital playing a key role</span>
<span id="cb8-11"><a href="#cb8-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-12"><a href="#cb8-12" aria-hidden="true" tabindex="-1"></a><span class="ex">...</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>The answer was very “verbose”, let’s specify a better prompt:</p>
<p><img src="./images/png/paris-phi3.png" class="img-fluid"></p>
<p>In this case, the answer was still longer than we expected, with an eval rate of 2.25 tokens/s, more than double that of Gemma and Llama.</p>
<blockquote class="blockquote">
<p>Choosing the most appropriate prompt is one of the most important skills to be used with LLMs, no matter its size.</p>
</blockquote>
<p>When we asked the same questions about distance and Latitude/Longitude, we did not get a good answer for a distance of <code>13,507 kilometers (8,429 miles)</code>, but it was OK for coordinates. Again, it could have been less verbose (more than 200 tokens for each answer).</p>
<p>We can use any model as an assistant since their speed is relatively decent, but in October 2024, the Llama2:3B or Gemma 3 are better choices. Try other models, depending on your needs. <a href="https://huggingface.co/spaces/open-llm-leaderboard/open_llm_leaderboard">🤗 Open LLM Leaderboard</a> can give you an idea about the best models in size, benchmark, license, etc.</p>
<blockquote class="blockquote">
<p>The best model to use is the one fit for your specific necessity. Also, take into consideration that this field evolves with new models everyday.</p>
</blockquote>
</section>
<section id="moondream" class="level3">
<h3 class="anchored" data-anchor-id="moondream">MoonDream</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="./images/png/moondream_ai_logo.png" class="img-fluid figure-img" style="width:30.0%"></p>
</figure>
</div>
<p><a href="https://moondream.ai/">Moondream</a> is an open-source visual language model that understands images using simple text prompts. It’s fast and wildly capable. It is 3 to 4 times faster than the Gemma3, for example.</p>
<p><img src="./images/png/moondream.png" class="img-fluid"></p>
<p>Moondream is a compact and efficient open-source vision-language model (VLM) designed to analyze images and generate descriptive text, answer questions, detect objects, count, point, and perform OCR—all through natural-language instructions, even on resource-limited devices like the Raspberry Pi or edge hardware.</p>
<p>Below is a diagram showing the flux from an image and its description:</p>
<p><img src="./images/png/mondream-flux.png" class="img-fluid"></p>
<p>Moondream can interpret images much like a human would, enabling tasks like: • Captioning and describing images in detail • Visual question answering (VQA) such as “What is the color of the shirt?” • Object detection and pointing out locations • Counting and contextual reasoning about visual scenes</p>
<p><img src="./images/png/moondream-infer.png" class="img-fluid"></p>
</section>
<section id="lava-phi-3" class="level3">
<h3 class="anchored" data-anchor-id="lava-phi-3">Lava-Phi-3</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="./images/png/logo-llava.png" class="img-fluid figure-img" style="width:60.0%"></p>
</figure>
</div>
<p>Another multimodal model is the <a href="https://ollama.com/library/llava-phi3:3.8b">LLaVA-Phi-3</a>, a fine-tuned LLaVA model from Phi 3 Mini 4k. It has strong performance benchmarks that are on par with the original <a href="https://llava-vl.github.io/">LLaVA</a> (Large Language and Vision Assistant) model.</p>
<blockquote class="blockquote">
<p>In terms of latency, it is a pair with Gemma3 and much slower than MoonDream</p>
</blockquote>
<p>The LLaVA-Phi-3 is an end-to-end trained large multimodal model designed to understand and generate content based on visual inputs (images) and textual instructions. It combines the capabilities of a visual encoder and a language model to process and respond to multimodal inputs.</p>
<p>Let’s install the model:</p>
<div class="sourceCode" id="cb9"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="ex">ollama</span> run llava-phi3:3.8b <span class="at">--verbose</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Let’s start with a text input:</p>
<div class="sourceCode" id="cb10"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="op">&gt;&gt;&gt;</span> You <span class="ex">are</span> a helpful AI assistant. What is the capital of France<span class="pp">?</span></span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a><span class="ex">As</span> an AI language model, I can tell you that the capital of France </span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a><span class="ex">is</span> Paris. It<span class="st">'s not only the largest city in the country but also </span></span>
<span id="cb10-5"><a href="#cb10-5" aria-hidden="true" tabindex="-1"></a><span class="st">serves as its political and administrative center. Paris is known </span></span>
<span id="cb10-6"><a href="#cb10-6" aria-hidden="true" tabindex="-1"></a><span class="st">for its iconic landmarks such as the Eiffel Tower, Notre-Dame </span></span>
<span id="cb10-7"><a href="#cb10-7" aria-hidden="true" tabindex="-1"></a><span class="st">Cathedral, and the Louvre Museum. The city has a rich history, </span></span>
<span id="cb10-8"><a href="#cb10-8" aria-hidden="true" tabindex="-1"></a><span class="st">beautiful architecture, and is widely considered to be one of the </span></span>
<span id="cb10-9"><a href="#cb10-9" aria-hidden="true" tabindex="-1"></a><span class="st">most romantic cities in the world.</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>The response took around 30s, with an eval rate of 3.93 tokens/s! Not bad!</p>
<p>But let us know to enter with an image as input. For that, let’s create a directory for working:</p>
<div class="sourceCode" id="cb11"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="bu">cd</span> Documents/</span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a><span class="fu">mkdir</span> OLLAMA</span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a><span class="bu">cd</span> OLLAMA</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Let’s download a 640x320 image from the internet, for example (Wikipedia: <a href="https://upload.wikimedia.org/wikipedia/commons/thumb/4/4b/La_Tour_Eiffel_vue_de_la_Tour_Saint-Jacques%2C_Paris_ao%C3%BBt_2014_%282%29.jpg/640px-La_Tour_Eiffel_vue_de_la_Tour_Saint-Jacques%2C_Paris_ao%C3%BBt_2014_%282%29.jpg">Paris, France)</a>:</p>
<p><img src="images/jpeg/paris.jpg" class="img-fluid"></p>
<p>Using FileZilla, for example, let’s upload the image to the OLLAMA folder on the Raspi-5 and name it <code>image_test_1.jpg</code>. We should have the whole image path (we can use <code>pwd</code> to get it).</p>
<p><code>/home/mjrovai/Documents/OLLAMA/image_test_1.jpg</code></p>
<p>If you use a desktop, you can copy the image path by right-clicking the image.</p>
<p><img src="images/png/image_test-path.png" class="img-fluid"></p>
<p>Let’s enter with this prompt:</p>
<div class="sourceCode" id="cb12"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="op">&gt;&gt;&gt;</span> Describe <span class="ex">the</span> image /home/mjrovai/Documents/OLLAMA/image_test_1.jpg</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>The result was great, but the overall latency was significant; almost 4 minutes to perform the inference.</p>
<p><img src="images/png/paris-infer-1.png" class="img-fluid"></p>
</section>
<section id="inspecting-local-resources" class="level3">
<h3 class="anchored" data-anchor-id="inspecting-local-resources">Inspecting local resources</h3>
<p>Using htop, we can monitor the resources running on our device.</p>
<div class="sourceCode" id="cb13"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="ex">htop</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>During the time that the model is running, we can inspect the resources:</p>
<p><img src="images/png/htop.png" class="img-fluid"></p>
<p>All four CPUs run at almost 100% of their capacity, and the memory used with the model loaded is <code>3.24GB</code>. Exiting Ollama, the memory goes down to around <code>377MB</code> (with no desktop).</p>
<p>It is also essential to monitor the temperature. When running the Raspberry with a desktop, you can have the temperature shown on the taskbar:</p>
<p><img src="images/png/resourses-temp.png" class="img-fluid"></p>
<p>If you are “headless”, the temperature can be monitored with the command:</p>
<div class="sourceCode" id="cb14"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a><span class="ex">vcgencmd</span> measure_temp</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>If you are doing nothing, the temperature is around <code>50°C</code> for CPUs running at 1%. During inference, with the CPUs at 100%, the temperature can rise to almost <code>70°C</code>. This is OK and means the active cooler is working, keeping the temperature below 80°C / 85°C (its limit).</p>
</section>
</section>
<section id="ollama-python-library" class="level2">
<h2 class="anchored" data-anchor-id="ollama-python-library">Ollama Python Library</h2>
<p>So far, we have explored SLMs’ chat capability using the command line on a terminal. However, we want to integrate those models into our projects, so Python seems to be the right path. The good news is that Ollama has such a library.</p>
<p>The <a href="https://github.com/ollama/ollama-python">Ollama Python library</a> simplifies interaction with advanced LLM models, enabling more sophisticated responses and capabilities, besides providing the easiest way to integrate Python 3.8+ projects with <a href="https://github.com/ollama/ollama">Ollama.</a></p>
<p>For a better understanding of how to create apps using Ollama with Python, we can follow <a href="https://www.youtube.com/@technovangelist">Matt Williams’s videos</a>, as the one below:</p>
<div class="quarto-video ratio ratio-16x9"><iframe data-external="1" src="https://www.youtube.com/embed/_4K20tOsXK8" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
<p><strong>Installation:</strong></p>
<p>In the terminal, run the command:</p>
<div class="sourceCode" id="cb15"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a><span class="ex">pip</span> install ollama</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>We will need a text editor or an IDE to create a Python script. If we run Raspberry Pi OS on a desktop, several options, such as Thonny and Geany, are already installed by default (accessed via <code>[Menu][Programming]</code>). You can download other IDEs, such as Visual Studio Code, from <code>[Menu][Recommended Software]</code>. When the window pops up, go to <code>[Programming]</code>, select your option, and press <code>[Apply]</code>.</p>
<p><img src="images/png/menu.png" class="img-fluid"></p>
<p>If you prefer using Jupyter Notebook for development:</p>
<div class="sourceCode" id="cb16"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a><span class="ex">pip</span> install jupyter</span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a><span class="ex">jupyter</span> notebook <span class="at">--generate-config</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>To run Jupyter Notebook, run the command (change the IP address for yours):</p>
<div class="sourceCode" id="cb17"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a><span class="ex">jupyter</span> notebook <span class="at">--ip</span><span class="op">=</span>192.168.4.209 <span class="at">--no-browser</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>On the terminal, you can see the local URL address to open the notebook:</p>
<p><img src="images/png/jupyter.png" class="img-fluid"></p>
<p>We can access it from another computer by entering the Raspberry Pi’s IP address and the provided token in a web browser (we should copy it from the terminal).</p>
<p>In our working directory in the Raspi, we will create a new Python 3 notebook.</p>
<p>Let’s enter with a very simple script to verify the installed models:</p>
<div class="sourceCode" id="cb18"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> ollama</span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a>models <span class="op">=</span> ollama.<span class="bu">list</span>()</span>
<span id="cb18-3"><a href="#cb18-3" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> model <span class="kw">in</span> models[<span class="st">'models'</span>]:</span>
<span id="cb18-4"><a href="#cb18-4" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(model[<span class="st">'model'</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="sourceCode" id="cb19"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a><span class="ex">gemma3:4b</span></span>
<span id="cb19-2"><a href="#cb19-2" aria-hidden="true" tabindex="-1"></a><span class="ex">riven/smolvlm:latest</span></span>
<span id="cb19-3"><a href="#cb19-3" aria-hidden="true" tabindex="-1"></a><span class="ex">gemma3n:e2b</span></span>
<span id="cb19-4"><a href="#cb19-4" aria-hidden="true" tabindex="-1"></a><span class="ex">moondream:latest</span></span>
<span id="cb19-5"><a href="#cb19-5" aria-hidden="true" tabindex="-1"></a><span class="ex">llama3.2:3b</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Same as we had on the terminal using <code>ollama show llama 3.2: 3b</code> , we can get model information:</p>
<div class="sourceCode" id="cb20"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a>info <span class="op">=</span> ollama.show(<span class="st">'llama3.2:3b'</span>)</span>
<span id="cb20-2"><a href="#cb20-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-3"><a href="#cb20-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Model/Format          :"</span>, <span class="bu">getattr</span>(info, <span class="st">'model'</span>, <span class="va">None</span>))</span>
<span id="cb20-4"><a href="#cb20-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Parameter Size        :"</span>, <span class="bu">getattr</span>(info.details, <span class="st">'parameter_size'</span>, <span class="va">None</span>))</span>
<span id="cb20-5"><a href="#cb20-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Quantization Level    :"</span>, <span class="bu">getattr</span>(info.details, <span class="st">'quantization_level'</span>, <span class="va">None</span>))</span>
<span id="cb20-6"><a href="#cb20-6" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Family                :"</span>, <span class="bu">getattr</span>(info.details, <span class="st">'family'</span>, <span class="va">None</span>))</span>
<span id="cb20-7"><a href="#cb20-7" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Supported Capabilities:"</span>, <span class="bu">getattr</span>(info, <span class="st">'capabilities'</span>, <span class="va">None</span>))</span>
<span id="cb20-8"><a href="#cb20-8" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Date Modified (Local) :"</span>, <span class="bu">getattr</span>(info, <span class="st">'modified_at'</span>, <span class="va">None</span>))</span>
<span id="cb20-9"><a href="#cb20-9" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"License Short         :"</span>, (<span class="bu">str</span>(<span class="bu">getattr</span>(info, <span class="st">'license'</span>, <span class="va">None</span>)).split(<span class="st">'</span><span class="ch">\n</span><span class="st">'</span>)[<span class="dv">0</span>]))</span>
<span id="cb20-10"><a href="#cb20-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-11"><a href="#cb20-11" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Key Architecture Details:"</span>)</span>
<span id="cb20-12"><a href="#cb20-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-13"><a href="#cb20-13" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> key <span class="kw">in</span> [</span>
<span id="cb20-14"><a href="#cb20-14" aria-hidden="true" tabindex="-1"></a>    <span class="st">'general.architecture'</span>, <span class="st">'general.finetune'</span>,</span>
<span id="cb20-15"><a href="#cb20-15" aria-hidden="true" tabindex="-1"></a>    <span class="st">'llama.context_length'</span>, <span class="st">'llama.embedding_length'</span>, <span class="st">'llama.block_count'</span></span>
<span id="cb20-16"><a href="#cb20-16" aria-hidden="true" tabindex="-1"></a>]:</span>
<span id="cb20-17"><a href="#cb20-17" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f" - </span><span class="sc">{</span>key<span class="sc">}</span><span class="ss">: </span><span class="sc">{</span>info<span class="sc">.</span>modelinfo<span class="sc">.</span>get(key)<span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>As a result, we have:</p>
<div class="sourceCode" id="cb21"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a><span class="ex">Model/Format</span>               : None</span>
<span id="cb21-2"><a href="#cb21-2" aria-hidden="true" tabindex="-1"></a><span class="ex">Parameter</span> Size             : 3.2B</span>
<span id="cb21-3"><a href="#cb21-3" aria-hidden="true" tabindex="-1"></a><span class="ex">Quantization</span> Level         : Q4_K_M</span>
<span id="cb21-4"><a href="#cb21-4" aria-hidden="true" tabindex="-1"></a><span class="ex">Family</span>                     : llama</span>
<span id="cb21-5"><a href="#cb21-5" aria-hidden="true" tabindex="-1"></a><span class="ex">Supported</span> Capabilities      : [<span class="st">'completion'</span>, <span class="st">'tools'</span>]</span>
<span id="cb21-6"><a href="#cb21-6" aria-hidden="true" tabindex="-1"></a><span class="ex">Date</span> Modified <span class="er">(</span><span class="ex">Local</span><span class="kw">)</span>      <span class="bu">:</span> 2025-09-15 15:39:48.153510-03:00</span>
<span id="cb21-7"><a href="#cb21-7" aria-hidden="true" tabindex="-1"></a><span class="ex">License</span> Short              : LLAMA 3.2 COMMUNITY LICENSE AGREEMENT</span>
<span id="cb21-8"><a href="#cb21-8" aria-hidden="true" tabindex="-1"></a><span class="ex">Key</span> Architecture Details   :</span>
<span id="cb21-9"><a href="#cb21-9" aria-hidden="true" tabindex="-1"></a> <span class="ex">-</span> general.architecture: llama</span>
<span id="cb21-10"><a href="#cb21-10" aria-hidden="true" tabindex="-1"></a> <span class="ex">-</span> general.finetune: Instruct</span>
<span id="cb21-11"><a href="#cb21-11" aria-hidden="true" tabindex="-1"></a> <span class="ex">-</span> llama.context_length: 131072</span>
<span id="cb21-12"><a href="#cb21-12" aria-hidden="true" tabindex="-1"></a> <span class="ex">-</span> llama.embedding_length: 3072</span>
<span id="cb21-13"><a href="#cb21-13" aria-hidden="true" tabindex="-1"></a> <span class="ex">-</span> llama.block_count: 28</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<section id="ollama-generate" class="level3">
<h3 class="anchored" data-anchor-id="ollama-generate">Ollama Generate</h3>
<p>Let’s repeat one of the questions that we did before, but now using <code>ollama.generate()</code> from the Ollama Python library. This API generates a response for the given prompt using the provided model. This is a streaming endpoint, so there will be a series of responses. The final response object will include statistics and additional data from the request.</p>
<div class="sourceCode" id="cb22"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a>response <span class="op">=</span> ollama.generate(</span>
<span id="cb22-2"><a href="#cb22-2" aria-hidden="true" tabindex="-1"></a>    model<span class="op">=</span><span class="st">"llama3.2:3b"</span>,</span>
<span id="cb22-3"><a href="#cb22-3" aria-hidden="true" tabindex="-1"></a>    prompt<span class="op">=</span><span class="st">"Qual a capital do Brasil"</span></span>
<span id="cb22-4"><a href="#cb22-4" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb22-5"><a href="#cb22-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(response[<span class="st">'response'</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>We get the response: <code>A capital do Brasil é Brasília.</code></p>
<p>If you are running the code as a Python script, you should save it as, for example, test_ollama.py. You can run it in the IDE or run it directly in the terminal. Also, remember always to call the model and define it when running a stand-alone script.</p>
<div class="sourceCode" id="cb23"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"></a><span class="ex">python</span> test_ollama.py</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Let’s print the full response now. As a result, we will have the model response in a JSON format:</p>
<div class="sourceCode" id="cb24"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a><span class="ex">GenerateResponse</span><span class="er">(</span><span class="va">model</span><span class="op">=</span><span class="st">'llama3.2:3b'</span>, <span class="va">created_at</span><span class="op">=</span><span class="st">'2025-10-14T18:30:43.877536633Z'</span>,</span>
<span id="cb24-2"><a href="#cb24-2" aria-hidden="true" tabindex="-1"></a><span class="va">done</span><span class="op">=</span>True, <span class="va">done_reason</span><span class="op">=</span><span class="st">'stop'</span>, <span class="va">total_duration</span><span class="op">=</span>28105898130, <span class="va">load_duration</span><span class="op">=</span>23201685971, </span>
<span id="cb24-3"><a href="#cb24-3" aria-hidden="true" tabindex="-1"></a><span class="va">prompt_eval_count</span><span class="op">=</span>30, <span class="va">prompt_eval_duration</span><span class="op">=</span>3364148131, <span class="va">eval_count</span><span class="op">=</span>9, </span>
<span id="cb24-4"><a href="#cb24-4" aria-hidden="true" tabindex="-1"></a><span class="va">eval_duration</span><span class="op">=</span>1539216446, <span class="va">response</span><span class="op">=</span><span class="st">'A capital do Brasil é Brasília.'</span>, <span class="va">thinking</span><span class="op">=</span>None, </span>
<span id="cb24-5"><a href="#cb24-5" aria-hidden="true" tabindex="-1"></a><span class="va">context</span><span class="op">=</span>[128006, <span class="ex">9125,</span> 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, </span>
<span id="cb24-6"><a href="#cb24-6" aria-hidden="true" tabindex="-1"></a><span class="ex">271,</span> 128009, 128006, 882, 128007, 271, 32129, 264, 6864, 656, 43025, 128009, 128006, </span>
<span id="cb24-7"><a href="#cb24-7" aria-hidden="true" tabindex="-1"></a><span class="ex">78191,</span> 128007, 271, 32, 6864, 656, 43025, 4046, 62224, 76472, 13]<span class="kw">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>As we can see, several pieces of information are generated, such as:</p>
<ul>
<li><p><strong>response</strong>: the main output text generated by the model in response to our prompt.</p>
<ul>
<li><code>A capital do Brasil é Brasília.</code></li>
</ul></li>
<li><p><strong>context</strong>: the token IDs representing the input and context used by the model. Tokens are numerical representations of text that the language model uses for processing.</p>
<p><img src="./images/png/context.png" class="img-fluid"></p></li>
</ul>
<p>The Performance Metrics:</p>
<ul>
<li><strong>total_duration</strong>: The total time taken for the operation in nanoseconds. In this case, approximately 2.81 seconds.</li>
<li><strong>load_duration</strong>: The time taken to load the model or components in nanoseconds. About 0.23 seconds.</li>
<li><strong>prompt_eval_duration</strong>: The time taken to evaluate the prompt in nanoseconds. Around 0.33 seconds.</li>
<li><strong>eval_count</strong>: The number of tokens evaluated during the generation. Here, 9 tokens.</li>
<li><strong>eval_duration</strong>: The time taken for the model to generate the response in nanoseconds. Approximately 1.54 seconds.</li>
</ul>
<p>But what we want is the plain ‘response’ and, perhaps for analysis, the total duration of the inference, so let’s change the code to extract it from the dictionary:</p>
<div class="sourceCode" id="cb25"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb25-1"><a href="#cb25-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="sc">{</span>response[<span class="st">'response'</span>]<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb25-2"><a href="#cb25-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss"> [INFO] Total Duration: </span><span class="sc">{</span>(response[<span class="st">'total_duration'</span>]<span class="op">/</span><span class="fl">1e9</span>)<span class="sc">:.2f}</span><span class="ss"> seconds"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Now, we got:</p>
<div class="sourceCode" id="cb26"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb26-1"><a href="#cb26-1" aria-hidden="true" tabindex="-1"></a><span class="ex">A</span> capital do Brasil é Brasília.</span>
<span id="cb26-2"><a href="#cb26-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-3"><a href="#cb26-3" aria-hidden="true" tabindex="-1"></a> <span class="ex">[INFO]</span> Total Duration: 2.81 seconds</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<section id="streaming-with-ollama.generate" class="level4">
<h4 class="anchored" data-anchor-id="streaming-with-ollama.generate">Streaming with ollama.generate()</h4>
<p>To stream the output from <code>ollama.generate()</code> in Python, set <code>stream=True</code> and iterate over the generator to print each chunk as it’s produced. This enables real-time response streaming, similar to chat models.</p>
<div class="sourceCode" id="cb27"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb27-1"><a href="#cb27-1" aria-hidden="true" tabindex="-1"></a>stream <span class="op">=</span> ollama.generate(</span>
<span id="cb27-2"><a href="#cb27-2" aria-hidden="true" tabindex="-1"></a>    model<span class="op">=</span><span class="st">'llama3.2:3b'</span>,</span>
<span id="cb27-3"><a href="#cb27-3" aria-hidden="true" tabindex="-1"></a>    prompt<span class="op">=</span><span class="st">'Tell me an interesting fact about Brazil'</span>,</span>
<span id="cb27-4"><a href="#cb27-4" aria-hidden="true" tabindex="-1"></a>    stream<span class="op">=</span><span class="va">True</span></span>
<span id="cb27-5"><a href="#cb27-5" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb27-6"><a href="#cb27-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-7"><a href="#cb27-7" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> chunk <span class="kw">in</span> stream:</span>
<span id="cb27-8"><a href="#cb27-8" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(chunk[<span class="st">'response'</span>], end<span class="op">=</span><span class="st">''</span>, flush<span class="op">=</span><span class="va">True</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p><img src="./images/png/resp-1.png" class="img-fluid"></p>
<ul>
<li>Each <code>chunk['response']</code> is a part of the generated text, streamed as it’s created</li>
<li>This allows responsive, real-time interaction in the terminal or UI.</li>
</ul>
<p>This approach is ideal for long or complex generations, making the user experience feel faster and more interactive.</p>
</section>
<section id="system-prompt" class="level4">
<h4 class="anchored" data-anchor-id="system-prompt">System Prompt</h4>
<p>Add a <code>system</code> parameter to set overall instructions or behavior for the model (useful for role assignment and tone control):</p>
<div class="sourceCode" id="cb28"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb28-1"><a href="#cb28-1" aria-hidden="true" tabindex="-1"></a>response <span class="op">=</span> ollama.generate(model<span class="op">=</span><span class="st">'llama3.2:3b'</span>, </span>
<span id="cb28-2"><a href="#cb28-2" aria-hidden="true" tabindex="-1"></a>                prompt<span class="op">=</span><span class="st">'Tell about industry'</span>, </span>
<span id="cb28-3"><a href="#cb28-3" aria-hidden="true" tabindex="-1"></a>                system<span class="op">=</span><span class="st">'You are an expert on Brazil.'</span>, </span>
<span id="cb28-4"><a href="#cb28-4" aria-hidden="true" tabindex="-1"></a>                stream<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb28-5"><a href="#cb28-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(response[<span class="st">'response'</span>]) </span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p><img src="./images/png/resp-2.png" class="img-fluid"></p>
</section>
<section id="temperature-and-sampling" class="level4">
<h4 class="anchored" data-anchor-id="temperature-and-sampling"><strong>Temperature and Sampling</strong></h4>
<p>Control creativity/randomness via <code>temperature</code>, and customize output style with extra settings like <code>top_p</code> and <code>num_ctx</code> (context window size)</p>
<div class="sourceCode" id="cb29"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb29-1"><a href="#cb29-1" aria-hidden="true" tabindex="-1"></a>response <span class="op">=</span> ollama.generate(</span>
<span id="cb29-2"><a href="#cb29-2" aria-hidden="true" tabindex="-1"></a>    model<span class="op">=</span><span class="st">'llama3.2:3b'</span>, </span>
<span id="cb29-3"><a href="#cb29-3" aria-hidden="true" tabindex="-1"></a>    prompt<span class="op">=</span><span class="st">'Why the sky is blue?'</span>, </span>
<span id="cb29-4"><a href="#cb29-4" aria-hidden="true" tabindex="-1"></a>    options<span class="op">=</span>{<span class="st">'temperature'</span>:<span class="fl">0.1</span>}, </span>
<span id="cb29-5"><a href="#cb29-5" aria-hidden="true" tabindex="-1"></a>    stream<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb29-6"><a href="#cb29-6" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> chunk <span class="kw">in</span> response:</span>
<span id="cb29-7"><a href="#cb29-7" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(chunk[<span class="st">'response'</span>], end<span class="op">=</span><span class="st">''</span>, flush<span class="op">=</span><span class="va">True</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p><img src="./images/png/resp-3.png" class="img-fluid"></p>
<div class="sourceCode" id="cb30"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb30-1"><a href="#cb30-1" aria-hidden="true" tabindex="-1"></a>response <span class="op">=</span> ollama.chat(</span>
<span id="cb30-2"><a href="#cb30-2" aria-hidden="true" tabindex="-1"></a>    messages<span class="op">=</span>[</span>
<span id="cb30-3"><a href="#cb30-3" aria-hidden="true" tabindex="-1"></a>        {<span class="st">"role"</span>: <span class="st">"user"</span>, </span>
<span id="cb30-4"><a href="#cb30-4" aria-hidden="true" tabindex="-1"></a>         <span class="st">"content"</span>: <span class="st">"Poetically describe Paris in one short sentence"</span>},</span>
<span id="cb30-5"><a href="#cb30-5" aria-hidden="true" tabindex="-1"></a>    ],</span>
<span id="cb30-6"><a href="#cb30-6" aria-hidden="true" tabindex="-1"></a>    model<span class="op">=</span><span class="st">'llama3.2:3b'</span>,</span>
<span id="cb30-7"><a href="#cb30-7" aria-hidden="true" tabindex="-1"></a>    options<span class="op">=</span>{<span class="st">"temperature"</span>: <span class="fl">1.0</span>} <span class="co"># Set. temp. to 1.0 for more creativity</span></span>
<span id="cb30-8"><a href="#cb30-8" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb30-9"><a href="#cb30-9" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(response[<span class="st">'message'</span>][<span class="st">'content'</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="sourceCode" id="cb31"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb31-1"><a href="#cb31-1" aria-hidden="true" tabindex="-1"></a><span class="ex">Like</span> a velvet-draped secret, Paris whispers ancient mysteries to the moonlit </span>
<span id="cb31-2"><a href="#cb31-2" aria-hidden="true" tabindex="-1"></a><span class="ex">Seine,</span> her sighing shadows weaving an eternal waltz of love and dreams.</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
</section>
<section id="ollama.chat" class="level3">
<h3 class="anchored" data-anchor-id="ollama.chat">Ollama.chat()</h3>
<p>Another way to get our response is to use <code>ollama.chat()</code>, which generates the next message in a chat with a provided model. This is a streaming endpoint, so a series of responses will occur. Streaming can be disabled using <code>"stream": false</code>. The final response object will also include statistics and additional data from the request.</p>
<div class="sourceCode" id="cb32"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb32-1"><a href="#cb32-1" aria-hidden="true" tabindex="-1"></a>PROMPT_1 <span class="op">=</span> <span class="st">'What is the capital of France?'</span></span>
<span id="cb32-2"><a href="#cb32-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-3"><a href="#cb32-3" aria-hidden="true" tabindex="-1"></a>response <span class="op">=</span> ollama.chat(model<span class="op">=</span>MODEL, messages<span class="op">=</span>[</span>
<span id="cb32-4"><a href="#cb32-4" aria-hidden="true" tabindex="-1"></a>{<span class="st">'role'</span>: <span class="st">'user'</span>,<span class="st">'content'</span>: PROMPT_1,},])</span>
<span id="cb32-5"><a href="#cb32-5" aria-hidden="true" tabindex="-1"></a>resp_1 <span class="op">=</span> response[<span class="st">'message'</span>][<span class="st">'content'</span>]</span>
<span id="cb32-6"><a href="#cb32-6" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="sc">{</span>resp_1<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb32-7"><a href="#cb32-7" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss"> [INFO] Total Duration: </span><span class="sc">{</span>(res[<span class="st">'total_duration'</span>]<span class="op">/</span><span class="fl">1e9</span>)<span class="sc">:.2f}</span><span class="ss"> seconds"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>The answer is the same as before.</p>
<p>An important consideration is that by using <code>ollama.generate()</code>, the response is “clear” from the model’s “memory” after the end of inference (only used once), but If we want to keep a conversation, we must use <code>ollama.chat()</code>. Let’s see it in action:</p>
<div class="sourceCode" id="cb33"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb33-1"><a href="#cb33-1" aria-hidden="true" tabindex="-1"></a>PROMPT_1 <span class="op">=</span> <span class="st">'What is the capital of France?'</span></span>
<span id="cb33-2"><a href="#cb33-2" aria-hidden="true" tabindex="-1"></a>response <span class="op">=</span> ollama.chat(model<span class="op">=</span>MODEL, messages<span class="op">=</span>[</span>
<span id="cb33-3"><a href="#cb33-3" aria-hidden="true" tabindex="-1"></a>{<span class="st">'role'</span>: <span class="st">'user'</span>,<span class="st">'content'</span>: PROMPT_1,},])</span>
<span id="cb33-4"><a href="#cb33-4" aria-hidden="true" tabindex="-1"></a>resp_1 <span class="op">=</span> response[<span class="st">'message'</span>][<span class="st">'content'</span>]</span>
<span id="cb33-5"><a href="#cb33-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="sc">{</span>resp_1<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb33-6"><a href="#cb33-6" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss"> [INFO] Total Duration: </span><span class="sc">{</span>(response[<span class="st">'total_duration'</span>]<span class="op">/</span><span class="fl">1e9</span>)<span class="sc">:.2f}</span><span class="ss"> seconds"</span>)</span>
<span id="cb33-7"><a href="#cb33-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-8"><a href="#cb33-8" aria-hidden="true" tabindex="-1"></a>PROMPT_2 <span class="op">=</span> <span class="st">'and of Italy?'</span></span>
<span id="cb33-9"><a href="#cb33-9" aria-hidden="true" tabindex="-1"></a>response <span class="op">=</span> ollama.chat(model<span class="op">=</span>MODEL, messages<span class="op">=</span>[</span>
<span id="cb33-10"><a href="#cb33-10" aria-hidden="true" tabindex="-1"></a>{<span class="st">'role'</span>: <span class="st">'user'</span>,<span class="st">'content'</span>: PROMPT_1,},</span>
<span id="cb33-11"><a href="#cb33-11" aria-hidden="true" tabindex="-1"></a>{<span class="st">'role'</span>: <span class="st">'assistant'</span>,<span class="st">'content'</span>: resp_1,},</span>
<span id="cb33-12"><a href="#cb33-12" aria-hidden="true" tabindex="-1"></a>{<span class="st">'role'</span>: <span class="st">'user'</span>,<span class="st">'content'</span>: PROMPT_2,},])</span>
<span id="cb33-13"><a href="#cb33-13" aria-hidden="true" tabindex="-1"></a>resp_2 <span class="op">=</span> response[<span class="st">'message'</span>][<span class="st">'content'</span>]</span>
<span id="cb33-14"><a href="#cb33-14" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="sc">{</span>resp_2<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb33-15"><a href="#cb33-15" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss"> [INFO] Total Duration: </span><span class="sc">{</span>(response[<span class="st">'total_duration'</span>]<span class="op">/</span><span class="fl">1e9</span>)<span class="sc">:.2f}</span><span class="ss"> seconds"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>In the above code, we are running two queries, and the second prompt considers the result of the first one.</p>
<p>Here is how the model responded:</p>
<div class="sourceCode" id="cb34"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb34-1"><a href="#cb34-1" aria-hidden="true" tabindex="-1"></a><span class="ex">The</span> capital of France is <span class="pp">**</span>Paris<span class="pp">**</span>. 🇫🇷 </span>
<span id="cb34-2"><a href="#cb34-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-3"><a href="#cb34-3" aria-hidden="true" tabindex="-1"></a> <span class="ex">[INFO]</span> Total Duration: 2.82 seconds</span>
<span id="cb34-4"><a href="#cb34-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-5"><a href="#cb34-5" aria-hidden="true" tabindex="-1"></a><span class="ex">The</span> capital of Italy is <span class="pp">**</span>Rome<span class="pp">**</span>. 🇮🇹 </span>
<span id="cb34-6"><a href="#cb34-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-7"><a href="#cb34-7" aria-hidden="true" tabindex="-1"></a> <span class="ex">[INFO]</span> Total Duration: 4.46 seconds</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>The above code works with two prompts. Let’s include a conversation variable to really provide the chat with a memory:</p>
<div class="sourceCode" id="cb35"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb35-1"><a href="#cb35-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Initialize conversation history</span></span>
<span id="cb35-2"><a href="#cb35-2" aria-hidden="true" tabindex="-1"></a>conversation <span class="op">=</span> []</span>
<span id="cb35-3"><a href="#cb35-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-4"><a href="#cb35-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Function to chat with memory</span></span>
<span id="cb35-5"><a href="#cb35-5" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> chat_with_memory(prompt, model<span class="op">=</span>MODEL):</span>
<span id="cb35-6"><a href="#cb35-6" aria-hidden="true" tabindex="-1"></a>    <span class="kw">global</span> conversation</span>
<span id="cb35-7"><a href="#cb35-7" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb35-8"><a href="#cb35-8" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Add user message to conversation</span></span>
<span id="cb35-9"><a href="#cb35-9" aria-hidden="true" tabindex="-1"></a>    conversation.append({<span class="st">"role"</span>: <span class="st">"user"</span>, <span class="st">"content"</span>: prompt})</span>
<span id="cb35-10"><a href="#cb35-10" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb35-11"><a href="#cb35-11" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Generate response with conversation history</span></span>
<span id="cb35-12"><a href="#cb35-12" aria-hidden="true" tabindex="-1"></a>    response <span class="op">=</span> ollama.chat(</span>
<span id="cb35-13"><a href="#cb35-13" aria-hidden="true" tabindex="-1"></a>        model<span class="op">=</span>MODEL,</span>
<span id="cb35-14"><a href="#cb35-14" aria-hidden="true" tabindex="-1"></a>        messages<span class="op">=</span>conversation</span>
<span id="cb35-15"><a href="#cb35-15" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb35-16"><a href="#cb35-16" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb35-17"><a href="#cb35-17" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Add assistant's response to conversation history</span></span>
<span id="cb35-18"><a href="#cb35-18" aria-hidden="true" tabindex="-1"></a>    conversation.append(response[<span class="st">"message"</span>])</span>
<span id="cb35-19"><a href="#cb35-19" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb35-20"><a href="#cb35-20" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Return just the response text</span></span>
<span id="cb35-21"><a href="#cb35-21" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> response[<span class="st">"message"</span>][<span class="st">"content"</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="sourceCode" id="cb36"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb36-1"><a href="#cb36-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Question</span></span>
<span id="cb36-2"><a href="#cb36-2" aria-hidden="true" tabindex="-1"></a>prompt <span class="op">=</span> <span class="st">"Qual a capital do Brasil"</span></span>
<span id="cb36-3"><a href="#cb36-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(chat_with_memory(prompt))</span>
<span id="cb36-4"><a href="#cb36-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-5"><a href="#cb36-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Ask a follow-up question that relies on memory</span></span>
<span id="cb36-6"><a href="#cb36-6" aria-hidden="true" tabindex="-1"></a>follow_up <span class="op">=</span> <span class="st">"E a do Perú?"</span></span>
<span id="cb36-7"><a href="#cb36-7" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(chat_with_memory(follow_up))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="sourceCode" id="cb37"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb37-1"><a href="#cb37-1" aria-hidden="true" tabindex="-1"></a><span class="ex">A</span> capital do Brasil é Brasília.</span>
<span id="cb37-2"><a href="#cb37-2" aria-hidden="true" tabindex="-1"></a><span class="ex">A</span> capital do Peru é Lima.</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="image-description" class="level3">
<h3 class="anchored" data-anchor-id="image-description">Image Description:</h3>
<p>As we did with the visual models and the command line to analyze an image, the same can be done here with Python. Let’s use the same image of Paris, but now with the <code>ollama.generate()</code>:</p>
<div class="sourceCode" id="cb38"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb38-1"><a href="#cb38-1" aria-hidden="true" tabindex="-1"></a>MODEL <span class="op">=</span> <span class="st">'llava-phi3:3.8b'</span></span>
<span id="cb38-2"><a href="#cb38-2" aria-hidden="true" tabindex="-1"></a>PROMPT <span class="op">=</span> <span class="st">"Describe this picture"</span></span>
<span id="cb38-3"><a href="#cb38-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-4"><a href="#cb38-4" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> <span class="bu">open</span>(<span class="st">'image_test_1.jpg'</span>, <span class="st">'rb'</span>) <span class="im">as</span> image_file:</span>
<span id="cb38-5"><a href="#cb38-5" aria-hidden="true" tabindex="-1"></a>    img <span class="op">=</span> image_file.read()</span>
<span id="cb38-6"><a href="#cb38-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-7"><a href="#cb38-7" aria-hidden="true" tabindex="-1"></a>response <span class="op">=</span> ollama.generate(</span>
<span id="cb38-8"><a href="#cb38-8" aria-hidden="true" tabindex="-1"></a>    model<span class="op">=</span>MODEL,</span>
<span id="cb38-9"><a href="#cb38-9" aria-hidden="true" tabindex="-1"></a>    prompt<span class="op">=</span>PROMPT,</span>
<span id="cb38-10"><a href="#cb38-10" aria-hidden="true" tabindex="-1"></a>    images<span class="op">=</span> [img]</span>
<span id="cb38-11"><a href="#cb38-11" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb38-12"><a href="#cb38-12" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="sc">{</span>response[<span class="st">'response'</span>]<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb38-13"><a href="#cb38-13" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss"> [INFO] Total Duration: </span><span class="sc">{</span>(res[<span class="st">'total_duration'</span>]<span class="op">/</span><span class="fl">1e9</span>)<span class="sc">:.2f}</span><span class="ss"> seconds"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Here is the result:</p>
<div class="sourceCode" id="cb39"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb39-1"><a href="#cb39-1" aria-hidden="true" tabindex="-1"></a><span class="ex">This</span> image captures the iconic cityscape of Paris, France. The vantage point </span>
<span id="cb39-2"><a href="#cb39-2" aria-hidden="true" tabindex="-1"></a><span class="ex">is</span> high, providing a panoramic view of the Seine River that meanders through </span>
<span id="cb39-3"><a href="#cb39-3" aria-hidden="true" tabindex="-1"></a><span class="ex">the</span> heart of the city. Several bridges arch gracefully over the river, </span>
<span id="cb39-4"><a href="#cb39-4" aria-hidden="true" tabindex="-1"></a><span class="ex">connecting</span> different parts of the city. The Eiffel Tower, an iron lattice </span>
<span id="cb39-5"><a href="#cb39-5" aria-hidden="true" tabindex="-1"></a><span class="ex">structure</span> with a pointed top and two antennas on its summit, stands tall in the </span>
<span id="cb39-6"><a href="#cb39-6" aria-hidden="true" tabindex="-1"></a><span class="ex">background,</span> piercing the sky. It is painted in a light gray color, contrasting </span>
<span id="cb39-7"><a href="#cb39-7" aria-hidden="true" tabindex="-1"></a><span class="ex">against</span> the blue sky speckled with white clouds.</span>
<span id="cb39-8"><a href="#cb39-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-9"><a href="#cb39-9" aria-hidden="true" tabindex="-1"></a><span class="ex">The</span> buildings that line the river are predominantly white or beige, their uniform</span>
<span id="cb39-10"><a href="#cb39-10" aria-hidden="true" tabindex="-1"></a><span class="ex">color</span> palette broken occasionally by red roofs peeking through. The Seine River </span>
<span id="cb39-11"><a href="#cb39-11" aria-hidden="true" tabindex="-1"></a><span class="ex">itself</span> appears calm and wide, reflecting the city<span class="st">'s architectural beauty in its </span></span>
<span id="cb39-12"><a href="#cb39-12" aria-hidden="true" tabindex="-1"></a><span class="st">surface. On either side of the river, trees add a touch of green to the urban </span></span>
<span id="cb39-13"><a href="#cb39-13" aria-hidden="true" tabindex="-1"></a><span class="st">landscape.</span></span>
<span id="cb39-14"><a href="#cb39-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-15"><a href="#cb39-15" aria-hidden="true" tabindex="-1"></a><span class="st">The image is taken from an elevated perspective, looking down on the city. This </span></span>
<span id="cb39-16"><a href="#cb39-16" aria-hidden="true" tabindex="-1"></a><span class="st">viewpoint allows for a comprehensive view of Paris'</span>s beautiful architecture and </span>
<span id="cb39-17"><a href="#cb39-17" aria-hidden="true" tabindex="-1"></a><span class="ex">layout.</span> The relative positions of the buildings, bridges, and other structures </span>
<span id="cb39-18"><a href="#cb39-18" aria-hidden="true" tabindex="-1"></a><span class="ex">create</span> a harmonious composition that showcases the city<span class="st">'s charm.</span></span>
<span id="cb39-19"><a href="#cb39-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-20"><a href="#cb39-20" aria-hidden="true" tabindex="-1"></a><span class="st">In summary, this image presents a serene day in Paris, with its architectural </span></span>
<span id="cb39-21"><a href="#cb39-21" aria-hidden="true" tabindex="-1"></a><span class="st">marvels - from the Eiffel Tower to the river-side buildings - all bathed in soft </span></span>
<span id="cb39-22"><a href="#cb39-22" aria-hidden="true" tabindex="-1"></a><span class="st">colors under a clear sky.</span></span>
<span id="cb39-23"><a href="#cb39-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-24"><a href="#cb39-24" aria-hidden="true" tabindex="-1"></a><span class="st"> [INFO] Total Duration: 256.45 seconds</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>The model took about 4 minutes (256.45 s) to return with a detailed image description.</p>
<p>Let’s capture an image from the Raspberry Pi camera and get the description, now using the MoonDream model:</p>
<div class="sourceCode" id="cb40"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb40-1"><a href="#cb40-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> time</span>
<span id="cb40-2"><a href="#cb40-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb40-3"><a href="#cb40-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb40-4"><a href="#cb40-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> picamera2 <span class="im">import</span> Picamera2</span>
<span id="cb40-5"><a href="#cb40-5" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> PIL <span class="im">import</span> Image</span>
<span id="cb40-6"><a href="#cb40-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-7"><a href="#cb40-7" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> capture_image(image_path):</span>
<span id="cb40-8"><a href="#cb40-8" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Initialize camera</span></span>
<span id="cb40-9"><a href="#cb40-9" aria-hidden="true" tabindex="-1"></a>    picam2 <span class="op">=</span> Picamera2() <span class="co"># default is index 0</span></span>
<span id="cb40-10"><a href="#cb40-10" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb40-11"><a href="#cb40-11" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Configure the camera</span></span>
<span id="cb40-12"><a href="#cb40-12" aria-hidden="true" tabindex="-1"></a>    config <span class="op">=</span> picam2.create_still_configuration(main<span class="op">=</span>{<span class="st">"size"</span>: (<span class="dv">520</span>, <span class="dv">520</span>)})</span>
<span id="cb40-13"><a href="#cb40-13" aria-hidden="true" tabindex="-1"></a>    picam2.configure(config)</span>
<span id="cb40-14"><a href="#cb40-14" aria-hidden="true" tabindex="-1"></a>    picam2.start()</span>
<span id="cb40-15"><a href="#cb40-15" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb40-16"><a href="#cb40-16" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Wait for the camera to warm up</span></span>
<span id="cb40-17"><a href="#cb40-17" aria-hidden="true" tabindex="-1"></a>    time.sleep(<span class="dv">2</span>)</span>
<span id="cb40-18"><a href="#cb40-18" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb40-19"><a href="#cb40-19" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Capture image</span></span>
<span id="cb40-20"><a href="#cb40-20" aria-hidden="true" tabindex="-1"></a>    picam2.capture_file(image_path)</span>
<span id="cb40-21"><a href="#cb40-21" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"Image captured: "</span><span class="op">+</span>image_path)</span>
<span id="cb40-22"><a href="#cb40-22" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb40-23"><a href="#cb40-23" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Stop camera</span></span>
<span id="cb40-24"><a href="#cb40-24" aria-hidden="true" tabindex="-1"></a>    picam2.stop()</span>
<span id="cb40-25"><a href="#cb40-25" aria-hidden="true" tabindex="-1"></a>    picam2.close()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Using the above code, we can capture an image, which can be displayed with:</p>
<div class="sourceCode" id="cb41"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb41-1"><a href="#cb41-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> show_image(image_path):</span>
<span id="cb41-2"><a href="#cb41-2" aria-hidden="true" tabindex="-1"></a>    img <span class="op">=</span> Image.<span class="bu">open</span>(image_path)</span>
<span id="cb41-3"><a href="#cb41-3" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb41-4"><a href="#cb41-4" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Display the image</span></span>
<span id="cb41-5"><a href="#cb41-5" aria-hidden="true" tabindex="-1"></a>    plt.figure(figsize<span class="op">=</span>(<span class="dv">6</span>, <span class="dv">6</span>))</span>
<span id="cb41-6"><a href="#cb41-6" aria-hidden="true" tabindex="-1"></a>    plt.imshow(img)</span>
<span id="cb41-7"><a href="#cb41-7" aria-hidden="true" tabindex="-1"></a>    plt.title(<span class="st">"Captured Image"</span>)</span>
<span id="cb41-8"><a href="#cb41-8" aria-hidden="true" tabindex="-1"></a>    plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Let’s also create a function to describe the image:</p>
<div class="sourceCode" id="cb42"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb42-1"><a href="#cb42-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> image_description(img_path, model):</span>
<span id="cb42-2"><a href="#cb42-2" aria-hidden="true" tabindex="-1"></a>    <span class="cf">with</span> <span class="bu">open</span>(img_path, <span class="st">'rb'</span>) <span class="im">as</span> <span class="bu">file</span>:</span>
<span id="cb42-3"><a href="#cb42-3" aria-hidden="true" tabindex="-1"></a>        response <span class="op">=</span> ollama.chat(</span>
<span id="cb42-4"><a href="#cb42-4" aria-hidden="true" tabindex="-1"></a>            model<span class="op">=</span>model,</span>
<span id="cb42-5"><a href="#cb42-5" aria-hidden="true" tabindex="-1"></a>            messages<span class="op">=</span>[</span>
<span id="cb42-6"><a href="#cb42-6" aria-hidden="true" tabindex="-1"></a>              {</span>
<span id="cb42-7"><a href="#cb42-7" aria-hidden="true" tabindex="-1"></a>                <span class="st">'role'</span>: <span class="st">'user'</span>,</span>
<span id="cb42-8"><a href="#cb42-8" aria-hidden="true" tabindex="-1"></a>                <span class="st">'content'</span>: <span class="st">'''return the description of the image'''</span>,</span>
<span id="cb42-9"><a href="#cb42-9" aria-hidden="true" tabindex="-1"></a>                <span class="st">'images'</span>: [<span class="bu">file</span>.read()],</span>
<span id="cb42-10"><a href="#cb42-10" aria-hidden="true" tabindex="-1"></a>              },</span>
<span id="cb42-11"><a href="#cb42-11" aria-hidden="true" tabindex="-1"></a>            ],</span>
<span id="cb42-12"><a href="#cb42-12" aria-hidden="true" tabindex="-1"></a>            options <span class="op">=</span> {</span>
<span id="cb42-13"><a href="#cb42-13" aria-hidden="true" tabindex="-1"></a>              <span class="st">'temperature'</span>: <span class="dv">0</span>,</span>
<span id="cb42-14"><a href="#cb42-14" aria-hidden="true" tabindex="-1"></a>              }</span>
<span id="cb42-15"><a href="#cb42-15" aria-hidden="true" tabindex="-1"></a>      )</span>
<span id="cb42-16"><a href="#cb42-16" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> response</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Now, let’s put all togheter and capture an image from the camera:</p>
<div class="sourceCode" id="cb43"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb43-1"><a href="#cb43-1" aria-hidden="true" tabindex="-1"></a>IMG_PATH <span class="op">=</span> <span class="st">"/home/mjrovai/Documents/OLLAMA/SST/capt_image.jpg"</span></span>
<span id="cb43-2"><a href="#cb43-2" aria-hidden="true" tabindex="-1"></a>MODEL <span class="op">=</span> <span class="st">"moondream:latest"</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="sourceCode" id="cb44"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb44-1"><a href="#cb44-1" aria-hidden="true" tabindex="-1"></a>apture_image(IMG_PATH)</span>
<span id="cb44-2"><a href="#cb44-2" aria-hidden="true" tabindex="-1"></a>show_image(IMG_PATH)</span>
<span id="cb44-3"><a href="#cb44-3" aria-hidden="true" tabindex="-1"></a>response <span class="op">=</span> image_description(IMG_PATH, MODEL)</span>
<span id="cb44-4"><a href="#cb44-4" aria-hidden="true" tabindex="-1"></a>caption <span class="op">=</span> response[<span class="st">'message'</span>][<span class="st">'content'</span>]</span>
<span id="cb44-5"><a href="#cb44-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span> (<span class="st">"</span><span class="ch">\n</span><span class="st">==&gt; AI Response:"</span>, caption)</span>
<span id="cb44-6"><a href="#cb44-6" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">[INFO] ==&gt; Total Duration: </span><span class="sc">{</span></span>
<span id="cb44-7"><a href="#cb44-7" aria-hidden="true" tabindex="-1"></a>(response[<span class="st">'total_duration'</span>]<span class="op">/</span><span class="fl">1e9</span>)<span class="sc">:.2f}</span><span class="ss"> seconds"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Pointing the camera at my table:</p>
<p><img src="./images/jpeg/capt_image.jpg" class="img-fluid"></p>
<p>We got the description:</p>
<div class="sourceCode" id="cb45"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb45-1"><a href="#cb45-1" aria-hidden="true" tabindex="-1"></a><span class="ex">The</span> image features a green table with various items on it. A white mug adorned with </span>
<span id="cb45-2"><a href="#cb45-2" aria-hidden="true" tabindex="-1"></a><span class="ex">black</span> faces is prominently displayed, and there are several other mugs scattered around </span>
<span id="cb45-3"><a href="#cb45-3" aria-hidden="true" tabindex="-1"></a><span class="ex">the</span> table as well. In addition to the mugs, there<span class="st">'s also a microphone placed near them,</span></span>
<span id="cb45-4"><a href="#cb45-4" aria-hidden="true" tabindex="-1"></a><span class="st">suggesting that this might be an office or workspace setting where someone could enjoy </span></span>
<span id="cb45-5"><a href="#cb45-5" aria-hidden="true" tabindex="-1"></a><span class="st">their coffee while recording podcasts or audio content.</span></span>
<span id="cb45-6"><a href="#cb45-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb45-7"><a href="#cb45-7" aria-hidden="true" tabindex="-1"></a><span class="st">A computer keyboard can be seen in the background, indicating that it is likely </span></span>
<span id="cb45-8"><a href="#cb45-8" aria-hidden="true" tabindex="-1"></a><span class="st">connected to a computer for work purposes. A mouse and a cell phone are also present on</span></span>
<span id="cb45-9"><a href="#cb45-9" aria-hidden="true" tabindex="-1"></a><span class="st">the table, further emphasizing the technology-oriented nature of this scene.</span></span>
<span id="cb45-10"><a href="#cb45-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb45-11"><a href="#cb45-11" aria-hidden="true" tabindex="-1"></a><span class="st">[INFO] ==&gt; Total Duration: 82.57 seconds</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>We can now change the <code>image_description</code> function to ask “Who are the faces in the mug?”. The answer:</p>
<div class="sourceCode" id="cb46"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb46-1"><a href="#cb46-1" aria-hidden="true" tabindex="-1"></a><span class="ex">==</span><span class="op">&gt;</span> AI Response: </span>
<span id="cb46-2"><a href="#cb46-2" aria-hidden="true" tabindex="-1"></a> <span class="ex">The</span> mug has a picture of the Beatles on it.</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<blockquote class="blockquote">
<p>In the <a href="https://github.com/Mjrovai/EdgeML-with-Raspberry-Pi/blob/main/OLLAMA_SLMs/Ollama_Python_Intro.ipynb">Ollama_Python_Library Intro notebook</a>, we can find the experiments using the Ollama Python library.</p>
</blockquote>
</section>
</section>
<section id="calling-direct-api" class="level2">
<h2 class="anchored" data-anchor-id="calling-direct-api">Calling Direct API</h2>
<p>One alternative to running an SLM in Python using Ollama is to call the API directly. Let’s explore some advantages and disadvantages of both methods.</p>
<p><strong>Python Library:</strong></p>
<div class="sourceCode" id="cb47"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb47-1"><a href="#cb47-1" aria-hidden="true" tabindex="-1"></a>response <span class="op">=</span> ollama.generate(</span>
<span id="cb47-2"><a href="#cb47-2" aria-hidden="true" tabindex="-1"></a>  model<span class="op">=</span>MODEL, </span>
<span id="cb47-3"><a href="#cb47-3" aria-hidden="true" tabindex="-1"></a>  prompt<span class="op">=</span>QUERY)</span>
<span id="cb47-4"><a href="#cb47-4" aria-hidden="true" tabindex="-1"></a>result <span class="op">=</span> response[<span class="st">'response'</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p><strong>Direct API Calls:</strong></p>
<div class="sourceCode" id="cb48"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb48-1"><a href="#cb48-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> requests</span>
<span id="cb48-2"><a href="#cb48-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> json</span>
<span id="cb48-3"><a href="#cb48-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb48-4"><a href="#cb48-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Configuration</span></span>
<span id="cb48-5"><a href="#cb48-5" aria-hidden="true" tabindex="-1"></a>OLLAMA_URL <span class="op">=</span> <span class="st">"http://localhost:11434/api"</span></span>
<span id="cb48-6"><a href="#cb48-6" aria-hidden="true" tabindex="-1"></a>MODEL <span class="op">=</span> MODEL </span>
<span id="cb48-7"><a href="#cb48-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb48-8"><a href="#cb48-8" aria-hidden="true" tabindex="-1"></a>response <span class="op">=</span> requests.post(</span>
<span id="cb48-9"><a href="#cb48-9" aria-hidden="true" tabindex="-1"></a>    <span class="ss">f"</span><span class="sc">{</span>OLLAMA_URL<span class="sc">}</span><span class="ss">/generate"</span>,</span>
<span id="cb48-10"><a href="#cb48-10" aria-hidden="true" tabindex="-1"></a>    json<span class="op">=</span>{</span>
<span id="cb48-11"><a href="#cb48-11" aria-hidden="true" tabindex="-1"></a>        <span class="st">"model"</span>: MODEL,</span>
<span id="cb48-12"><a href="#cb48-12" aria-hidden="true" tabindex="-1"></a>        <span class="st">"prompt"</span>: QUERY,</span>
<span id="cb48-13"><a href="#cb48-13" aria-hidden="true" tabindex="-1"></a>        <span class="st">"stream"</span>: <span class="va">False</span></span>
<span id="cb48-14"><a href="#cb48-14" aria-hidden="true" tabindex="-1"></a>    }</span>
<span id="cb48-15"><a href="#cb48-15" aria-hidden="true" tabindex="-1"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="sourceCode" id="cb49"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb49-1"><a href="#cb49-1" aria-hidden="true" tabindex="-1"></a>response <span class="op">=</span> requests.post(</span>
<span id="cb49-2"><a href="#cb49-2" aria-hidden="true" tabindex="-1"></a>    <span class="ss">f"</span><span class="sc">{</span>OLLAMA_URL<span class="sc">}</span><span class="ss">/generate"</span>,</span>
<span id="cb49-3"><a href="#cb49-3" aria-hidden="true" tabindex="-1"></a>    json<span class="op">=</span>{<span class="st">"model"</span>: MODEL, </span>
<span id="cb49-4"><a href="#cb49-4" aria-hidden="true" tabindex="-1"></a>          <span class="st">"prompt"</span>: query, </span>
<span id="cb49-5"><a href="#cb49-5" aria-hidden="true" tabindex="-1"></a>          <span class="st">"stream"</span>: <span class="va">False</span>}</span>
<span id="cb49-6"><a href="#cb49-6" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb49-7"><a href="#cb49-7" aria-hidden="true" tabindex="-1"></a>result <span class="op">=</span> response.json().get(<span class="st">"response"</span>, <span class="st">""</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<blockquote class="blockquote">
<p>One clear advantage of the Python library is that it handles URL construction, request formatting, and response parsing.</p>
</blockquote>
<section id="error-handling" class="level3">
<h3 class="anchored" data-anchor-id="error-handling">Error Handling</h3>
<p><strong>Python Library:</strong></p>
<ul>
<li>Raises specific exceptions (e.g., <code>ollama.ResponseError</code>, <code>ollama.RequestError</code>)</li>
<li>Better typed error messages</li>
<li>Automatically handles connection issues</li>
</ul>
<p><strong>Direct API Calls:</strong></p>
<ul>
<li>We should manually check the <code>response.status_code</code></li>
<li>Generic HTTP errors</li>
<li>Need to handle connection exceptions yourself</li>
</ul>
</section>
<section id="connection-management" class="level3">
<h3 class="anchored" data-anchor-id="connection-management">Connection Management</h3>
<p><strong>Python Library:</strong></p>
<ul>
<li>Automatically detects Ollama instance (checks <code>OLLAMA_HOST</code> env variable or defaults to <code>localhost:11434</code>)</li>
<li>Built-in connection pooling and retry logic</li>
<li>Handles timeouts gracefully</li>
</ul>
<p><strong>Direct API Calls:</strong></p>
<ul>
<li>We should specify the URL manually</li>
<li>Must implement our own retry logic</li>
<li>Need to manage connection pooling yourself</li>
</ul>
</section>
<section id="features-functionality" class="level3">
<h3 class="anchored" data-anchor-id="features-functionality">Features &amp; Functionality</h3>
<p><strong>Python Library:</strong></p>
<ul>
<li>Clean access to all Ollama features (generate, chat, embeddings, list models, pull, etc.)</li>
<li>Streaming is simple: <code>for chunk in ollama.generate(..., stream=True)</code></li>
<li>Type hints and better IDE support</li>
</ul>
<p><strong>Direct API Calls:</strong></p>
<ul>
<li>Access to any API endpoint, including undocumented ones</li>
<li>Full control over request headers, timeouts, etc.</li>
<li>Can use any HTTP library (requests, httpx, urllib, etc.)</li>
</ul>
</section>
<section id="dependencies" class="level3">
<h3 class="anchored" data-anchor-id="dependencies">Dependencies</h3>
<p><strong>Python Library:</strong></p>
<div class="sourceCode" id="cb50"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb50-1"><a href="#cb50-1" aria-hidden="true" tabindex="-1"></a><span class="ex">pip</span> install ollama</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<ul>
<li>Adds a dependency to our project</li>
<li>Depends on <code>httpx</code> under the hood</li>
</ul>
<p><strong>Direct API Calls:</strong></p>
<div class="sourceCode" id="cb51"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb51-1"><a href="#cb51-1" aria-hidden="true" tabindex="-1"></a><span class="ex">pip</span> install requests</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<ul>
<li>We choose our HTTP library</li>
<li>More lightweight if we only need basic functionality</li>
</ul>
</section>
<section id="advanced-features" class="level3">
<h3 class="anchored" data-anchor-id="advanced-features">Advanced Features</h3>
<p><strong>Python Library:</strong></p>
<div class="sourceCode" id="cb52"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb52-1"><a href="#cb52-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Streaming</span></span>
<span id="cb52-2"><a href="#cb52-2" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> chunk <span class="kw">in</span> ollama.generate(model<span class="op">=</span>MODEL, prompt<span class="op">=</span>query, stream<span class="op">=</span><span class="va">True</span>):</span>
<span id="cb52-3"><a href="#cb52-3" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(chunk[<span class="st">'response'</span>], end<span class="op">=</span><span class="st">''</span>)</span>
<span id="cb52-4"><a href="#cb52-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb52-5"><a href="#cb52-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Chat history</span></span>
<span id="cb52-6"><a href="#cb52-6" aria-hidden="true" tabindex="-1"></a>response <span class="op">=</span> ollama.chat(</span>
<span id="cb52-7"><a href="#cb52-7" aria-hidden="true" tabindex="-1"></a>    model<span class="op">=</span>MODEL,</span>
<span id="cb52-8"><a href="#cb52-8" aria-hidden="true" tabindex="-1"></a>    messages<span class="op">=</span>[</span>
<span id="cb52-9"><a href="#cb52-9" aria-hidden="true" tabindex="-1"></a>        {<span class="st">'role'</span>: <span class="st">'user'</span>, <span class="st">'content'</span>: <span class="st">'Hello!'</span>},</span>
<span id="cb52-10"><a href="#cb52-10" aria-hidden="true" tabindex="-1"></a>        {<span class="st">'role'</span>: <span class="st">'assistant'</span>, <span class="st">'content'</span>: <span class="st">'Hi there!'</span>},</span>
<span id="cb52-11"><a href="#cb52-11" aria-hidden="true" tabindex="-1"></a>        {<span class="st">'role'</span>: <span class="st">'user'</span>, <span class="st">'content'</span>: <span class="st">'How are you?'</span>}</span>
<span id="cb52-12"><a href="#cb52-12" aria-hidden="true" tabindex="-1"></a>    ]</span>
<span id="cb52-13"><a href="#cb52-13" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb52-14"><a href="#cb52-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb52-15"><a href="#cb52-15" aria-hidden="true" tabindex="-1"></a><span class="co"># List models</span></span>
<span id="cb52-16"><a href="#cb52-16" aria-hidden="true" tabindex="-1"></a>models <span class="op">=</span> ollama.<span class="bu">list</span>()</span>
<span id="cb52-17"><a href="#cb52-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb52-18"><a href="#cb52-18" aria-hidden="true" tabindex="-1"></a><span class="co"># Pull models</span></span>
<span id="cb52-19"><a href="#cb52-19" aria-hidden="true" tabindex="-1"></a>ollama.pull(<span class="st">'llama3.2:3b'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p><strong>Direct API Calls:</strong></p>
<ul>
<li>Require us to implement all these patterns manually</li>
<li>More boilerplate code</li>
</ul>
</section>
<section id="when-to-use-each" class="level3">
<h3 class="anchored" data-anchor-id="when-to-use-each">When to Use Each?</h3>
<section id="use-the-python-library-when" class="level4">
<h4 class="anchored" data-anchor-id="use-the-python-library-when">Use the Python Library when:</h4>
<ul>
<li>✅ Building standard applications</li>
<li>✅ Need cleaner, more maintainable code</li>
<li>✅ Need good error handling out of the box</li>
<li>✅ Need streaming support</li>
<li>✅ Okay with adding a dependency</li>
</ul>
</section>
<section id="use-direct-api-calls-when" class="level4">
<h4 class="anchored" data-anchor-id="use-direct-api-calls-when">Use Direct API Calls when:</h4>
<ul>
<li>✅Need fine-grained control over HTTP requests</li>
<li>✅ Are working in a constrained environment</li>
<li>✅ Need to customize timeouts, headers, or proxies</li>
<li>✅ Need to minimize dependencies</li>
<li>✅ Are debugging API issues</li>
<li>✅ Need to access experimental/undocumented endpoints</li>
</ul>
</section>
</section>
<section id="performance-difference" class="level3">
<h3 class="anchored" data-anchor-id="performance-difference">Performance Difference</h3>
<p>Minimal difference in practice! The Python library uses <code>httpx</code> which is comparable to <code>requests</code>. Both make the same underlying HTTP calls to Ollama.</p>
<blockquote class="blockquote">
<p><strong>Bottom line:</strong> For most use cases, the Python library is the better choice due to its simplicity and built-in features. Use direct API calls only when you need specific control or have constraints that prevent adding the dependency.</p>
</blockquote>
</section>
</section>
<section id="going-further" class="level2">
<h2 class="anchored" data-anchor-id="going-further">Going Further</h2>
<p>The small LLM models tested worked well at the edge, both with text and with images, but, of course, the last one had high latency. A combination of specific, dedicated models can lead to better results; for example, in real cases, an Object Detection model (such as YOLO) can provide a general description and count of objects in an image, which, once passed to an LLM, can help extract essential insights and actions.</p>
<p>According to Avi Baum, CTO at Hailo,</p>
<blockquote class="blockquote">
<p>In the vast landscape of artificial intelligence (AI), one of the most intriguing journeys has been the evolution of AI on the edge. This journey has taken us from classic machine vision to the realms of discriminative AI, enhancive AI, and now, the groundbreaking frontier of generative AI. Each step has brought us closer to a future where intelligent systems seamlessly integrate with our daily lives, offering an immersive experience of not just perception but also creation at the palm of our hand.</p>
</blockquote>
<p><img src="images/jpeg/halo.jpg" class="img-fluid"></p>
</section>
<section id="conclusion" class="level2">
<h2 class="anchored" data-anchor-id="conclusion">Conclusion</h2>
<p>This chapter has demonstrated how a Raspberry Pi 5 can be transformed into a potent AI hub capable of running large language models (LLMs) for real-time, on-site data analysis and insights using Ollama and Python. The Raspberry Pi’s versatility and power, coupled with the capabilities of lightweight LLMs like Llama 3.2 and MoonDream, make it an excellent platform for edge computing applications.</p>
<p>The potential of running LLMs on the edge extends far beyond simple data processing, as in this lab’s examples. Here are some innovative suggestions for using this project:</p>
<p><strong>1. Smart Home Automation:</strong></p>
<ul>
<li>Integrate SLMs to interpret voice commands or analyze sensor data for intelligent home automation. This could include real-time monitoring and control of home devices, security systems, and energy management, all processed locally without relying on cloud services.</li>
</ul>
<p><strong>2. Field Data Collection and Analysis:</strong></p>
<ul>
<li>Deploy SLMs on Raspberry Pi in remote or mobile setups for real-time data collection and analysis. This can be used in agriculture to monitor crop health, in environmental studies for wildlife tracking, or in disaster response for situational awareness and resource management.</li>
</ul>
<p><strong>3. Educational Tools:</strong></p>
<ul>
<li>Create interactive educational tools that leverage SLMs to provide instant feedback, language translation, and tutoring. This can be particularly useful in developing regions with limited access to advanced technology and internet connectivity.</li>
</ul>
<p><strong>4. Healthcare Applications:</strong></p>
<ul>
<li>Use SLMs for medical diagnostics and patient monitoring. They can provide real-time analysis of symptoms and suggest potential treatments. This can be integrated into telemedicine platforms or portable health devices.</li>
</ul>
<p><strong>5. Local Business Intelligence:</strong></p>
<ul>
<li>Implement SLMs in retail or small business environments to analyze customer behavior, manage inventory, and optimize operations. The ability to process data locally ensures privacy and reduces dependency on external services.</li>
</ul>
<p><strong>6. Industrial IoT:</strong></p>
<ul>
<li>Integrate SLMs into industrial IoT systems for predictive maintenance, quality control, and process optimization. The Raspberry Pi can serve as a localized data processing unit, reducing latency and improving the reliability of automated systems.</li>
</ul>
<p><strong>7. Autonomous Vehicles:</strong></p>
<ul>
<li>Use SLMs to process sensory data from autonomous vehicles, enabling real-time decision-making and navigation. This can be applied to drones, robots, and self-driving cars for enhanced autonomy and safety.</li>
</ul>
<p><strong>8. Cultural Heritage and Tourism:</strong></p>
<ul>
<li>Implement SLMs to provide interactive and informative cultural heritage sites and museum guides. Visitors can use these systems to get real-time information and insights, enhancing their experience without internet connectivity.</li>
</ul>
<p><strong>9. Artistic and Creative Projects:</strong></p>
<ul>
<li>Use SLMs to analyze and generate creative content, such as music, art, and literature. This can foster innovative projects in the creative industries and allow for unique interactive experiences in exhibitions and performances.</li>
</ul>
<p><strong>10. Customized Assistive Technologies:</strong></p>
<ul>
<li>Develop assistive technologies for individuals with disabilities, providing personalized and adaptive support through real-time text-to-speech, language translation, and other accessible tools.</li>
</ul>
</section>
<section id="resources" class="level2">
<h2 class="anchored" data-anchor-id="resources">Resources</h2>
<ul>
<li><a href="https://github.com/Mjrovai/EdgeML-with-Raspberry-Pi/blob/main/OLLAMA_SLMs/Ollama_Python_Intro.ipynb">Ollama_Python_Library Intro notebook</a></li>
</ul>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
          // target, if specified
          link.setAttribute("target", "_blank");
          // default icon
          link.classList.add("external");
      }
    }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="../../raspi/kd_intro/kd_intro.html" class="pagination-link">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text">Knowledge Distillation in Practice</span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="../../raspi/llm/slm_opt_tech.html" class="pagination-link">
        <span class="nav-page-text">SLM: Basic Optimization Techniques</span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">Written and edited by Prof.&nbsp;Marcelo Rovai (UNIFEI University)</div>   
    <div class="nav-footer-center">
      &nbsp;
    </div>
    <div class="nav-footer-right">This book was built with <a href="https://quarto.org/">Quarto</a>.</div>
  </div>
</footer>



</body></html>