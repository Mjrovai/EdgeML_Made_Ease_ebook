[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Edge AI Engineering",
    "section": "",
    "text": "Preface\nIn the rapidly evolving landscape of technology, the convergence of artificial intelligence and edge computing stands as one of the most exciting frontiers. This intersection promises to revolutionize how we interact with the world around us, bringing intelligence and decision-making capabilities directly to the devices we use every day. At the heart of this revolution lies the Raspberry Pi, a powerful yet accessible single-board computer that has democratized computing and now stands poised to do the same for edge AI.\nThe journey to this book began with a simple question: How can we make advanced machine learning accessible to everyone, not just those with access to powerful cloud resources or specialized hardware? The answer we found was the Raspberry Pi, which is the size of a credit card.\nEdge AI Engineering: Hands-on with the Raspberry Pi is the product of a passion for technology and a belief in its power to solve real-world problems. It represents countless hours of experimentation, learning, and teaching distilled into a format that will inspire and empower you to explore the fascinating world of edge AI.\nThis book is not just about theory or abstract concepts. It’s about getting your hands dirty, writing code, training models, and seeing your creations come to life. We’ve designed each chapter to blend foundational knowledge and practical application, always with an eye toward what’s possible on the Raspberry Pi platform.\nFrom the compact Raspberry Pi Zero to the more powerful Pi 5, we explore how these incredible devices can become the brains of intelligent systems—recognizing images, understanding speech, detecting objects, and even running small language models. Each project in this book is a stepping stone, building your skills and confidence as you progress.\nBut beyond the technical skills, we hope this book instills something more valuable – a sense of curiosity and possibility. The field of edge AI is still in its infancy, with new applications and techniques emerging daily. By mastering the fundamentals presented here, you’ll be well-equipped to explore these frontiers, perhaps even pushing the boundaries of what’s possible on edge devices.\nWhether you’re a student seeking to understand AI’s practical applications, a professional seeking to expand your skill set, or an enthusiast eager to add intelligence to your projects, we hope this book serves as both a guide and an inspiration.\nAs you embark on this journey, remember that every expert was once a beginner. The learning path is filled with challenges and moments of joy and discovery. Embrace both, and let your creativity guide you.\nThank you for joining us on this exciting adventure into edge machine learning. Let’s begin exploring what’s possible when we bring AI to the edge, one Raspberry Pi at a time.\nHappy coding, and may your models always converge!\nProf. Marcelo Rovai February, 2025"
  },
  {
    "objectID": "Acknowledgements.html",
    "href": "Acknowledgements.html",
    "title": "Acknowledgments",
    "section": "",
    "text": "I extend my deepest gratitude to the entire TinyML4D Academic Network, comprised of distinguished professors, researchers, and professionals. Notable contributions from Marco Zennaro, Ermanno Petrosemoli, Brian Plancher, José Alberto Ferreira, Jesus Lopez, Diego Mendez, Shawn Hymel, Dan Situnayake, Pete Warden, and Laurence Moroney have been instrumental in advancing our understanding of Embedded Machine Learning (TinyML) and Edge AI.\nSpecial commendation is reserved for Professor Vijay Janapa Reddi of Harvard University. His steadfast belief in the transformative potential of open-source communities, coupled with his invaluable guidance and teachings, has served as a beacon and a cornerstone for our efforts from the beginning.\nAcknowledging these individuals, we pay tribute to the collective wisdom and dedication that have enriched this field and our work.\n\n\nGoogle ImageFX and OpenAI’s DALL-E generated illustrations of some of the images on the book and chapter covers. Claude 3.5 Sonnet helped with code and text reviews."
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "Introduction",
    "section": "",
    "text": "In the rapidly evolving landscape of technology, the convergence of artificial intelligence and edge computing is revolutionizing how we interact with and understand the world around us. At the forefront of this transformation is the Raspberry Pi, a powerful yet accessible single-board computer that has become a cornerstone for innovators, educators, and hobbyists alike.\n“Edge AI Engineering: Hands-on with the Raspberry Pi” is designed to bridge the gap between complex machine learning concepts and practical, real-world applications using the Raspberry Pi platform. This book is your guide to harnessing the power of edge AI, bringing sophisticated machine learning capabilities directly to where data is generated and actions are taken.\nWhy Raspberry Pi for Edge ML?\nThe Raspberry Pi, with its compact form factor, robust processing capabilities, and vibrant community support, offers an ideal platform for exploring and implementing edge machine learning solutions. Unlike more constrained microcontrollers, the Raspberry Pi provides:\n\nSufficient computational power to run complex ML models\nA full-fledged operating system, enabling easier development and deployment\nExtensive connectivity options, facilitating integration with various sensors and actuators\nA rich ecosystem of libraries and tools optimized for machine learning tasks\n\nThis book will explore leveraging these advantages to implement cutting-edge ML applications, from image classification and object detection to pose estimation and natural language processing.\nWhat You’ll Learn\nThis hands-on guide will take you through the entire process of developing ML applications on the Raspberry Pi:\n\nSetting up your Raspberry Pi for ML development\nCollecting and preparing data for various ML tasks\nTraining and optimizing models for edge deployment\nImplementing real-time inference on the Raspberry Pi\nBuilding practical projects that combine multiple ML techniques\n\nWhether you’re a student, educator, maker, or professional looking to expand your skills, this book provides the knowledge and practical experience needed to bring your ML ideas to life on the Raspberry Pi platform.\nEmpowering Innovation at the Edge\nBy the end of this journey, you’ll be equipped with the skills to create intelligent, responsive systems that can see, understand, and interact with their environment. From smart cameras and voice assistants to industrial automation and IoT solutions, the possibilities are limited only by your imagination.\nJoin us as we explore the exciting intersection of machine learning and edge computing and discover how the Raspberry Pi can become your gateway to innovating at the edge. Let’s embark on this journey to make edge ML accessible, practical, and impactful in solving real-world challenges."
  },
  {
    "objectID": "about_book.html",
    "href": "about_book.html",
    "title": "About this Book",
    "section": "",
    "text": "Several chapters in this book are also part of the open book Machine Learning Systems, which we invite you to read.\n\n“Edge AI Engineering: Hands-on with the Raspberry Pi” is an accessible, practical guide designed to empower readers with the knowledge and skills needed to implement artificial intelligence (DL and GenAI) at the edge using the Raspberry Pi platform. This book is part of the open-source Machine Learning Systems initiative, which aims to democratize AI education and application.\nKey Features:\n\nPractical Approach: Each chapter is built around hands-on projects demonstrating real-world applications of edge ML on Raspberry Pi.\nProgressive Learning: The book starts with fundamental concepts and progresses to more advanced topics, ensuring a smooth learning curve for readers of various skill levels.\nRaspberry Pi Focus: All examples and projects are optimized for various Raspberry Pi models, including the Pi Zero 2W, Pi 4, and Pi 5, highlighting each model’s unique capabilities.\nComprehensive Coverage: From image processing and computer vision to natural language processing and sensor data analysis, this book covers various ML (and GenAI) applications relevant to edge computing.\nOpen-Source Tools: We emphasize using open-source models, frameworks, and libraries, such as Edge Impulse Studio, TensorFlow Lite, OpenCV, PyTorch, Transformers, and Ollama, ensuring accessibility and continuity in your learning journey.\nResource Optimization: Learn techniques to optimize ML models for the constrained resources of edge devices, balancing performance with efficiency.\nDeployment Ready: Gain insights into best practices for deploying and maintaining ML models on Raspberry Pi in production environments.\n\nPrerequisites:\nWhile this book is designed to be accessible to a broad audience, readers will benefit from:\n\nBasic familiarity with Python programming\nFundamental understanding of machine learning concepts\nExperience with Raspberry Pi or similar single-board computers (helpful but not required)\n\nStructure of the Book:\nThe book is divided into chapters, each focusing on a specific aspect of edge ML on Raspberry Pi. Every chapter includes:\n\nTheoretical background to understand the concepts\nStep-by-step tutorials for implementing ML models\nPractical projects that apply the learned techniques\nTips for troubleshooting and optimizing performance\nSuggestions for further exploration and experimentation\n\nWhat’s Inside\n\nIntroduction to EdgeAI and TinyML: A foundational look at embedded machine learning, including the differences between traditional AI, cloud AI, and AI at the edge.\nGetting Started with the Raspberry Pi: Learn to set up your Raspberry Pi for EdgeAI projects, including installation, system setup, and required libraries.\nHands-On Projects: Step-by-step guides on implementing popular machine learning applications, such as image classification, object detection, anomaly detection, and more, directly on Raspberry Pi.\nLarge Language Models at the Edge (SLMs): Explores running large language models (LLMs) on edge devices like the Raspberry Pi. It covers setting up Ollama and Python to leverage these models for tasks such as text generation, summarization, and conversational AI, making powerful language AI accessible at the edge.\nVision-Language Models: Focusing on deploying Florence-2, Microsoft’s state-of-the-art Vision-Language Model, for various computer vision tasks such as captioning, object detection, segmentation, and visual grounding.\nPhysical Computing and IoT Integration: Explores the implementation of Small Language Models (SLMs) in IoT control systems, demonstrating the possibility of creating a monitoring and control system using edge AI. These models will be integrated with physical sensors and actuators, creating an intelligent IoT system capable of natural language interaction.\n\nBy the end of this book, you’ll have a solid foundation in implementing various ML applications on Raspberry Pi and the confidence to tackle your edge AI projects."
  },
  {
    "objectID": "raspi/setup/setup.html#introduction",
    "href": "raspi/setup/setup.html#introduction",
    "title": "Setup",
    "section": "Introduction",
    "text": "Introduction\nThe Raspberry Pi is a powerful and versatile single-board computer that has become an essential tool for engineers across various disciplines. Developed by the Raspberry Pi Foundation, these compact devices offer a unique combination of affordability, computational power, and extensive GPIO (General Purpose Input/Output) capabilities, making them ideal for prototyping, embedded systems development, and advanced engineering projects.\n\nKey Features\n\nComputational Power: Despite their small size, Raspberry Pis offers significant processing capabilities, with the latest models featuring multi-core ARM processors and up to 8GB of RAM.\nGPIO Interface: The 40-pin GPIO header allows direct interaction with sensors, actuators, and other electronic components, facilitating hardware-software integration projects.\nExtensive Connectivity: Built-in Wi-Fi, Bluetooth, Ethernet, and multiple USB ports enable diverse communication and networking projects.\nLow-Level Hardware Access: Raspberry Pis provides access to interfaces like I2C, SPI, and UART, allowing for detailed control and communication with external devices.\nReal-Time Capabilities: With proper configuration, Raspberry Pis can be used for soft real-time applications, making them suitable for control systems and signal processing tasks.\nPower Efficiency: Low power consumption enables battery-powered and energy-efficient designs, especially in models like the Pi Zero.\n\n\n\nRaspberry Pi Models (covered in this book)\n\nRaspberry Pi Zero 2 W (Raspi-Zero):\n\nIdeal for: Compact embedded systems\nKey specs: 1GHz single-core CPU (ARM Cortex-A53), 512MB RAM, minimal power consumption\n\nRaspberry Pi 5 (Raspi-5):\n\nIdeal for: More demanding applications such as edge computing, computer vision, and edgeAI applications, including LLMs.\nKey specs: 2.4GHz quad-core CPU (ARM Cortex A-76), up to 8GB RAM, PCIe interface for expansions\n\n\n\n\nEngineering Applications\n\nEmbedded Systems Design: Develop and prototype embedded systems for real-world applications.\nIoT and Networked Devices: Create interconnected devices and explore protocols like MQTT, CoAP, and HTTP/HTTPS.\nControl Systems: Implement feedback control loops, PID controllers, and interface with actuators.\nComputer Vision and AI: Utilize libraries like OpenCV and TensorFlow Lite for image processing and machine learning at the edge.\nData Acquisition and Analysis: Collect sensor data, perform real-time analysis, and create data logging systems.\nRobotics: Build robot controllers, implement motion planning algorithms, and interface with motor drivers.\nSignal Processing: Perform real-time signal analysis, filtering, and DSP applications.\nNetwork Security: Set up VPNs, firewalls, and explore network penetration testing.\n\nThis tutorial will guide you through setting up the most common Raspberry Pi models, enabling you to start on your machine learning project quickly. We’ll cover hardware setup, operating system installation, and initial configuration, focusing on preparing your Pi for Machine Learning applications."
  },
  {
    "objectID": "raspi/setup/setup.html#hardware-overview",
    "href": "raspi/setup/setup.html#hardware-overview",
    "title": "Setup",
    "section": "Hardware Overview",
    "text": "Hardware Overview\n\nRaspberry Pi Zero 2W\n\n\nProcessor: 1GHz quad-core 64-bit Arm Cortex-A53 CPU\nRAM: 512MB SDRAM\nWireless: 2.4GHz 802.11 b/g/n wireless LAN, Bluetooth 4.2, BLE\nPorts: Mini HDMI, micro USB OTG, CSI-2 camera connector\nPower: 5V via micro USB port\n\n\n\nRaspberry Pi 5\n\n\nProcessor:\n\nPi 5: Quad-core 64-bit Arm Cortex-A76 CPU @ 2.4GHz\nPi 4: Quad-core Cortex-A72 (ARM v8) 64-bit SoC @ 1.5GHz\n\nRAM: 2GB, 4GB, or 8GB options (8GB recommended for AI tasks)\nWireless: Dual-band 802.11ac wireless, Bluetooth 5.0\nPorts: 2 × micro HDMI ports, 2 × USB 3.0 ports, 2 × USB 2.0 ports, CSI camera port, DSI display port\nPower: 5V DC via USB-C connector (3A)\n\n\nIn the labs, we will use different names to address the Raspberry: Raspi, Raspi-5, Raspi-Zero, etc. Usually, Raspi is used when the instructions or comments apply to every model."
  },
  {
    "objectID": "raspi/setup/setup.html#installing-the-operating-system",
    "href": "raspi/setup/setup.html#installing-the-operating-system",
    "title": "Setup",
    "section": "Installing the Operating System",
    "text": "Installing the Operating System\n\nThe Operating System (OS)\nAn operating system (OS) is fundamental software that manages computer hardware and software resources, providing standard services for computer programs. It is the core software that runs on a computer, acting as an intermediary between hardware and application software. The OS manages the computer’s memory, processes, device drivers, files, and security protocols.\n\nKey functions:\n\nProcess management: Allocating CPU time to different programs\nMemory management: Allocating and freeing up memory as needed\nFile system management: Organizing and keeping track of files and directories\nDevice management: Communicating with connected hardware devices\nUser interface: Providing a way for users to interact with the computer\n\nComponents:\n\nKernel: The core of the OS that manages hardware resources\nShell: The user interface for interacting with the OS\nFile system: Organizes and manages data storage\nDevice drivers: Software that allows the OS to communicate with hardware\n\n\nThe Raspberry Pi runs a specialized version of Linux designed for embedded systems. This operating system, typically a variant of Debian called Raspberry Pi OS (formerly Raspbian), is optimized for the Pi’s ARM-based architecture and limited resources.\n\nThe latest version of Raspberry Pi OS is based on Debian Bookworm.\n\nKey features:\n\nLightweight: Tailored to run efficiently on the Pi’s hardware.\nVersatile: Supports a wide range of applications and programming languages.\nOpen-source: Allows for customization and community-driven improvements.\nGPIO support: Enables interaction with sensors and other hardware through the Pi’s pins.\nRegular updates: Continuously improved for performance and security.\n\nEmbedded Linux on the Raspberry Pi provides a full-featured operating system in a compact package, making it ideal for projects ranging from simple IoT devices to more complex edge machine-learning applications. Its compatibility with standard Linux tools and libraries makes it a powerful platform for development and experimentation.\n\n\nInstallation\nTo use the Raspberry Pi, we will need an operating system. By default, Raspberry Pis checks for an operating system on any SD card inserted in the slot, so we should install an operating system using Raspberry Pi Imager.\nRaspberry Pi Imager is a tool for downloading and writing images on macOS, Windows, and Linux. It includes many popular operating system images for Raspberry Pi. We will also use the Imager to preconfigure credentials and remote access settings.\nFollow the steps to install the OS in your Raspi.\n\nDownload and install the Raspberry Pi Imager on your computer.\nInsert a microSD card into your computer (a 32GB SD card is recommended) .\nOpen Raspberry Pi Imager and select your Raspberry Pi model.\nChoose the appropriate operating system:\n\nFor Raspi-Zero: For example, you can select: Raspberry Pi OS Lite (64-bit).\n\n\n\nDue to its reduced SDRAM (512MB), the recommended OS for the Raspi-Zero is the 32-bit version. However, to run some machine learning models, such as the YOLOv8 from Ultralitics, we should use the 64-bit version. Although Raspi-Zero can run a desktop, we will choose the LITE version (no Desktop) to reduce the RAM needed for regular operation.\n\n\nFor Raspi-5: We can select the full 64-bit version, which includes a desktop: Raspberry Pi OS (64-bit)\n\n\nSelect your microSD card as the storage device.\nClick on Next and then the gear icon to access advanced options.\nSet the hostname, the Raspi username and password, configure WiFi and enable SSH (Very important!)\n\n\n\nWrite the image to the microSD card.\n\n\nIn the examples here, we will use different hostnames depending on the device used: raspi, raspi-5, raspi-Zero, etc. It would help if you replaced it with the one you are using.\n\n\n\nInitial Configuration\n\nInsert the microSD card into your Raspberry Pi.\nConnect power to boot up the Raspberry Pi.\nPlease wait for the initial boot process to complete (it may take a few minutes).\n\n\nYou can find the most common Linux commands to be used with the Raspi here or here."
  },
  {
    "objectID": "raspi/setup/setup.html#remote-access",
    "href": "raspi/setup/setup.html#remote-access",
    "title": "Setup",
    "section": "Remote Access",
    "text": "Remote Access\n\nSSH Access\nThe easiest way to interact with the Raspi-Zero is via SSH (“Headless”). You can use a Terminal (MAC/Linux), PuTTy (Windows), or any other.\n\nFind your Raspberry Pi’s IP address (for example, check your router).\nOn your computer, open a terminal and connect via SSH:\nssh username@[raspberry_pi_ip_address]   \nAlternatively, if you do not have the IP address, you can try the following: bash ssh username@hostname.local for example, ssh mjrovai@rpi-5.local , ssh mjrovai@raspi.local , etc.\n\nWhen you see the prompt:\nmjrovai@rpi-5:~ $\nIt means that you are interacting remotely with your Raspi. It is a good practice to update/upgrade the system regularly. For that, you should run:\nsudo apt-get update\nsudo apt upgrade\nYou should confirm the Raspi IP address. On the terminal, you can use:\nhostname -I\n\n\n\n\nTo shut down the Raspi via terminal:\nWhen you want to turn off your Raspberry Pi, there are better ideas than just pulling the power cord. This is because the Raspi may still be writing data to the SD card, in which case merely powering down may result in data loss or, even worse, a corrupted SD card.\nFor safety shut down, use the command line:\nsudo shutdown -h now\n\nTo avoid possible data loss and SD card corruption, before removing the power, you should wait a few seconds after shutdown for the Raspberry Pi’s LED to stop blinking and go dark. Once the LED goes out, it’s safe to power down.\n\n\n\nTransfer Files between the Raspi and a computer\nTransferring files between the Raspi and our main computer can be done using a pen drive, directly on the terminal (with scp), or an FTP program over the network.\n\nUsing Secure Copy Protocol (scp):\n\nCopy files to your Raspberry Pi\nLet’s create a text file on our computer, for example, test.txt.\n\n\nYou can use any text editor. In the same terminal, an option is the nano.\n\nTo copy the file named test.txt from your personal computer to a user’s home folder on your Raspberry Pi, run the following command from the directory containing test.txt, replacing the &lt;username&gt; placeholder with the username you use to log in to your Raspberry Pi and the &lt;pi_ip_address&gt; placeholder with your Raspberry Pi’s IP address:\n$ scp test.txt &lt;username&gt;@&lt;pi_ip_address&gt;:~/\n\nNote that ~/ means that we will move the file to the ROOT of our Raspi. You can choose any folder in your Raspi. But you should create the folder before you run scp, since scp won’t create folders automatically.\n\nFor example, let’s transfer the file test.txt to the ROOT of my Raspi-zero, which has an IP of 192.168.4.210:\nscp test.txt mjrovai@192.168.4.210:~/\n\nI use a different profile to differentiate the terminals. The above action happens on your computer. Now, let’s go to our Raspi (using the SSH) and check if the file is there:\n\n\n\nCopy files from your Raspberry Pi\nTo copy a file named test.txt from a user’s home directory on a Raspberry Pi to the current directory on another computer, run the following command on your Host Computer:\n$ scp &lt;username&gt;@&lt;pi_ip_address&gt;:myfile.txt .\nFor example:\nOn the Raspi, let’s create a copy of the file with another name:\ncp test.txt test_2.txt\nAnd on the Host Computer (in my case, a Mac)\nscp mjrovai@192.168.4.210:test_2.txt .\n\n\n\n\nTransferring files using FTP\nTransferring files using FTP, such as FileZilla FTP Client, is also possible. Follow the instructions, install the program for your Desktop OS, and use the Raspi IP address as the Host. For example:\nsftp://192.168.4.210\nand enter your Raspi username and password. Pressing Quickconnect will open two windows, one for your host computer desktop (right) and another for the Raspi (left)."
  },
  {
    "objectID": "raspi/setup/setup.html#increasing-swap-memory",
    "href": "raspi/setup/setup.html#increasing-swap-memory",
    "title": "Setup",
    "section": "Increasing SWAP Memory",
    "text": "Increasing SWAP Memory\nUsing htop, a cross-platform interactive process viewer, you can easily monitor the resources running on your Raspi, such as the list of processes, the running CPUs, and the memory used in real-time. To lunch hop, enter with the command on the terminal:\nhtop\n\nRegarding memory, among the devices in the Raspberry Pi family, the Raspi-Zero has the smallest amount of SRAM (500MB), compared to a selection of 2GB to 8GB on the Raspis 4 or 5. For any Raspi, it is possible to increase the memory available to the system with “Swap.” Swap memory, also known as swap space, is a technique used in computer operating systems to temporarily store data from RAM (Random Access Memory) on the SD card when the physical RAM is fully utilized. This allows the operating system (OS) to continue running even when RAM is full, which can prevent system crashes or slowdowns.\nSwap memory benefits devices with limited RAM, such as the Raspi-Zero. Increasing swap can help run more demanding applications or processes, but it’s essential to balance this with the potential performance impact of frequent disk access.\nBy default, the Rapi-Zero’s SWAP (Swp) memory is only 100MB, which is very small for running some more complex and demanding Machine Learning applications (for example, YOLO). Let’s increase it to 2MB:\nFirst, turn off swap-file:\nsudo dphys-swapfile swapoff\nNext, you should open and change the file /etc/dphys-swapfile. For that, we will use the nano:\nsudo nano /etc/dphys-swapfile\nSearch for the CONF_SWAPSIZE variable (default is 200) and update it to 2000:\nCONF_SWAPSIZE=2000\nAnd save the file.\nNext, turn on the swapfile again and reboot the Raspi-zero:\nsudo dphys-swapfile setup\nsudo dphys-swapfile swapon\nsudo reboot\nWhen your device is rebooted (you should enter with the SSH again), you will realize that the maximum swap memory value shown on top is now something near 2GB (in my case, 1.95GB).\n\nTo keep the htop running, you should open another terminal window to interact continuously with your Raspi."
  },
  {
    "objectID": "raspi/setup/setup.html#installing-a-camera",
    "href": "raspi/setup/setup.html#installing-a-camera",
    "title": "Setup",
    "section": "Installing a Camera",
    "text": "Installing a Camera\nThe Raspi is an excellent device for computer vision applications; a camera is needed for it. We can install a standard USB webcam on the micro-USB port using a USB OTG adapter (Raspi-Zero and Raspi-5) or a camera module connected to the Raspi CSI (Camera Serial Interface) port.\n\nUSB Webcams generally have inferior quality to the camera modules that connect to the CSI port. They can also not be controlled using the raspistill and rasivid commands in the terminal or the picamera recording package in Python. Nevertheless, there may be reasons why you want to connect a USB camera to your Raspberry Pi, such as because of the benefit that it is much easier to set up multiple cameras with a single Raspberry Pi, long cables, or simply because you have such a camera on hand.\n\n\nInstalling a USB WebCam\n\nPower off the Raspi:\n\nsudo shutdown -h no\n\nConnect the USB Webcam (USB Camera Module 30fps,1280x720) to your Raspi (In this example, I am using the Raspi-Zero, but the instructions work for all Raspis).\n\n\n\nPower on again and run the SSH\nTo check if your USB camera is recognized, run:\n\nlsusb\nYou should see your camera listed in the output.\n\n\nTo take a test picture with your USB camera, use:\n\nfswebcam test_image.jpg\nThis will save an image named “test_image.jpg” in your current directory.\n\n\nSince we are using SSH to connect to our Rapsi, we must transfer the image to our main computer so we can view it. We can use FileZilla or SCP for this:\n\nOpen a terminal on your host computer and run:\nscp mjrovai@raspi-zero.local:~/test_image.jpg .\n\nReplace “mjrovai” with your username and “raspi-zero” with Pi’s hostname.\n\n\n\nIf the image quality isn’t satisfactory, you can adjust various settings; for example, define a resolution that is suitable for YOLO (640x640):\n\nfswebcam -r 640x640 --no-banner test_image_yolo.jpg\nThis captures a higher-resolution image without the default banner.\n\nAn ordinary USB Webcam can also be used:\n\nAnd verified using lsusb\n\n\nVideo Streaming\nFor stream video (which is more resource-intensive), we can install and use mjpg-streamer:\nFirst, install Git:\nsudo apt install git\nNow, we should install the necessary dependencies for mjpg-streamer, clone the repository, and proceed with the installation:\nsudo apt install cmake libjpeg62-turbo-dev\ngit clone https://github.com/jacksonliam/mjpg-streamer.git\ncd mjpg-streamer/mjpg-streamer-experimental\nmake\nsudo make install\nThen start the stream with:\nmjpg_streamer -i \"input_uvc.so\" -o \"output_http.so -w ./www\"\nWe can then access the stream by opening a web browser and navigating to:\nhttp://&lt;your_pi_ip_address&gt;:8080. In my case: http://192.168.4.210:8080\nWe should see a webpage with options to view the stream. Click on the link that says “Stream” or try accessing:\nhttp://&lt;raspberry_pi_ip_address&gt;:8080/?action=stream\n\n\n\n\nInstalling a Camera Module on the CSI port\nThere are now several Raspberry Pi camera modules. The original 5-megapixel model was releasedin 2013, followed by an 8-megapixel Camera Module 2, released in 2016. The latest camera model is the 12-megapixel Camera Module 3, released in 2023.\nThe original 5MP camera (Arducam OV5647) is no longer available from Raspberry Pi but can be found from several alternative suppliers. Below is an example of such a camera on a Raspi-Zero.\n\nHere is another example of a v2 Camera Module, which has a Sony IMX219 8-megapixel sensor:\n\nAny camera module will work on the Raspis, but for that, the onfiguration.txt file must be updated:\nsudo nano /boot/firmware/config.txt\nAt the bottom of the file, for example, to use the 5MP Arducam OV5647 camera, add the line:\ndtoverlay=ov5647,cam0\nOr for the v2 module, wich has the 8MP Sony IMX219 camera:\ndtoverlay=imx219,cam0\nSave the file (CTRL+O [ENTER] CRTL+X) and reboot the Raspi:\nSudo reboot\nAfter the boot, you can see if the camera is listed:\nlibcamera-hello --list-cameras\n\n\n\nlibcamerais an open-source software library that supports camera systems directly from the Linux operating system on Arm processors. It minimizes proprietary code running on the Broadcom GPU.\n\nLet’s capture a jpeg image with a resolution of 640 x 480 for testing and save it to a file named test_cli_camera.jpg\nrpicam-jpeg --output test_cli_camera.jpg --width 640 --height 480\nif we want to see the file saved, we should use ls -f, which lists all current directory content in long format. As before, we can use scp to view the image:"
  },
  {
    "objectID": "raspi/setup/setup.html#running-the-raspi-desktop-remotely",
    "href": "raspi/setup/setup.html#running-the-raspi-desktop-remotely",
    "title": "Setup",
    "section": "Running the Raspi Desktop remotely",
    "text": "Running the Raspi Desktop remotely\nWhile we’ve primarily interacted with the Raspberry Pi using terminal commands via SSH, we can access the whole graphical desktop environment remotely if we have installed the complete Raspberry Pi OS (for example, Raspberry Pi OS (64-bit). This can be particularly useful for tasks that benefit from a visual interface. To enable this functionality, we must set up a VNC (Virtual Network Computing) server on the Raspberry Pi. Here’s how to do it:\n\nEnable the VNC Server:\n\nConnect to your Raspberry Pi via SSH.\nRun the Raspberry Pi configuration tool by entering:\nsudo raspi-config\nNavigate to Interface Options using the arrow keys.\n\n\n\nSelect VNC and Yes to enable the VNC server.\n\n\n\nExit the configuration tool, saving changes when prompted.\n\n\nInstall a VNC Viewer on Your Computer:\n\nDownload and install a VNC viewer application on your main computer. Popular options include RealVNC Viewer, TightVNC, or VNC Viewer by RealVNC. We will install VNC Viewer by RealVNC.\n\nOnce installed, you should confirm the Raspi IP address. For example, on the terminal, you can use:\nhostname -I\n\nConnect to Your Raspberry Pi:\n\nOpen your VNC viewer application.\n\n\n\nEnter your Raspberry Pi’s IP address and hostname.\nWhen prompted, enter your Raspberry Pi’s username and password.\n\n\nThe Raspberry Pi 5 Desktop should appear on your computer monitor.\n\nAdjust Display Settings (if needed):\n\nOnce connected, adjust the display resolution for optimal viewing. This can be done through the Raspberry Pi’s desktop settings or by modifying the config.txt file.\nLet’s do it using the desktop settings. Reach the menu (the Raspberry Icon at the left upper corner) and select the best screen definition for your monitor:"
  },
  {
    "objectID": "raspi/setup/setup.html#updating-and-installing-software",
    "href": "raspi/setup/setup.html#updating-and-installing-software",
    "title": "Setup",
    "section": "Updating and Installing Software",
    "text": "Updating and Installing Software\n\nUpdate your system:\nsudo apt update && sudo apt upgrade -y\nInstall essential software:\nsudo apt install python3-pip -y\nEnable pip for Python projects:\nsudo rm /usr/lib/python3.11/EXTERNALLY-MANAGED"
  },
  {
    "objectID": "raspi/setup/setup.html#model-specific-considerations",
    "href": "raspi/setup/setup.html#model-specific-considerations",
    "title": "Setup",
    "section": "Model-Specific Considerations",
    "text": "Model-Specific Considerations\n\nRaspberry Pi Zero (Raspi-Zero)\n\nLimited processing power, best for lightweight projects\nIt is better to use a headless setup (SSH) to conserve resources.\nConsider increasing swap space for memory-intensive tasks.\nIt can be used for Image Classification and Object Detection Labs but not for the LLM (SLM).\n\n\n\nRaspberry Pi 4 or 5 (Raspi-4 or Raspi-5)\n\nSuitable for more demanding projects, including AI and machine learning.\nIt can run the whole desktop environment smoothly.\nRaspi-4 can be used for Image Classification and Object Detection Labs but will not work well with LLMs (SLM).\nFor Raspi-5, consider using an active cooler for temperature management during intensive tasks, as in the LLMs (SLMs) lab.\n\nRemember to adjust your project requirements based on the specific Raspberry Pi model you’re using. The Raspi-Zero is great for low-power, space-constrained projects, while the Raspi-4 or 5 models are better suited for more computationally intensive tasks."
  },
  {
    "objectID": "raspi/image_classification/image_classification.html#introduction",
    "href": "raspi/image_classification/image_classification.html#introduction",
    "title": "Image Classification",
    "section": "Introduction",
    "text": "Introduction\nImage classification is a fundamental task in computer vision that involves categorizing an image into one of several predefined classes. It’s a cornerstone of artificial intelligence, enabling machines to interpret and understand visual information in a way that mimics human perception.\nImage classification refers to assigning a label or category to an entire image based on its visual content. This task is crucial in computer vision and has numerous applications across various industries. Image classification’s importance lies in its ability to automate visual understanding tasks that would otherwise require human intervention.\n\nApplications in Real-World Scenarios\nImage classification has found its way into numerous real-world applications, revolutionizing various sectors:\n\nHealthcare: Assisting in medical image analysis, such as identifying abnormalities in X-rays or MRIs.\nAgriculture: Monitoring crop health and detecting plant diseases through aerial imagery.\nAutomotive: Enabling advanced driver assistance systems and autonomous vehicles to recognize road signs, pedestrians, and other vehicles.\nRetail: Powering visual search capabilities and automated inventory management systems.\nSecurity and Surveillance: Enhancing threat detection and facial recognition systems.\nEnvironmental Monitoring: Analyzing satellite imagery for deforestation, urban planning, and climate change studies.\n\n\n\nAdvantages of Running Classification on Edge Devices like Raspberry Pi\nImplementing image classification on edge devices such as the Raspberry Pi offers several compelling advantages:\n\nLow Latency: Processing images locally eliminates the need to send data to cloud servers, significantly reducing response times.\nOffline Functionality: Classification can be performed without an internet connection, making it suitable for remote or connectivity-challenged environments.\nPrivacy and Security: Sensitive image data remains on the local device, addressing data privacy concerns and compliance requirements.\nCost-Effectiveness: Eliminates the need for expensive cloud computing resources, especially for continuous or high-volume classification tasks.\nScalability: Enables distributed computing architectures where multiple devices can work independently or in a network.\nEnergy Efficiency: Optimized models on dedicated hardware can be more energy-efficient than cloud-based solutions, which is crucial for battery-powered or remote applications.\nCustomization: Deploying specialized or frequently updated models tailored to specific use cases is more manageable.\n\nWe can create more responsive, secure, and efficient computer vision solutions by leveraging the power of edge devices like Raspberry Pi for image classification. This approach opens up new possibilities for integrating intelligent visual processing into various applications and environments.\nIn the following sections, we’ll explore how to implement and optimize image classification on the Raspberry Pi, harnessing these advantages to create powerful and efficient computer vision systems."
  },
  {
    "objectID": "raspi/image_classification/image_classification.html#setting-up-the-environment",
    "href": "raspi/image_classification/image_classification.html#setting-up-the-environment",
    "title": "Image Classification",
    "section": "Setting Up the Environment",
    "text": "Setting Up the Environment\n\nUpdating the Raspberry Pi\nFirst, ensure your Raspberry Pi is up to date:\nsudo apt update\nsudo apt upgrade -y\n\n\nInstalling Required Libraries\nInstall the necessary libraries for image processing and machine learning:\nsudo apt install python3-pip\nsudo rm /usr/lib/python3.11/EXTERNALLY-MANAGED\npip3 install --upgrade pip\n\n\nSetting up a Virtual Environment (Optional but Recommended)\nCreate a virtual environment to manage dependencies:\npython3 -m venv ~/tflite\nsource ~/tflite/bin/activate\n\n\nInstalling TensorFlow Lite\nWe are interested in performing inference, which refers to executing a TensorFlow Lite model on a device to make predictions based on input data. To perform an inference with a TensorFlow Lite model, we must run it through an interpreter. The TensorFlow Lite interpreter is designed to be lean and fast. The interpreter uses a static graph ordering and a custom (less-dynamic) memory allocator to ensure minimal load, initialization, and execution latency.\nWe’ll use the TensorFlow Lite runtime for Raspberry Pi, a simplified library for running machine learning models on mobile and embedded devices, without including all TensorFlow packages.\npip install tflite_runtime --no-deps\n\nThe wheel installed: tflite_runtime-2.14.0-cp311-cp311-manylinux_2_34_aarch64.whl\n\n\n\nInstalling Additional Python Libraries\nInstall required Python libraries for use with Image Classification:\nIf you have another version of Numpy installed, first uninstall it.\npip3 uninstall numpy\nInstall version 1.23.2, which is compatible with the tflite_runtime.\n pip3 install numpy==1.23.2\npip3 install Pillow matplotlib\n\n\nCreating a working directory:\nIf you are working on the Raspi-Zero with the minimum OS (No Desktop), you may not have a user-pre-defined directory tree (you can check it with ls. So, let’s create one:\nmkdir Documents\ncd Documents/\nmkdir TFLITE\ncd TFLITE/\nmkdir IMG_CLASS\ncd IMG_CLASS\nmkdir models\ncd models\n\nOn the Raspi-5, the /Documents should be there.\n\nGet a pre-trained Image Classification model:\nAn appropriate pre-trained model is crucial for successful image classification on resource-constrained devices like the Raspberry Pi. MobileNet is designed for mobile and embedded vision applications with a good balance between accuracy and speed. Versions: MobileNetV1, MobileNetV2, MobileNetV3. Let’s download the V2:\nwget https://storage.googleapis.com/download.tensorflow.org/models/\ntflite_11_05_08/mobilenet_v2_1.0_224_quant.tgz\n\ntar xzf mobilenet_v2_1.0_224_quant.tgz\nGet its labels:\nwget https://github.com/Mjrovai/EdgeML-with-Raspberry-Pi/blob/main/IMG_CLASS/models/labels.txt\nIn the end, you should have the models in its directory:\n\n\nWe will only need the mobilenet_v2_1.0_224_quant.tflite model and the labels.txt. You can delete the other files.\n\n\n\nSetting up Jupyter Notebook (Optional)\nIf you prefer using Jupyter Notebook for development:\npip3 install jupyter\njupyter notebook --generate-config\nTo run Jupyter Notebook, run the command (change the IP address for yours):\njupyter notebook --ip=192.168.4.210 --no-browser\nOn the terminal, you can see the local URL address to open the notebook:\n\nYou can access it from another device by entering the Raspberry Pi’s IP address and the provided token in a web browser (you can copy the token from the terminal).\n\nDefine your working directory in the Raspi and create a new Python 3 notebook.\n\n\nVerifying the Setup\nTest your setup by running a simple Python script:\nimport tflite_runtime.interpreter as tflite\nimport numpy as np\nfrom PIL import Image\n\nprint(\"NumPy:\", np.__version__)\nprint(\"Pillow:\", Image.__version__)\n\n# Try to create a TFLite Interpreter\nmodel_path = \"./models/mobilenet_v2_1.0_224_quant.tflite\"\ninterpreter = tflite.Interpreter(model_path=model_path)\ninterpreter.allocate_tensors()\nprint(\"TFLite Interpreter created successfully!\")\nYou can create the Python script using nano on the terminal, saving it with CTRL+0 + ENTER + CTRL+X\n\nAnd run it with the command:\n\nOr you can run it directly on the Notebook:"
  },
  {
    "objectID": "raspi/image_classification/image_classification.html#making-inferences-with-mobilenet-v2",
    "href": "raspi/image_classification/image_classification.html#making-inferences-with-mobilenet-v2",
    "title": "Image Classification",
    "section": "Making inferences with Mobilenet V2",
    "text": "Making inferences with Mobilenet V2\nIn the last section, we set up the environment, including downloading a popular pre-trained model, Mobilenet V2, trained on ImageNet’s 224x224 images (1.2 million) for 1,001 classes (1,000 object categories plus 1 background). The model was converted to a compact 3.5MB TensorFlow Lite format, making it suitable for the limited storage and memory of a Raspberry Pi.\n\nLet’s start a new notebook to follow all the steps to classify one image:\nImport the needed libraries:\nimport time\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom PIL import Image\nimport tflite_runtime.interpreter as tflite\nLoad the TFLite model and allocate tensors:\nmodel_path = \"./models/mobilenet_v2_1.0_224_quant.tflite\"\ninterpreter = tflite.Interpreter(model_path=model_path)\ninterpreter.allocate_tensors()\nGet input and output tensors.\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\nInput details will give us information about how the model should be fed with an image. The shape of (1, 224, 224, 3) informs us that an image with dimensions (224x224x3) should be input one by one (Batch Dimension: 1).\n\nThe output details show that the inference will result in an array of 1,001 integer values. Those values result from the image classification, where each value is the probability of that specific label being related to the image.\n\nLet’s also inspect the dtype of input details of the model\ninput_dtype = input_details[0]['dtype']\ninput_dtype\ndtype('uint8')\nThis shows that the input image should be raw pixels (0 - 255).\nLet’s get a test image. You can transfer it from your computer or download one for testing. Let’s first create a folder under our working directory:\nmkdir images\ncd images\nwget https://upload.wikimedia.org/wikipedia/commons/3/3a/Cat03.jpg\nLet’s load and display the image:\n# Load he image\nimg_path = \"./images/Cat03.jpg\"\nimg = Image.open(img_path)\n\n# Display the image\nplt.figure(figsize=(8, 8))\nplt.imshow(img)\nplt.title(\"Original Image\")\nplt.show()\n\nWe can see the image size running the command:\nwidth, height = img.size\nThat shows us that the image is an RGB image with a width of 1600 and a height of 1600 pixels. So, to use our model, we should reshape it to (224, 224, 3) and add a batch dimension of 1, as defined in input details: (1, 224, 224, 3). The inference result, as shown in output details, will be an array with a 1001 size, as shown below:\n\nSo, let’s reshape the image, add the batch dimension, and see the result:\nimg = img.resize((input_details[0]['shape'][1], input_details[0]['shape'][2]))\ninput_data = np.expand_dims(img, axis=0)\ninput_data.shape\nThe input_data shape is as expected: (1, 224, 224, 3)\nLet’s confirm the dtype of the input data:\ninput_data.dtype\ndtype('uint8')\nThe input data dtype is ‘uint8’, which is compatible with the dtype expected for the model.\nUsing the input_data, let’s run the interpreter and get the predictions (output):\ninterpreter.set_tensor(input_details[0]['index'], input_data)\ninterpreter.invoke()\npredictions = interpreter.get_tensor(output_details[0]['index'])[0]\nThe prediction is an array with 1001 elements. Let’s get the Top-5 indices where their elements have high values:\ntop_k_results = 5\ntop_k_indices = np.argsort(predictions)[::-1][:top_k_results]\ntop_k_indices \nThe top_k_indices is an array with 5 elements: array([283, 286, 282])\nSo, 283, 286, 282, 288, and 479 are the image’s most probable classes. Having the index, we must find to what class it appoints (such as car, cat, or dog). The text file downloaded with the model has a label associated with each index from 0 to 1,000. Let’s use a function to load the .txt file as a list:\ndef load_labels(filename):\n    with open(filename, 'r') as f:\n        return [line.strip() for line in f.readlines()]\nAnd get the list, printing the labels associated with the indexes:\nlabels_path = \"./models/labels.txt\"\nlabels = load_labels(labels_path)\n\nprint(labels[286])\nprint(labels[283])\nprint(labels[282])\nprint(labels[288])\nprint(labels[479])\nAs a result, we have:\nEgyptian cat\ntiger cat\ntabby\nlynx\ncarton\nAt least the four top indices are related to felines. The prediction content is the probability associated with each one of the labels. As we saw on output details, those values are quantized and should be dequantized and apply softmax.\nscale, zero_point = output_details[0]['quantization']\ndequantized_output = (predictions.astype(np.float32) - zero_point) * scale\nexp_output = np.exp(dequantized_output - np.max(dequantized_output))\nprobabilities = exp_output / np.sum(exp_output)\nLet’s print the top-5 probabilities:\nprint (probabilities[286])\nprint (probabilities[283])\nprint (probabilities[282])\nprint (probabilities[288])\nprint (probabilities[479])\n0.27741462\n0.3732285\n0.16919471\n0.10319158\n0.023410844\nFor clarity, let’s create a function to relate the labels with the probabilities:\nfor i in range(top_k_results):\n    print(\"\\t{:20}: {}%\".format(\n        labels[top_k_indices[i]],\n        (int(probabilities[top_k_indices[i]]*100))))\ntiger cat           : 37%\nEgyptian cat        : 27%\ntabby               : 16%\nlynx                : 10%\ncarton              : 2%\n\nDefine a general Image Classification function\nLet’s create a general function to give an image as input, and we get the Top-5 possible classes:\n\ndef image_classification(img_path, model_path, labels, top_k_results=5):\n    # load the image\n    img = Image.open(img_path)\n    plt.figure(figsize=(4, 4))\n    plt.imshow(img)\n    plt.axis('off')\n\n    # Load the TFLite model\n    interpreter = tflite.Interpreter(model_path=model_path)\n    interpreter.allocate_tensors()\n    \n    # Get input and output tensors\n    input_details = interpreter.get_input_details()\n    output_details = interpreter.get_output_details()\n    \n    # Preprocess\n    img = img.resize((input_details[0]['shape'][1], \n                      input_details[0]['shape'][2]))\n    input_data = np.expand_dims(img, axis=0)\n    \n    # Inference on Raspi-Zero\n    interpreter.set_tensor(input_details[0]['index'], input_data)\n    interpreter.invoke()\n    \n    # Obtain results and map them to the classes\n    predictions = interpreter.get_tensor(output_details[0]['index'])[0]\n    \n    # Get indices of the top k results\n    top_k_indices = np.argsort(predictions)[::-1][:top_k_results]\n    \n    # Get quantization parameters\n    scale, zero_point = output_details[0]['quantization']\n    \n    # Dequantize the output and apply softmax\n    dequantized_output = (predictions.astype(np.float32) - zero_point) * scale\n    exp_output = np.exp(dequantized_output - np.max(dequantized_output))\n    probabilities = exp_output / np.sum(exp_output)\n    \n    print(\"\\n\\t[PREDICTION]        [Prob]\\n\")\n    for i in range(top_k_results):\n        print(\"\\t{:20}: {}%\".format(\n            labels[top_k_indices[i]],\n            (int(probabilities[top_k_indices[i]]*100))))\n\nAnd loading some images for testing, we have:\n\n\n\nTesting with a model trained from scratch\nLet’s get a TFLite model trained from scratch. For that, you can follow the Notebook:\nCNN to classify Cifar-10 dataset\nIn the notebook, we trained a model using the CIFAR10 dataset, which contains 60,000 images from 10 classes of CIFAR (airplane, automobile, bird, cat, deer, dog, frog, horse, ship, and truck). CIFAR has 32x32 color images (3 color channels) where the objects are not centered and can have the object with a background, such as airplanes that might have a cloudy sky behind them! In short, small but real images.\nThe CNN trained model (cifar10_model.keras) had a size of 2.0MB. Using the TFLite Converter, the model cifar10.tflite became with 674MB (around 1/3 of the original size).\n\nOn the notebook Cifar 10 - Image Classification on a Raspi with TFLite (which can be run over the Raspi), we can follow the same steps we did with the mobilenet_v2_1.0_224_quant.tflite. Below are examples of images using the General Function for Image Classification on a Raspi-Zero, as shown in the last section.\n\n\n\nInstalling Picamera2\nPicamera2, a Python library for interacting with Raspberry Pi’s camera, is based on the libcamera camera stack, and the Raspberry Pi foundation maintains it. The Picamera2 library is supported on all Raspberry Pi models, from the Pi Zero to the RPi 5. It is already installed system-wide on the Raspi, but we should make it accessible within the virtual environment.\n\nFirst, activate the virtual environment if it’s not already activated:\nsource ~/tflite/bin/activate\nNow, let’s create a .pth file in your virtual environment to add the system site-packages path:\necho \"/usr/lib/python3/dist-packages\" &gt; $VIRTUAL_ENV/lib/python3.11/\nsite-packages/system_site_packages.pth\n\nNote: If your Python version differs, replace python3.11 with the appropriate version.\n\nAfter creating this file, try importing picamera2 in Python:\npython3\n&gt;&gt;&gt; import picamera2\n&gt;&gt;&gt; print(picamera2.__file__)\n\nThe above code will show the file location of the picamera2 module itself, proving that the library can be accessed from the environment.\n/home/mjrovai/tflite/lib/python3.11/site-packages/picamera2/__init__.py\nYou can also list the available cameras in the system:\n&gt;&gt;&gt; print(Picamera2.global_camera_info())\nIn my case, with a USB installed, I got:\n\nNow that we’ve confirmed picamera2 is working in the environment with an index 0, let’s try a simple Python script to capture an image from your USB camera:\nfrom picamera2 import Picamera2\nimport time\n\n# Initialize the camera\npicam2 = Picamera2() # default is index 0\n\n# Configure the camera\nconfig = picam2.create_still_configuration(main={\"size\": (640, 480)})\npicam2.configure(config)\n\n# Start the camera\npicam2.start()\n\n# Wait for the camera to warm up\ntime.sleep(2)\n\n# Capture an image\npicam2.capture_file(\"usb_camera_image.jpg\")\nprint(\"Image captured and saved as 'usb_camera_image.jpg'\")\n\n# Stop the camera\npicam2.stop()\nUse the Nano text editor, the Jupyter Notebook, or any other editor. Save this as a Python script (e.g., capture_image.py) and run it. This should capture an image from your camera and save it as “usb_camera_image.jpg” in the same directory as your script.\n\nIf the Jupyter is open, you can see the captured image on your computer. Otherwise, transfer the file from the Raspi to your computer.\n\n\nIf you are working with a Raspi-5 with a whole desktop, you can open the file directly on the device."
  },
  {
    "objectID": "raspi/image_classification/image_classification.html#image-classification-project",
    "href": "raspi/image_classification/image_classification.html#image-classification-project",
    "title": "Image Classification",
    "section": "Image Classification Project",
    "text": "Image Classification Project\nNow, we will develop a complete Image Classification project using the Edge Impulse Studio. As we did with the Movilinet V2, the trained and converted TFLite model will be used for inference.\n\nThe Goal\nThe first step in any ML project is to define its goal. In this case, it is to detect and classify two specific objects present in one image. For this project, we will use two small toys: a robot and a small Brazilian parrot (named Periquito). We will also collect images of a background where those two objects are absent.\n\n\n\nData Collection\nOnce we have defined our Machine Learning project goal, the next and most crucial step is collecting the dataset. We can use a phone for the image capture, but we will use the Raspi here. Let’s set up a simple web server on our Raspberry Pi to view the QVGA (320 x 240) captured images in a browser.\n\nFirst, let’s install Flask, a lightweight web framework for Python:\npip3 install flask\nLet’s create a new Python script combining image capture with a web server. We’ll call it get_img_data.py:\n\n\nfrom flask import Flask, Response, render_template_string, request, redirect, url_for\nfrom picamera2 import Picamera2\nimport io\nimport threading\nimport time\nimport os\nimport signal\n\napp = Flask(__name__)\n\n# Global variables\nbase_dir = \"dataset\"\npicam2 = None\nframe = None\nframe_lock = threading.Lock()\ncapture_counts = {}\ncurrent_label = None\nshutdown_event = threading.Event()\n\ndef initialize_camera():\n    global picam2\n    picam2 = Picamera2()\n    config = picam2.create_preview_configuration(main={\"size\": (320, 240)})\n    picam2.configure(config)\n    picam2.start()\n    time.sleep(2)  # Wait for camera to warm up\n\ndef get_frame():\n    global frame\n    while not shutdown_event.is_set():\n        stream = io.BytesIO()\n        picam2.capture_file(stream, format='jpeg')\n        with frame_lock:\n            frame = stream.getvalue()\n        time.sleep(0.1)  # Adjust as needed for smooth preview\n\ndef generate_frames():\n    while not shutdown_event.is_set():\n        with frame_lock:\n            if frame is not None:\n                yield (b'--frame\\r\\n'\n                       b'Content-Type: image/jpeg\\r\\n\\r\\n' + frame + b'\\r\\n')\n        time.sleep(0.1)  # Adjust as needed for smooth streaming\n\ndef shutdown_server():\n    shutdown_event.set()\n    if picam2:\n        picam2.stop()\n    # Give some time for other threads to finish\n    time.sleep(2)\n    # Send SIGINT to the main process\n    os.kill(os.getpid(), signal.SIGINT)\n\n@app.route('/', methods=['GET', 'POST'])\ndef index():\n    global current_label\n    if request.method == 'POST':\n        current_label = request.form['label']\n        if current_label not in capture_counts:\n            capture_counts[current_label] = 0\n        os.makedirs(os.path.join(base_dir, current_label), exist_ok=True)\n        return redirect(url_for('capture_page'))\n    return render_template_string('''\n        &lt;!DOCTYPE html&gt;\n        &lt;html&gt;\n        &lt;head&gt;\n            &lt;title&gt;Dataset Capture - Label Entry&lt;/title&gt;\n        &lt;/head&gt;\n        &lt;body&gt;\n            &lt;h1&gt;Enter Label for Dataset&lt;/h1&gt;\n            &lt;form method=\"post\"&gt;\n                &lt;input type=\"text\" name=\"label\" required&gt;\n                &lt;input type=\"submit\" value=\"Start Capture\"&gt;\n            &lt;/form&gt;\n        &lt;/body&gt;\n        &lt;/html&gt;\n    ''')\n\n@app.route('/capture')\ndef capture_page():\n    return render_template_string('''\n        &lt;!DOCTYPE html&gt;\n        &lt;html&gt;\n        &lt;head&gt;\n            &lt;title&gt;Dataset Capture&lt;/title&gt;\n            &lt;script&gt;\n                var shutdownInitiated = false;\n                function checkShutdown() {\n                    if (!shutdownInitiated) {\n                        fetch('/check_shutdown')\n                            .then(response =&gt; response.json())\n                            .then(data =&gt; {\n                                if (data.shutdown) {\n                                    shutdownInitiated = true;\n                                    document.getElementById('video-feed').src = '';\n                                    document.getElementById('shutdown-message')\n                                    .style.display = 'block';\n                                }\n                            });\n                    }\n                }\n                setInterval(checkShutdown, 1000);  // Check every second\n            &lt;/script&gt;\n        &lt;/head&gt;\n        &lt;body&gt;\n            &lt;h1&gt;Dataset Capture&lt;/h1&gt;\n            &lt;p&gt;Current Label: {{ label }}&lt;/p&gt;\n            &lt;p&gt;Images captured for this label: {{ capture_count }}&lt;/p&gt;\n            &lt;img id=\"video-feed\" src=\"{{ url_for('video_feed') }}\" width=\"640\" \n            height=\"480\" /&gt;\n            &lt;div id=\"shutdown-message\" style=\"display: none; color: red;\"&gt;\n                Capture process has been stopped. You can close this window.\n            &lt;/div&gt;\n            &lt;form action=\"/capture_image\" method=\"post\"&gt;\n                &lt;input type=\"submit\" value=\"Capture Image\"&gt;\n            &lt;/form&gt;\n            &lt;form action=\"/stop\" method=\"post\"&gt;\n                &lt;input type=\"submit\" value=\"Stop Capture\" \n                style=\"background-color: #ff6666;\"&gt;\n            &lt;/form&gt;\n            &lt;form action=\"/\" method=\"get\"&gt;\n                &lt;input type=\"submit\" value=\"Change Label\" \n                style=\"background-color: #ffff66;\"&gt;\n            &lt;/form&gt;\n        &lt;/body&gt;\n        &lt;/html&gt;\n    ''', label=current_label, capture_count=capture_counts.get(current_label, 0))\n\n@app.route('/video_feed')\ndef video_feed():\n    return Response(generate_frames(),\n                    mimetype='multipart/x-mixed-replace; boundary=frame')\n\n@app.route('/capture_image', methods=['POST'])\ndef capture_image():\n    global capture_counts\n    if current_label and not shutdown_event.is_set():\n        capture_counts[current_label] += 1\n        timestamp = time.strftime(\"%Y%m%d-%H%M%S\")\n        filename = f\"image_{timestamp}.jpg\"\n        full_path = os.path.join(base_dir, current_label, filename)\n        \n        picam2.capture_file(full_path)\n    \n    return redirect(url_for('capture_page'))\n\n@app.route('/stop', methods=['POST'])\ndef stop():\n    summary = render_template_string('''\n        &lt;!DOCTYPE html&gt;\n        &lt;html&gt;\n        &lt;head&gt;\n            &lt;title&gt;Dataset Capture - Stopped&lt;/title&gt;\n        &lt;/head&gt;\n        &lt;body&gt;\n            &lt;h1&gt;Dataset Capture Stopped&lt;/h1&gt;\n            &lt;p&gt;The capture process has been stopped. You can close this window.&lt;/p&gt;\n            &lt;p&gt;Summary of captures:&lt;/p&gt;\n            &lt;ul&gt;\n            {% for label, count in capture_counts.items() %}\n                &lt;li&gt;{{ label }}: {{ count }} images&lt;/li&gt;\n            {% endfor %}\n            &lt;/ul&gt;\n        &lt;/body&gt;\n        &lt;/html&gt;\n    ''', capture_counts=capture_counts)\n    \n    # Start a new thread to shutdown the server\n    threading.Thread(target=shutdown_server).start()\n    \n    return summary\n\n@app.route('/check_shutdown')\ndef check_shutdown():\n    return {'shutdown': shutdown_event.is_set()}\n\nif __name__ == '__main__':\n    initialize_camera()\n    threading.Thread(target=get_frame, daemon=True).start()\n    app.run(host='0.0.0.0', port=5000, threaded=True)\n\n\nRun this script:\npython3 get_img_data.py\nAccess the web interface:\n\nOn the Raspberry Pi itself (if you have a GUI): Open a web browser and go to http://localhost:5000\nFrom another device on the same network: Open a web browser and go to http://&lt;raspberry_pi_ip&gt;:5000 (Replace &lt;raspberry_pi_ip&gt; with your Raspberry Pi’s IP address). For example: http://192.168.4.210:5000/\n\n\nThis Python script creates a web-based interface for capturing and organizing image datasets using a Raspberry Pi and its camera. It’s handy for machine learning projects that require labeled image data.\n\nKey Features:\n\nWeb Interface: Accessible from any device on the same network as the Raspberry Pi.\nLive Camera Preview: This shows a real-time feed from the camera.\nLabeling System: Allows users to input labels for different categories of images.\nOrganized Storage: Automatically saves images in label-specific subdirectories.\nPer-Label Counters: Keeps track of how many images are captured for each label.\nSummary Statistics: Provides a summary of captured images when stopping the capture process.\n\n\n\nMain Components:\n\nFlask Web Application: Handles routing and serves the web interface.\nPicamera2 Integration: Controls the Raspberry Pi camera.\nThreaded Frame Capture: Ensures smooth live preview.\nFile Management: Organizes captured images into labeled directories.\n\n\n\nKey Functions:\n\ninitialize_camera(): Sets up the Picamera2 instance.\nget_frame(): Continuously captures frames for the live preview.\ngenerate_frames(): Yields frames for the live video feed.\nshutdown_server(): Sets the shutdown event, stops the camera, and shuts down the Flask server\nindex(): Handles the label input page.\ncapture_page(): Displays the main capture interface.\nvideo_feed(): Shows a live preview to position the camera\ncapture_image(): Saves an image with the current label.\nstop(): Stops the capture process and displays a summary.\n\n\n\nUsage Flow:\n\nStart the script on your Raspberry Pi.\nAccess the web interface from a browser.\nEnter a label for the images you want to capture and press Start Capture.\n\n\n\nUse the live preview to position the camera.\nClick Capture Image to save images under the current label.\n\n\n\nChange labels as needed for different categories, selecting Change Label.\nClick Stop Capture when finished to see a summary.\n\n\n\n\nTechnical Notes:\n\nThe script uses threading to handle concurrent frame capture and web serving.\nImages are saved with timestamps in their filenames for uniqueness.\nThe web interface is responsive and can be accessed from mobile devices.\n\n\n\nCustomization Possibilities:\n\nAdjust image resolution in the initialize_camera() function. Here we used QVGA (320X240).\nModify the HTML templates for a different look and feel.\nAdd additional image processing or analysis steps in the capture_image() function.\n\n\n\nNumber of samples on Dataset:\nGet around 60 images from each category (periquito, robot and background). Try to capture different angles, backgrounds, and light conditions. On the Raspi, we will end with a folder named dataset, witch contains 3 sub-folders periquito, robot, and background. one for each class of images.\nYou can use Filezilla to transfer the created dataset to your main computer."
  },
  {
    "objectID": "raspi/image_classification/image_classification.html#training-the-model-with-edge-impulse-studio",
    "href": "raspi/image_classification/image_classification.html#training-the-model-with-edge-impulse-studio",
    "title": "Image Classification",
    "section": "Training the model with Edge Impulse Studio",
    "text": "Training the model with Edge Impulse Studio\nWe will use the Edge Impulse Studio to train our model. Go to the Edge Impulse Page, enter your account credentials, and create a new project:\n\n\nHere, you can clone a similar project: Raspi - Img Class.\n\n\nDataset\nWe will walk through four main steps using the EI Studio (or Studio). These steps are crucial in preparing our model for use on the Raspi: Dataset, Impulse, Tests, and Deploy (on the Edge Device, in this case, the Raspi).\n\nRegarding the Dataset, it is essential to point out that our Original Dataset, captured with the Raspi, will be split into Training, Validation, and Test. The Test Set will be separated from the beginning and reserved for use only in the Test phase after training. The Validation Set will be used during training.\n\nOn Studio, follow the steps to upload the captured data:\n\nGo to the Data acquisition tab, and in the UPLOAD DATA section, upload the files from your computer in the chosen categories.\nLeave to the Studio the splitting of the original dataset into train and test and choose the label about\nRepeat the procedure for all three classes. At the end, you should see your “raw data” in the Studio:\n\n\nThe Studio allows you to explore your data, showing a complete view of all the data in your project. You can clear, inspect, or change labels by clicking on individual data items. In our case, a straightforward project, the data seems OK."
  },
  {
    "objectID": "raspi/image_classification/image_classification.html#the-impulse-design",
    "href": "raspi/image_classification/image_classification.html#the-impulse-design",
    "title": "Image Classification",
    "section": "The Impulse Design",
    "text": "The Impulse Design\nIn this phase, we should define how to:\n\nPre-process our data, which consists of resizing the individual images and determining the color depth to use (be it RGB or Grayscale) and\nSpecify a Model. In this case, it will be the Transfer Learning (Images) to fine-tune a pre-trained MobileNet V2 image classification model on our data. This method performs well even with relatively small image datasets (around 180 images in our case).\n\nTransfer Learning with MobileNet offers a streamlined approach to model training, which is especially beneficial for resource-constrained environments and projects with limited labeled data. MobileNet, known for its lightweight architecture, is a pre-trained model that has already learned valuable features from a large dataset (ImageNet).\n\nBy leveraging these learned features, we can train a new model for your specific task with fewer data and computational resources and achieve competitive accuracy.\n\nThis approach significantly reduces training time and computational cost, making it ideal for quick prototyping and deployment on embedded devices where efficiency is paramount.\nGo to the Impulse Design Tab and create the impulse, defining an image size of 160x160 and squashing them (squared form, without cropping). Select Image and Transfer Learning blocks. Save the Impulse.\n\n\nImage Pre-Processing\nAll the input QVGA/RGB565 images will be converted to 76,800 features (160x160x3).\n\nPress Save parameters and select Generate features in the next tab.\n\n\nModel Design\nMobileNet is a family of efficient convolutional neural networks designed for mobile and embedded vision applications. The key features of MobileNet are:\n\nLightweight: Optimized for mobile devices and embedded systems with limited computational resources.\nSpeed: Fast inference times, suitable for real-time applications.\nAccuracy: Maintains good accuracy despite its compact size.\n\nMobileNetV2, introduced in 2018, improves the original MobileNet architecture. Key features include:\n\nInverted Residuals: Inverted residual structures are used where shortcut connections are made between thin bottleneck layers.\nLinear Bottlenecks: Removes non-linearities in the narrow layers to prevent the destruction of information.\nDepth-wise Separable Convolutions: Continues to use this efficient operation from MobileNetV1.\n\nIn our project, we will do a Transfer Learning with the MobileNetV2 160x160 1.0, which means that the images used for training (and future inference) should have an input Size of 160x160 pixels and a Width Multiplier of 1.0 (full width, not reduced). This configuration balances between model size, speed, and accuracy.\n\n\nModel Training\nAnother valuable deep learning technique is Data Augmentation. Data augmentation improves the accuracy of machine learning models by creating additional artificial data. A data augmentation system makes small, random changes to the training data during the training process (such as flipping, cropping, or rotating the images).\nLooking under the hood, here you can see how Edge Impulse implements a data Augmentation policy on your data:\n# Implements the data augmentation policy\ndef augment_image(image, label):\n    # Flips the image randomly\n    image = tf.image.random_flip_left_right(image)\n\n    # Increase the image size, then randomly crop it down to\n    # the original dimensions\n    resize_factor = random.uniform(1, 1.2)\n    new_height = math.floor(resize_factor * INPUT_SHAPE[0])\n    new_width = math.floor(resize_factor * INPUT_SHAPE[1])\n    image = tf.image.resize_with_crop_or_pad(image, new_height, new_width)\n    image = tf.image.random_crop(image, size=INPUT_SHAPE)\n\n    # Vary the brightness of the image\n    image = tf.image.random_brightness(image, max_delta=0.2)\n\n    return image, label\nExposure to these variations during training can help prevent your model from taking shortcuts by “memorizing” superficial clues in your training data, meaning it may better reflect the deep underlying patterns in your dataset.\nThe final dense layer of our model will have 0 neurons with a 10% dropout for overfitting prevention. Here is the Training result:\n\nThe result is excellent, with a reasonable 35ms of latency (for a Raspi-4), which should result in around 30 fps (frames per second) during inference. A Raspi-Zero should be slower, and the Raspi-5, faster.\n\n\nTrading off: Accuracy versus speed\nIf faster inference is needed, we should train the model using smaller alphas (0.35, 0.5, and 0.75) or even reduce the image input size, trading with accuracy. However, reducing the input image size and decreasing the alpha (width multiplier) can speed up inference for MobileNet V2, but they have different trade-offs. Let’s compare:\n\nReducing Image Input Size:\n\nPros:\n\nSignificantly reduces the computational cost across all layers.\nDecreases memory usage.\nIt often provides a substantial speed boost.\n\nCons:\n\nIt may reduce the model’s ability to detect small features or fine details.\nIt can significantly impact accuracy, especially for tasks requiring fine-grained recognition.\n\n\nReducing Alpha (Width Multiplier):\n\nPros:\n\nReduces the number of parameters and computations in the model.\nMaintains the original input resolution, potentially preserving more detail.\nIt can provide a good balance between speed and accuracy.\n\nCons:\n\nIt may not speed up inference as dramatically as reducing input size.\nIt can reduce the model’s capacity to learn complex features.\n\nComparison:\n\nSpeed Impact:\n\nReducing input size often provides a more substantial speed boost because it reduces computations quadratically (halving both width and height reduces computations by about 75%).\nReducing alpha provides a more linear reduction in computations.\n\nAccuracy Impact:\n\nReducing input size can severely impact accuracy, especially when detecting small objects or fine details.\nReducing alpha tends to have a more gradual impact on accuracy.\n\nModel Architecture:\n\nChanging input size doesn’t alter the model’s architecture.\nChanging alpha modifies the model’s structure by reducing the number of channels in each layer.\n\n\nRecommendation:\n\nIf our application doesn’t require detecting tiny details and can tolerate some loss in accuracy, reducing the input size is often the most effective way to speed up inference.\nReducing alpha might be preferable if maintaining the ability to detect fine details is crucial or if you need a more balanced trade-off between speed and accuracy.\nFor best results, you might want to experiment with both:\n\nTry MobileNet V2 with input sizes like 160x160 or 92x92\nExperiment with alpha values like 1.0, 0.75, 0.5 or 0.35.\n\nAlways benchmark the different configurations on your specific hardware and with your particular dataset to find the optimal balance for your use case.\n\n\nRemember, the best choice depends on your specific requirements for accuracy, speed, and the nature of the images you’re working with. It’s often worth experimenting with combinations to find the optimal configuration for your particular use case.\n\n\n\nModel Testing\nNow, you should take the data set aside at the start of the project and run the trained model using it as input. Again, the result is excellent (92.22%).\n\n\nDeploying the model\nAs we did in the previous section, we can deploy the trained model as .tflite and use Raspi to run it using Python.\nOn the Dashboard tab, go to Transfer learning model (int8 quantized) and click on the download icon:\n\n\nLet’s also download the float32 version for comparasion\n\nTransfer the model from your computer to the Raspi (./models), for example, using FileZilla. Also, capture some images for inference (./images).\nImport the needed libraries:\nimport time\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom PIL import Image\nimport tflite_runtime.interpreter as tflite\nDefine the paths and labels:\nimg_path = \"./images/robot.jpg\"\nmodel_path = \"./models/ei-raspi-img-class-int8-quantized-model.tflite\"\nlabels = ['background', 'periquito', 'robot']\n\nNote that the models trained on the Edge Impulse Studio will output values with index 0, 1, 2, etc., where the actual labels will follow an alphabetic order.\n\nLoad the model, allocate the tensors, and get the input and output tensor details:\n# Load the TFLite model\ninterpreter = tflite.Interpreter(model_path=model_path)\ninterpreter.allocate_tensors()\n\n# Get input and output tensors\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\nOne important difference to note is that the dtype of the input details of the model is now int8, which means that the input values go from -128 to +127, while each pixel of our image goes from 0 to 256. This means that we should pre-process the image to match it. We can check here:\ninput_dtype = input_details[0]['dtype']\ninput_dtype\nnumpy.int8\nSo, let’s open the image and show it:\nimg = Image.open(img_path)\nplt.figure(figsize=(4, 4))\nplt.imshow(img)\nplt.axis('off')\nplt.show()\n\nAnd perform the pre-processing:\nscale, zero_point = input_details[0]['quantization']\nimg = img.resize((input_details[0]['shape'][1], \n                  input_details[0]['shape'][2]))\nimg_array = np.array(img, dtype=np.float32) / 255.0\nimg_array = (img_array / scale + zero_point).clip(-128, 127).astype(np.int8)\ninput_data = np.expand_dims(img_array, axis=0)\nChecking the input data, we can verify that the input tensor is compatible with what is expected by the model:\ninput_data.shape, input_data.dtype\n((1, 160, 160, 3), dtype('int8'))\nNow, it is time to perform the inference. Let’s also calculate the latency of the model:\n# Inference on Raspi-Zero\nstart_time = time.time()\ninterpreter.set_tensor(input_details[0]['index'], input_data)\ninterpreter.invoke()\nend_time = time.time()\ninference_time = (end_time - start_time) * 1000  # Convert to milliseconds\nprint (\"Inference time: {:.1f}ms\".format(inference_time))\nThe model will take around 125ms to perform the inference in the Raspi-Zero, which is 3 to 4 times longer than a Raspi-5.\nNow, we can get the output labels and probabilities. It is also important to note that the model trained on the Edge Impulse Studio has a softmax in its output (different from the original Movilenet V2), and we should use the model’s raw output as the “probabilities.”\n# Obtain results and map them to the classes\npredictions = interpreter.get_tensor(output_details[0]['index'])[0]\n\n# Get indices of the top k results\ntop_k_results=3\ntop_k_indices = np.argsort(predictions)[::-1][:top_k_results]\n\n# Get quantization parameters\nscale, zero_point = output_details[0]['quantization']\n\n# Dequantize the output\ndequantized_output = (predictions.astype(np.float32) - zero_point) * scale\nprobabilities = dequantized_output\n\nprint(\"\\n\\t[PREDICTION]        [Prob]\\n\")\nfor i in range(top_k_results):\n    print(\"\\t{:20}: {:.2f}%\".format(\n        labels[top_k_indices[i]],\n        probabilities[top_k_indices[i]] * 100))\n\nLet’s modify the function created before so that we can handle different type of models:\n\ndef image_classification(img_path, model_path, labels, top_k_results=3, \n                         apply_softmax=False):\n    # Load the image\n    img = Image.open(img_path)\n    plt.figure(figsize=(4, 4))\n    plt.imshow(img)\n    plt.axis('off')\n\n    # Load the TFLite model\n    interpreter = tflite.Interpreter(model_path=model_path)\n    interpreter.allocate_tensors()\n    \n    # Get input and output tensors\n    input_details = interpreter.get_input_details()\n    output_details = interpreter.get_output_details()\n    \n    # Preprocess\n    img = img.resize((input_details[0]['shape'][1], \n                      input_details[0]['shape'][2]))\n    \n    input_dtype = input_details[0]['dtype']\n    \n    if input_dtype == np.uint8:\n        input_data = np.expand_dims(np.array(img), axis=0)\n    elif input_dtype == np.int8:\n        scale, zero_point = input_details[0]['quantization']\n        img_array = np.array(img, dtype=np.float32) / 255.0\n        img_array = (img_array / scale + zero_point).clip(-128, 127).astype(np.int8)\n        input_data = np.expand_dims(img_array, axis=0)\n    else:  # float32\n        input_data = np.expand_dims(np.array(img, dtype=np.float32), axis=0) / 255.0\n    \n    # Inference on Raspi-Zero\n    start_time = time.time()\n    interpreter.set_tensor(input_details[0]['index'], input_data)\n    interpreter.invoke()\n    end_time = time.time()\n    inference_time = (end_time - start_time) * 1000  # Convert to milliseconds\n    \n    # Obtain results\n    predictions = interpreter.get_tensor(output_details[0]['index'])[0]\n    \n    # Get indices of the top k results\n    top_k_indices = np.argsort(predictions)[::-1][:top_k_results]\n    \n    # Handle output based on type\n    output_dtype = output_details[0]['dtype']\n    if output_dtype in [np.int8, np.uint8]:\n        # Dequantize the output\n        scale, zero_point = output_details[0]['quantization']\n        predictions = (predictions.astype(np.float32) - zero_point) * scale\n    \n    if apply_softmax:\n        # Apply softmax\n        exp_preds = np.exp(predictions - np.max(predictions))\n        probabilities = exp_preds / np.sum(exp_preds)\n    else:\n        probabilities = predictions\n    \n    print(\"\\n\\t[PREDICTION]        [Prob]\\n\")\n    for i in range(top_k_results):\n        print(\"\\t{:20}: {:.1f}%\".format(\n            labels[top_k_indices[i]],\n            probabilities[top_k_indices[i]] * 100))\n    print (\"\\n\\tInference time: {:.1f}ms\".format(inference_time))\n\nAnd test it with different images and the int8 quantized model (160x160 alpha =1.0).\n\nLet’s download a smaller model, such as the one trained for the Nicla Vision Lab (int8 quantized model (96x96 alpha = 0.1), as a test. We can use the same function:\n\nThe model lost some accuracy, but it is still OK once our model does not look for many details. Regarding latency, we are around ten times faster on the Raspi-Zero."
  },
  {
    "objectID": "raspi/image_classification/image_classification.html#live-image-classification",
    "href": "raspi/image_classification/image_classification.html#live-image-classification",
    "title": "Image Classification",
    "section": "Live Image Classification",
    "text": "Live Image Classification\nLet’s develop an app to capture images with the USB camera in real time, showing its classification.\nUsing the nano on the terminal, save the code below, such as img_class_live_infer.py.\n\nfrom flask import Flask, Response, render_template_string, request, jsonify\nfrom picamera2 import Picamera2\nimport io\nimport threading\nimport time\nimport numpy as np\nfrom PIL import Image\nimport tflite_runtime.interpreter as tflite\nfrom queue import Queue\n\napp = Flask(__name__)\n\n# Global variables\npicam2 = None\nframe = None\nframe_lock = threading.Lock()\nis_classifying = False\nconfidence_threshold = 0.8\nmodel_path = \"./models/ei-raspi-img-class-int8-quantized-model.tflite\"\nlabels = ['background', 'periquito', 'robot']\ninterpreter = None\nclassification_queue = Queue(maxsize=1)\n\ndef initialize_camera():\n    global picam2\n    picam2 = Picamera2()\n    config = picam2.create_preview_configuration(main={\"size\": (320, 240)})\n    picam2.configure(config)\n    picam2.start()\n    time.sleep(2)  # Wait for camera to warm up\n\ndef get_frame():\n    global frame\n    while True:\n        stream = io.BytesIO()\n        picam2.capture_file(stream, format='jpeg')\n        with frame_lock:\n            frame = stream.getvalue()\n        time.sleep(0.1)  # Capture frames more frequently\n\ndef generate_frames():\n    while True:\n        with frame_lock:\n            if frame is not None:\n                yield (b'--frame\\r\\n'\n                       b'Content-Type: image/jpeg\\r\\n\\r\\n' + frame + b'\\r\\n')\n        time.sleep(0.1)\n\ndef load_model():\n    global interpreter\n    if interpreter is None:\n        interpreter = tflite.Interpreter(model_path=model_path)\n        interpreter.allocate_tensors()\n    return interpreter\n\ndef classify_image(img, interpreter):\n    input_details = interpreter.get_input_details()\n    output_details = interpreter.get_output_details()\n\n    img = img.resize((input_details[0]['shape'][1], \n                      input_details[0]['shape'][2]))\n    input_data = np.expand_dims(np.array(img), axis=0)\\\n                             .astype(input_details[0]['dtype'])\n\n    interpreter.set_tensor(input_details[0]['index'], input_data)\n    interpreter.invoke()\n\n    predictions = interpreter.get_tensor(output_details[0]['index'])[0]\n    # Handle output based on type\n    output_dtype = output_details[0]['dtype']\n    if output_dtype in [np.int8, np.uint8]:\n        # Dequantize the output\n        scale, zero_point = output_details[0]['quantization']\n        predictions = (predictions.astype(np.float32) - zero_point) * scale\n    return predictions\n\ndef classification_worker():\n    interpreter = load_model()\n    while True:\n        if is_classifying:\n            with frame_lock:\n                if frame is not None:\n                    img = Image.open(io.BytesIO(frame))\n            predictions = classify_image(img, interpreter)\n            max_prob = np.max(predictions)\n            if max_prob &gt;= confidence_threshold:\n                label = labels[np.argmax(predictions)]\n            else:\n                label = 'Uncertain'\n            classification_queue.put({'label': label, \n                                      'probability': float(max_prob)})\n        time.sleep(0.1)  # Adjust based on your needs\n\n@app.route('/')\ndef index():\n    return render_template_string('''\n        &lt;!DOCTYPE html&gt;\n        &lt;html&gt;\n        &lt;head&gt;\n            &lt;title&gt;Image Classification&lt;/title&gt;\n            &lt;script \n                src=\"https://code.jquery.com/jquery-3.6.0.min.js\"&gt;\n            &lt;/script&gt;\n            &lt;script&gt;\n                function startClassification() {\n                    $.post('/start');\n                    $('#startBtn').prop('disabled', true);\n                    $('#stopBtn').prop('disabled', false);\n                }\n                function stopClassification() {\n                    $.post('/stop');\n                    $('#startBtn').prop('disabled', false);\n                    $('#stopBtn').prop('disabled', true);\n                }\n                function updateConfidence() {\n                    var confidence = $('#confidence').val();\n                    $.post('/update_confidence', {confidence: confidence});\n                }\n                function updateClassification() {\n                    $.get('/get_classification', function(data) {\n                        $('#classification').text(data.label + ': ' \n                        + data.probability.toFixed(2));\n                    });\n                }\n                $(document).ready(function() {\n                    setInterval(updateClassification, 100);  \n                    // Update every 100ms\n                });\n            &lt;/script&gt;\n        &lt;/head&gt;\n        &lt;body&gt;\n            &lt;h1&gt;Image Classification&lt;/h1&gt;\n            &lt;img src=\"{{ url_for('video_feed') }}\" width=\"640\" height=\"480\" /&gt;\n            &lt;br&gt;\n            &lt;button id=\"startBtn\" onclick=\"startClassification()\"&gt;\n            Start Classification&lt;/button&gt;\n            &lt;button id=\"stopBtn\" onclick=\"stopClassification()\" disabled&gt;\n            Stop Classification&lt;/button&gt;\n            &lt;br&gt;\n            &lt;label for=\"confidence\"&gt;Confidence Threshold:&lt;/label&gt;\n            &lt;input type=\"number\" id=\"confidence\" name=\"confidence\" min=\"0\" \n            max=\"1\" step=\"0.1\" value=\"0.8\" onchange=\"updateConfidence()\"&gt;\n            &lt;br&gt;\n            &lt;div id=\"classification\"&gt;Waiting for classification...&lt;/div&gt;\n        &lt;/body&gt;\n        &lt;/html&gt;\n    ''')\n\n@app.route('/video_feed')\ndef video_feed():\n    return Response(generate_frames(),\n                    mimetype='multipart/x-mixed-replace; boundary=frame')\n\n@app.route('/start', methods=['POST'])\ndef start_classification():\n    global is_classifying\n    is_classifying = True\n    return '', 204\n\n@app.route('/stop', methods=['POST'])\ndef stop_classification():\n    global is_classifying\n    is_classifying = False\n    return '', 204\n\n@app.route('/update_confidence', methods=['POST'])\ndef update_confidence():\n    global confidence_threshold\n    confidence_threshold = float(request.form['confidence'])\n    return '', 204\n\n@app.route('/get_classification')\ndef get_classification():\n    if not is_classifying:\n        return jsonify({'label': 'Not classifying', 'probability': 0})\n    try:\n        result = classification_queue.get_nowait()\n    except Queue.Empty:\n        result = {'label': 'Processing', 'probability': 0}\n    return jsonify(result)\n\nif __name__ == '__main__':\n    initialize_camera()\n    threading.Thread(target=get_frame, daemon=True).start()\n    threading.Thread(target=classification_worker, daemon=True).start()\n    app.run(host='0.0.0.0', port=5000, threaded=True)\n\nOn the terminal, run:\npython3 img_class_live_infer.py\nAnd access the web interface:\n\nOn the Raspberry Pi itself (if you have a GUI): Open a web browser and go to http://localhost:5000\nFrom another device on the same network: Open a web browser and go to http://&lt;raspberry_pi_ip&gt;:5000 (Replace &lt;raspberry_pi_ip&gt; with your Raspberry Pi’s IP address). For example: http://192.168.4.210:5000/\n\nHere are some screenshots of the app running on an external desktop\n\nHere, you can see the app running on the YouTube:\n\nThe code creates a web application for real-time image classification using a Raspberry Pi, its camera module, and a TensorFlow Lite model. The application uses Flask to serve a web interface where is possible to view the camera feed and see live classification results.\n\nKey Components:\n\nFlask Web Application: Serves the user interface and handles requests.\nPiCamera2: Captures images from the Raspberry Pi camera module.\nTensorFlow Lite: Runs the image classification model.\nThreading: Manages concurrent operations for smooth performance.\n\n\n\nMain Features:\n\nLive camera feed display\nReal-time image classification\nAdjustable confidence threshold\nStart/Stop classification on demand\n\n\n\nCode Structure:\n\nImports and Setup:\n\nFlask for web application\nPiCamera2 for camera control\nTensorFlow Lite for inference\nThreading and Queue for concurrent operations\n\nGlobal Variables:\n\nCamera and frame management\nClassification control\nModel and label information\n\nCamera Functions:\n\ninitialize_camera(): Sets up the PiCamera2\nget_frame(): Continuously captures frames\ngenerate_frames(): Yields frames for the web feed\n\nModel Functions:\n\nload_model(): Loads the TFLite model\nclassify_image(): Performs inference on a single image\n\nClassification Worker:\n\nRuns in a separate thread\nContinuously classifies frames when active\nUpdates a queue with the latest results\n\nFlask Routes:\n\n/: Serves the main HTML page\n/video_feed: Streams the camera feed\n/start and /stop: Controls classification\n/update_confidence: Adjusts the confidence threshold\n/get_classification: Returns the latest classification result\n\nHTML Template:\n\nDisplays camera feed and classification results\nProvides controls for starting/stopping and adjusting settings\n\nMain Execution:\n\nInitializes camera and starts necessary threads\nRuns the Flask application\n\n\n\n\nKey Concepts:\n\nConcurrent Operations: Using threads to handle camera capture and classification separately from the web server.\nReal-time Updates: Frequent updates to the classification results without page reloads.\nModel Reuse: Loading the TFLite model once and reusing it for efficiency.\nFlexible Configuration: Allowing users to adjust the confidence threshold on the fly.\n\n\n\nUsage:\n\nEnsure all dependencies are installed.\nRun the script on a Raspberry Pi with a camera module.\nAccess the web interface from a browser using the Raspberry Pi’s IP address.\nStart classification and adjust settings as needed."
  },
  {
    "objectID": "raspi/image_classification/image_classification.html#conclusion",
    "href": "raspi/image_classification/image_classification.html#conclusion",
    "title": "Image Classification",
    "section": "Conclusion:",
    "text": "Conclusion:\nImage classification has emerged as a powerful and versatile application of machine learning, with significant implications for various fields, from healthcare to environmental monitoring. This chapter has demonstrated how to implement a robust image classification system on edge devices like the Raspi-Zero and Raspi-5, showcasing the potential for real-time, on-device intelligence.\nWe’ve explored the entire pipeline of an image classification project, from data collection and model training using Edge Impulse Studio to deploying and running inferences on a Raspi. The process highlighted several key points:\n\nThe importance of proper data collection and preprocessing for training effective models.\nThe power of transfer learning, allowing us to leverage pre-trained models like MobileNet V2 for efficient training with limited data.\nThe trade-offs between model accuracy and inference speed, especially crucial for edge devices.\nThe implementation of real-time classification using a web-based interface, demonstrating practical applications.\n\nThe ability to run these models on edge devices like the Raspi opens up numerous possibilities for IoT applications, autonomous systems, and real-time monitoring solutions. It allows for reduced latency, improved privacy, and operation in environments with limited connectivity.\nAs we’ve seen, even with the computational constraints of edge devices, it’s possible to achieve impressive results in terms of both accuracy and speed. The flexibility to adjust model parameters, such as input size and alpha values, allows for fine-tuning to meet specific project requirements.\nLooking forward, the field of edge AI and image classification continues to evolve rapidly. Advances in model compression techniques, hardware acceleration, and more efficient neural network architectures promise to further expand the capabilities of edge devices in computer vision tasks.\nThis project serves as a foundation for more complex computer vision applications and encourages further exploration into the exciting world of edge AI and IoT. Whether it’s for industrial automation, smart home applications, or environmental monitoring, the skills and concepts covered here provide a solid starting point for a wide range of innovative projects."
  },
  {
    "objectID": "raspi/image_classification/image_classification.html#resources",
    "href": "raspi/image_classification/image_classification.html#resources",
    "title": "Image Classification",
    "section": "Resources",
    "text": "Resources\n\nDataset Example\nSetup Test Notebook on a Raspi\nImage Classification Notebook on a Raspi\nCNN to classify Cifar-10 dataset at CoLab\nCifar 10 - Image Classification on a Raspi\nPython Scripts\nEdge Impulse Project"
  },
  {
    "objectID": "raspi/object_detection/object_detection.html#introduction",
    "href": "raspi/object_detection/object_detection.html#introduction",
    "title": "Object Detection",
    "section": "Introduction",
    "text": "Introduction\nBuilding upon our exploration of image classification, we now turn our attention to a more advanced computer vision task: object detection. While image classification assigns a single label to an entire image, object detection goes further by identifying and locating multiple objects within a single image. This capability opens up many new applications and challenges, particularly in edge computing and IoT devices like the Raspberry Pi.\nObject detection combines the tasks of classification and localization. It not only determines what objects are present in an image but also pinpoints their locations by, for example, drawing bounding boxes around them. This added complexity makes object detection a more powerful tool for understanding visual scenes, but it also requires more sophisticated models and training techniques.\nIn edge AI, where we work with constrained computational resources, implementing efficient object detection models becomes crucial. The challenges we faced with image classification—balancing model size, inference speed, and accuracy—are amplified in object detection. However, the rewards are also more significant, as object detection enables more nuanced and detailed visual data analysis.\nSome applications of object detection on edge devices include:\n\nSurveillance and security systems\nAutonomous vehicles and drones\nIndustrial quality control\nWildlife monitoring\nAugmented reality applications\n\nAs we put our hands into object detection, we’ll build upon the concepts and techniques we explored in image classification. We’ll examine popular object detection architectures designed for efficiency, such as:\n\nSingle Stage Detectors, such as MobileNet and EfficientDet,\nFOMO (Faster Objects, More Objects), and\nYOLO (You Only Look Once).\n\n\nTo learn more about object detection models, follow the tutorial A Gentle Introduction to Object Recognition With Deep Learning.\n\nWe will explore those object detection models using\n\nTensorFlow Lite Runtime (now changed to LiteRT),\nEdge Impulse Linux Python SDK and\nUltralitics\n\n\nThroughout this lab, we’ll cover the fundamentals of object detection and how it differs from image classification. We’ll also learn how to train, fine-tune, test, optimize, and deploy popular object detection architectures using a dataset created from scratch.\n\nObject Detection Fundamentals\nObject detection builds upon the foundations of image classification but extends its capabilities significantly. To understand object detection, it’s crucial first to recognize its key differences from image classification:\n\nImage Classification vs. Object Detection\nImage Classification:\n\nAssigns a single label to an entire image\nAnswers the question: “What is this image’s primary object or scene?”\nOutputs a single class prediction for the whole image\n\nObject Detection:\n\nIdentifies and locates multiple objects within an image\nAnswers the questions: “What objects are in this image, and where are they located?”\nOutputs multiple predictions, each consisting of a class label and a bounding box\n\nTo visualize this difference, let’s consider an example: \nThis diagram illustrates the critical difference: image classification provides a single label for the entire image, while object detection identifies multiple objects, their classes, and their locations within the image.\n\n\nKey Components of Object Detection\nObject detection systems typically consist of two main components:\n\nObject Localization: This component identifies where objects are located in the image. It typically outputs bounding boxes, rectangular regions encompassing each detected object.\nObject Classification: This component determines the class or category of each detected object, similar to image classification but applied to each localized region.\n\n\n\nChallenges in Object Detection\nObject detection presents several challenges beyond those of image classification:\n\nMultiple objects: An image may contain multiple objects of various classes, sizes, and positions.\nVarying scales: Objects can appear at different sizes within the image.\nOcclusion: Objects may be partially hidden or overlapping.\nBackground clutter: Distinguishing objects from complex backgrounds can be challenging.\nReal-time performance: Many applications require fast inference times, especially on edge devices.\n\n\n\nApproaches to Object Detection\nThere are two main approaches to object detection:\n\nTwo-stage detectors: These first propose regions of interest and then classify each region. Examples include R-CNN and its variants (Fast R-CNN, Faster R-CNN).\nSingle-stage detectors: These predict bounding boxes (or centroids) and class probabilities in one forward pass of the network. Examples include YOLO (You Only Look Once), EfficientDet, SSD (Single Shot Detector), and FOMO (Faster Objects, More Objects). These are often faster and more suitable for edge devices like Raspberry Pi.\n\n\n\nEvaluation Metrics\nObject detection uses different metrics compared to image classification:\n\nIntersection over Union (IoU): Measures the overlap between predicted and ground truth bounding boxes.\nMean Average Precision (mAP): Combines precision and recall across all classes and IoU thresholds.\nFrames Per Second (FPS): Measures detection speed, crucial for real-time applications on edge devices."
  },
  {
    "objectID": "raspi/object_detection/object_detection.html#pre-trained-object-detection-models-overview",
    "href": "raspi/object_detection/object_detection.html#pre-trained-object-detection-models-overview",
    "title": "Object Detection",
    "section": "Pre-Trained Object Detection Models Overview",
    "text": "Pre-Trained Object Detection Models Overview\nAs we saw in the introduction, given an image or a video stream, an object detection model can identify which of a known set of objects might be present and provide information about their positions within the image.\n\nYou can test some common models online by visiting Object Detection - MediaPipe Studio\n\nOn Kaggle, we can find the most common pre-trained tflite models to use with the Raspi, ssd_mobilenet_v1, and efficiendet. Those models were trained on the COCO (Common Objects in Context) dataset, with over 200,000 labeled images in 91 categories. Go, download the models, and upload them to the ./models folder in the Raspi.\n\nAlternatively, you can find the models and the COCO labels on GitHub.\n\nFor the first part of this lab, we will focus on a pre-trained 300x300 SSD-Mobilenet V1 model and compare it with the 320x320 EfficientDet-lite0, also trained using the COCO 2017 dataset. Both models were converted to a TensorFlow Lite format (4.2MB for the SSD Mobilenet and 4.6MB for the EfficienDet).\n\nSSD-Mobilenet V2 or V3 is recommended for transfer learning projects, but once the V1 TFLite model is publicly available, we will use it for this overview.\n\n\n\nSetting Up the TFLite Environment\nWe should confirm the steps done on the last Hands-On Lab, Image Classification, as follows:\n\nUpdating the Raspberry Pi\nInstalling Required Libraries\nSetting up a Virtual Environment (Optional but Recommended)\n\nsource ~/tflite/bin/activate\n\nInstalling TensorFlow Lite Runtime\nInstalling Additional Python Libraries (inside the environment)\n\n\n\nCreating a Working Directory:\nConsidering that we have created the Documents/TFLITE folder in the last Lab, let’s now create the specific folders for this object detection lab:\ncd Documents/TFLITE/\nmkdir OBJ_DETECT\ncd OBJ_DETECT\nmkdir images\nmkdir models\ncd models\n\n\nInference and Post-Processing\nLet’s start a new notebook to follow all the steps to detect objects on an image:\nImport the needed libraries:\nimport time\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom PIL import Image\nimport tflite_runtime.interpreter as tflite\nLoad the TFLite model and allocate tensors:\nmodel_path = \"./models/ssd-mobilenet-v1-tflite-default-v1.tflite\"\ninterpreter = tflite.Interpreter(model_path=model_path)\ninterpreter.allocate_tensors()\nGet input and output tensors.\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\nInput details will inform us how the model should be fed with an image. The shape of (1, 300, 300, 3) with a dtype of uint8 tells us that a non-normalized (pixel value range from 0 to 255) image with dimensions (300x300x3) should be input one by one (Batch Dimension: 1).\nThe output details include not only the labels (“classes”) and probabilities (“scores”) but also the relative window position of the bounding boxes (“boxes”) about where the object is located on the image and the number of detected objects (“num_detections”). The output details also tell us that the model can detect a maximum of 10 objects in the image.\n\nSo, for the above example, using the same cat image used with the Image Classification Lab looking for the output, we have a 76% probability of having found an object with a class ID of 16 on an area delimited by a bounding box of [0.028011084, 0.020121813, 0.9886069, 0.802299]. Those four numbers are related to ymin, xmin, ymax and xmax, the box coordinates.\nTaking into consideration that y goes from the top (ymin) to the bottom (ymax) and x goes from left (xmin) to the right (xmax), we have, in fact, the coordinates of the top/left corner and the bottom/right one. With both edges and knowing the shape of the picture, it is possible to draw a rectangle around the object, as shown in the figure below:\n\nNext, we should find what class ID equal to 16 means. Opening the file coco_labels.txt, as a list, each element has an associated index, and inspecting index 16, we get, as expected, cat. The probability is the value returning from the score.\nLet’s now upload some images with multiple objects on it for testing.\nimg_path = \"./images/cat_dog.jpeg\"\norig_img = Image.open(img_path)\n\n# Display the image\nplt.figure(figsize=(8, 8))\nplt.imshow(orig_img)\nplt.title(\"Original Image\")\nplt.show()\n\nBased on the input details, let’s pre-process the image, changing its shape and expanding its dimension:\nimg = orig_img.resize((input_details[0]['shape'][1], \n                  input_details[0]['shape'][2]))\ninput_data = np.expand_dims(img, axis=0)\ninput_data.shape, input_data.dtype \nThe new input_data shape is(1, 300, 300, 3) with a dtype of uint8, which is compatible with what the model expects.\nUsing the input_data, let’s run the interpreter, measure the latency, and get the output:\nstart_time = time.time()\ninterpreter.set_tensor(input_details[0]['index'], input_data)\ninterpreter.invoke()\nend_time = time.time()\ninference_time = (end_time - start_time) * 1000  # Convert to milliseconds\nprint (\"Inference time: {:.1f}ms\".format(inference_time))\nWith a latency of around 800ms, we can get 4 distinct outputs:\nboxes = interpreter.get_tensor(output_details[0]['index'])[0] \nclasses = interpreter.get_tensor(output_details[1]['index'])[0]  \nscores = interpreter.get_tensor(output_details[2]['index'])[0]   \nnum_detections = int(interpreter.get_tensor(output_details[3]['index'])[0])\nOn a quick inspection, we can see that the model detected 2 objects with a score over 0.5:\nfor i in range(num_detections):\n    if scores[i] &gt; 0.5:  # Confidence threshold\n        print(f\"Object {i}:\")\n        print(f\"  Bounding Box: {boxes[i]}\")\n        print(f\"  Confidence: {scores[i]}\")\n        print(f\"  Class: {classes[i]}\")\n\nAnd we can also visualize the results:\nplt.figure(figsize=(12, 8))\nplt.imshow(orig_img)\nfor i in range(num_detections):\n    if scores[i] &gt; 0.5:  # Adjust threshold as needed\n        ymin, xmin, ymax, xmax = boxes[i]\n        (left, right, top, bottom) = (xmin * orig_img.width, \n                                      xmax * orig_img.width, \n                                      ymin * orig_img.height, \n                                      ymax * orig_img.height)\n        rect = plt.Rectangle((left, top), right-left, bottom-top, \n                             fill=False, color='red', linewidth=2)\n        plt.gca().add_patch(rect)\n        class_id = int(classes[i])\n        class_name = labels[class_id]\n        plt.text(left, top-10, f'{class_name}: {scores[i]:.2f}', \n                 color='red', fontsize=12, backgroundcolor='white')\n\n\n\nEfficientDet\nEfficientDet is not technically an SSD (Single Shot Detector) model, but it shares some similarities and builds upon ideas from SSD and other object detection architectures:\n\nEfficientDet:\n\nDeveloped by Google researchers in 2019\nUses EfficientNet as the backbone network\nEmploys a novel bi-directional feature pyramid network (BiFPN)\nIt uses compound scaling to scale the backbone network and the object detection components efficiently.\n\nSimilarities to SSD:\n\nBoth are single-stage detectors, meaning they perform object localization and classification in a single forward pass.\nBoth use multi-scale feature maps to detect objects at different scales.\n\nKey differences:\n\nBackbone: SSD typically uses VGG or MobileNet, while EfficientDet uses EfficientNet.\nFeature fusion: SSD uses a simple feature pyramid, while EfficientDet uses the more advanced BiFPN.\nScaling method: EfficientDet introduces compound scaling for all components of the network\n\nAdvantages of EfficientDet:\n\nGenerally achieves better accuracy-efficiency trade-offs than SSD and many other object detection models.\nMore flexible scaling allows for a family of models with different size-performance trade-offs.\n\n\nWhile EfficientDet is not an SSD model, it can be seen as an evolution of single-stage detection architectures, incorporating more advanced techniques to improve efficiency and accuracy. When using EfficientDet, we can expect similar output structures to SSD (e.g., bounding boxes and class scores).\n\nOn GitHub, you can find another notebook exploring the EfficientDet model that we did with SSD MobileNet."
  },
  {
    "objectID": "raspi/object_detection/object_detection.html#object-detection-project",
    "href": "raspi/object_detection/object_detection.html#object-detection-project",
    "title": "Object Detection",
    "section": "Object Detection Project",
    "text": "Object Detection Project\nNow, we will develop a complete Image Classification project from data collection to training and deployment. As we did with the Image Classification project, the trained and converted model will be used for inference.\nWe will use the same dataset to train 3 models: SSD-MobileNet V2, FOMO, and YOLO.\n\nThe Goal\nAll Machine Learning projects need to start with a goal. Let’s assume we are in an industrial facility and must sort and count wheels and special boxes.\n\nIn other words, we should perform a multi-label classification, where each image can have three classes:\n\nBackground (no objects)\nBox\nWheel\n\n\n\nRaw Data Collection\nOnce we have defined our Machine Learning project goal, the next and most crucial step is collecting the dataset. We can use a phone, the Raspi, or a mix to create the raw dataset (with no labels). Let’s use the simple web app on our Raspberry Pi to view the QVGA (320 x 240) captured images in a browser.\nFrom GitHub, get the Python script get_img_data.py and open it in the terminal:\npython3 get_img_data.py \nAccess the web interface:\n\nOn the Raspberry Pi itself (if you have a GUI): Open a web browser and go to http://localhost:5000\nFrom another device on the same network: Open a web browser and go to http://&lt;raspberry_pi_ip&gt;:5000 (Replace &lt;raspberry_pi_ip&gt; with your Raspberry Pi’s IP address). For example: http://192.168.4.210:5000/\n\nThe Python script creates a web-based interface for capturing and organizing image datasets using a Raspberry Pi and its camera. It’s handy for machine learning projects that require labeled image data or not, as in our case here.\nAccess the web interface from a browser, enter a generic label for the images you want to capture, and press Start Capture.\n\n\nNote that the images to be captured will have multiple labels that should be defined later.\n\nUse the live preview to position the camera and click Capture Image to save images under the current label (in this case, box-wheel.\n\nWhen we have enough images, we can press Stop Capture. The captured images are saved on the folder dataset/box-wheel:\n\n\nGet around 60 images. Try to capture different angles, backgrounds, and light conditions. Filezilla can transfer the created raw dataset to your main computer.\n\n\n\nLabeling Data\nThe next step in an Object Detect project is to create a labeled dataset. We should label the raw dataset images, creating bounding boxes around each picture’s objects (box and wheel). We can use labeling tools like LabelImg, CVAT, Roboflow, or even the Edge Impulse Studio. Once we have explored the Edge Impulse tool in other labs, let’s use Roboflow here.\n\nWe are using Roboflow (free version) here for two main reasons. 1) We can have auto-labeler, and 2) The annotated dataset is available in several formats and can be used both on Edge Impulse Studio (we will use it for MobileNet V2 and FOMO train) and on CoLab (YOLOv8 train), for example. Having the annotated dataset on Edge Impulse (Free account), it is not possible to use it for training on other platforms.\n\nWe should upload the raw dataset to Roboflow. Create a free account there and start a new project, for example, (“box-versus-wheel”).\n\n\nWe will not enter in deep details about the Roboflow process once many tutorials are available.\n\n\nAnnotate\nOnce the project is created and the dataset is uploaded, you should make the annotations using the “Auto-Label” Tool. Note that you can also upload images with only a background, which should be saved w/o any annotations.\n\nOnce all images are annotated, you should split them into training, validation, and testing.\n\n\n\nData Pre-Processing\nThe last step with the dataset is preprocessing to generate a final version for training. Let’s resize all images to 320x320 and generate augmented versions of each image (augmentation) to create new training examples from which our model can learn.\nFor augmentation, we will rotate the images (+/-15o), crop, and vary the brightness and exposure.\n\nAt the end of the process, we will have 153 images.\n\nNow, you should export the annotated dataset in a format that Edge Impulse, Ultralitics, and other frameworks/tools understand, for example, YOLOv8. Let’s download a zipped version of the dataset to our desktop.\n\nHere, it is possible to review how the dataset was structured\n\nThere are 3 separate folders, one for each split (train/test/valid). For each of them, there are 2 subfolders, images, and labels. The pictures are stored as image_id.jpg and images_id.txt, where “image_id” is unique for every picture.\nThe labels file format will be class_id bounding box coordinates, where in our case, class_id will be 0 for box and 1 for wheel. The numerical id (o, 1, 2…) will follow the alphabetical order of the class name.\nThe data.yaml file has info about the dataset as the classes’ names (names: ['box', 'wheel']) following the YOLO format.\nAnd that’s it! We are ready to start training using the Edge Impulse Studio (as we will do in the following step), Ultralytics (as we will when discussing YOLO), or even training from scratch on CoLab (as we did with the Cifar-10 dataset on the Image Classification lab).\n\nThe pre-processed dataset can be found at the Roboflow site, or here:"
  },
  {
    "objectID": "raspi/object_detection/object_detection.html#training-an-ssd-mobilenet-model-on-edge-impulse-studio",
    "href": "raspi/object_detection/object_detection.html#training-an-ssd-mobilenet-model-on-edge-impulse-studio",
    "title": "Object Detection",
    "section": "Training an SSD MobileNet Model on Edge Impulse Studio",
    "text": "Training an SSD MobileNet Model on Edge Impulse Studio\nGo to Edge Impulse Studio, enter your credentials at Login (or create an account), and start a new project.\n\nHere, you can clone the project developed for this hands-on lab: Raspi - Object Detection.\n\nOn the Project Dashboard tab, go down and on Project info, and for Labeling method select Bounding boxes (object detection)\n\nUploading the annotated data\nOn Studio, go to the Data acquisition tab, and on the UPLOAD DATA section, upload from your computer the raw dataset.\nWe can use the option Select a folder, choosing, for example, the folder train in your computer, which contains two sub-folders, images, and labels. Select the Image label format, “YOLO TXT”, upload into the caegory Training, and press Upload data.\n\nRepeat the process for the test data (upload both folders, test, and validation). At the end of the upload process, you should end with the annotated dataset of 153 images split in the train/test (84%/16%).\n\nNote that labels will be stored at the labels files 0 and 1 , which are equivalent to box and wheel.\n\n\n\n\nThe Impulse Design\nThe first thing to define when we enter the Create impulse step is to describe the target device for deployment. A pop-up window will appear. We will select Raspberry 4, an intermediary device between the Raspi-Zero and the Raspi-5.\n\nThis choice will not interfere with the training; it will only give us an idea about the latency of the model on that specific target.\n\n\nIn this phase, you should define how to:\n\nPre-processing consists of resizing the individual images. In our case, the images were pre-processed on Roboflow, to 320x320 , so let’s keep it. The resize will not matter here because the images are already squared. If you upload a rectangular image, squash it (squared form, without cropping). Afterward, you could define if the images are converted from RGB to Grayscale or not.\nDesign a Model, in this case, “Object Detection.”\n\n\n\n\nPreprocessing all dataset\nIn the section Image, select Color depth as RGB, and press Save parameters.\n\nThe Studio moves automatically to the next section, Generate features, where all samples will be pre-processed, resulting in 480 objects: 207 boxes and 273 wheels.\n\nThe feature explorer shows that all samples evidence a good separation after the feature generation.\n\n\nModel Design, Training, and Test\nFor training, we should select a pre-trained model. Let’s use the MobileNetV2 SSD FPN-Lite (320x320 only) . It is a pre-trained object detection model designed to locate up to 10 objects within an image, outputting a bounding box for each object detected. The model is around 3.7MB in size. It supports an RGB input at 320x320px.\nRegarding the training hyper-parameters, the model will be trained with:\n\nEpochs: 25\nBatch size: 32\nLearning Rate: 0.15.\n\nFor validation during training, 20% of the dataset (validation_dataset) will be spared.\n\nAs a result, the model ends with an overall precision score (based on COCO mAP) of 88.8%, higher than the result when using the test data (83.3%).\n\n\nDeploying the model\nWe have two ways to deploy our model:\n\nTFLite model, which lets deploy the trained model as .tflite for the Raspi to run it using Python.\nLinux (AARCH64), a binary for Linux (AARCH64), implements the Edge Impulse Linux protocol, which lets us run our models on any Linux-based development board, with SDKs for Python, for example. See the documentation for more information and setup instructions.\n\nLet’s deploy the TFLite model. On the Dashboard tab, go to Transfer learning model (int8 quantized) and click on the download icon:\n\nTransfer the model from your computer to the Raspi folder./models and capture or get some images for inference and save them in the folder ./images.\n\n\nInference and Post-Processing\nThe inference can be made as discussed in the Pre-Trained Object Detection Models Overview. Let’s start a new notebook to follow all the steps to detect cubes and wheels on an image.\nImport the needed libraries:\nimport time\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as patches\nfrom PIL import Image\nimport tflite_runtime.interpreter as tflite\nDefine the model path and labels:\nmodel_path = \"./models/ei-raspi-object-detection-SSD-MobileNetv2-320x0320-\\\nint8.lite\"\nlabels = ['box', 'wheel']\n\nRemember that the model will output the class ID as values (0 and 1), following an alphabetic order regarding the class names.\n\nLoad the model, allocate the tensors, and get the input and output tensor details:\n# Load the TFLite model\ninterpreter = tflite.Interpreter(model_path=model_path)\ninterpreter.allocate_tensors()\n\n# Get input and output tensors\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\nOne crucial difference to note is that the dtype of the input details of the model is now int8, which means that the input values go from -128 to +127, while each pixel of our raw image goes from 0 to 256. This means that we should pre-process the image to match it. We can check here:\ninput_dtype = input_details[0]['dtype']\ninput_dtype\nnumpy.int8\nSo, let’s open the image and show it:\n# Load the image\nimg_path = \"./images/box_2_wheel_2.jpg\"\norig_img = Image.open(img_path)\n\n# Display the image\nplt.figure(figsize=(6, 6))\nplt.imshow(orig_img)\nplt.title(\"Original Image\")\nplt.show()\n\nAnd perform the pre-processing:\nscale, zero_point = input_details[0]['quantization']\nimg = orig_img.resize((input_details[0]['shape'][1], \n                  input_details[0]['shape'][2]))\nimg_array = np.array(img, dtype=np.float32) / 255.0\nimg_array = (img_array / scale + zero_point).clip(-128, 127).astype(np.int8)\ninput_data = np.expand_dims(img_array, axis=0)\nChecking the input data, we can verify that the input tensor is compatible with what is expected by the model:\ninput_data.shape, input_data.dtype\n((1, 320, 320, 3), dtype('int8'))\nNow, it is time to perform the inference. Let’s also calculate the latency of the model:\n# Inference on Raspi-Zero\nstart_time = time.time()\ninterpreter.set_tensor(input_details[0]['index'], input_data)\ninterpreter.invoke()\nend_time = time.time()\ninference_time = (end_time - start_time) * 1000  # Convert to milliseconds\nprint (\"Inference time: {:.1f}ms\".format(inference_time))\nThe model will take around 600ms to perform the inference in the Raspi-Zero, which is around 5 times longer than a Raspi-5.\nNow, we can get the output classes of objects detected, its bounding boxes coordinates, and probabilities.\nboxes = interpreter.get_tensor(output_details[1]['index'])[0]  \nclasses = interpreter.get_tensor(output_details[3]['index'])[0]  \nscores = interpreter.get_tensor(output_details[0]['index'])[0]        \nnum_detections = int(interpreter.get_tensor(output_details[2]['index'])[0])\nfor i in range(num_detections):\n    if scores[i] &gt; 0.5:  # Confidence threshold\n        print(f\"Object {i}:\")\n        print(f\"  Bounding Box: {boxes[i]}\")\n        print(f\"  Confidence: {scores[i]}\")\n        print(f\"  Class: {classes[i]}\")\n\nFrom the results, we can see that 4 objects were detected: two with class ID 0 (box)and two with class ID 1 (wheel), what is correct!\nLet’s visualize the result for a threshold of 0.5\nthreshold = 0.5\nplt.figure(figsize=(6,6))\nplt.imshow(orig_img)\nfor i in range(num_detections):\n    if scores[i] &gt; threshold:  \n        ymin, xmin, ymax, xmax = boxes[i]\n        (left, right, top, bottom) = (xmin * orig_img.width, \n                                      xmax * orig_img.width, \n                                      ymin * orig_img.height, \n                                      ymax * orig_img.height)\n        rect = plt.Rectangle((left, top), right-left, bottom-top, \n                             fill=False, color='red', linewidth=2)\n        plt.gca().add_patch(rect)\n        class_id = int(classes[i])\n        class_name = labels[class_id]\n        plt.text(left, top-10, f'{class_name}: {scores[i]:.2f}', \n                 color='red', fontsize=12, backgroundcolor='white')\n\nBut what happens if we reduce the threshold to 0.3, for example?\n\nWe start to see false positives and multiple detections, where the model detects the same object multiple times with different confidence levels and slightly different bounding boxes.\nCommonly, sometimes, we need to adjust the threshold to smaller values to capture all objects, avoiding false negatives, which would lead to multiple detections.\nTo improve the detection results, we should implement Non-Maximum Suppression (NMS), which helps eliminate overlapping bounding boxes and keeps only the most confident detection.\nFor that, let’s create a general function named non_max_suppression(), with the role of refining object detection results by eliminating redundant and overlapping bounding boxes. It achieves this by iteratively selecting the detection with the highest confidence score and removing other significantly overlapping detections based on an Intersection over Union (IoU) threshold.\ndef non_max_suppression(boxes, scores, threshold):\n    # Convert to corner coordinates\n    x1 = boxes[:, 0]\n    y1 = boxes[:, 1]\n    x2 = boxes[:, 2]\n    y2 = boxes[:, 3]\n\n    areas = (x2 - x1 + 1) * (y2 - y1 + 1)\n    order = scores.argsort()[::-1]\n\n    keep = []\n    while order.size &gt; 0:\n        i = order[0]\n        keep.append(i)\n        xx1 = np.maximum(x1[i], x1[order[1:]])\n        yy1 = np.maximum(y1[i], y1[order[1:]])\n        xx2 = np.minimum(x2[i], x2[order[1:]])\n        yy2 = np.minimum(y2[i], y2[order[1:]])\n\n        w = np.maximum(0.0, xx2 - xx1 + 1)\n        h = np.maximum(0.0, yy2 - yy1 + 1)\n        inter = w * h\n        ovr = inter / (areas[i] + areas[order[1:]] - inter)\n\n        inds = np.where(ovr &lt;= threshold)[0]\n        order = order[inds + 1]\n\n    return keep\nHow it works:\n\nSorting: It starts by sorting all detections by their confidence scores, highest to lowest.\nSelection: It selects the highest-scoring box and adds it to the final list of detections.\nComparison: This selected box is compared with all remaining lower-scoring boxes.\nElimination: Any box that overlaps significantly (above the IoU threshold) with the selected box is eliminated.\nIteration: This process repeats with the next highest-scoring box until all boxes are processed.\n\nNow, we can define a more precise visualization function that will take into consideration an IoU threshold, detecting only the objects that were selected by the non_max_suppression function:\ndef visualize_detections(image, boxes, classes, scores, \n                         labels, threshold, iou_threshold):\n    if isinstance(image, Image.Image):\n        image_np = np.array(image)\n    else:\n        image_np = image\n\n    height, width = image_np.shape[:2]\n    \n    # Convert normalized coordinates to pixel coordinates\n    boxes_pixel = boxes * np.array([height, width, height, width])\n    \n    # Apply NMS\n    keep = non_max_suppression(boxes_pixel, scores, iou_threshold)\n    \n    # Set the figure size to 12x8 inches\n    fig, ax = plt.subplots(1, figsize=(12, 8))\n\n    ax.imshow(image_np)\n    \n    for i in keep:\n        if scores[i] &gt; threshold:\n            ymin, xmin, ymax, xmax = boxes[i]\n            rect = patches.Rectangle((xmin * width, ymin * height),\n                                     (xmax - xmin) * width,\n                                     (ymax - ymin) * height,\n                                     linewidth=2, edgecolor='r', facecolor='none')\n            ax.add_patch(rect)\n            class_name = labels[int(classes[i])]\n            ax.text(xmin * width, ymin * height - 10,\n                    f'{class_name}: {scores[i]:.2f}', color='red',\n                    fontsize=12, backgroundcolor='white')\n\n    plt.show()\nNow we can create a function that will call the others, performing inference on any image:\ndef detect_objects(img_path, conf=0.5, iou=0.5):\n    orig_img = Image.open(img_path)\n    scale, zero_point = input_details[0]['quantization']\n    img = orig_img.resize((input_details[0]['shape'][1], \n                      input_details[0]['shape'][2]))\n    img_array = np.array(img, dtype=np.float32) / 255.0\n    img_array = (img_array / scale + zero_point).clip(-128, 127).\\\n    astype(np.int8)\n    input_data = np.expand_dims(img_array, axis=0)\n    \n    # Inference on Raspi-Zero\n    start_time = time.time()\n    interpreter.set_tensor(input_details[0]['index'], input_data)\n    interpreter.invoke()\n    end_time = time.time()\n    inference_time = (end_time - start_time) * 1000  # Convert to ms\n    print (\"Inference time: {:.1f}ms\".format(inference_time))\n    \n    # Extract the outputs\n    boxes = interpreter.get_tensor(output_details[1]['index'])[0]  \n    classes = interpreter.get_tensor(output_details[3]['index'])[0]  \n    scores = interpreter.get_tensor(output_details[0]['index'])[0]        \n    num_detections = int(interpreter.get_tensor(output_details[2]['index'])[0])\n\n    visualize_detections(orig_img, boxes, classes, scores, labels, \n                         threshold=conf, \n                         iou_threshold=iou)\nNow, running the code, having the same image again with a confidence threshold of 0.3, but with a small IoU:\nimg_path = \"./images/box_2_wheel_2.jpg\"\ndetect_objects(img_path, conf=0.3,iou=0.05)"
  },
  {
    "objectID": "raspi/object_detection/object_detection.html#training-a-fomo-model-at-edge-impulse-studio",
    "href": "raspi/object_detection/object_detection.html#training-a-fomo-model-at-edge-impulse-studio",
    "title": "Object Detection",
    "section": "Training a FOMO Model at Edge Impulse Studio",
    "text": "Training a FOMO Model at Edge Impulse Studio\nThe inference with the SSD MobileNet model worked well, but the latency was significantly high. The inference varied from 0.5 to 1.3 seconds on a Raspi-Zero, which means around or less than 1 FPS (1 frame per second). One alternative to speed up the process is to use FOMO (Faster Objects, More Objects).\nThis novel machine learning algorithm lets us count multiple objects and find their location in an image in real-time using up to 30x less processing power and memory than MobileNet SSD or YOLO. The main reason this is possible is that while other models calculate the object’s size by drawing a square around it (bounding box), FOMO ignores the size of the image, providing only the information about where the object is located in the image through its centroid coordinates.\n\nHow FOMO works?\nIn a typical object detection pipeline, the first stage is extracting features from the input image. FOMO leverages MobileNetV2 to perform this task. MobileNetV2 processes the input image to produce a feature map that captures essential characteristics, such as textures, shapes, and object edges, in a computationally efficient way.\n\nOnce these features are extracted, FOMO’s simpler architecture, focused on center-point detection, interprets the feature map to determine where objects are located in the image. The output is a grid of cells, where each cell represents whether or not an object center is detected. The model outputs one or more confidence scores for each cell, indicating the likelihood of an object being present.\nLet’s see how it works on an image.\nFOMO divides the image into blocks of pixels using a factor of 8. For the input of 96x96, the grid would be 12x12 (96/8=12). For a 160x160, the grid will be 20x20, and so on. Next, FOMO will run a classifier through each pixel block to calculate the probability that there is a box or a wheel in each of them and, subsequently, determine the regions that have the highest probability of containing the object (If a pixel block has no objects, it will be classified as background). From the overlap of the final region, the FOMO provides the coordinates (related to the image dimensions) of the centroid of this region.\n\nTrade-off Between Speed and Precision:\n\nGrid Resolution: FOMO uses a grid of fixed resolution, meaning each cell can detect if an object is present in that part of the image. While it doesn’t provide high localization accuracy, it makes a trade-off by being fast and computationally light, which is crucial for edge devices.\nMulti-Object Detection: Since each cell is independent, FOMO can detect multiple objects simultaneously in an image by identifying multiple centers.\n\n\n\nImpulse Design, new Training and Testing\nReturn to Edge Impulse Studio, and in the Experiments tab, create another impulse. Now, the input images should be 160x160 (this is the expected input size for MobilenetV2).\n\nOn the Image tab, generate the features and go to the Object detection tab.\nWe should select a pre-trained model for training. Let’s use the FOMO (Faster Objects, More Objects) MobileNetV2 0.35.\n\nRegarding the training hyper-parameters, the model will be trained with:\n\nEpochs: 30\nBatch size: 32\nLearning Rate: 0.001.\n\nFor validation during training, 20% of the dataset (validation_dataset) will be spared. We will not apply Data Augmentation for the remaining 80% (train_dataset) because our dataset was already augmented during the labeling phase at Roboflow.\nAs a result, the model ends with an overall F1 score of 93.3% with an impressive latency of 8ms (Raspi-4), around 60X less than we got with the SSD MovileNetV2.\n\n\nNote that FOMO automatically added a third label background to the two previously defined boxes (0) and wheels (1).\n\nOn the Model testing tab, we can see that the accuracy was 94%. Here is one of the test sample results:\n\n\nIn object detection tasks, accuracy is generally not the primary evaluation metric. Object detection involves classifying objects and providing bounding boxes around them, making it a more complex problem than simple classification. The issue is that we do not have the bounding box, only the centroids. In short, using accuracy as a metric could be misleading and may not provide a complete understanding of how well the model is performing.\n\n\n\nDeploying the model\nAs we did in the previous section, we can deploy the trained model as TFLite or Linux (AARCH64). Let’s do it now as Linux (AARCH64), a binary that implements the Edge Impulse Linux protocol.\nEdge Impulse for Linux models is delivered in .eim format. This executable contains our “full impulse” created in Edge Impulse Studio. The impulse consists of the signal processing block(s) and any learning and anomaly block(s) we added and trained. It is compiled with optimizations for our processor or GPU (e.g., NEON instructions on ARM cores), plus a straightforward IPC layer (over a Unix socket).\nAt the Deploy tab, select the option Linux (AARCH64), the int8model and press Build.\n\nThe model will be automatically downloaded to your computer.\nOn our Raspi, let’s create a new working area:\ncd ~\ncd Documents\nmkdir EI_Linux\ncd EI_Linux\nmkdir models\nmkdir images\nRename the model for easy identification:\nFor example, raspi-object-detection-linux-aarch64-FOMO-int8.eim and transfer it to the new Raspi folder./models and capture or get some images for inference and save them in the folder ./images.\n\n\nInference and Post-Processing\nThe inference will be made using the Linux Python SDK. This library lets us run machine learning models and collect sensor data on Linux machines using Python. The SDK is open source and hosted on GitHub: edgeimpulse/linux-sdk-python.\nLet’s set up a Virtual Environment for working with the Linux Python SDK\npython3 -m venv ~/eilinux\nsource ~/eilinux/bin/activate\nAnd Install the all the libraries needed:\nsudo apt-get update\nsudo apt-get install libatlas-base-dev libportaudio0 libportaudio2\nsudo apt-get installlibportaudiocpp0 portaudio19-dev\n\npip3 install edge_impulse_linux -i https://pypi.python.org/simple\npip3 install Pillow matplotlib pyaudio opencv-contrib-python\n\nsudo apt-get install portaudio19-dev\npip3 install pyaudio \npip3 install opencv-contrib-python\nPermit our model to be executable.\nchmod +x raspi-object-detection-linux-aarch64-FOMO-int8.eim\nInstall the Jupiter Notebook on the new environment\npip3 install jupyter\nRun a notebook locally (on the Raspi-4 or 5 with desktop)\njupyter notebook\nor on the browser on your computer:\njupyter notebook --ip=192.168.4.210 --no-browser\nLet’s start a new notebook by following all the steps to detect cubes and wheels on an image using the FOMO model and the Edge Impulse Linux Python SDK.\nImport the needed libraries:\nimport sys, time\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as patches\nfrom PIL import Image\nimport cv2\nfrom edge_impulse_linux.image import ImageImpulseRunner\nDefine the model path and labels:\nmodel_file = \"raspi-object-detection-linux-aarch64-int8.eim\"\nmodel_path = \"models/\"+ model_file # Trained ML model from Edge Impulse\nlabels = ['box', 'wheel']\n\nRemember that the model will output the class ID as values (0 and 1), following an alphabetic order regarding the class names.\n\nLoad and initialize the model:\n# Load the model file\nrunner = ImageImpulseRunner(model_path)\n\n# Initialize model\nmodel_info = runner.init()\nThe model_info will contain critical information about our model. However, unlike the TFLite interpreter, the EI Linux Python SDK library will now prepare the model for inference.\nSo, let’s open the image and show it (Now, for compatibility, we will use OpenCV, the CV Library used internally by EI. OpenCV reads the image as BGR, so we will need to convert it to RGB :\n# Load the image\nimg_path = \"./images/1_box_1_wheel.jpg\"\norig_img = cv2.imread(img_path)\nimg_rgb = cv2.cvtColor(orig_img, cv2.COLOR_BGR2RGB)\n\n# Display the image\nplt.imshow(img_rgb)\nplt.title(\"Original Image\")\nplt.show()\n\nNow we will get the features and the preprocessed image (cropped) using the runner:\nfeatures, cropped = runner.get_features_from_image_auto_studio_setings(img_rgb)\nAnd perform the inference. Let’s also calculate the latency of the model:\nres = runner.classify(features)\nLet’s get the output classes of objects detected, their bounding boxes centroids, and probabilities.\nprint('Found %d bounding boxes (%d ms.)' % (\n  len(res[\"result\"][\"bounding_boxes\"]), \n  res['timing']['dsp'] + res['timing']['classification']))\nfor bb in res[\"result\"][\"bounding_boxes\"]:\n    print('\\t%s (%.2f): x=%d y=%d w=%d h=%d' % (\n      bb['label'], bb['value'], bb['x'], \n      bb['y'], bb['width'], bb['height']))\nFound 2 bounding boxes (29 ms.)\n    1 (0.91): x=112 y=40 w=16 h=16\n    0 (0.75): x=48 y=56 w=8 h=8\nThe results show that two objects were detected: one with class ID 0 (box) and one with class ID 1 (wheel), which is correct!\nLet’s visualize the result (The threshold is 0.5, the default value set during the model testing on the Edge Impulse Studio).\nprint('\\tFound %d bounding boxes (latency: %d ms)' % (\n  len(res[\"result\"][\"bounding_boxes\"]), \n  res['timing']['dsp'] + res['timing']['classification']))\nplt.figure(figsize=(5,5))\nplt.imshow(cropped)\n\n# Go through each of the returned bounding boxes\nbboxes = res['result']['bounding_boxes']\nfor bbox in bboxes:\n\n    # Get the corners of the bounding box\n    left = bbox['x']\n    top = bbox['y']\n    width = bbox['width']\n    height = bbox['height']\n    \n    # Draw a circle centered on the detection\n    circ = plt.Circle((left+width//2, top+height//2), 5, \n                     fill=False, color='red', linewidth=3)\n    plt.gca().add_patch(circ)\n    class_id = int(bbox['label'])\n    class_name = labels[class_id]\n    plt.text(left, top-10, f'{class_name}: {bbox[\"value\"]:.2f}', \n              color='red', fontsize=12, backgroundcolor='white')\nplt.show()"
  },
  {
    "objectID": "raspi/object_detection/object_detection.html#exploring-a-yolo-model-using-ultralitics",
    "href": "raspi/object_detection/object_detection.html#exploring-a-yolo-model-using-ultralitics",
    "title": "Object Detection",
    "section": "Exploring a YOLO Model using Ultralitics",
    "text": "Exploring a YOLO Model using Ultralitics\nFor this lab, we will explore YOLOv8. Ultralytics YOLOv8 is a version of the acclaimed real-time object detection and image segmentation model, YOLO. YOLOv8 is built on cutting-edge advancements in deep learning and computer vision, offering unparalleled performance in terms of speed and accuracy. Its streamlined design makes it suitable for various applications and easily adaptable to different hardware platforms, from edge devices to cloud APIs.\n\nTalking about the YOLO Model\nThe YOLO (You Only Look Once) model is a highly efficient and widely used object detection algorithm known for its real-time processing capabilities. Unlike traditional object detection systems that repurpose classifiers or localizers to perform detection, YOLO frames the detection problem as a single regression task. This innovative approach enables YOLO to simultaneously predict multiple bounding boxes and their class probabilities from full images in one evaluation, significantly boosting its speed.\n\nKey Features:\n\nSingle Network Architecture:\n\nYOLO employs a single neural network to process the entire image. This network divides the image into a grid and, for each grid cell, directly predicts bounding boxes and associated class probabilities. This end-to-end training improves speed and simplifies the model architecture.\n\nReal-Time Processing:\n\nOne of YOLO’s standout features is its ability to perform object detection in real-time. Depending on the version and hardware, YOLO can process images at high frames per second (FPS). This makes it ideal for applications requiring quick and accurate object detection, such as video surveillance, autonomous driving, and live sports analysis.\n\nEvolution of Versions:\n\nOver the years, YOLO has undergone significant improvements, from YOLOv1 to the latest YOLOv10. Each iteration has introduced enhancements in accuracy, speed, and efficiency. YOLOv8, for instance, incorporates advancements in network architecture, improved training methodologies, and better support for various hardware, ensuring a more robust performance.\nAlthough YOLOv10 is the family’s newest member with an encouraging performance based on its paper, it was just released (May 2024) and is not fully integrated with the Ultralitycs library. Conversely, the precision-recall curve analysis suggests that YOLOv8 generally outperforms YOLOv9, capturing a higher proportion of true positives while minimizing false positives more effectively (for more details, see this article). So, this lab is based on the YOLOv8n.\n\n\nAccuracy and Efficiency:\n\nWhile early versions of YOLO traded off some accuracy for speed, recent versions have made substantial strides in balancing both. The newer models are faster and more accurate, detecting small objects (such as bees) and performing well on complex datasets.\n\nWide Range of Applications:\n\nYOLO’s versatility has led to its adoption in numerous fields. It is used in traffic monitoring systems to detect and count vehicles, security applications to identify potential threats and agricultural technology to monitor crops and livestock. Its application extends to any domain requiring efficient and accurate object detection.\n\nCommunity and Development:\n\nYOLO continues to evolve and is supported by a strong community of developers and researchers (being the YOLOv8 very strong). Open-source implementations and extensive documentation have made it accessible for customization and integration into various projects. Popular deep learning frameworks like Darknet, TensorFlow, and PyTorch support YOLO, further broadening its applicability.\nUltralitics YOLOv8 can not only Detect (our case here) but also Segment and Pose models pre-trained on the COCO dataset and YOLOv8 Classify models pre-trained on the ImageNet dataset. Track mode is available for all Detect, Segment, and Pose models.\n\n\n\n\n\n\nInstallation\nOn our Raspi, let’s deactivate the current environment to create a new working area:\ndeactivate\ncd ~\ncd Documents/\nmkdir YOLO\ncd YOLO\nmkdir models\nmkdir images\nLet’s set up a Virtual Environment for working with the Ultralytics YOLOv8\npython3 -m venv ~/yolo\nsource ~/yolo/bin/activate\nAnd install the Ultralytics packages for local inference on the Raspi\n\nUpdate the packages list, install pip, and upgrade to the latest:\n\nsudo apt update\nsudo apt install python3-pip -y\npip install -U pip\n\nInstall the ultralytics pip package with optional dependencies:\n\npip install ultralytics[export]\n\nReboot the device:\n\nsudo reboot\n\n\nTesting the YOLO\nAfter the Raspi-Zero booting, let’s activate the yolo env, go to the working directory,\nsource ~/yolo/bin/activate\ncd /Documents/YOLO\nand run inference on an image that will be downloaded from the Ultralytics website, using the YOLOV8n model (the smallest in the family) at the Terminal (CLI):\nyolo predict model='yolov8n' source='https://ultralytics.com/images/bus.jpg'\n\nThe YOLO model family is pre-trained with the COCO dataset.\n\nThe inference result will appear in the terminal. In the image (bus.jpg), 4 persons, 1 bus, and 1 stop signal were detected:\n\nAlso, we got a message that Results saved to runs/detect/predict. Inspecting that directory, we can see a new image saved (bus.jpg). Let’s download it from the Raspi-Zero to our desktop for inspection:\n\nSo, the Ultrayitics YOLO is correctly installed on our Raspi. But, on the Raspi-Zero, an issue is the high latency for this inference, around 18 seconds, even with the most miniature model of the family (YOLOv8n).\n\n\nExport Model to NCNN format\nDeploying computer vision models on edge devices with limited computational power, such as the Raspi-Zero, can cause latency issues. One alternative is to use a format optimized for optimal performance. This ensures that even devices with limited processing power can handle advanced computer vision tasks well.\nOf all the model export formats supported by Ultralytics, the NCNN is a high-performance neural network inference computing framework optimized for mobile platforms. From the beginning of the design, NCNN was deeply considerate about deployment and use on mobile phones and did not have third-party dependencies. It is cross-platform and runs faster than all known open-source frameworks (such as TFLite).\nNCNN delivers the best inference performance when working with Raspberry Pi devices. NCNN is highly optimized for mobile embedded platforms (such as ARM architecture).\nSo, let’s convert our model and rerun the inference:\n\nExport a YOLOv8n PyTorch model to NCNN format, creating: ‘/yolov8n_ncnn_model’\n\nyolo export model=yolov8n.pt format=ncnn \n\nRun inference with the exported model (now the source could be the bus.jpg image that was downloaded from the website to the current directory on the last inference):\n\nyolo predict model='./yolov8n_ncnn_model' source='bus.jpg'\n\nThe first inference, when the model is loaded, usually has a high latency (around 17s), but from the 2nd, it is possible to note that the inference goes down to around 2s.\n\n\n\nExploring YOLO with Python\nTo start, let’s call the Python Interpreter so we can explore how the YOLO model works, line by line:\npython3\nNow, we should call the YOLO library from Ultralitics and load the model:\nfrom ultralytics import YOLO\nmodel = YOLO('yolov8n_ncnn_model')\nNext, run inference over an image (let’s use again bus.jpg):\nimg = 'bus.jpg'\nresult = model.predict(img, save=True, imgsz=640, conf=0.5, iou=0.3)\n\nWe can verify that the result is almost identical to the one we get running the inference at the terminal level (CLI), except that the bus stop was not detected with the reduced NCNN model. Note that the latency was reduced.\nLet’s analyze the “result” content.\nFor example, we can see result[0].boxes.data, showing us the main inference result, which is a tensor shape (4, 6). Each line is one of the objects detected, being the 4 first columns, the bounding boxes coordinates, the 5th, the confidence, and the 6th, the class (in this case, 0: person and 5: bus):\n\nWe can access several inference results separately, as the inference time, and have it printed in a better format:\ninference_time = int(result[0].speed['inference'])\nprint(f\"Inference Time: {inference_time} ms\")\nOr we can have the total number of objects detected:\nprint(f'Number of objects: {len (result[0].boxes.cls)}')\n\nWith Python, we can create a detailed output that meets our needs (See Model Prediction with Ultralytics YOLO for more details). Let’s run a Python script instead of manually entering it line by line in the interpreter, as shown below. Let’s use nano as our text editor. First, we should create an empty Python script named, for example, yolov8_tests.py:\nnano yolov8_tests.py\nEnter with the code lines:\nfrom ultralytics import YOLO\n\n# Load the YOLOv8 model\nmodel = YOLO('yolov8n_ncnn_model')\n\n# Run inference\nimg = 'bus.jpg'\nresult = model.predict(img, save=False, imgsz=640, conf=0.5, iou=0.3)\n\n# print the results\ninference_time = int(result[0].speed['inference'])\nprint(f\"Inference Time: {inference_time} ms\")\nprint(f'Number of objects: {len (result[0].boxes.cls)}')\n\nAnd enter with the commands: [CTRL+O] + [ENTER] +[CTRL+X] to save the Python script.\nRun the script:\npython yolov8_tests.py\nThe result is the same as running the inference at the terminal level (CLI) and with the built-in Python interpreter.\n\nCalling the YOLO library and loading the model for inference for the first time takes a long time, but the inferences after that will be much faster. For example, the first single inference can take several seconds, but after that, the inference time should be reduced to less than 1 second.\n\n\n\nTraining YOLOv8 on a Customized Dataset\nReturn to our “Boxe versus Wheel” dataset, labeled on Roboflow. On the Download Dataset, instead of Download a zip to computer option done for training on Edge Impulse Studio, we will opt for Show download code. This option will open a pop-up window with a code snippet that should be pasted into our training notebook.\n\nFor training, let’s adapt one of the public examples available from Ultralitytics and run it on Google Colab. Below, you can find mine to be adapted in your project:\n\nYOLOv8 Box versus Wheel Dataset Training [Open In Colab]\n\n\nCritical points on the Notebook:\n\nRun it with GPU (the NVidia T4 is free)\nInstall Ultralytics using PIP.\n\nNow, you can import the YOLO and upload your dataset to the CoLab, pasting the Download code that we get from Roboflow. Note that our dataset will be mounted under /content/datasets/:\n\n\n\nIt is essential to verify and change the file data.yaml with the correct path for the images (copy the path on each images folder).\n\nnames:\n- box\n- wheel\nnc: 2\nroboflow:\n  license: CC BY 4.0\n  project: box-versus-wheel-auto-dataset\n  url: https://universe.roboflow.com/marcelo-rovai-riila/box-versus-wheel-auto-dataset/dataset/5\n  version: 5\n  workspace: marcelo-rovai-riila\ntest: /content/datasets/Box-versus-Wheel-auto-dataset-5/test/images\ntrain: /content/datasets/Box-versus-Wheel-auto-dataset-5/train/images\nval: /content/datasets/Box-versus-Wheel-auto-dataset-5/valid/images\n\nDefine the main hyperparameters that you want to change from default, for example:\nMODEL = 'yolov8n.pt'\nIMG_SIZE = 640\nEPOCHS = 25 # For a final project, you should consider at least 100 epochs \nRun the training (using CLI):\n!yolo task=detect mode=train model={MODEL} data={dataset.location}/data.yaml epochs={EPOCHS} imgsz={IMG_SIZE} plots=True \n\n\n​ The model took a few minutes to be trained and has an excellent result (mAP50 of 0.995). At the end of the training, all results are saved in the folder listed, for example: /runs/detect/train/. There, you can find, for example, the confusion matrix.\n\n\nNote that the trained model (best.pt) is saved in the folder /runs/detect/train/weights/. Now, you should validate the trained model with the valid/images.\n\n!yolo task=detect mode=val model={HOME}/runs/detect/train/weights/best.pt data={dataset.location}/data.yaml\n​ The results were similar to training.\n\nNow, we should perform inference on the images left aside for testing\n\n!yolo task=detect mode=predict model={HOME}/runs/detect/train/weights/best.pt conf=0.25 source={dataset.location}/test/images save=True\nThe inference results are saved in the folder runs/detect/predict. Let’s see some of them:\n\n\nIt is advised to export the train, validation, and test results for a Drive at Google. To do so, we should mount the drive.\nfrom google.colab import drive\ndrive.mount('/content/gdrive')\nand copy the content of /runs folder to a folder that you should create in your Drive, for example:\n!scp -r /content/runs '/content/gdrive/MyDrive/10_UNIFEI/Box_vs_Wheel_Project'\n\n\n\n\nInference with the trained model, using the Raspi\nDownload the trained model /runs/detect/train/weights/best.pt to your computer. Using the FileZilla FTP, let’s transfer the best.pt to the Raspi models folder (before the transfer, you may change the model name, for example, box_wheel_320_yolo.pt).\nUsing the FileZilla FTP, let’s transfer a few images from the test dataset to .\\YOLO\\images:\nLet’s return to the YOLO folder and use the Python Interpreter:\ncd ..\npython\nAs before, we will import the YOLO library and define our converted model to detect bees:\nfrom ultralytics import YOLO\nmodel = YOLO('./models/box_wheel_320_yolo.pt')\nNow, let’s define an image and call the inference (we will save the image result this time to external verification):\nimg = './images/1_box_1_wheel.jpg'\nresult = model.predict(img, save=True, imgsz=320, conf=0.5, iou=0.3)\nLet’s repeat for several images. The inference result is saved on the variable result, and the processed image on runs/detect/predict8\n\nUsing FileZilla FTP, we can send the inference result to our Desktop for verification:\n\nWe can see that the inference result is excellent! The model was trained based on the smaller base model of the YOLOv8 family (YOLOv8n). The issue is the latency, around 1 second (or 1 FPS on the Raspi-Zero). Of course, we can reduce this latency and convert the model to TFLite or NCNN."
  },
  {
    "objectID": "raspi/object_detection/object_detection.html#object-detection-on-a-live-stream",
    "href": "raspi/object_detection/object_detection.html#object-detection-on-a-live-stream",
    "title": "Object Detection",
    "section": "Object Detection on a live stream",
    "text": "Object Detection on a live stream\nAll the models explored in this lab can detect objects in real-time using a camera. The captured image should be the input for the trained and converted model. For the Raspi-4 or 5 with a desktop, OpenCV can capture the frames and display the inference result.\nHowever, creating a live stream with a webcam to detect objects in real-time is also possible. For example, let’s start with the script developed for the Image Classification app and adapt it for a Real-Time Object Detection Web Application Using TensorFlow Lite and Flask.\nThis app version will work for all TFLite models. Verify if the model is in its correct folder, for example:\nmodel_path = \"./models/ssd-mobilenet-v1-tflite-default-v1.tflite\"\nDownload the Python script object_detection_app.py from GitHub.\nAnd on the terminal, run:\npython3 object_detection_app.py\nAnd access the web interface:\n\nOn the Raspberry Pi itself (if you have a GUI): Open a web browser and go to http://localhost:5000\nFrom another device on the same network: Open a web browser and go to http://&lt;raspberry_pi_ip&gt;:5000 (Replace &lt;raspberry_pi_ip&gt; with your Raspberry Pi’s IP address). For example: http://192.168.4.210:5000/\n\nHere are some screenshots of the app running on an external desktop\n\nLet’s see a technical description of the key modules used in the object detection application:\n\nTensorFlow Lite (tflite_runtime):\n\nPurpose: Efficient inference of machine learning models on edge devices.\nWhy: TFLite offers reduced model size and optimized performance compared to full TensorFlow, which is crucial for resource-constrained devices like Raspberry Pi. It supports hardware acceleration and quantization, further improving efficiency.\nKey functions: Interpreter for loading and running the model,get_input_details(), and get_output_details() for interfacing with the model.\n\nFlask:\n\nPurpose: Lightweight web framework for creating the backend server.\nWhy: Flask’s simplicity and flexibility make it ideal for rapidly developing and deploying web applications. It’s less resource-intensive than larger frameworks suitable for edge devices.\nKey components: route decorators for defining API endpoints, Response objects for streaming video, render_template_string for serving dynamic HTML.\n\nPicamera2:\n\nPurpose: Interface with the Raspberry Pi camera module.\nWhy: Picamera2 is the latest library for controlling Raspberry Pi cameras, offering improved performance and features over the original Picamera library.\nKey functions: create_preview_configuration() for setting up the camera, capture_file() for capturing frames.\n\nPIL (Python Imaging Library):\n\nPurpose: Image processing and manipulation.\nWhy: PIL provides a wide range of image processing capabilities. It’s used here to resize images, draw bounding boxes, and convert between image formats.\nKey classes: Image for loading and manipulating images, ImageDraw for drawing shapes and text on images.\n\nNumPy:\n\nPurpose: Efficient array operations and numerical computing.\nWhy: NumPy’s array operations are much faster than pure Python lists, which is crucial for efficiently processing image data and model inputs/outputs.\nKey functions: array() for creating arrays, expand_dims() for adding dimensions to arrays.\n\nThreading:\n\nPurpose: Concurrent execution of tasks.\nWhy: Threading allows simultaneous frame capture, object detection, and web server operation, crucial for maintaining real-time performance.\nKey components: Thread class creates separate execution threads, and Lock is used for thread synchronization.\n\nio.BytesIO:\n\nPurpose: In-memory binary streams.\nWhy: Allows efficient handling of image data in memory without needing temporary files, improving speed and reducing I/O operations.\n\ntime:\n\nPurpose: Time-related functions.\nWhy: Used for adding delays (time.sleep()) to control frame rate and for performance measurements.\n\njQuery (client-side):\n\nPurpose: Simplified DOM manipulation and AJAX requests.\nWhy: It makes it easy to update the web interface dynamically and communicate with the server without page reloads.\nKey functions: .get() and .post() for AJAX requests, DOM manipulation methods for updating the UI.\n\n\nRegarding the main app system architecture:\n\nMain Thread: Runs the Flask server, handling HTTP requests and serving the web interface.\nCamera Thread: Continuously captures frames from the camera.\nDetection Thread: Processes frames through the TFLite model for object detection.\nFrame Buffer: Shared memory space (protected by locks) storing the latest frame and detection results.\n\nAnd the app data flow, we can describe in short:\n\nCamera captures frame → Frame Buffer\nDetection thread reads from Frame Buffer → Processes through TFLite model → Updates detection results in Frame Buffer\nFlask routes access Frame Buffer to serve the latest frame and detection results\nWeb client receives updates via AJAX and updates UI\n\nThis architecture allows for efficient, real-time object detection while maintaining a responsive web interface running on a resource-constrained edge device like a Raspberry Pi. Threading and efficient libraries like TFLite and PIL enable the system to process video frames in real-time, while Flask and jQuery provide a user-friendly way to interact with them.\nYou can test the app with another pre-processed model, such as the EfficientDet, changing the app line:\nmodel_path = \"./models/lite-model_efficientdet_lite0_detection_metadata_1.tflite\"\n\nIf we want to use the app for the SSD-MobileNetV2 model, trained on Edge Impulse Studio with the “Box versus Wheel” dataset, the code should also be adapted depending on the input details, as we have explored on its notebook."
  },
  {
    "objectID": "raspi/object_detection/object_detection.html#conclusion",
    "href": "raspi/object_detection/object_detection.html#conclusion",
    "title": "Object Detection",
    "section": "Conclusion",
    "text": "Conclusion\nThis lab has explored the implementation of object detection on edge devices like the Raspberry Pi, demonstrating the power and potential of running advanced computer vision tasks on resource-constrained hardware. We’ve covered several vital aspects:\n\nModel Comparison: We examined different object detection models, including SSD-MobileNet, EfficientDet, FOMO, and YOLO, comparing their performance and trade-offs on edge devices.\nTraining and Deployment: Using a custom dataset of boxes and wheels (labeled on Roboflow), we walked through the process of training models using Edge Impulse Studio and Ultralytics and deploying them on Raspberry Pi.\nOptimization Techniques: To improve inference speed on edge devices, we explored various optimization methods, such as model quantization (TFLite int8) and format conversion (e.g., to NCNN).\nReal-time Applications: The lab exemplified a real-time object detection web application, demonstrating how these models can be integrated into practical, interactive systems.\nPerformance Considerations: Throughout the lab, we discussed the balance between model accuracy and inference speed, a critical consideration for edge AI applications.\n\nThe ability to perform object detection on edge devices opens up numerous possibilities across various domains, from precision agriculture, industrial automation, and quality control to smart home applications and environmental monitoring. By processing data locally, these systems can offer reduced latency, improved privacy, and operation in environments with limited connectivity.\nLooking ahead, potential areas for further exploration include: - Implementing multi-model pipelines for more complex tasks - Exploring hardware acceleration options for Raspberry Pi - Integrating object detection with other sensors for more comprehensive edge AI systems - Developing edge-to-cloud solutions that leverage both local processing and cloud resources\nObject detection on edge devices can create intelligent, responsive systems that bring the power of AI directly into the physical world, opening up new frontiers in how we interact with and understand our environment."
  },
  {
    "objectID": "raspi/object_detection/object_detection.html#resources",
    "href": "raspi/object_detection/object_detection.html#resources",
    "title": "Object Detection",
    "section": "Resources",
    "text": "Resources\n\nDataset (“Box versus Wheel”)\nSSD-MobileNet Notebook on a Raspi\nEfficientDet Notebook on a Raspi\nFOMO - EI Linux Notebook on a Raspi\nYOLOv8 Box versus Wheel Dataset Training on CoLab\nEdge Impulse Project - SSD MobileNet and FOMO\nPython Scripts\nModels"
  },
  {
    "objectID": "raspi/counting_objects_yolo/counting_objects_yolo.html#introduction",
    "href": "raspi/counting_objects_yolo/counting_objects_yolo.html#introduction",
    "title": "Counting objects with YOLO",
    "section": "Introduction",
    "text": "Introduction\nAt the Federal University of Itajuba in Brazil, with the master’s student José Anderson Reis and Professor José Alberto Ferreira Filho, we are exploring a project that delves into the intersection of technology and nature. This tutorial will review our first steps and share our observations on deploying YOLOv8, a cutting-edge machine learning model, on the compact and efficient Raspberry Pi Zero 2W (Raspi-Zero). We aim to estimate the number of bees entering and exiting their hive—a task crucial for beekeeping and ecological studies.\nWhy is this important? Bee populations are vital indicators of environmental health, and their monitoring can provide essential data for ecological research and conservation efforts. However, manual counting is labor-intensive and prone to errors. By leveraging the power of embedded machine learning, or tinyML, we automate this process, enhancing accuracy and efficiency.\n\n\n\nimg\n\n\nThis tutorial will cover setting up the Raspberry Pi, integrating a camera module, optimizing and deploying YOLOv8 for real-time image processing, and analyzing the data gathered."
  },
  {
    "objectID": "raspi/counting_objects_yolo/counting_objects_yolo.html#installing-and-using-ultralytics-yolov8",
    "href": "raspi/counting_objects_yolo/counting_objects_yolo.html#installing-and-using-ultralytics-yolov8",
    "title": "Counting objects with YOLO",
    "section": "Installing and using Ultralytics YOLOv8",
    "text": "Installing and using Ultralytics YOLOv8\nUltralytics YOLOv8, is a version of the acclaimed real-time object detection and image segmentation model, YOLO. YOLOv8 is built on cutting-edge advancements in deep learning and computer vision, offering unparalleled performance in terms of speed and accuracy. Its streamlined design makes it suitable for various applications and easily adaptable to different hardware platforms, from edge devices to cloud APIs.\nLet’s start installing the Ultarlytics packages for local inference on the Rasp-Zero:\n\nUpdate the packages list, install pip, and upgrade to the latest:\n\nsudo apt update\nsudo apt install python3-pip -y\npip install -U pip\n\nInstall the ultralytics pip package with optional dependencies:\n\npip install ultralytics[export]\n\nReboot the device:\n\nsudo reboot\n\nTesting the YOLO\nAfter the Rasp-Zero booting, let’s create a directory for working with YOLO and change the current location to it::\nmkdir Documents/YOLO\ncd Documents/YOLO\nLet’s run inference on an image that will be downloaded from the Ultralytics website, using the YOLOV8n model (the smallest in the family) at the Terminal (CLI):\nyolo predict model='yolov8n' source='https://ultralytics.com/images/bus.jpg'\nThe inference result will appear in the terminal. In the image (bus.jpg), 4 persons, 1 bus, and 1 stop signal were detected:\n\nAlso, we got a message that Results saved to runs/detect/predict4. Inspecting that directory, we can see a new image saved (bus.jpg). Let’s download it from the Rasp-Zero to our desktop for inspection:\n\nSo, the Ultrayitics YOLO is correctly installed on our Rasp-Zero.\n\n\nExport Model to NCNN format\nSo, let’s convert our model and rerun the inference:\n\nExport a YOLOv8n PyTorch model to NCNN format, creating: ‘/yolov8n_ncnn_model’\n\nyolo export model=yolov8n.pt format=ncnn \n\nRun inference with the exported model (now the source could be the bus.jpg image that was downloaded from the website to the current directory on the last inference):\n\nyolo predict model='./yolov8n_ncnn_model' source='bus.jpg'\nNow, we can see that the latency was reduced by half.\n\n\n\nExploring YOLO with Python\nTo start, let’s call the Python Interpreter so we can explore how the YOLO model works, line by line:\npython3\n\nNow, we should call the YOLO library from Ultralitics and load the model:\nfrom ultralytics import YOLO\nmodel = YOLO('yolov8n_ncnn_model')\nNext, run inference over an image (let’s use again bus.jpg):\nimg = 'bus.jpg'\nresult = model.predict(img, save=True, imgsz=640, conf=0.5, iou=0.3)\n\nWe can verify that the result is the same as the one we get running the inference at the terminal level (CLI).\nimage 1/1 /home/mjrovai/Documents/YOLO/bus.jpg: 640x640 3 persons, 1 bus, 4048.5ms\nSpeed: 635.7ms preprocess, 4048.5ms inference, 33897.6ms postprocess per image at shape (1, 3, 640, 640)\n\nResults saved to runs/detect/predict7 \nBut, we are interested in analyzing the “result” content.\nFor example, we can see result[0].boxes.data, showing us the main inference result, which is a tensor shape (4, 6). Each line is one of the objects detected, being the 4 first columns, the bounding boxes coordinates, the 5th, the confidence, and the 6th, the class (in this case, 0: person and 5: bus):\n\nWe can access several inference results separately, as the inference time, and have it printed in a better format:\ninference_time = int(result[0].speed['inference'])\nprint(f\"Inference Time: {inference_time} ms\")\nOr we can have the total number of objects detected:\nprint(f'Number of objects: {len (result[0].boxes.cls)}')\n\nWith Python, we can create a detailed output that meets our needs. In our final project, we will run a Python script at once rather than manually entering it line by line in the interpreter.\nFor that, let’s use nano as our text editor. First, we should create an empty Python script named, for example, yolov8_tests.py:\nnano yolov8_tests.py\nEnter with the code lines:\nfrom ultralytics import YOLO\n\n# Load the YOLOv8 model\nmodel = YOLO('yolov8n_ncnn_model')\n\n# Run inference\nimg = 'bus.jpg'\nresult = model.predict(img, save=False, imgsz=640, conf=0.5, iou=0.3)\n\n# print the results\ninference_time = int(result[0].speed['inference'])\nprint(f\"Inference Time: {inference_time} ms\")\nprint(f'Number of objects: {len (result[0].boxes.cls)}')\n\nAnd enter with the commands: [CTRL+O] + [ENTER] +[CTRL+X] to save the Python script.\nRun the script:\npython yolov8_tests.py\n\nWe can verify again that the result is precisely the same as when we run the inference at the terminal level (CLI) and with the built-in Python interpreter.\n\nNote about the Latency:\nThe process of calling the YOLO library and loading the model for inference for the first time takes a long time, but the inferences after that will be much faster. For example, the first single inference took 3 to 4 seconds, but after that, the inference time is reduced to less than 1 second."
  },
  {
    "objectID": "raspi/counting_objects_yolo/counting_objects_yolo.html#estimating-the-number-of-bees",
    "href": "raspi/counting_objects_yolo/counting_objects_yolo.html#estimating-the-number-of-bees",
    "title": "Counting objects with YOLO",
    "section": "Estimating the number of Bees",
    "text": "Estimating the number of Bees\nFor our project at the university, we are preparing to collect a dataset of bees at the entrance of a beehive using the same camera connected to the Rasp-Zero. The images should be collected every 10 seconds. With the Arducam OV5647, the horizontal Field of View (FoV) is 53.5o, which means that a camera positioned at the top of a standard Hive (46 cm) will capture all of its entrance (about 47 cm).\n\n\nDataset\nThe dataset collection is the most critical phase of the project and should take several weeks or months. For this tutorial, we will use a public dataset: “Sledevic, Tomyslav (2023), “[Labeled dataset for bee detection and direction estimation on beehive landing boards,” Mendeley Data, V5, doi: 10.17632/8gb9r2yhfc.5”\nThe original dataset has 6,762 images (1920 x 1080), and around 8% of them (518) have no bees (only background). This is very important with Object Detection, where we should keep around 10% of the dataset with only background (without any objects to be detected).\nThe images contain from zero to up to 61 bees:\n\nWe downloaded the dataset (images and annotations) and uploaded it to Roboflow. There, you should create a free account and start a new project, for example, (“Bees_on_Hive_landing_boards”):\n\n\nWe will not enter details about the Roboflow process once many tutorials are available.\n\nOnce the project is created and the dataset is uploaded, you should review the annotations using the “Auto-Label” Tool. Note that all images with only a background should be saved w/o any annotations. At this step, you can also add additional images.\n\nOnce all images are annotated, you should split them into training, validation, and testing.\n\n\n\nPre-Processing\nThe last step with the dataset is preprocessing to generate a final version for training. The Yolov8 model can be trained with 640 x 640 pixels (RGB) images. Let’s resize all images and generate augmented versions of each image (augmentation) to create new training examples from which our model can learn.\nFor augmentation, we will rotate the images (+/-15o) and vary the brightness and exposure.\n\nThis will create a final dataset of 16,228 images.\n\nNow, you should export the annotateddataset in a YOLOv8 format. You can download a zipped version of the dataset to your desktop or get a downloaded code to be used with a Jupyter Notebook:\n\nAnd that is it! We are prepared to start our training using Google Colab.\n\nThe pre-processed dataset can be found at the Roboflow site.\n\n\n\nTraining YOLOv8 on a Customized Dataset\nFor training, let’s adapt one of the public examples available from Ultralitytics and run it on Google Colab:\n\nyolov8_bees_on_hive_landing_board.ipynb [Open In Colab]\n\n\nCritical points on the Notebook:\n\nRun it with GPU (the NVidia T4 is free)\nInstall Ultralytics using PIP.\n\nNow, you can import the YOLO and upload your dataset to the CoLab, pasting the Download code that you get from Roboflow. Note that your dataset will be mounted under /content/datasets/:\n\n\n\nIt is important to verify and change, if needed, the file data.yaml with the correct path for the images:\n\nnames:\n- bee\nnc: 1\nroboflow:\n  license: CC BY 4.0\n  project: bees_on_hive_landing_boards\n  url: https://universe.roboflow.com/marcelo-rovai-riila/bees_on_hive_landing_boards/dataset/1\n  version: 1\n  workspace: marcelo-rovai-riila\ntest: /content/datasets/Bees_on_Hive_landing_boards-1test/images\ntrain: /content/datasets/Bees_on_Hive_landing_boards-1/train/images\nval: /content/datasets/Bees_on_Hive_landing_boards-1/valid/images\n\nDefine the main hyperparameters that you want to change from default, for example:\nMODEL = 'yolov8n.pt'\nIMG_SIZE = 640\nEPOCHS = 25 # For a final project, you should consider at least 100 epochs \nRun the training (using CLI):\n!yolo task=detect mode=train model={MODEL} data={dataset.location}/data.yaml epochs={EPOCHS} imgsz={IMG_SIZE} plots=True \n\n\n​ The model took 2.7 hours to train and has an excellent result (mAP50 of 0.984). At the end of the training, all results are saved in the folder listed, for example: /runs/detect/train3/. There, you can find, for example, the confusion matrix and the metrics curves per epoch.\n\n\nNote that the trained model (best.pt) is saved in the folder /runs/detect/train3/weights/. Now, you should validade the trained model with the valid/images.\n\n!yolo task=detect mode=val model={HOME}/runs/detect/train3/weights/best.pt data={dataset.location}/data.yaml\n​ The results were similar to training.\n\nNow, we should perform inference on the images left aside for testing\n\n!yolo task=detect mode=predict model={HOME}/runs/detect/train3/weights/best.pt conf=0.25 source={dataset.location}/test/images save=True\nThe inference results are saved in the folder runs/detect/predict. Let’s see some of them:\n\nWe can also perform inference with a completely new and complex image from another beehive with a different background (the beehive of Professor Maurilio of our University). The results were great (but not perfect and with a lower confidence score). The model found 41 bees.\n\n\nThe last thing to do is export the train, validation, and test results for your Drive at Google. To do so, you should mount your drive.\nfrom google.colab import drive\ndrive.mount('/content/gdrive')\nand copy the content of /runs folder to a folder that you should create in your Drive, for example:\n!scp -r /content/runs '/content/gdrive/MyDrive/10_UNIFEI/Bee_Project/YOLO/bees_on_hive_landing'"
  },
  {
    "objectID": "raspi/counting_objects_yolo/counting_objects_yolo.html#inference-with-the-trained-model-using-the-rasp-zero",
    "href": "raspi/counting_objects_yolo/counting_objects_yolo.html#inference-with-the-trained-model-using-the-rasp-zero",
    "title": "Counting objects with YOLO",
    "section": "Inference with the trained model, using the Rasp-Zero",
    "text": "Inference with the trained model, using the Rasp-Zero\nUsing the FileZilla FTP, let’s transfer the best.pt to our Rasp-Zero (before the transfer, you may change the model name, for example, bee_landing_640_best.pt).\nThe first thing to do is convert the model to an NCNN format:\nyolo export model=bee_landing_640_best.pt format=ncnn \nAs a result, a new converted model, bee_landing_640_best_ncnn_model is created in the same directory.\nLet’s create a folder to receive some test images (under Documents/YOLO/:\nmkdir test_images\nUsing the FileZilla FTP, let’s transfer a few images from the test dataset to our Rasp-Zero:\n\nLet’s use the Python Interpreter:\npython\nAs before, we will import the YOLO library and define our converted model to detect bees:\nfrom ultralytics import YOLO\nmodel = YOLO('bee_landing_640_best_ncnn_model')\nNow, let’s define an image and call the inference (we will save the image result this time to external verification):\nimg = 'test_images/15_bees.jpg'\nresult = model.predict(img, save=True, imgsz=640, conf=0.2, iou=0.3)\nThe inference result is saved on the variable result, and the processed image on runs/detect/predict9\n\nUsing FileZilla FTP, we can send the inference result to our Desktop for verification:\n\nlet’s go over the other images, analyzing the number of objects (bees) found:\n\nDepending on the confidence, we can have some false positives or negatives. But in general, with a model trained based on the smaller base model of the YOLOv8 family (YOLOv8n) and also converted to NCNN, the result is pretty good, running on an Edge device such as the Rasp-Zero. Also, note that the inference latency is around 730ms.\nFor example, by running the inference on Maurilio-bee.jpeg, we can find 40 bees. During the test phase on Colab, 41 bees were found (we only missed one here.)"
  },
  {
    "objectID": "raspi/counting_objects_yolo/counting_objects_yolo.html#considerations-about-the-post-processing",
    "href": "raspi/counting_objects_yolo/counting_objects_yolo.html#considerations-about-the-post-processing",
    "title": "Counting objects with YOLO",
    "section": "Considerations about the Post-Processing",
    "text": "Considerations about the Post-Processing\nOur final project should be very simple in terms of code. We will use the camera to capture an image every 10 seconds. As we did in the previous section, the captured image should be the input for the trained and converted model. We should get the number of bees for each image and save it in a database (for example, timestamp: number of bees).\nWe can do it with a single Python script or use a Linux system timer, like cron, to periodically capture images every 10 seconds and have a separate Python script to process these images as they are saved. This method can be particularly efficient in managing system resources and can be more robust against potential delays in image processing.\n\nSetting Up the Image Capture with cron\nFirst, we should set up a cron job to use the rpicam-jpeg command to capture an image every 10 seconds.\n\nEdit the crontab:\n\nOpen the terminal and type crontab -e to edit the cron jobs.\ncron normally doesn’t support sub-minute intervals directly, so we should use a workaround like a loop or watch for file changes.\n\nCreate a Bash Script (capture.sh):\n\nImage Capture: This bash script captures images every 10 seconds using rpicam-jpeg, a command that is part of the raspijpeg tool. This command lets us control the camera and capture JPEG images directly from the command line. This is especially useful because we are looking for a lightweight and straightforward method to capture images without the need for additional libraries like Picamera or external software. The script also saves the captured image with a timestamp.\n\n#!/bin/bash\n# Script to capture an image every 10 seconds\n\nwhile true\ndo\n  DATE=$(date +\"%Y-%m-%d_%H%M%S\")\n  rpicam-jpeg --output test_images/$DATE.jpg --width 640 --height 640\n  sleep 10\ndone\n\nWe should make the script executable with chmod +x capture.sh.\nThe script must start at boot or use a @reboot entry in cron to start it automatically.\n\n\n\n\nSetting Up the Python Script for Inference\nImage Processing: The Python script continuously monitors the designated directory for new images, processes each new image using the YOLOv8 model, updates the database with the count of detected bees, and optionally deletes the image to conserve disk space.\nDatabase Updates: The results, along with the timestamps, are saved in an SQLite database. For that, a simple option is to use sqlite3.\nIn short, we need to write a script that continuously monitors the directory for new images, processes them using a YOLO model, and then saves the results to a SQLite database. Here’s how we can create and make the script executable:\n#!/usr/bin/env python3\nimport os\nimport time\nimport sqlite3\nfrom datetime import datetime\nfrom ultralytics import YOLO\n\n# Constants and paths\nIMAGES_DIR = 'test_images/'\nMODEL_PATH = 'bee_landing_640_best_ncnn_model'\nDB_PATH = 'bee_count.db'\n\ndef setup_database():\n    \"\"\" Establishes a database connection and creates the table if it doesn't exist. \"\"\"\n    conn = sqlite3.connect(DB_PATH)\n    cursor = conn.cursor()\n    cursor.execute('''\n        CREATE TABLE IF NOT EXISTS bee_counts\n        (timestamp TEXT, count INTEGER)\n    ''')\n    conn.commit()\n    return conn\n\ndef process_image(image_path, model, conn):\n    \"\"\" Processes an image to detect objects and logs the count to the database. \"\"\"\n    result = model.predict(image_path, save=False, imgsz=640, conf=0.2, iou=0.3, verbose=False)\n    num_bees = len(result[0].boxes.cls) \n    timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n    cursor = conn.cursor()\n    cursor.execute(\"INSERT INTO bee_counts (timestamp, count) VALUES (?, ?)\", (timestamp, num_bees))\n    conn.commit()\n    print(f'Processed {image_path}: Number of bees detected = {num_bees}')\n\ndef monitor_directory(model, conn):\n    \"\"\" Monitors the directory for new images and processes them as they appear. \"\"\"\n    processed_files = set()\n    while True:\n        try:\n            files = set(os.listdir(IMAGES_DIR))\n            new_files = files - processed_files\n            for file in new_files:\n                if file.endswith('.jpg'):\n                    full_path = os.path.join(IMAGES_DIR, file)\n                    process_image(full_path, model, conn)\n                    processed_files.add(file)\n            time.sleep(1)  # Check every second\n        except KeyboardInterrupt:\n            print(\"Stopping...\")\n            break\n\ndef main():\n    conn = setup_database()\n    model = YOLO(MODEL_PATH)\n    monitor_directory(model, conn)\n    conn.close()\n\nif __name__ == \"__main__\":\n    main()\nThe python script must be executable, for that:\n\nSave the script: For example, as process_images.py.\nChange file permissions to make it executable:\nchmod +x process_images.py\nRun the script directly from the command line:\n./process_images.py\n\nWe should consider keeping the script running even after closing the terminal; for that, we can use nohup or screen:\nnohup ./process_images.py &\nor\nscreen -S bee_monitor\n./process_images.py\nNote that we are capturing images with their own timestamp and then log a separate timestamp for when the inference results are saved to the database. This approach can be beneficial for the following reasons:\n\nAccuracy in Data Logging:\n\nCapture Timestamp: The timestamp associated with each image capture represents the exact moment the image was taken. This is crucial for applications where precise timing of events (like bee activity) is important for analysis.\nInference Timestamp: This timestamp indicates when the image was processed and the results were recorded in the database. This can differ from the capture time due to processing delays or if the image processing is batched or queued.\n\nPerformance Monitoring:\n\nHaving separate timestamps allows us to monitor the performance and efficiency of your image processing pipeline. We can measure the delay between image capture and result logging, which helps optimize the system for real-time processing needs.\n\nTroubleshooting and Audit:\n\nSeparate timestamps provide a better audit trail and troubleshooting data. If there are issues with the image processing or data recording, having distinct timestamps can help isolate whether delays or problems occurred during capture, processing, or logging.\n\n\n\n\nScript For Reading the SQLite Database\nHere is an example of a code to retrieve the data from the database:\n#!/usr/bin/env python3\nimport sqlite3\n\ndef main():\n    db_path = 'bee_count.db'\n    conn = sqlite3.connect(db_path)\n    cursor = conn.cursor()\n    query = \"SELECT * FROM bee_counts\"\n    cursor.execute(query)\n    data = cursor.fetchall()\n    for row in data:\n        print(f\"Timestamp: {row[0]}, Number of bees: {row[1]}\")\n    conn.close()\n\nif __name__ == \"__main__\":\n    main()\n\n\nAdding Environment data\nBesides bee counting, environmental data, such as temperature and humidity, are essential for monitoring the bee-have health. Using a Rasp-Zero, it is straightforward to add a digital sensor such as the DHT-22 to get this data.\n\nEnvironmental data will be part of our final project. If you want to know more about connecting sensors to a Raspberry Pi and, even more, how to save the data to a local database and send it to the web, follow this tutorial: From Data to Graph: A Web Journey With Flask and SQLite."
  },
  {
    "objectID": "raspi/counting_objects_yolo/counting_objects_yolo.html#conclusion",
    "href": "raspi/counting_objects_yolo/counting_objects_yolo.html#conclusion",
    "title": "Counting objects with YOLO",
    "section": "Conclusion",
    "text": "Conclusion\nIn this tutorial, we have thoroughly explored integrating the YOLOv8 model with a Raspberry Pi Zero 2W to address the practical and pressing task of counting (or better, “estimating”) bees at a beehive entrance. Our project underscores the robust capability of embedding advanced machine learning technologies within compact edge computing devices, highlighting their potential impact on environmental monitoring and ecological studies.\nThis tutorial provides a step-by-step guide to the practical deployment of the YOLOv8 model. We demonstrate a tangible example of a real-world application by optimizing it for edge computing in terms of efficiency and processing speed (using NCNN format). This not only serves as a functional solution but also as an instructional tool for similar projects.\nThe technical insights and methodologies shared in this tutorial are the basis for the complete work to be developed at our university in the future. We envision further development, such as integrating additional environmental sensing capabilities and refining the model’s accuracy and processing efficiency. Implementing alternative energy solutions like the proposed solar power setup will expand the project’s sustainability and applicability in remote or underserved locations."
  },
  {
    "objectID": "raspi/counting_objects_yolo/counting_objects_yolo.html#resources",
    "href": "raspi/counting_objects_yolo/counting_objects_yolo.html#resources",
    "title": "Counting objects with YOLO",
    "section": "Resources",
    "text": "Resources\n\nThe Dataset paper, Notebooks, and PDF version are in the Project repository."
  },
  {
    "objectID": "raspi/llm/llm.html#introduction",
    "href": "raspi/llm/llm.html#introduction",
    "title": "Small Language Models (SLM)",
    "section": "Introduction",
    "text": "Introduction\nIn the fast-growing area of artificial intelligence, edge computing presents an opportunity to decentralize capabilities traditionally reserved for powerful, centralized servers. This lab explores the practical integration of small versions of traditional large language models (LLMs) into a Raspberry Pi 5, transforming this edge device into an AI hub capable of real-time, on-site data processing.\nAs large language models grow in size and complexity, Small Language Models (SLMs) offer a compelling alternative for edge devices, striking a balance between performance and resource efficiency. By running these models directly on Raspberry Pi, we can create responsive, privacy-preserving applications that operate even in environments with limited or no internet connectivity.\nThis lab will guide you through setting up, optimizing, and leveraging SLMs on Raspberry Pi. We will explore the installation and utilization of Ollama. This open-source framework allows us to run LLMs locally on our machines (our desktops or edge devices such as the Raspberry Pis or NVidia Jetsons). Ollama is designed to be efficient, scalable, and easy to use, making it a good option for deploying AI models such as Microsoft Phi, Google Gemma, Meta Llama, and LLaVa (Multimodal). We will integrate some of those models into projects using Python’s ecosystem, exploring their potential in real-world scenarios (or at least point in this direction)."
  },
  {
    "objectID": "raspi/llm/llm.html#setup",
    "href": "raspi/llm/llm.html#setup",
    "title": "Small Language Models (SLM)",
    "section": "Setup",
    "text": "Setup\nWe could use any Raspi model in the previous labs, but here, the choice must be the Raspberry Pi 5 (Raspi-5). It is a robust platform that substantially upgrades the last version 4, equipped with the Broadcom BCM2712, a 2.4GHz quad-core 64-bit Arm Cortex-A76 CPU featuring Cryptographic Extension and enhanced caching capabilities. It boasts a VideoCore VII GPU, dual 4Kp60 HDMI® outputs with HDR, and a 4Kp60 HEVC decoder. Memory options include 4GB and 8GB of high-speed LPDDR4X SDRAM, with 8GB being our choice to run SLMs. It also features expandable storage via a microSD card slot and a PCIe 2.0 interface for fast peripherals such as M.2 SSDs (Solid State Drives).\n\nFor real SSL applications, SSDs are a better option than SD cards.\n\nBy the way, as Alasdair Allan discussed, inferencing directly on the Raspberry Pi 5 CPU—with no GPU acceleration—is now on par with the performance of the Coral TPU.\n\nFor more info, please see the complete article: Benchmarking TensorFlow and TensorFlow Lite on Raspberry Pi 5.\n\nRaspberry Pi Active Cooler\nWe suggest installing an Active Cooler, a dedicated clip-on cooling solution for Raspberry Pi 5 (Raspi-5), for this lab. It combines an aluminum heatsink with a temperature-controlled blower fan to keep the Raspi-5 operating comfortably under heavy loads, such as running SLMs.\n\nThe Active Cooler has pre-applied thermal pads for heat transfer and is mounted directly to the Raspberry Pi 5 board using spring-loaded push pins. The Raspberry Pi firmware actively manages it: at 60°C, the blower’s fan will be turned on; at 67.5°C, the fan speed will be increased; and finally, at 75°C, the fan increases to full speed. The blower’s fan will spin down automatically when the temperature drops below these limits.\n\n\nTo prevent overheating, all Raspberry Pi boards begin to throttle the processor when the temperature reaches 80°Cand throttle even further when it reaches the maximum temperature of 85°C (more detail here)."
  },
  {
    "objectID": "raspi/llm/llm.html#generative-ai-genai",
    "href": "raspi/llm/llm.html#generative-ai-genai",
    "title": "Small Language Models (SLM)",
    "section": "Generative AI (GenAI)",
    "text": "Generative AI (GenAI)\nGenerative AI is an artificial intelligence system capable of creating new, original content across various mediums such as text, images, audio, and video. These systems learn patterns from existing data and use that knowledge to generate novel outputs that didn’t previously exist. Large Language Models (LLMs), Small Language Models (SLMs), and multimodal models can all be considered types of GenAI when used for generative tasks.\nGenAI provides the conceptual framework for AI-driven content creation, with LLMs serving as powerful general-purpose text generators. SLMs adapt this technology for edge computing, while multimodal models extend GenAI capabilities across different data types. Together, they represent a spectrum of generative AI technologies, each with its strengths and applications, collectively driving AI-powered content creation and understanding.\n\nLarge Language Models (LLMs)\nLarge Language Models (LLMs) are advanced artificial intelligence systems that understand, process, and generate human-like text. These models are characterized by their massive scale in terms of the amount of data they are trained on and the number of parameters they contain. Critical aspects of LLMs include:\n\nSize: LLMs typically contain billions of parameters. For example, GPT-3 has 175 billion parameters, while some newer models exceed a trillion parameters.\nTraining Data: They are trained on vast amounts of text data, often including books, websites, and other diverse sources, amounting to hundreds of gigabytes or even terabytes of text.\nArchitecture: Most LLMs use transformer-based architectures, which allow them to process and generate text by paying attention to different parts of the input simultaneously.\nCapabilities: LLMs can perform a wide range of language tasks without specific fine-tuning, including:\n\nText generation\nTranslation\nSummarization\nQuestion answering\nCode generation\nLogical reasoning\n\nFew-shot Learning: They can often understand and perform new tasks with minimal examples or instructions.\nResource-Intensive: Due to their size, LLMs typically require significant computational resources to run, often needing powerful GPUs or TPUs.\nContinual Development: The field of LLMs is rapidly evolving, with new models and techniques constantly emerging.\nEthical Considerations: The use of LLMs raises important questions about bias, misinformation, and the environmental impact of training such large models.\nApplications: LLMs are used in various fields, including content creation, customer service, research assistance, and software development.\nLimitations: Despite their power, LLMs can produce incorrect or biased information and lack true understanding or reasoning capabilities.\n\nWe must note that we use large models beyond text, calling them multi-modal models. These models integrate and process information from multiple types of input simultaneously. They are designed to understand and generate content across various forms of data, such as text, images, audio, and video.\nCertainly. Let’s define open and closed models in the context of AI and language models:\n\n\nClosed vs Open Models:\nClosed models, also called proprietary models, are AI models whose internal workings, code, and training data are not publicly disclosed. Examples: GPT-4 (by OpenAI), Claude (by Anthropic), Gemini (by Google).\nOpen models, also known as open-source models, are AI models whose underlying code, architecture, and often training data are publicly available and accessible. Examples: Gemma (by Google), LLaMA (by Meta) and Phi (by Microsoft)/\nOpen models are particularly relevant for running models on edge devices like Raspberry Pi as they can be more easily adapted, optimized, and deployed in resource-constrained environments. Still, it is crucial to verify their Licenses. Open models come with various open-source licenses that may affect their use in commercial applications, while closed models have clear, albeit restrictive, terms of service.\n\n\n\nAdapted from https://arxiv.org/pdf/2304.13712\n\n\n\n\nSmall Language Models (SLMs)\nIn the context of edge computing on devices like Raspberry Pi, full-scale LLMs are typically too large and resource-intensive to run directly. This limitation has driven the development of smaller, more efficient models, such as the Small Language Models (SLMs).\nSLMs are compact versions of LLMs designed to run efficiently on resource-constrained devices such as smartphones, IoT devices, and single-board computers like the Raspberry Pi. These models are significantly smaller in size and computational requirements than their larger counterparts while still retaining impressive language understanding and generation capabilities.\nKey characteristics of SLMs include:\n\nReduced parameter count: Typically ranging from a few hundred million to a few billion parameters, compared to two-digit billions in larger models.\nLower memory footprint: Requiring, at most, a few gigabytes of memory rather than tens or hundreds of gigabytes.\nFaster inference time: Can generate responses in milliseconds to seconds on edge devices.\nEnergy efficiency: Consuming less power, making them suitable for battery-powered devices.\nPrivacy-preserving: Enabling on-device processing without sending data to cloud servers.\nOffline functionality: Operating without an internet connection.\n\nSLMs achieve their compact size through various techniques such as knowledge distillation, model pruning, and quantization. While they may not match the broad capabilities of larger models, SLMs excel in specific tasks and domains, making them ideal for targeted applications on edge devices.\n\nWe will generally consider SLMs, language models with less than 5 billion parameters quantized to 4 bits.\n\nExamples of SLMs include compressed versions of models like Meta Llama, Microsoft PHI, and Google Gemma. These models enable a wide range of natural language processing tasks directly on edge devices, from text classification and sentiment analysis to question answering and limited text generation.\nFor more information on SLMs, the paper, LLM Pruning and Distillation in Practice: The Minitron Approach, provides an approach applying pruning and distillation to obtain SLMs from LLMs. And, SMALL LANGUAGE MODELS: SURVEY, MEASUREMENTS, AND INSIGHTS, presents a comprehensive survey and analysis of Small Language Models (SLMs), which are language models with 100 million to 5 billion parameters designed for resource-constrained devices."
  },
  {
    "objectID": "raspi/llm/llm.html#ollama",
    "href": "raspi/llm/llm.html#ollama",
    "title": "Small Language Models (SLM)",
    "section": "Ollama",
    "text": "Ollama\n\n\n\nollama logo\n\n\nOllama is an open-source framework that allows us to run language models (LMs), large or small, locally on our machines. Here are some critical points about Ollama:\n\nLocal Model Execution: Ollama enables running LMs on personal computers or edge devices such as the Raspi-5, eliminating the need for cloud-based API calls.\nEase of Use: It provides a simple command-line interface for downloading, running, and managing different language models.\nModel Variety: Ollama supports various LLMs, including Phi, Gemma, Llama, Mistral, and other open-source models.\nCustomization: Users can create and share custom models tailored to specific needs or domains.\nLightweight: Designed to be efficient and run on consumer-grade hardware.\nAPI Integration: Offers an API that allows integration with other applications and services.\nPrivacy-Focused: By running models locally, it addresses privacy concerns associated with sending data to external servers.\nCross-Platform: Available for macOS, Windows, and Linux systems (our case, here).\nActive Development: Regularly updated with new features and model support.\nCommunity-Driven: Benefits from community contributions and model sharing.\n\nTo learn more about what Ollama is and how it works under the hood, you should see this short video from Matt Williams, one of the founders of Ollama:\n\n\nMatt has an entirely free course about Ollama that we recommend: \n\n\nInstalling Ollama\nLet’s set up and activate a Virtual Environment for working with Ollama:\npython3 -m venv ~/ollama\nsource ~/ollama/bin/activate\nAnd run the command to install Ollama:\ncurl -fsSL https://ollama.com/install.sh | sh\nAs a result, an API will run in the background on 127.0.0.1:11434. From now on, we can run Ollama via the terminal. For starting, let’s verify the Ollama version, which will also tell us that it is correctly installed:\nollama -v\n\nOn the Ollama Library page, we can find the models Ollama supports. For example, by filtering by Most popular, we can see Meta Llama, Google Gemma, Microsoft Phi, LLaVa, etc.\n\n\nMeta Llama 3.2 1B/3B\n\nLet’s install and run our first small language model, Llama 3.2 1B (and 3B). The Meta Llama, 3.2 collections of multilingual large language models (LLMs), is a collection of pre-trained and instruction-tuned generative models in 1B and 3B sizes (text in/text out). The Llama 3.2 instruction-tuned text-only models are optimized for multilingual dialogue use cases, including agentic retrieval and summarization tasks.\nThe 1B and 3B models were pruned from the Llama 8B, and then logits from the 8B and 70B models were used as token-level targets (token-level distillation). Knowledge distillation was used to recover performance (they were trained with 9 trillion tokens). The 1B model has 1,24B, quantized to integer (Q8_0), and the 3B, 3.12B parameters, with a Q4_0 quantization, which ends with a size of 1.3 GB and 2GB, respectively. Its context window is 131,072 tokens.\n\nInstall and run the Model\nollama run llama3.2:1b\nRunning the model with the command before, we should have the Ollama prompt available for us to input a question and start chatting with the LLM model; for example,\n&gt;&gt;&gt; What is the capital of France?\nAlmost immediately, we get the correct answer:\nThe capital of France is Paris.\nUsing the option --verbose when calling the model will generate several statistics about its performance (The model will be polling only the first time we run the command).\n\nEach metric gives insights into how the model processes inputs and generates outputs. Here’s a breakdown of what each metric means:\n\nTotal Duration (2.620170326s): This is the complete time taken from the start of the command to the completion of the response. It encompasses loading the model, processing the input prompt, and generating the response.\nLoad Duration (39.947908ms): This duration indicates the time to load the model or necessary components into memory. If this value is minimal, it can suggest that the model was preloaded or that only a minimal setup was required.\nPrompt Eval Count (32 tokens): The number of tokens in the input prompt. In NLP, tokens are typically words or subwords, so this count includes all the tokens that the model evaluated to understand and respond to the query.\nPrompt Eval Duration (1.644773s): This measures the model’s time to evaluate or process the input prompt. It accounts for the bulk of the total duration, implying that understanding the query and preparing a response is the most time-consuming part of the process.\nPrompt Eval Rate (19.46 tokens/s): This rate indicates how quickly the model processes tokens from the input prompt. It reflects the model’s speed in terms of natural language comprehension.\nEval Count (8 token(s)): This is the number of tokens in the model’s response, which in this case was, “The capital of France is Paris.”\nEval Duration (889.941ms): This is the time taken to generate the output based on the evaluated input. It’s much shorter than the prompt evaluation, suggesting that generating the response is less complex or computationally intensive than understanding the prompt.\nEval Rate (8.99 tokens/s): Similar to the prompt eval rate, this indicates the speed at which the model generates output tokens. It’s a crucial metric for understanding the model’s efficiency in output generation.\n\nThis detailed breakdown can help understand the computational demands and performance characteristics of running SLMs like Llama on edge devices like the Raspberry Pi 5. It shows that while prompt evaluation is more time-consuming, the actual generation of responses is relatively quicker. This analysis is crucial for optimizing performance and diagnosing potential bottlenecks in real-time applications.\nLoading and running the 3B model, we can see the difference in performance for the same prompt;\n\nThe eval rate is lower, 5.3 tokens/s versus 9 tokens/s with the smaller model.\nWhen question about\n&gt;&gt;&gt; What is the distance between Paris and Santiago, Chile?\nThe 1B model answered 9,841 kilometers (6,093 miles), which is inaccurate, and the 3B model answered 7,300 miles (11,700 km), which is close to the correct (11,642 km).\nLet’s ask for the Paris’s coordinates:\n&gt;&gt;&gt; what is the latitude and longitude of Paris?\nThe latitude and longitude of Paris are 48.8567° N (48°55' \n42\" N) and 2.3510° E (2°22' 8\" E), respectively.\n\nBoth 1B and 3B models gave correct answers.\n\n\nGoogle Gemma 2 2B\nLet’s install Gemma 2, a high-performing and efficient model available in three sizes: 2B, 9B, and 27B. We will install Gemma 2 2B, a lightweight model trained with 2 trillion tokens that produces outsized results by learning from larger models through distillation. The model has 2.6 billion parameters and a Q4_0 quantization, which ends with a size of 1.6 GB. Its context window is 8,192 tokens.\n\nInstall and run the Model\nollama run gemma2:2b --verbose\nRunning the model with the command before, we should have the Ollama prompt available for us to input a question and start chatting with the LLM model; for example,\n&gt;&gt;&gt; What is the capital of France?\nAlmost immediately, we get the correct answer:\nThe capital of France is **Paris**. 🗼\nAnd it’ statistics.\n\nWe can see that Gemma 2:2B has around the same performance as Lama 3.2:3B, but having less parameters.\nOther examples:\n&gt;&gt;&gt; What is the distance between Paris and Santiago, Chile?\n\nThe distance between Paris, France and Santiago, Chile is \napproximately **7,000 miles (11,267 kilometers)**. \n\nKeep in mind that this is a straight-line distance, and actual \ntravel distance can vary depending on the chosen routes and any \nstops along the way. ✈️`\nAlso, a good response but less accurate than Llama3.2:3B.\n&gt;&gt;&gt; what is the latitude and longitude of Paris?\n\nYou got it! Here are the latitudes and longitudes of Paris, \nFrance:\n\n* **Latitude:** 48.8566° N (north)\n* **Longitude:** 2.3522° E (east) \n\nLet me know if you'd like to explore more about Paris or its \nlocation! 🗼🇫🇷 \nA good and accurate answer (a little more verbose than the Llama answers).\n\n\nMicrosoft Phi3.5 3.8B\nLet’s pull a bigger (but still tiny) model, the PHI3.5, a 3.8B lightweight state-of-the-art open model by Microsoft. The model belongs to the Phi-3 model family and supports 128K token context length and the languages: Arabic, Chinese, Czech, Danish, Dutch, English, Finnish, French, German, Hebrew, Hungarian, Italian, Japanese, Korean, Norwegian, Polish, Portuguese, Russian, Spanish, Swedish, Thai, Turkish and Ukrainian.\nThe model size, in terms of bytes, will depend on the specific quantization format used. The size can go from 2-bit quantization (q2_k) of 1.4GB (higher performance/lower quality) to 16-bit quantization (fp-16) of 7.6GB (lower performance/higher quality).\nLet’s run the 4-bit quantization (Q4_0), which will need 2.2GB of RAM, with an intermediary trade-off regarding output quality and performance.\nollama run phi3.5:3.8b --verbose\n\nYou can use run or pull to download the model. What happens is that Ollama keeps note of the pulled models, and once the PHI3 does not exist, before running it, Ollama pulls it.\n\nLet’s enter with the same prompt used before:\n&gt;&gt;&gt; What is the capital of France?\n\nThe capital of France is Paris. It' extradites significant \nhistorical, cultural, and political importance to the country as \nwell as being a major European city known for its art, fashion, \ngastronomy, and culture. Its influence extends beyond national \nborders, with millions of tourists visiting each year from around \nthe globe. The Seine River flows through Paris before it reaches \nthe broader English Channel at Le Havre. Moreover, France is one \nof Europe's leading economies with its capital playing a key role \n\n...\nThe answer was very “verbose”, let’s specify a better prompt:\n\nIn this case, the answer was still longer than we expected, with an eval rate of 2.25 tokens/s, more than double that of Gemma and Llama.\n\nChoosing the most appropriate prompt is one of the most important skills to be used with LLMs, no matter its size.\n\nWhen we asked the same questions about distance and Latitude/Longitude, we did not get a good answer for a distance of 13,507 kilometers (8,429 miles), but it was OK for coordinates. Again, it could have been less verbose (more than 200 tokens for each answer).\nWe can use any model as an assistant since their speed is relatively decent, but on September 24, the Llama2:3B is a better choice. You should try other models, depending on your needs. 🤗 Open LLM Leaderboard can give you an idea about the best models in size, benchmark, license, etc.\n\nThe best model to use is the one fit for your specific necessity. Also, take into consideration that this field evolves with new models every day,\n\n\n\nMultimodal Models\nMultimodal models are artificial intelligence (AI) systems that can process and understand information from multiple sources, such as images, text, audio, and video. In our context, multimodal LLMs can process various inputs, including text, images, and audio, as prompts and convert those prompts into various outputs, not just the source type.\nWe will work here with LLaVA-Phi-3, a fine-tuned LLaVA model from Phi 3 Mini 4k. It has strong performance benchmarks that are on par with the original LLaVA (Large Language and Vision Assistant) model.\nThe LLaVA-Phi-3 is an end-to-end trained large multimodal model designed to understand and generate content based on visual inputs (images) and textual instructions. It combines the capabilities of a visual encoder and a language model to process and respond to multimodal inputs.\nLet’s install the model:\nollama run llava-phi3:3.8b --verbose\nLet’s start with a text input:\n&gt;&gt;&gt; You are a helpful AI assistant. What is the capital of France?\n\nAs an AI language model, I can tell you that the capital of France \nis Paris. It's not only the largest city in the country but also \nserves as its political and administrative center. Paris is known \nfor its iconic landmarks such as the Eiffel Tower, Notre-Dame \nCathedral, and the Louvre Museum. The city has a rich history, \nbeautiful architecture, and is widely considered to be one of the \nmost romantic cities in the world.\nThe response took around 30s, with an eval rate of 3.93 tokens/s! Not bad!\nBut let us know to enter with an image as input. For that, let’s create a directory for working:\ncd Documents/\nmkdir OLLAMA\ncd OLLAMA\nLet’s download a 640x320 image from the internet, for example (Wikipedia: Paris, France):\n\nUsing FileZilla, for example, let’s upload the image to the OLLAMA folder at the Raspi-5 and name it image_test_1.jpg. We should have the whole image path (we can use pwd to get it).\n/home/mjrovai/Documents/OLLAMA/image_test_1.jpg\nIf you use a desktop, you can copy the image path by clicking the image with the mouse’s right button.\n\nLet’s enter with this prompt:\n&gt;&gt;&gt; Describe the image /home/mjrovai/Documents/OLLAMA/image_test_1.jpg\nThe result was great, but the overall latency was significant; almost 4 minutes to perform the inference.\n\n\n\nInspecting local resources\nUsing htop, we can monitor the resources running on our device.\nhtop\nDuring the time that the model is running, we can inspect the resources:\n\nAll four CPUs run at almost 100% of their capacity, and the memory used with the model loaded is 3.24GB. Exiting Ollama, the memory goes down to around 377MB (with no desktop).\nIt is also essential to monitor the temperature. When running the Raspberry with a desktop, you can have the temperature shown on the taskbar:\n\nIf you are “headless”, the temperature can be monitored with the command:\nvcgencmd measure_temp\nIf you are doing nothing, the temperature is around 50°C for CPUs running at 1%. During inference, with the CPUs at 100%, the temperature can rise to almost 70°C. This is OK and means the active cooler is working, keeping the temperature below 80°C / 85°C (its limit)."
  },
  {
    "objectID": "raspi/llm/llm.html#ollama-python-library",
    "href": "raspi/llm/llm.html#ollama-python-library",
    "title": "Small Language Models (SLM)",
    "section": "Ollama Python Library",
    "text": "Ollama Python Library\nSo far, we have explored SLMs’ chat capability using the command line on a terminal. However, we want to integrate those models into our projects, so Python seems to be the right path. The good news is that Ollama has such a library.\nThe Ollama Python library simplifies interaction with advanced LLM models, enabling more sophisticated responses and capabilities, besides providing the easiest way to integrate Python 3.8+ projects with Ollama.\nFor a better understanding of how to create apps using Ollama with Python, we can follow Matt Williams’s videos, as the one below:\n\nInstallation:\nIn the terminal, run the command:\npip install ollama\nWe will need a text editor or an IDE to create a Python script. If you run the Raspberry OS on a desktop, several options, such as Thonny and Geany, have already been installed by default (accessed by [Menu][Programming]). You can download other IDEs, such as Visual Studio Code, from [Menu][Recommended Software]. When the window pops up, go to [Programming], select the option of your choice, and press [Apply].\n\nIf you prefer using Jupyter Notebook for development:\npip install jupyter\njupyter notebook --generate-config\nTo run Jupyter Notebook, run the command (change the IP address for yours):\njupyter notebook --ip=192.168.4.209 --no-browser\nOn the terminal, you can see the local URL address to open the notebook:\n\nWe can access it from another computer by entering the Raspberry Pi’s IP address and the provided token in a web browser (we should copy it from the terminal).\nIn our working directory in the Raspi, we will create a new Python 3 notebook.\nLet’s enter with a very simple script to verify the installed models:\nimport ollama\nollama.list()\nAll the models will be printed as a dictionary, for example:\n  {'name': 'gemma2:2b',\n   'model': 'gemma2:2b',\n   'modified_at': '2024-09-24T19:30:40.053898094+01:00',\n   'size': 1629518495,\n   'digest': '8ccf136fdd5298f3ffe2d69862750ea7fb56555fa4d5b18c04e3fa4d82ee09d7',\n   'details': {'parent_model': '',\n    'format': 'gguf',\n    'family': 'gemma2',\n    'families': ['gemma2'],\n    'parameter_size': '2.6B',\n    'quantization_level': 'Q4_0'}}]}\nLet’s repeat one of the questions that we did before, but now using ollama.generate() from Ollama python library. This API will generate a response for the given prompt with the provided model. This is a streaming endpoint, so there will be a series of responses. The final response object will include statistics and additional data from the request.\nMODEL = 'gemma2:2b'\nPROMPT = 'What is the capital of France?'\n\nres = ollama.generate(model=MODEL, prompt=PROMPT)\nprint (res)\nIn case you are running the code as a Python script, you should save it, for example, test_ollama.py. You can use the IDE to run it or do it directly on the terminal. Also, remember that you should always call the model and define it when running a stand-alone script.\npython test_ollama.py\nAs a result, we will have the model response in a JSON format:\n{'model': 'gemma2:2b', 'created_at': '2024-09-25T14:43:31.869633807Z', \n'response': 'The capital of France is **Paris**. 🇫🇷 \\n', 'done': True, \n'done_reason': 'stop', 'context': [106, 1645, 108, 1841, 603, 573, 6037, 576,\n6081, 235336, 107, 108, 106, 2516, 108, 651, 6037, 576, 6081, 603, 5231, 29437, \n168428, 235248, 244304, 241035, 235248, 108], 'total_duration': 24259469458, \n'load_duration': 19830013859, 'prompt_eval_count': 16, 'prompt_eval_duration': \n1908757000, 'eval_count': 14, 'eval_duration': 2475410000}\nAs we can see, several pieces of information are generated, such as:\n\nresponse: the main output text generated by the model in response to our prompt.\n\nThe capital of France is **Paris**. 🇫🇷\n\ncontext: the token IDs representing the input and context used by the model. Tokens are numerical representations of text used for processing by the language model.\n\n[106, 1645, 108, 1841, 603, 573, 6037, 576, 6081, 235336, 107, 108, 106, 2516, 108, 651, 6037, 576, 6081, 603, 5231, 29437, 168428, 235248, 244304, 241035, 235248, 108]\n\n\nThe Performance Metrics:\n\ntotal_duration: The total time taken for the operation in nanoseconds. In this case, approximately 24.26 seconds.\nload_duration: The time taken to load the model or components in nanoseconds. About 19.38 seconds.\nprompt_eval_duration: The time taken to evaluate the prompt in nanoseconds. Around 1.9.0 seconds.\neval_count: The number of tokens evaluated during the generation. Here, 14 tokens.\neval_duration: The time taken for the model to generate the response in nanoseconds. Approximately 2.5 seconds.\n\nBut, what we want is the plain ‘response’ and, perhaps for analysis, the total duration of the inference, so let’s change the code to extract it from the dictionary:\nprint(f\"\\n{res['response']}\")\nprint(f\"\\n [INFO] Total Duration: {(res['total_duration']/1e9):.2f} seconds\")\nNow, we got:\nThe capital of France is **Paris**. 🇫🇷 \n\n [INFO] Total Duration: 24.26 seconds\nUsing Ollama.chat()\nAnother way to get our response is to use ollama.chat(), which generates the next message in a chat with a provided model. This is a streaming endpoint, so a series of responses will occur. Streaming can be disabled using \"stream\": false. The final response object will also include statistics and additional data from the request.\nPROMPT_1 = 'What is the capital of France?'\n\nresponse = ollama.chat(model=MODEL, messages=[\n{'role': 'user','content': PROMPT_1,},])\nresp_1 = response['message']['content']\nprint(f\"\\n{resp_1}\")\nprint(f\"\\n [INFO] Total Duration: {(res['total_duration']/1e9):.2f} seconds\")\nThe answer is the same as before.\nAn important consideration is that by using ollama.generate(), the response is “clear” from the model’s “memory” after the end of inference (only used once), but If we want to keep a conversation, we must use ollama.chat(). Let’s see it in action:\nPROMPT_1 = 'What is the capital of France?'\nresponse = ollama.chat(model=MODEL, messages=[\n{'role': 'user','content': PROMPT_1,},])\nresp_1 = response['message']['content']\nprint(f\"\\n{resp_1}\")\nprint(f\"\\n [INFO] Total Duration: {(response['total_duration']/1e9):.2f} seconds\")\n\nPROMPT_2 = 'and of Italy?'\nresponse = ollama.chat(model=MODEL, messages=[\n{'role': 'user','content': PROMPT_1,},\n{'role': 'assistant','content': resp_1,},\n{'role': 'user','content': PROMPT_2,},])\nresp_2 = response['message']['content']\nprint(f\"\\n{resp_2}\")\nprint(f\"\\n [INFO] Total Duration: {(response['total_duration']/1e9):.2f} seconds\")\nIn the above code, we are running two queries, and the second prompt considers the result of the first one.\nHere is how the model responded:\nThe capital of France is **Paris**. 🇫🇷 \n\n [INFO] Total Duration: 2.82 seconds\n\nThe capital of Italy is **Rome**. 🇮🇹 \n\n [INFO] Total Duration: 4.46 seconds\nGetting an image description:\nIn the same way that we have used the LlaVa-PHI-3 model with the command line to analyze an image, the same can be done here with Python. Let’s use the same image of Paris, but now with the ollama.generate():\nMODEL = 'llava-phi3:3.8b'\nPROMPT = \"Describe this picture\"\n\nwith open('image_test_1.jpg', 'rb') as image_file:\n    img = image_file.read()\n\nresponse = ollama.generate(\n    model=MODEL,\n    prompt=PROMPT,\n    images= [img]\n)\nprint(f\"\\n{response['response']}\")\nprint(f\"\\n [INFO] Total Duration: {(res['total_duration']/1e9):.2f} seconds\")\nHere is the result:\nThis image captures the iconic cityscape of Paris, France. The vantage point \nis high, providing a panoramic view of the Seine River that meanders through \nthe heart of the city. Several bridges arch gracefully over the river, \nconnecting different parts of the city. The Eiffel Tower, an iron lattice \nstructure with a pointed top and two antennas on its summit, stands tall in the \nbackground, piercing the sky. It is painted in a light gray color, contrasting \nagainst the blue sky speckled with white clouds.\n\nThe buildings that line the river are predominantly white or beige, their uniform\ncolor palette broken occasionally by red roofs peeking through. The Seine River \nitself appears calm and wide, reflecting the city's architectural beauty in its \nsurface. On either side of the river, trees add a touch of green to the urban \nlandscape.\n\nThe image is taken from an elevated perspective, looking down on the city. This \nviewpoint allows for a comprehensive view of Paris's beautiful architecture and \nlayout. The relative positions of the buildings, bridges, and other structures \ncreate a harmonious composition that showcases the city's charm.\n\nIn summary, this image presents a serene day in Paris, with its architectural \nmarvels - from the Eiffel Tower to the river-side buildings - all bathed in soft \ncolors under a clear sky.\n\n [INFO] Total Duration: 256.45 seconds\nThe model took about 4 minutes (256.45 s) to return with a detailed image description.\n\nIn the 10-Ollama_Python_Library notebook, it is possible to find the experiments with the Ollama Python library.\n\n\nFunction Calling\nSo far, we can see that, with the model’s (“response”) answer to a variable, we can efficiently work with it, integrating it into real-world projects. However, a big problem is that the model can respond differently to the same prompt. Let’s say that what we want, as the model’s response in the last examples, is only the name of a given country’s capital and its coordinates, nothing more, even with very verbose models such as the Microsoft Phi. We can use the Ollama function's calling to guarantee the same answers, which is perfectly compatible with OpenAI API.\n\nBut what exactly is “function calling”?\nIn modern artificial intelligence, function calling with Large Language Models (LLMs) allows these models to perform actions beyond generating text. By integrating with external functions or APIs, LLMs can access real-time data, automate tasks, and interact with various systems.\nFor instance, instead of merely responding to a query about the weather, an LLM can call a weather API to fetch the current conditions and provide accurate, up-to-date information. This capability enhances the relevance and accuracy of the model’s responses and makes it a powerful tool for driving workflows and automating processes, transforming it into an active participant in real-world applications.\nFor more details about Function Calling, please see this video made by Marvin Prison:\n\n\n\nLet’s create a project.\nWe want to create an app where the user enters a country’s name and gets, as an output, the distance in km from the capital city of such a country and the app’s location (for simplicity, We will use Santiago, Chile, as the app location).\n\nOnce the user enters a country name, the model will return the name of its capital city (as a string) and the latitude and longitude of such city (in float). Using those coordinates, we can use a simple Python library (haversine) to calculate the distance between those 2 points.\nThe idea of this project is to demonstrate a combination of language model interaction (IA), structured data handling with Pydantic, and geospatial calculations using the Haversine formula (traditional computing).\nFirst, let us install some libraries. Besides Haversine, the main one is the OpenAI Python library, which provides convenient access to the OpenAI REST API from any Python 3.7+ application. The other one is Pydantic (and instructor), a robust data validation and settings management library engineered by Python to enhance the robustness and reliability of our codebase. In short, Pydantic will help ensure that our model’s response will always be consistent.\npip install haversine\npip install openai \npip install pydantic \npip install instructor\nNow, we should create a Python script designed to interact with our model (LLM) to determine the coordinates of a country’s capital city and calculate the distance from Santiago de Chile to that capital.\nLet’s go over the code:\n\n\n\n1. Importing Libraries\nimport sys\nfrom haversine import haversine\nfrom openai import OpenAI\nfrom pydantic import BaseModel, Field\nimport instructor\n\nsys: Provides access to system-specific parameters and functions. It’s used to get command-line arguments.\nhaversine: A function from the haversine library that calculates the distance between two geographic points using the Haversine formula.\nopenAI: A module for interacting with the OpenAI API (although it’s used in conjunction with a local setup, Ollama). Everything is off-line here.\npydantic: Provides data validation and settings management using Python-type annotations. It’s used to define the structure of expected response data.\ninstructor: A module is used to patch the OpenAI client to work in a specific mode (likely related to structured data handling).\n\n\n\n2. Defining Input and Model\ncountry = sys.argv[1]       # Get the country from command-line arguments\nMODEL = 'phi3.5:3.8b'     # The name of the model to be used\nmylat = -33.33              # Latitude of Santiago de Chile\nmylon = -70.51              # Longitude of Santiago de Chile\n\ncountry: On a Python script, getting the country name from command-line arguments is possible. On a Jupyter notebook, we can enter its name, for example,\n\ncountry = \"France\"\n\nMODEL: Specifies the model being used, which is, in this example, the phi3.5.\nmylat and mylon: Coordinates of Santiago de Chile, used as the starting point for the distance calculation.\n\n\n\n3. Defining the Response Data Structure\nclass CityCoord(BaseModel):\n    city: str = Field(..., description=\"Name of the city\")\n    lat: float = Field(..., description=\"Decimal Latitude of the city\")\n    lon: float = Field(..., description=\"Decimal Longitude of the city\")\n\nCityCoord: A Pydantic model that defines the expected structure of the response from the LLM. It expects three fields: city (name of the city), lat (latitude), and lon (longitude).\n\n\n\n4. Setting Up the OpenAI Client\nclient = instructor.patch(\n    OpenAI(\n        base_url=\"http://localhost:11434/v1\",  # Local API base URL (Ollama)\n        api_key=\"ollama\",                      # API key (not used)\n    ),\n    mode=instructor.Mode.JSON,                 # Mode for structured JSON output\n)\n\nOpenAI: This setup initializes an OpenAI client with a local base URL and an API key (ollama). It uses a local server.\ninstructor.patch: Patches the OpenAI client to work in JSON mode, enabling structured output that matches the Pydantic model.\n\n\n\n5. Generating the Response\nresp = client.chat.completions.create(\n    model=MODEL,\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": f\"return the decimal latitude and decimal longitude \\\n            of the capital of the {country}.\"\n        }\n    ],\n    response_model=CityCoord,\n    max_retries=10\n)\n\nclient.chat.completions.create: Calls the LLM to generate a response.\nmodel: Specifies the model to use (llava-phi3).\nmessages: Contains the prompt for the LLM, asking for the latitude and longitude of the capital city of the specified country.\nresponse_model: Indicates that the response should conform to the CityCoord model.\nmax_retries: The maximum number of retry attempts if the request fails.\n\n\n\n6. Calculating the Distance\ndistance = haversine((mylat, mylon), (resp.lat, resp.lon), unit='km')\nprint(f\"Santiago de Chile is about {int(round(distance, -1)):,} \\\n        kilometers away from {resp.city}.\")\n\nhaversine: Calculates the distance between Santiago de Chile and the capital city returned by the LLM using their respective coordinates.\n(mylat, mylon): Coordinates of Santiago de Chile.\nresp.city: Name of the country’s capital\n(resp.lat, resp.lon): Coordinates of the capital city are provided by the LLM response.\nunit=‘km’: Specifies that the distance should be calculated in kilometers.\nprint: Outputs the distance, rounded to the nearest 10 kilometers, with thousands of separators for readability.\n\nRunning the code\nIf we enter different countries, for example, France, Colombia, and the United States, We can note that we always receive the same structured information:\nSantiago de Chile is about 8,060 kilometers away from Washington, D.C..\nSantiago de Chile is about 4,250 kilometers away from Bogotá.\nSantiago de Chile is about 11,630 kilometers away from Paris.\nIf you run the code as a script, the result will be printed on the terminal:\n\nAnd the calculations are pretty good!\n\n\nIn the 20-Ollama_Function_Calling notebook, it is possible to find experiments with all models installed.\n\n\n\nAdding images\nNow it is time to wrap up everything so far! Let’s modify the script so that instead of entering the country name (as a text), the user enters an image, and the application (based on SLM) returns the city in the image and its geographic location. With those data, we can calculate the distance as before.\n\nFor simplicity, we will implement this new code in two steps. First, the LLM will analyze the image and create a description (text). This text will be passed on to another instance, where the model will extract the information needed to pass along.\nWe will start importing the libraries\nimport sys\nimport time\nfrom haversine import haversine\nimport ollama\nfrom openai import OpenAI\nfrom pydantic import BaseModel, Field\nimport instructor\nWe can see the image if you run the code on the Jupyter Notebook. For that we need also import:\nimport matplotlib.pyplot as plt\nfrom PIL import Image\n\nThose libraries are unnecessary if we run the code as a script.\n\nNow, we define the model and the local coordinates:\nMODEL = 'llava-phi3:3.8b'\nmylat = -33.33\nmylon = -70.51\nWe can download a new image, for example, Machu Picchu from Wikipedia. On the Notebook we can see it:\n# Load the image\nimg_path = \"image_test_3.jpg\"\nimg = Image.open(img_path)\n\n# Display the image\nplt.figure(figsize=(8, 8))\nplt.imshow(img)\nplt.axis('off')\n#plt.title(\"Image\")\nplt.show()\n\nNow, let’s define a function that will receive the image and will return the decimal latitude and decimal longitude of the city in the image, its name, and what country it is located\ndef image_description(img_path):\n    with open(img_path, 'rb') as file:\n        response = ollama.chat(\n            model=MODEL,\n            messages=[\n              {\n                'role': 'user',\n                'content': '''return the decimal latitude and decimal longitude \n                              of the city in the image, its name, and \n                              what country it is located''',\n                'images': [file.read()],\n              },\n            ],\n            options = {\n              'temperature': 0,\n              }\n      )\n    #print(response['message']['content'])\n    return response['message']['content']\n\nWe can print the entire response for debug purposes.\n\nThe image description generated for the function will be passed as a prompt for the model again.\nstart_time = time.perf_counter()  # Start timing\n\nclass CityCoord(BaseModel):\n    city: str = Field(..., description=\"Name of the city in the image\")\n    country: str = Field(..., description=\"\"\"Name of the country where\"\n                                             the city in the image is located\n                                             \"\"\")\n    lat: float = Field(..., description=\"\"\"Decimal Latitude of the city in\"\n                                            the image\"\"\")\n    lon: float = Field(..., description=\"\"\"Decimal Longitude of the city in\"\n                                           the image\"\"\")\n\n# enables `response_model` in create call\nclient = instructor.patch(\n    OpenAI(\n        base_url=\"http://localhost:11434/v1\",\n        api_key=\"ollama\"\n    ),\n    mode=instructor.Mode.JSON,\n)\n\nimage_description = image_description(img_path)\n# Send this description to the model\nresp = client.chat.completions.create(\n    model=MODEL,\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": image_description,\n        }\n    ],\n    response_model=CityCoord,\n    max_retries=10,\n    temperature=0,\n)\nIf we print the image description , we will get:\nThe image shows the ancient city of Machu Picchu, located in Peru. The city is\nperched on a steep hillside and consists of various structures made of stone. It \nis surrounded by lush greenery and towering mountains. The sky above is blue with\nscattered clouds. \n\nMachu Picchu's latitude is approximately 13.5086° S, and its longitude is around\n72.5494° W.\nAnd the second response from the model (resp) will be:\nCityCoord(city='Machu Picchu', country='Peru', lat=-13.5086, lon=-72.5494)\nNow, we can do a “Post-Processing”, calculating the distance and preparing the final answer:\ndistance = haversine((mylat, mylon), (resp.lat, resp.lon), unit='km')\n\nprint(f\"\\n The image shows {resp.city}, with lat:{round(resp.lat, 2)} and \\\n      long: {round(resp.lon, 2)}, located in {resp.country} and about \\\n            {int(round(distance, -1)):,} kilometers away from \\\n            Santiago, Chile.\\n\")\n\nend_time = time.perf_counter()  # End timing\nelapsed_time = end_time - start_time  # Calculate elapsed time\nprint(f\" [INFO] ==&gt; The code (running {MODEL}), took {elapsed_time:.1f} \\\n      seconds to execute.\\n\")\nAnd we will get:\n The image shows Machu Picchu, with lat:-13.16 and long: -72.54, located in Peru\n and about 2,250 kilometers away from Santiago, Chile.\n\n [INFO] ==&gt; The code (running llava-phi3:3.8b), took 491.3 seconds to execute.\nIn the 30-Function_Calling_with_images notebook, it is possible to find the experiments with multiple images.\nLet’s now download the script calc_distance_image.py from the GitHub and run it on the terminal with the command:\npython calc_distance_image.py /home/mjrovai/Documents/OLLAMA/image_test_3.jpg\nEnter with the Machu Picchu image full patch as an argument. We will get the same previous result.\n\nHow about Paris?\n\nOf course, there are many ways to optimize the code used here. Still, the idea is to explore the considerable potential of function calling with SLMs at the edge, allowing those models to integrate with external functions or APIs. Going beyond text generation, SLMs can access real-time data, automate tasks, and interact with various systems."
  },
  {
    "objectID": "raspi/llm/llm.html#slms-optimization-techniques",
    "href": "raspi/llm/llm.html#slms-optimization-techniques",
    "title": "Small Language Models (SLM)",
    "section": "SLMs: Optimization Techniques",
    "text": "SLMs: Optimization Techniques\nLarge Language Models (LLMs) have revolutionized natural language processing, but their deployment and optimization come with unique challenges. One significant issue is the tendency for LLMs (and more, the SLMs) to generate plausible-sounding but factually incorrect information, a phenomenon known as hallucination. This occurs when models produce content that seems coherent but is not grounded in truth or real-world facts.\nOther challenges include the immense computational resources required for training and running these models, the difficulty in maintaining up-to-date knowledge within the model, and the need for domain-specific adaptations. Privacy concerns also arise when handling sensitive data during training or inference. Additionally, ensuring consistent performance across diverse tasks and maintaining ethical use of these powerful tools present ongoing challenges. Addressing these issues is crucial for the effective and responsible deployment of LLMs in real-world applications.\nThe fundamental techniques for enhancing LLM (and SLM) performance and efficiency are Fine-tuning, Prompt engineering, and Retrieval-Augmented Generation (RAG).\n\nFine-tuning, while more resource-intensive, offers a way to specialize LLMs for particular domains or tasks. This process involves further training the model on carefully curated datasets, allowing it to adapt its vast general knowledge to specific applications. Fine-tuning can lead to substantial improvements in performance, especially in specialized fields or for unique use cases.\nPrompt engineering is at the forefront of LLM optimization. By carefully crafting input prompts, we can guide models to produce more accurate and relevant outputs. This technique involves structuring queries that leverage the model’s pre-trained knowledge and capabilities, often incorporating examples or specific instructions to shape the desired response.\nRetrieval-Augmented Generation (RAG) represents another powerful approach to improving LLM performance. This method combines the vast knowledge embedded in pre-trained models with the ability to access and incorporate external, up-to-date information. By retrieving relevant data to supplement the model’s decision-making process, RAG can significantly enhance accuracy and reduce the likelihood of generating outdated or false information.\n\nFor edge applications, it is more beneficial to focus on techniques like RAG that can enhance model performance without needing on-device fine-tuning. Let’s explore it."
  },
  {
    "objectID": "raspi/llm/llm.html#rag-implementation",
    "href": "raspi/llm/llm.html#rag-implementation",
    "title": "Small Language Models (SLM)",
    "section": "RAG Implementation",
    "text": "RAG Implementation\nIn a basic interaction between a user and a language model, the user asks a question, which is sent as a prompt to the model. The model generates a response based solely on its pre-trained knowledge. In a RAG process, there’s an additional step between the user’s question and the model’s response. The user’s question triggers a retrieval process from a knowledge base.\n\n\nA simple RAG project\nHere are the steps to implement a basic Retrieval Augmented Generation (RAG):\n\nDetermine the type of documents you’ll be using: The best types are documents from which we can get clean and unobscured text. PDFs can be problematic because they are designed for printing, not for extracting sensible text. To work with PDFs, we should get the source document or use tools to handle it.\nChunk the text: We can’t store the text as one long stream because of context size limitations and the potential for confusion. Chunking involves splitting the text into smaller pieces. Chunk text has many ways, such as character count, tokens, words, paragraphs, or sections. It is also possible to overlap chunks.\nCreate embeddings: Embeddings are numerical representations of text that capture semantic meaning. We create embeddings by passing each chunk of text through a particular embedding model. The model outputs a vector, the length of which depends on the embedding model used. We should pull one (or more) embedding models from Ollama, to perform this task. Here are some examples of embedding models available at Ollama.\n\n\n\nModel\nParameter Size\nEmbedding Size\n\n\n\n\nmxbai-embed-large\n334M\n1024\n\n\nnomic-embed-text\n137M\n768\n\n\nall-minilm\n23M\n384\n\n\n\n\nGenerally, larger embedding sizes capture more nuanced information about the input. Still, they also require more computational resources to process, and a higher number of parameters should increase the latency (but also the quality of the response).\n\nStore the chunks and embeddings in a vector database: We will need a way to efficiently find the most relevant chunks of text for a given prompt, which is where a vector database comes in. We will use Chromadb, an AI-native open-source vector database, which simplifies building RAGs by creating knowledge, facts, and skills pluggable for LLMs. Both the embedding and the source text for each chunk are stored.\nBuild the prompt: When we have a question, we create an embedding and query the vector database for the most similar chunks. Then, we select the top few results and include their text in the prompt.\n\nThe goal of RAG is to provide the model with the most relevant information from our documents, allowing it to generate more accurate and informative responses. So, let’s implement a simple example of an SLM incorporating a particular set of facts about bees (“Bee Facts”).\nInside the ollama env, enter the command in the terminal for Chromadb instalation:\npip install ollama chromadb\nLet’s pull an intermediary embedding model, nomic-embed-text\nollama pull nomic-embed-text\nAnd create a working directory:\ncd Documents/OLLAMA/\nmkdir RAG-simple-bee\ncd RAG-simple-bee/\nLet’s create a new Jupyter notebook, 40-RAG-simple-bee for some exploration:\nImport the needed libraries:\nimport ollama\nimport chromadb\nimport time\nAnd define aor models:\nEMB_MODEL = \"nomic-embed-text\"\nMODEL = 'llama3.2:3B'\nInitially, a knowledge base about bee facts should be created. This involves collecting relevant documents and converting them into vector embeddings. These embeddings are then stored in a vector database, allowing for efficient similarity searches later. Enter with the “document,” a base of “bee facts” as a list:\ndocuments = [\n    \"Bee-keeping, also known as apiculture, involves the maintenance of bee \\\n    colonies, typically in hives, by humans.\",\n    \"The most commonly kept species of bees is the European honey bee (Apis \\\n    mellifera).\",\n    \n    ...\n    \n    \"There are another 20,000 different bee species in the world.\",  \n    \"Brazil alone has more than 300 different bee species, and the \\\n    vast majority, unlike western honey bees, don’t sting.\", \n    \"Reports written in 1577 by Hans Staden, mention three native bees \\\n    used by indigenous people in Brazil.\",\n    \"The indigenous people in Brazil used bees for medicine and food purposes\",\n    \"From Hans Staden report: probable species: mandaçaia (Melipona \\\n    quadrifasciata), mandaguari (Scaptotrigona postica) and jataí-amarela \\\n    (Tetragonisca angustula).\"\n]\n\nWe do not need to “chunk” the document here because we will use each element of the list and a chunk.\n\nNow, we will create our vector embedding database bee_facts and store the document in it:\nclient = chromadb.Client()\ncollection = client.create_collection(name=\"bee_facts\")\n\n# store each document in a vector embedding database\nfor i, d in enumerate(documents):\n  response = ollama.embeddings(model=EMB_MODEL, prompt=d)\n  embedding = response[\"embedding\"]\n  collection.add(\n    ids=[str(i)],\n    embeddings=[embedding],\n    documents=[d]\n  )\nNow that we have our “Knowledge Base” created, we can start making queries, retrieving data from it:\n\nUser Query: The process begins when a user asks a question, such as “How many bees are in a colony? Who lays eggs, and how much? How about common pests and diseases?”\nprompt = \"How many bees are in a colony? Who lays eggs and how much? How about\\\n          common pests and diseases?\"\nQuery Embedding: The user’s question is converted into a vector embedding using the same embedding model used for the knowledge base.\nresponse = ollama.embeddings(\n  prompt=prompt,\n  model=EMB_MODEL\n)\nRelevant Document Retrieval: The system searches the knowledge base using the query embedding to find the most relevant documents (in this case, the 5 more probable). This is done using a similarity search, which compares the query embedding to the document embeddings in the database.\nresults = collection.query(\n  query_embeddings=[response[\"embedding\"]],\n  n_results=5\n)\ndata = results['documents']\nPrompt Augmentation: The retrieved relevant information is combined with the original user query to create an augmented prompt. This prompt now contains the user’s question and pertinent facts from the knowledge base.\nprompt=f\"Using this data: {data}. Respond to this prompt: {prompt}\",\nAnswer Generation: The augmented prompt is then fed into a language model, in this case, the llama3.2:3b model. The model uses this enriched context to generate a comprehensive answer. Parameters like temperature, top_k, and top_p are set to control the randomness and quality of the generated response.\noutput = ollama.generate(\n  model=MODEL,\n  prompt=f\"Using this data: {data}. Respond to this prompt: {prompt}\",\n  options={\n    \"temperature\": 0.0,\n    \"top_k\":10,\n    \"top_p\":0.5                          }\n)\nResponse Delivery: Finally, the system returns the generated answer to the user.\nprint(output['response'])\nBased on the provided data, here are the answers to your questions:\n\n1. How many bees are in a colony?\nA typical bee colony can contain between 20,000 and 80,000 bees.\n\n2. Who lays eggs and how much?\nThe queen bee lays up to 2,000 eggs per day during peak seasons.\n\n3. What about common pests and diseases?\nCommon pests and diseases that affect bees include varroa mites, hive beetles,\nand foulbrood.\nLet’s create a function to help answer new questions:\ndef rag_bees(prompt, n_results=5, temp=0.0, top_k=10, top_p=0.5):\n    start_time = time.perf_counter()  # Start timing\n    \n    # generate an embedding for the prompt and retrieve the data \n    response = ollama.embeddings(\n      prompt=prompt,\n      model=EMB_MODEL\n    )\n    \n    results = collection.query(\n      query_embeddings=[response[\"embedding\"]],\n      n_results=n_results\n    )\n    data = results['documents']\n    \n    # generate a response combining the prompt and data retrieved\n    output = ollama.generate(\n      model=MODEL,\n      prompt=f\"Using this data: {data}. Respond to this prompt: {prompt}\",\n      options={\n        \"temperature\": temp,\n        \"top_k\": top_k,\n        \"top_p\": top_p                          }\n    )\n    \n    print(output['response'])\n    \n    end_time = time.perf_counter()  # End timing\n    elapsed_time = round((end_time - start_time), 1)  # Calculate elapsed time\n    \n    print(f\"\\n [INFO] ==&gt; The code for model: {MODEL}, took {elapsed_time}s \\\n          to generate the answer.\\n\")\nWe can now create queries and call the function:\nprompt = \"Are bees in Brazil?\"\nrag_bees(prompt)\nYes, bees are found in Brazil. According to the data, Brazil has more than 300\ndifferent bee species, and indigenous people in Brazil used bees for medicine and\nfood purposes. Additionally, reports from 1577 mention three native bees used by\nindigenous people in Brazil.\n\n [INFO] ==&gt; The code for model: llama3.2:3b, took 22.7s to generate the answer.\nBy the way, if the model used supports multiple languages, we can use it (for example, Portuguese), even if the dataset was created in English:\nprompt = \"Existem abelhas no Brazil?\"\nrag_bees(prompt)\nSim, existem abelhas no Brasil! De acordo com o relato de Hans Staden, há três \nespécies de abelhas nativas do Brasil que foram mencionadas: mandaçaia (Melipona\nquadrifasciata), mandaguari (Scaptotrigona postica) e jataí-amarela (Tetragonisca\nangustula). Além disso, o Brasil é conhecido por ter mais de 300 espécies diferentes de abelhas, a maioria das quais não é agressiva e não põe veneno.\n\n [INFO] ==&gt; The code for model: llama3.2:3b, took 54.6s to generate the answer.\n\n\nGoing Further\nThe small LLM models tested worked well at the edge, both in text and with images, but of course, they had high latency regarding the last one. A combination of specific and dedicated models can lead to better results; for example, in real cases, an Object Detection model (such as YOLO) can get a general description and count of objects on an image that, once passed to an LLM, can help extract essential insights and actions.\nAccording to Avi Baum, CTO at Hailo,\n\nIn the vast landscape of artificial intelligence (AI), one of the most intriguing journeys has been the evolution of AI on the edge. This journey has taken us from classic machine vision to the realms of discriminative AI, enhancive AI, and now, the groundbreaking frontier of generative AI. Each step has brought us closer to a future where intelligent systems seamlessly integrate with our daily lives, offering an immersive experience of not just perception but also creation at the palm of our hand."
  },
  {
    "objectID": "raspi/llm/llm.html#conclusion",
    "href": "raspi/llm/llm.html#conclusion",
    "title": "Small Language Models (SLM)",
    "section": "Conclusion",
    "text": "Conclusion\nThis lab has demonstrated how a Raspberry Pi 5 can be transformed into a potent AI hub capable of running large language models (LLMs) for real-time, on-site data analysis and insights using Ollama and Python. The Raspberry Pi’s versatility and power, coupled with the capabilities of lightweight LLMs like Llama 3.2 and LLaVa-Phi-3-mini, make it an excellent platform for edge computing applications.\nThe potential of running LLMs on the edge extends far beyond simple data processing, as in this lab’s examples. Here are some innovative suggestions for using this project:\n1. Smart Home Automation:\n\nIntegrate SLMs to interpret voice commands or analyze sensor data for intelligent home automation. This could include real-time monitoring and control of home devices, security systems, and energy management, all processed locally without relying on cloud services.\n\n2. Field Data Collection and Analysis:\n\nDeploy SLMs on Raspberry Pi in remote or mobile setups for real-time data collection and analysis. This can be used in agriculture to monitor crop health, in environmental studies for wildlife tracking, or in disaster response for situational awareness and resource management.\n\n3. Educational Tools:\n\nCreate interactive educational tools that leverage SLMs to provide instant feedback, language translation, and tutoring. This can be particularly useful in developing regions with limited access to advanced technology and internet connectivity.\n\n4. Healthcare Applications:\n\nUse SLMs for medical diagnostics and patient monitoring. They can provide real-time analysis of symptoms and suggest potential treatments. This can be integrated into telemedicine platforms or portable health devices.\n\n5. Local Business Intelligence:\n\nImplement SLMs in retail or small business environments to analyze customer behavior, manage inventory, and optimize operations. The ability to process data locally ensures privacy and reduces dependency on external services.\n\n6. Industrial IoT:\n\nIntegrate SLMs into industrial IoT systems for predictive maintenance, quality control, and process optimization. The Raspberry Pi can serve as a localized data processing unit, reducing latency and improving the reliability of automated systems.\n\n7. Autonomous Vehicles:\n\nUse SLMs to process sensory data from autonomous vehicles, enabling real-time decision-making and navigation. This can be applied to drones, robots, and self-driving cars for enhanced autonomy and safety.\n\n8. Cultural Heritage and Tourism:\n\nImplement SLMs to provide interactive and informative cultural heritage sites and museum guides. Visitors can use these systems to get real-time information and insights, enhancing their experience without internet connectivity.\n\n9. Artistic and Creative Projects:\n\nUse SLMs to analyze and generate creative content, such as music, art, and literature. This can foster innovative projects in the creative industries and allow for unique interactive experiences in exhibitions and performances.\n\n10. Customized Assistive Technologies:\n\nDevelop assistive technologies for individuals with disabilities, providing personalized and adaptive support through real-time text-to-speech, language translation, and other accessible tools."
  },
  {
    "objectID": "raspi/llm/llm.html#resources",
    "href": "raspi/llm/llm.html#resources",
    "title": "Small Language Models (SLM)",
    "section": "Resources",
    "text": "Resources\n\n10-Ollama_Python_Library notebook\n20-Ollama_Function_Calling notebook\n30-Function_Calling_with_images notebook\n40-RAG-simple-bee notebook\ncalc_distance_image python script"
  },
  {
    "objectID": "raspi/vlm/vlm.html#why-florence-2-at-the-edge",
    "href": "raspi/vlm/vlm.html#why-florence-2-at-the-edge",
    "title": "Vision-Language Models at the Edge",
    "section": "Why Florence-2 at the Edge?",
    "text": "Why Florence-2 at the Edge?\nFlorence-2 is a vision-language model open-sourced by Microsoft under the MIT license, which significantly advances vision-language models by combining a lightweight architecture with robust capabilities. Thanks to its training on the massive FLD-5B dataset, which contains 126 million images and 5.4 billion visual annotations, it achieves performance comparable to larger models. This makes Florence-2 ideal for deployment at the edge, where power and computational resources are limited.\nIn this tutorial, we will explore how to use Florence-2 for real-time computer vision applications, such as:\n\nImage captioning\nObject detection\nSegmentation\nVisual grounding\n\n\nVisual grounding involves linking textual descriptions to specific regions within an image. This enables the model to understand where particular objects or entities described in a prompt are in the image. For example, if the prompt is “a red car,” the model will identify and highlight the region where the red car is found in the image. Visual grounding is helpful for applications where precise alignment between text and visual content is needed, such as human-computer interaction, image annotation, and interactive AI systems.\n\nIn the tutorial, we will walk through:\n\nSetting up Florence-2 on the Raspberry Pi\nRunning inference tasks such as object detection and captioning\nOptimizing the model to get the best performance from the edge device\nExploring practical, real-world applications with fine-tuning."
  },
  {
    "objectID": "raspi/vlm/vlm.html#florence-2-model-architecture",
    "href": "raspi/vlm/vlm.html#florence-2-model-architecture",
    "title": "Vision-Language Models at the Edge",
    "section": "Florence-2 Model Architecture",
    "text": "Florence-2 Model Architecture\nFlorence-2 utilizes a unified, prompt-based representation to handle various vision-language tasks. The model architecture consists of two main components: an image encoder and a multi-modal transformer encoder-decoder.\n\n\nImage Encoder: The image encoder is based on the DaViT (Dual Attention Vision Transformers) architecture. It converts input images into a series of visual token embeddings. These embeddings serve as the foundational representations of the visual content, capturing both spatial and contextual information about the image.\nMulti-Modal Transformer Encoder-Decoder: Florence-2’s core is the multi-modal transformer encoder-decoder, which combines visual token embeddings from the image encoder with textual embeddings generated by a BERT-like model. This combination allows the model to simultaneously process visual and textual inputs, enabling a unified approach to tasks such as image captioning, object detection, and segmentation.\n\nThe model’s training on the extensive FLD-5B dataset ensures it can effectively handle diverse vision tasks without requiring task-specific modifications. Florence-2 uses textual prompts to activate specific tasks, making it highly flexible and capable of zero-shot generalization. For tasks like object detection or visual grounding, the model incorporates additional location tokens to represent regions within the image, ensuring a precise understanding of spatial relationships.\n\nFlorence-2’s compact architecture and innovative training approach allow it to perform computer vision tasks accurately, even on resource-constrained devices like the Raspberry Pi."
  },
  {
    "objectID": "raspi/vlm/vlm.html#technical-overview",
    "href": "raspi/vlm/vlm.html#technical-overview",
    "title": "Vision-Language Models at the Edge",
    "section": "Technical Overview",
    "text": "Technical Overview\nFlorence-2 introduces several innovative features that set it apart:\n\nArchitecture\n\n\nLightweight Design: Two variants available\n\nFlorence-2-Base: 232 million parameters\nFlorence-2-Large: 771 million parameters\n\nUnified Representation: Handles multiple vision tasks through a single architecture\nDaViT Vision Encoder: Converts images into visual token embeddings\nTransformer-based Multi-modal Encoder-Decoder: Processes combined visual and text embeddings\n\n\n\nTraining Dataset (FLD-5B)\n\n\n126 million unique images\n5.4 billion comprehensive annotations, including:\n\n500M text annotations\n1.3B region-text annotations\n3.6B text-phrase-region annotations\n\nAutomated annotation pipeline using specialist models\nIterative refinement process for high-quality labels\n\n\n\nKey Capabilities\nFlorence-2 excels in multiple vision tasks:\n\nZero-shot Performance\n\nImage Captioning: Achieves 135.6 CIDEr score on COCO\nVisual Grounding: 84.4% recall@1 on Flickr30k\nObject Detection: 37.5 mAP on COCO val2017\nReferring Expression: 67.0% accuracy on RefCOCO\n\n\n\nFine-tuned Performance\n\nCompetitive with specialist models despite the smaller size\nOutperforms larger models in specific benchmarks\nEfficient adaptation to new tasks\n\n\n\n\nPractical Applications\nFlorence-2 can be applied across various domains:\n\nContent Understanding\n\nAutomated image captioning for accessibility\nVisual content moderation\nMedia asset management\n\nE-commerce\n\nProduct image analysis\nVisual search\nAutomated product tagging\n\nHealthcare\n\nMedical image analysis\nDiagnostic assistance\nResearch data processing\n\nSecurity & Surveillance\n\nObject detection and tracking\nAnomaly detection\nScene understanding\n\n\n\n\nComparing Florence-2 with other VLMs\nFlorence-2 stands out from other visual language models due to its impressive zero-shot capabilities. Unlike models like Google PaliGemma, which rely on extensive fine-tuning to adapt to various tasks, Florence-2 works right out of the box, as we will see in this lab. It can also compete with larger models like GPT-4V and Flamingo, which often have many more parameters but only sometimes match Florence-2’s performance. For example, Florence-2 achieves better zero-shot results than Kosmos-2 despite having over twice the parameters.\nIn benchmark tests, Florence-2 has shown remarkable performance in tasks like COCO captioning and referring expression comprehension. It outperformed models like PolyFormer and UNINEXT in object detection and segmentation tasks on the COCO dataset. It is a highly competitive choice for real-world applications where both performance and resource efficiency are crucial."
  },
  {
    "objectID": "raspi/vlm/vlm.html#setup-and-installation",
    "href": "raspi/vlm/vlm.html#setup-and-installation",
    "title": "Vision-Language Models at the Edge",
    "section": "Setup and Installation",
    "text": "Setup and Installation\nOur choice of edge device is the Raspberry Pi 5 (Raspi-5). Its robust platform is equipped with the Broadcom BCM2712, a 2.4GHz quad-core 64-bit Arm Cortex-A76 CPU featuring Cryptographic Extension and enhanced caching capabilities. It boasts a VideoCore VII GPU, dual 4Kp60 HDMI® outputs with HDR, and a 4Kp60 HEVC decoder. Memory options include 4GB and 8GB of high-speed LPDDR4X SDRAM, with 8GB being our choice to run Florence-2. It also features expandable storage via a microSD card slot and a PCIe 2.0 interface for fast peripherals such as M.2 SSDs (Solid State Drives).\n\nFor real applications, SSDs are a better option than SD cards.\n\nWe suggest installing an Active Cooler, a dedicated clip-on cooling solution for Raspberry Pi 5 (Raspi-5), for this lab. It combines an aluminum heatsink with a temperature-controlled blower fan to keep the Raspi-5 operating comfortably under heavy loads, such as running Florense-2.\n\n\nEnvironment configuration\nTo run Microsoft Florense-2 on the Raspberry Pi 5, we’ll need a few libraries:\n\nTransformers:\n\nFlorence-2 uses the transformers library from Hugging Face for model loading and inference. This library provides the architecture for working with pre-trained vision-language models, making it easy to perform tasks like image captioning, object detection, and more. Essentially, transformers helps in interacting with the model, processing input prompts, and obtaining outputs.\n\nPyTorch:\n\nPyTorch is a deep learning framework that provides the infrastructure needed to run the Florence-2 model, which includes tensor operations, GPU acceleration (if a GPU is available), and model training/inference functionalities. The Florence-2 model is trained in PyTorch, and we need it to leverage its functions, layers, and computation capabilities to perform inferences on the Raspberry Pi.\n\nTimm (PyTorch Image Models):\n\nFlorence-2 uses timm to access efficient implementations of vision models and pre-trained weights. Specifically, the timm library is utilized for the image encoder part of Florence-2, particularly for managing the DaViT architecture. It provides model definitions and optimized code for common vision tasks and allows the easy integration of different backbones that are lightweight and suitable for edge devices.\n\nEinops:\n\nEinops is a library for flexible and powerful tensor operations. It makes it easy to reshape and manipulate tensor dimensions, which is especially important for the multi-modal processing done in Florence-2. Vision-language models like Florence-2 often need to rearrange image data, text embeddings, and visual embeddings to align correctly for the transformer blocks, and einops simplifies these complex operations, making the code more readable and concise.\n\n\nIn short, these libraries enable different essential components of Florence-2:\n\nTransformers and PyTorch are needed to load the model and run the inference.\nTimm is used to access and efficiently implement the vision encoder.\nEinops helps reshape data, facilitating the integration of visual and text features.\n\nAll these components work together to help Florence-2 run seamlessly on our Raspberry Pi, allowing it to perform complex vision-language tasks relatively quickly.\nConsidering that the Raspberry Pi already has its OS installed, let’s use SSH to reach it from another computer:\nssh mjrovai@raspi-5.local\nAnd check the IP allocated to it:\nhostname -I\n192.168.4.209\n\nUpdating the Raspberry Pi\nFirst, ensure your Raspberry Pi is up to date:\nsudo apt update\nsudo apt upgrade -y\nInitial setup for using PIP:\nsudo apt install python3-pip\nsudo rm /usr/lib/python3.11/EXTERNALLY-MANAGED\npip3 install --upgrade pip\nInstall Dependencies\nsudo apt-get install libjpeg-dev libopenblas-dev libopenmpi-dev libomp-dev\nLet’s set up and activate a Virtual Environment for working with Florence-2:\npython3 -m venv ~/florence\nsource ~/florence/bin/activate\nInstall PyTorch\npip3 install setuptools numpy Cython\npip3 install requests\npip3 install torch torchvision --index-url https://download.pytorch.org/whl/cpu\npip3 install torchaudio --index-url https://download.pytorch.org/whl/cpu\nLet’s verify that PyTorch is correctly installed:\n\nInstall Transformers, Timm and Einops:\npip3 install transformers\npip3 install timm einops\nInstall the model:\npip3 install autodistill-florence-2\nJupyter Notebook and Python libraries\nInstalling a Jupyter Notebook to run and test our Python scripts is possible.\npip3 install jupyter\npip3 install numpy Pillow matplotlib\njupyter notebook --generate-config\n\n\nTesting the installation\nRunning the Jupyter Notebook on the remote computer\njupyter notebook --ip=192.168.4.209 --no-browser\nRunning the above command on the SSH terminal, we can see the local URL address to open the notebook:\n\nThe notebook with the code used on this initial test can be found on the Lab GitHub:\n\n10-florence2_test.ipynb\n\nWe can access it on the remote computer by entering the Raspberry Pi’s IP address and the provided token in a web browser ( copy the entire URL from the terminal).\nFrom the Home page, create a new notebook [Python 3 (ipykernel) ] and copy and paste the example code from Hugging Face Hub.\nThe code is designed to run Florence-2 on a given image to perform object detection. It loads the model, processes an image and a prompt, and then generates a response to identify and describe the objects in the image.\n\nThe processor helps prepare text and image inputs.\nThe model takes the processed inputs to generate a meaningful response.\nThe post-processing step refines the generated output into a more interpretable form, like bounding boxes for detected objects.\n\n\nThis workflow leverages the versatility of Florence-2 to handle vision-language tasks and is implemented efficiently using PyTorch, Transformers, and related image-processing tools.\n\nimport requests\nfrom PIL import Image\nimport torch\nfrom transformers import AutoProcessor, AutoModelForCausalLM \n\ndevice = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\ntorch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32\n\nmodel = AutoModelForCausalLM.from_pretrained(\"microsoft/Florence-2-base\",\n                                             torch_dtype=torch_dtype, \n                                             trust_remote_code=True).to(device)\nprocessor = AutoProcessor.from_pretrained(\"microsoft/Florence-2-base\", \n                                          trust_remote_code=True)\n\nprompt = \"&lt;OD&gt;\"\n\nurl = \"https://huggingface.co/datasets/huggingface/documentation-\nimages/resolve/main/transformers/tasks/car.jpg?download=true\"\nimage = Image.open(requests.get(url, stream=True).raw)\n\ninputs = processor(text=prompt, images=image, return_tensors=\"pt\").to(\n  device, torch_dtype)\n\ngenerated_ids = model.generate(\n    input_ids=inputs[\"input_ids\"],\n    pixel_values=inputs[\"pixel_values\"],\n    max_new_tokens=1024,\n    do_sample=False,\n    num_beams=3,\n)\ngenerated_text = processor.batch_decode(generated_ids, skip_special_tokens=False)[0]\n\nparsed_answer = processor.post_process_generation(generated_text, task=\"&lt;OD&gt;\", \n                                                  image_size=(image.width, \n                                                              image.height))\n\nprint(parsed_answer)\nLet’s break down the provided code step by step:\n\n1. Importing Required Libraries\nimport requests\nfrom PIL import Image\nimport torch\nfrom transformers import AutoProcessor, AutoModelForCausalLM \n\nrequests: Used to make HTTP requests. In this case, it downloads an image from a URL.\nPIL (Pillow): Provides tools for manipulating images. Here, it’s used to open the downloaded image.\ntorch: PyTorch is imported to handle tensor operations and determine the hardware availability (CPU or GPU).\ntransformers: This module provides easy access to Florence-2 by using AutoProcessor and AutoModelForCausalLM to load pre-trained models and process inputs.\n\n\n\n2. Determining the Device and Data Type\ndevice = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\ntorch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32\n\nDevice Setup: The code checks if a CUDA-enabled GPU is available (torch.cuda.is_available()). The device is set to “cuda:0” if a GPU is available. Otherwise, it defaults to \"cpu\" (our case here).\nData Type Setup: If a GPU is available, torch.float16 is chosen, which uses half-precision floats to speed up processing and reduce memory usage. On the CPU, it defaults to torch.float32 to maintain compatibility.\n\n\n\n3. Loading the Model and Processor\nmodel = AutoModelForCausalLM.from_pretrained(\"microsoft/Florence-2-base\", \n                                             torch_dtype=torch_dtype,\n                                             trust_remote_code=True).to(device)\nprocessor = AutoProcessor.from_pretrained(\"microsoft/Florence-2-base\",\n                                          trust_remote_code=True)\n\nModel Initialization:\n\nAutoModelForCausalLM.from_pretrained() loads the pre-trained Florence-2 model from Microsoft’s repository on Hugging Face. The torch_dtype is set according to the available hardware (GPU/CPU), and trust_remote_code=True allows the use of any custom code that might be provided with the model.\n.to(device) moves the model to the appropriate device (either CPU or GPU). In our case, it will be set to CPU.\n\nProcessor Initialization:\n\nAutoProcessor.from_pretrained() loads the processor for Florence-2. The processor is responsible for transforming text and image inputs into a format the model can work with (e.g., encoding text, normalizing images, etc.).\n\n\n\n\n\n4. Defining the Prompt\nprompt = \"&lt;OD&gt;\"\n\nPrompt Definition: The string \"&lt;OD&gt;\" is used as a prompt. This refers to “Object Detection”, instructing the model to detect objects on the image.\n\n\n5. Downloading and Loading the Image\nurl = \"https://huggingface.co/datasets/huggingface/documentation-\\\nimages/resolve/main/transformers/tasks/car.jpg?download=true\"\nimage = Image.open(requests.get(url, stream=True).raw)\n\nDownloading the Image: The requests.get() function fetches the image from the specified URL. The stream=True parameter ensures the image is streamed rather than downloaded completely at once.\nOpening the Image: Image.open() opens the image so the model can process it.\n\n\n\n6. Processing Inputs\ninputs = processor(text=prompt, images=image, return_tensors=\"pt\").to(device, \n                                                                      torch_dtype)\n\nProcessing Input Data: The processor() function processes the text (prompt) and the image (image). The return_tensors=\"pt\" argument converts the processed data into PyTorch tensors, which are necessary for inputting data into the model.\nMoving Inputs to Device: .to(device, torch_dtype) moves the inputs to the correct device (CPU or GPU) and assigns the appropriate data type.\n\n\n\n\n7. Generating the Output\ngenerated_ids = model.generate(\n    input_ids=inputs[\"input_ids\"],\n    pixel_values=inputs[\"pixel_values\"],\n    max_new_tokens=1024,\n    do_sample=False,\n    num_beams=3,\n)\n\nModel Generation: model.generate() is used to generate the output based on the input data.\n\ninput_ids: Represents the tokenized form of the prompt.\npixel_values: Contains the processed image data.\nmax_new_tokens=1024: Specifies the maximum number of new tokens to be generated in the response. This limits the response length.\ndo_sample=False: Disables sampling; instead, the generation uses deterministic methods (beam search).\nnum_beams=3: Enables beam search with three beams, which improves output quality by considering multiple possibilities during generation.\n\n\n\n8. Decoding the Generated Text\ngenerated_text = processor.batch_decode(generated_ids, skip_special_tokens=False)[0]\n\nBatch Decode: processor.batch_decode() decodes the generated IDs (tokens) into readable text. The skip_special_tokens=False parameter means that the output will include any special tokens that may be part of the response.\n\n\n\n9. Post-processing the Generation\nparsed_answer = processor.post_process_generation(generated_text, task=\"&lt;OD&gt;\", \n                                                  image_size=(image.width, \n                                                              image.height))\n\nPost-Processing: processor.post_process_generation() is called to process the generated text further, interpreting it based on the task (\"&lt;OD&gt;\" for object detection) and the size of the image.\nThis function extracts specific information from the generated text, such as bounding boxes for detected objects, making the output more useful for visual tasks.\n\n\n\n10. Printing the Output\nprint(parsed_answer)\n\nFinally, print(parsed_answer) displays the output, which could include object detection results, such as bounding box coordinates and labels for the detected objects in the image.\n\n\n\nResult\nRunning the code, we get as the Parsed Answer:\n{'&lt;OD&gt;': {'bboxes': [[34.23999786376953, 160.0800018310547, 597.4400024414062, \n371.7599792480469], [272.32000732421875, 241.67999267578125, 303.67999267578125, \n247.4399871826172], [454.0799865722656, 276.7200012207031, 553.9199829101562, \n370.79998779296875], [96.31999969482422, 280.55999755859375, 198.0800018310547, \n371.2799987792969]], 'labels': ['car', 'door handle', 'wheel', 'wheel']}}\nFirst, Let’s inspect the image:\nimport matplotlib.pyplot as plt\nplt.figure(figsize=(8, 8))\nplt.imshow(image)\nplt.axis('off')\nplt.show()\n\nBy the Object Detection result, we can see that:\n'labels': ['car', 'door handle', 'wheel', 'wheel']\nIt seems that at least a few objects were detected. we can also implement a code to draw the bounding boxes in the find objects:\ndef plot_bbox(image, data):\n   # Create a figure and axes\n    fig, ax = plt.subplots()\n\n    # Display the image\n    ax.imshow(image)\n\n    # Plot each bounding box\n    for bbox, label in zip(data['bboxes'], data['labels']):\n        # Unpack the bounding box coordinates\n        x1, y1, x2, y2 = bbox\n        # Create a Rectangle patch\n        rect = patches.Rectangle((x1, y1), x2-x1, y2-y1, linewidth=1, \n                                 edgecolor='r', facecolor='none')\n        # Add the rectangle to the Axes\n        ax.add_patch(rect)\n        # Annotate the label\n        plt.text(x1, y1, label, color='white', fontsize=8, \n                 bbox=dict(facecolor='red', alpha=0.5))\n\n    # Remove the axis ticks and labels\n    ax.axis('off')\n\n    # Show the plot\n    plt.show()\n\nBox (x0, y0, x1, y1): Location tokens correspond to the top-left and bottom-right corners of a box.\n\nAnd running\nplot_bbox(image, parsed_answer['&lt;OD&gt;'])\nWe get:"
  },
  {
    "objectID": "raspi/vlm/vlm.html#florence-2-tasks",
    "href": "raspi/vlm/vlm.html#florence-2-tasks",
    "title": "Vision-Language Models at the Edge",
    "section": "Florence-2 Tasks",
    "text": "Florence-2 Tasks\nFlorence-2 is designed to perform a variety of computer vision and vision-language tasks through prompts. These tasks can be activated by providing a specific textual prompt to the model, as we saw with &lt;OD&gt; (Object Detection).\nFlorence-2’s versatility comes from combining these prompts, allowing us to guide the model’s behavior to perform specific vision tasks. Changing the prompt allows us to adapt Florence-2 to different tasks without needing task-specific modifications in the architecture. This capability directly results from Florence-2’s unified model architecture and large-scale multi-task training on the FLD-5B dataset.\nHere are some of the key tasks that Florence-2 can perform, along with example prompts:\n\n1. Object Detection (OD)\n\nPrompt: \"&lt;OD&gt;\"\nDescription: Identifies objects in an image and provides bounding boxes for each detected object. This task is helpful for applications like visual inspection, surveillance, and general object recognition.\n\n\n\n2. Image Captioning\n\nPrompt: \"&lt;CAPTION&gt;\"\nDescription: Generates a textual description for an input image. This task helps the model describe what is happening in the image, providing a human-readable caption for content understanding.\n\n\n\n3. Detailed Captioning\n\nPrompt: \"&lt;DETAILED_CAPTION&gt;\"\nDescription: Generates a more detailed caption with more nuanced information about the scene, such as the objects present and their relationships.\n\n\n\n4. Visual Grounding\n\nPrompt: \"&lt;CAPTION_TO_PHRASE_GROUNDING&gt;\"\nDescription: Links a textual description to specific regions in an image. For example, given a prompt like “a green car,” the model highlights where the red car is in the image. This is useful for human-computer interaction, where you must find specific objects based on text.\n\n\n\n5. Segmentation\n\nPrompt: \"&lt;REFERRING_EXPRESSION_SEGMENTATION&gt;\"\nDescription: Performs segmentation based on a referring expression, such as “the blue cup.” The model identifies and segments the specific region containing the object mentioned in the prompt (all related pixels).\n\n\n\n6. Dense Region Captioning\n\nPrompt: \"&lt;DENSE_REGION_CAPTION&gt;\"\nDescription: Provides captions for multiple regions within an image, offering a detailed breakdown of all visible areas, including different objects and their relationships.\n\n\n\n7. OCR with Region\n\nPrompt: \"&lt;OCR_WITH_REGION&gt;\"\nDescription: Performs Optical Character Recognition (OCR) on an image and provides bounding boxes for the detected text. This is useful for extracting and locating textual information in images, such as reading signs, labels, or other forms of text in images.\n\n\n\n8. Phrase Grounding for Specific Expressions\n\nPrompt: \"&lt;CAPTION_TO_PHRASE_GROUNDING&gt;\" along with a specific expression, such as \"a wine glass\".\nDescription: Locates the area in the image that corresponds to a specific textual phrase. This task allows for identifying particular objects or elements when prompted with a word or keyword.\n\n\n\n9. Open Vocabulary Object Detection\n\nPrompt: \"&lt;OPEN_VOCABULARY_OD&gt;\"\nDescription: The model can detect objects without being restricted to a predefined list of classes, making it helpful in recognizing a broader range of items based on general visual understanding."
  },
  {
    "objectID": "raspi/vlm/vlm.html#exploring-computer-vision-and-vision-language-tasks",
    "href": "raspi/vlm/vlm.html#exploring-computer-vision-and-vision-language-tasks",
    "title": "Vision-Language Models at the Edge",
    "section": "Exploring computer vision and vision-language tasks",
    "text": "Exploring computer vision and vision-language tasks\nFor exploration, all codes can be found on the GitHub:\n\n20-florence_2.ipynb\n\nLet’s use a couple of images created by Dall-E and upload them to the Rasp-5 (FileZilla can be used for that). The images will be saved on a sub-folder named images :\ndogs_cats = Image.open('./images/dogs-cats.jpg')\ntable = Image.open('./images/table.jpg')\n\nLet’s create a function to facilitate our exploration and to keep track of the latency of the model for different tasks:\ndef run_example(task_prompt, text_input=None, image=None):\n    start_time = time.perf_counter()  # Start timing\n    if text_input is None:\n        prompt = task_prompt\n    else:\n        prompt = task_prompt + text_input\n    inputs = processor(text=prompt, images=image, \n                       return_tensors=\"pt\").to(device)\n    generated_ids = model.generate(\n      input_ids=inputs[\"input_ids\"],\n      pixel_values=inputs[\"pixel_values\"],\n      max_new_tokens=1024,\n      early_stopping=False,\n      do_sample=False,\n      num_beams=3,\n    )\n    generated_text = processor.batch_decode(generated_ids, \n                                            skip_special_tokens=False)[0]\n    parsed_answer = processor.post_process_generation(\n        generated_text,\n        task=task_prompt,\n        image_size=(image.width, image.height)\n    )\n    \n    end_time = time.perf_counter()  # End timing\n    elapsed_time = end_time - start_time  # Calculate elapsed time\n    print(f\" \\n[INFO] ==&gt; Florence-2-base ({task_prompt}), \n          took {elapsed_time:.1f} seconds to execute.\\n\")\n    \n    return parsed_answer\n\nCaption\n1. Dogs and Cats\nrun_example(task_prompt='&lt;CAPTION&gt;',image=dogs_cats)\n[INFO] ==&gt; Florence-2-base (&lt;CAPTION&gt;), took 16.1 seconds to execute.\n\n{'&lt;CAPTION&gt;': 'A group of dogs and cats sitting in a garden.'}\n2. Table\nrun_example(task_prompt='&lt;CAPTION&gt;',image=table)\n[INFO] ==&gt; Florence-2-base (&lt;CAPTION&gt;), took 16.5 seconds to execute.\n\n{'&lt;CAPTION&gt;': 'A wooden table topped with a plate of fruit and a glass of wine.'}\n\n\nDETAILED_CAPTION\n1. Dogs and Cats\nrun_example(task_prompt='&lt;DETAILED_CAPTION&gt;',image=dogs_cats)\n[INFO] ==&gt; Florence-2-base (&lt;DETAILED_CAPTION&gt;), took 25.5 seconds to execute.\n\n{'&lt;DETAILED_CAPTION&gt;': 'The image shows a group of cats and dogs sitting on top of a\nlush green field, surrounded by plants with flowers, trees, and a house in the \nbackground. The sky is visible above them, creating a peaceful atmosphere.'}\n2. Table\nrun_example(task_prompt='&lt;DETAILED_CAPTION&gt;',image=table)\n[INFO] ==&gt; Florence-2-base (&lt;DETAILED_CAPTION&gt;), took 26.8 seconds to execute.\n\n{'&lt;DETAILED_CAPTION&gt;': 'The image shows a wooden table with a bottle of wine and a \nglass of wine on it, surrounded by a variety of fruits such as apples, oranges, and \ngrapes. In the background, there are chairs, plants, trees, and a house, all slightly \nblurred.'}\n\n\nMORE_DETAILED_CAPTION\n1. Dogs and Cats\nrun_example(task_prompt='&lt;MORE_DETAILED_CAPTION&gt;',image=dogs_cats)\n[INFO] ==&gt; Florence-2-base (&lt;MORE_DETAILED_CAPTION&gt;), took 49.8 seconds to execute.\n\n{'&lt;MORE_DETAILED_CAPTION&gt;': 'The image shows a group of four cats and a dog in a garden. \nThe garden is filled with colorful flowers and plants, and there is a pathway leading up \nto a house in the background. The main focus of the image is a large German Shepherd dog \nstanding on the left side of the garden, with its tongue hanging out and its mouth open, \nas if it is panting or panting. On the right side, there are two smaller cats, one orange \nand one gray, sitting on the grass. In the background, there is another golden retriever \ndog sitting and looking at the camera. The sky is blue and the sun is shining, creating a \nwarm and inviting atmosphere.'}\n2. Table\nrun_example(task_prompt='&lt; MORE_DETAILED_CAPTION&gt;',image=table)\nINFO] ==&gt; Florence-2-base (&lt;MORE_DETAILED_CAPTION&gt;), took 32.4 seconds to execute.\n\n{'&lt;MORE_DETAILED_CAPTION&gt;': 'The image shows a wooden table with a wooden tray on it. On \nthe tray, there are various fruits such as grapes, oranges, apples, and grapes. There is \nalso a bottle of red wine on the table. The background shows a garden with trees and a \nhouse. The overall mood of the image is peaceful and serene.'}\n\nWe can note that the more detailed the caption task, the longer the latency and the possibility of mistakes (like “The image shows a group of four cats and a dog in a garden”, instead of two dogs and three cats).\n\n\n\nOD - Object Detection\nWe can run the same previous function for object detection using the prompt &lt;OD&gt;.\ntask_prompt = '&lt;OD&gt;'\nresults = run_example(task_prompt,image=dogs_cats)\nprint(results)\nLet’s see the result:\n[INFO] ==&gt; Florence-2-base (&lt;OD&gt;), took 20.9 seconds to execute.\n\n{'&lt;OD&gt;': {'bboxes': [[737.7920532226562, 571.904052734375, 1022.4640502929688, \n980.4800415039062], [0.5120000243186951, 593.4080200195312, 211.4560089111328, \n991.7440185546875], [445.9520263671875, 721.4080200195312, 680.4480590820312, \n850.4320678710938], [39.42400360107422, 91.64800262451172, 491.0080261230469, \n933.3760375976562], [570.8800048828125, 184.83201599121094, 974.3360595703125, \n782.8480224609375]], 'labels': ['cat', 'cat', 'cat', 'dog', 'dog']}}\nOnly by the labels ['cat,' 'cat,' 'cat,' 'dog,' 'dog'] is it possible to see that the main objects in the image were captured. Let’s apply the function used before to draw the bounding boxes:\nplot_bbox(dogs_cats, results['&lt;OD&gt;'])\n\nLet’s also do it with the Table image:\ntask_prompt = '&lt;OD&gt;'\nresults = run_example(task_prompt,image=table)\nplot_bbox(table, results['&lt;OD&gt;'])\n[INFO] ==&gt; Florence-2-base (&lt;OD&gt;), took 40.8 seconds to execute.\n\n\n\nDENSE_REGION_CAPTION\nIt is possible to mix the classic Object Detection with the Caption task in specific sub-regions of the image:\ntask_prompt = '&lt;DENSE_REGION_CAPTION&gt;'\n\nresults = run_example(task_prompt,image=dogs_cats)\nplot_bbox(dogs_cats, results['&lt;DENSE_REGION_CAPTION&gt;'])\n\nresults = run_example(task_prompt,image=table)\nplot_bbox(table, results['&lt;DENSE_REGION_CAPTION&gt;'])\n\n\n\nCAPTION_TO_PHRASE_GROUNDING\nWith this task, we can enter with a caption, such as “a wine glass”, “a wine bottle,” or “a half orange,” and Florence-2 will localize the object in the image:\ntask_prompt = '&lt;CAPTION_TO_PHRASE_GROUNDING&gt;'\n\nresults = run_example(task_prompt, text_input=\"a wine bottle\",image=table)\nplot_bbox(table, results['&lt;CAPTION_TO_PHRASE_GROUNDING&gt;'])\n\nresults = run_example(task_prompt, text_input=\"a wine glass\",image=table)\nplot_bbox(table, results['&lt;CAPTION_TO_PHRASE_GROUNDING&gt;'])\n\nresults = run_example(task_prompt, text_input=\"a half orange\",image=table)\nplot_bbox(table, results['&lt;CAPTION_TO_PHRASE_GROUNDING&gt;'])\n\n[INFO] ==&gt; Florence-2-base (&lt;CAPTION_TO_PHRASE_GROUNDING&gt;), took 15.7 seconds to execute\neach task.\n\n\nCascade Tasks\nWe can also enter the image caption as the input text to push Florence-2 to find more objects:\ntask_prompt = '&lt;CAPTION&gt;'\nresults = run_example(task_prompt,image=dogs_cats)\ntext_input = results[task_prompt]\ntask_prompt = '&lt;CAPTION_TO_PHRASE_GROUNDING&gt;'\nresults = run_example(task_prompt, text_input,image=dogs_cats)\nplot_bbox(dogs_cats, results['&lt;CAPTION_TO_PHRASE_GROUNDING&gt;'])\nChanging the task_prompt among &lt;CAPTION,&gt; &lt;DETAILED_CAPTION&gt; and &lt;MORE_DETAILED_CAPTION&gt;, we will get more objects in the image.\n\n\n\nOPEN_VOCABULARY_DETECTION\n&lt;OPEN_VOCABULARY_DETECTION&gt; allows Florence-2 to detect recognizable objects in an image without relying on a predefined list of categories, making it a versatile tool for identifying various items that may not have been explicitly labeled during training. Unlike &lt;CAPTION_TO_PHRASE_GROUNDING&gt;, which requires a specific text phrase to locate and highlight a particular object in an image, &lt;OPEN_VOCABULARY_DETECTION&gt; performs a broad scan to find and classify all objects present.\nThis makes &lt;OPEN_VOCABULARY_DETECTION&gt; particularly useful for applications where you need a comprehensive overview of everything in an image without prior knowledge of what to expect. Enter with a text describing specific objects not previously detected, resulting in their detection. For example:\ntask_prompt = '&lt;OPEN_VOCABULARY_DETECTION&gt;'\ntext = [\"a house\", \"a tree\", \"a standing cat at the left\", \n        \"a sleeping cat on the ground\", \"a standing cat at the right\", \n        \"a yellow cat\"]\nfor txt in text:\n    results = run_example(task_prompt, text_input=txt,image=dogs_cats)\n    bbox_results  = convert_to_od_format(results['&lt;OPEN_VOCABULARY_DETECTION&gt;'])\n    plot_bbox(dogs_cats, bbox_results)\n\n[INFO] ==&gt; Florence-2-base (&lt;OPEN_VOCABULARY_DETECTION&gt;), took 15.1 seconds to execute \neach task.\n\nNote: Trying to use Florence-2 to find objects that were not found can leads to mistakes (see exaamples on the Notebook).\n\n\n\nReferring expression segmentation\nWe can also segment a specific object in the image and give its description (caption), such as “a wine bottle” on the table image or “a German Sheppard” on the dogs_cats.\nReferring expression segmentation results format: {'&lt;REFERRING_EXPRESSION_SEGMENTATION&gt;': {'Polygons': [[[polygon]], ...], 'labels': ['', '', ...]}}, one object is represented by a list of polygons. each polygon is [x1, y1, x2, y2, ..., xn, yn].\n\nPolygon (x1, y1, …, xn, yn): Location tokens represent the vertices of a polygon in clockwise order.\n\nSo, let’s first create a function to plot the segmentation:\nfrom PIL import Image, ImageDraw, ImageFont\nimport copy\nimport random\nimport numpy as np\ncolormap = ['blue','orange','green','purple','brown','pink','gray','olive',\n    'cyan','red','lime','indigo','violet','aqua','magenta','coral','gold',\n    'tan','skyblue']\n\ndef draw_polygons(image, prediction, fill_mask=False):\n    \"\"\"\n    Draws segmentation masks with polygons on an image.\n\n    Parameters:\n    - image_path: Path to the image file.\n    - prediction: Dictionary containing 'polygons' and 'labels' keys.\n                  'polygons' is a list of lists, each containing vertices \n                  of a polygon.\n                  'labels' is a list of labels corresponding to each polygon.\n    - fill_mask: Boolean indicating whether to fill the polygons with color.\n    \"\"\"\n    # Load the image\n\n    draw = ImageDraw.Draw(image)\n\n\n    # Set up scale factor if needed (use 1 if not scaling)\n    scale = 1\n\n    # Iterate over polygons and labels\n    for polygons, label in zip(prediction['polygons'], prediction['labels']):\n        color = random.choice(colormap)\n        fill_color = random.choice(colormap) if fill_mask else None\n\n        for _polygon in polygons:\n            _polygon = np.array(_polygon).reshape(-1, 2)\n            if len(_polygon) &lt; 3:\n                print('Invalid polygon:', _polygon)\n                continue\n\n            _polygon = (_polygon * scale).reshape(-1).tolist()\n\n            # Draw the polygon\n            if fill_mask:\n                draw.polygon(_polygon, outline=color, fill=fill_color)\n            else:\n                draw.polygon(_polygon, outline=color)\n\n            # Draw the label text\n            draw.text((_polygon[0] + 8, _polygon[1] + 2), label, fill=color)\n    \n    # Save or display the image\n    #image.show()  # Display the image\n    display(image)\nNow we can run the functions:\ntask_prompt = '&lt;REFERRING_EXPRESSION_SEGMENTATION&gt;'\n\nresults = run_example(task_prompt, text_input=\"a wine bottle\",image=table)\noutput_image = copy.deepcopy(table)\ndraw_polygons(output_image, \n              results['&lt;REFERRING_EXPRESSION_SEGMENTATION&gt;'], \n              fill_mask=True)\n\nresults = run_example(task_prompt, text_input=\"a german sheppard\",image=dogs_cats)\noutput_image = copy.deepcopy(dogs_cats)\ndraw_polygons(output_image, \n              results['&lt;REFERRING_EXPRESSION_SEGMENTATION&gt;'], \n              fill_mask=True)\n\n[INFO] ==&gt; Florence-2-base (&lt;REFERRING_EXPRESSION_SEGMENTATION&gt;), took 207.0 seconds to \nexecute each task.\n\n\nRegion to Segmentation\nWith this task, it is also possible to give the object coordinates in the image to segment it. The input format is '&lt;loc_x1&gt;&lt;loc_y1&gt;&lt;loc_x2&gt;&lt;loc_y2&gt;', [x1, y1, x2, y2] , which is the quantized coordinates in [0, 999].\nFor example, when running the code:\ntask_prompt = '&lt;CAPTION_TO_PHRASE_GROUNDING&gt;'\nresults = run_example(task_prompt, text_input=\"a half orange\",image=table)\nresults\nThe results were:\n{'&lt;CAPTION_TO_PHRASE_GROUNDING&gt;': {'bboxes': [[343.552001953125,\n    689.6640625,\n    530.9440307617188,\n    873.9840698242188]],\n  'labels': ['a half']}}\nUsing the bboxes rounded coordinates:\ntask_prompt = '&lt;REGION_TO_SEGMENTATION&gt;'\nresults = run_example(task_prompt, \n                      text_input=\"&lt;loc_343&gt;&lt;loc_690&gt;&lt;loc_531&gt;&lt;loc_874&gt;\",\n                      image=table)\noutput_image = copy.deepcopy(table)\ndraw_polygons(output_image, results['&lt;REGION_TO_SEGMENTATION&gt;'], fill_mask=True)  \nWe got the segmentation of the object on those coordinates (Latency: 83 seconds):\n\n\n\nRegion to Texts\nWe can also give the region (coordinates and ask for a caption):\ntask_prompt = '&lt;REGION_TO_CATEGORY&gt;'\nresults = run_example(task_prompt, text_input=\"&lt;loc_343&gt;&lt;loc_690&gt;&lt;loc_531&gt;\n                      &lt;loc_874&gt;\",image=table)\nresults\n[INFO] ==&gt; Florence-2-base (&lt;REGION_TO_CATEGORY&gt;), took 14.3 seconds to execute.\n{'&lt;REGION_TO_CATEGORY&gt;': 'orange&lt;loc_343&gt;&lt;loc_690&gt;&lt;loc_531&gt;&lt;loc_874&gt;'}\nThe model identified an orange in that region. Let’s ask for a description:\ntask_prompt = '&lt;REGION_TO_DESCRIPTION&gt;'\nresults = run_example(task_prompt, text_input=\"&lt;loc_343&gt;&lt;loc_690&gt;&lt;loc_531&gt;\n                      &lt;loc_874&gt;\",image=table)\nresults\n[INFO] ==&gt; Florence-2-base (&lt;REGION_TO_CATEGORY&gt;), took 14.6 seconds to execute.\n\n{'&lt;REGION_TO_CATEGORY&gt;': 'orange&lt;loc_343&gt;&lt;loc_690&gt;&lt;loc_531&gt;&lt;loc_874&gt;'}\nIn this case, the description did not provide more details, but it could. Try another example.\n\n\nOCR\nWith Florence-2, we can perform Optical Character Recognition (OCR) on an image, getting what is written on it (task_prompt = '&lt;OCR&gt;' and also get the bounding boxes (location) for the detected text (ask_prompt = '&lt;OCR_WITH_REGION&gt;'). Those tasks can help extract and locate textual information in images, such as reading signs, labels, or other forms of text in images.\nLet’s upload a flyer from a talk in Brazil to Raspi. Let’s test works in another language, here Portuguese):\nflayer = Image.open('./images/embarcados.jpg')\n# Display the image\nplt.figure(figsize=(8, 8))\nplt.imshow(flayer)\nplt.axis('off')\n#plt.title(\"Image\")\nplt.show()\n\nLet’s examine the image with '&lt;MORE_DETAILED_CAPTION&gt;' :\n[INFO] ==&gt; Florence-2-base (&lt;MORE_DETAILED_CAPTION&gt;), took 85.2 seconds to execute.\n\n{'&lt;MORE_DETAILED_CAPTION&gt;': 'The image is a promotional poster for an event called \n\"Machine Learning Embarcados\" hosted by Marcelo Roval. The poster has a black background\nwith white text. On the left side of the poster, there is a logo of a coffee cup with the\ntext \"Café Com Embarcados\" above it. Below the logo, it says \"25 de Setembro as 17th\" \nwhich translates to \"25th of September as 17\" in English. \\n\\nOn the right side, there \naretwo smaller text boxes with the names of the participants and their names. The first \ntext box reads \"Democratizando a Inteligência Artificial para Paises em Desenvolvimento\" \nand the second text box says \"Toda quarta-feira\" which is Portuguese for \"Transmissão via \nin Portuguese\".\\n\\nIn the center of the image, there has a photo of Marcelo, a man with a \nbeard and glasses, smiling at the camera. He is wearing a white hard hat and a white \nshirt. The text boxes are in orange and yellow colors.'}\nThe description is very accurate. Let’s get to the more important words with the task OCR:\ntask_prompt = '&lt;OCR&gt;'\nrun_example(task_prompt,image=flayer)\n[INFO] ==&gt; Florence-2-base (&lt;OCR&gt;), took 37.7 seconds to execute.\n{'&lt;OCR&gt;': 'Machine LearningCafécomEmbarcadoEmbarcadosDemocratizando a \nInteligênciaArtificial para Paises em25 de Setembro ás 17hDesenvolvimentoToda quarta-\nfeiraMarcelo RovalProfessor na UNIFIEI eTransmissão viainCo-Director do TinyML4D'}\nLet’s locate the words in the flyer:\ntask_prompt = '&lt;OCR_WITH_REGION&gt;'\nresults = run_example(task_prompt,image=flayer)\nLet’s also create a function to draw bounding boxes around the detected words:\ndef draw_ocr_bboxes(image, prediction):\n    scale = 1\n    draw = ImageDraw.Draw(image)\n    bboxes, labels = prediction['quad_boxes'], prediction['labels']\n    for box, label in zip(bboxes, labels):\n        color = random.choice(colormap)\n        new_box = (np.array(box) * scale).tolist()\n        draw.polygon(new_box, width=3, outline=color)\n        draw.text((new_box[0]+8, new_box[1]+2),\n                    \"{}\".format(label),\n                    align=\"right\",\n\n                    fill=color)\n    display(image)\noutput_image = copy.deepcopy(flayer)\ndraw_ocr_bboxes(output_image, results['&lt;OCR_WITH_REGION&gt;'])\n\nWe can inspect the detected words:\nresults['&lt;OCR_WITH_REGION&gt;']['labels']\n'&lt;/s&gt;Machine Learning',\n 'Café',\n 'com',\n 'Embarcado',\n 'Embarcados',\n 'Democratizando a Inteligência',\n 'Artificial para Paises em',\n '25 de Setembro ás 17h',\n 'Desenvolvimento',\n 'Toda quarta-feira',\n 'Marcelo Roval',\n 'Professor na UNIFIEI e',\n 'Transmissão via',\n 'in',\n 'Co-Director do TinyML4D']"
  },
  {
    "objectID": "raspi/vlm/vlm.html#latency-summary",
    "href": "raspi/vlm/vlm.html#latency-summary",
    "title": "Vision-Language Models at the Edge",
    "section": "Latency Summary",
    "text": "Latency Summary\nThe latency observed for different tasks using Florence-2 on the Raspberry Pi (Raspi-5) varied depending on the complexity of the task:\n\nImage Captioning: It took approximately 16-17 seconds to generate a caption for an image.\nDetailed Captioning: Increased latency to around 25-27 seconds, requiring generating more nuanced scene descriptions.\nMore Detailed Captioning: It took about 32-50 seconds, and the latency increased as the description grew more complex.\nObject Detection: It took approximately 20-41 seconds, depending on the image’s complexity and the number of detected objects.\nVisual Grounding: Approximately 15-16 seconds to localize specific objects based on textual prompts.\nOCR (Optical Character Recognition): Extracting text from an image took around 37-38 seconds.\nSegmentation and Region to Segmentation: Segmentation tasks took considerably longer, with a latency of around 83-207 seconds, depending on the complexity and the number of regions to be segmented.\n\nThese latency times highlight the resource constraints of edge devices like the Raspberry Pi and emphasize the need to optimize the model and the environment to achieve real-time performance.\n\n\nRunning complex tasks can use all 8GB of the Raspi-5’s memory. For example, the above screenshot during the Florence OD task shows 4 CPUs at full speed and over 5GB of memory in use. Consider increasing the SWAP memory to 2 GB.\n\nChecking the CPU temperature with vcgencmd measure_temp , showed that temperature can go up to +80oC."
  },
  {
    "objectID": "raspi/vlm/vlm.html#fine-tunning",
    "href": "raspi/vlm/vlm.html#fine-tunning",
    "title": "Vision-Language Models at the Edge",
    "section": "Fine-Tunning",
    "text": "Fine-Tunning\nAs explored in this lab, Florence supports many tasks out of the box, including captioning, object detection, OCR, and more. However, like other pre-trained foundational models, Florence-2 may need domain-specific knowledge. For example, it may need to improve with medical or satellite imagery. In such cases, fine-tuning with a custom dataset is necessary. The Roboflow tutorial, How to Fine-tune Florence-2 for Object Detection Tasks, shows how to fine-tune Florence-2 on object detection datasets to improve model performance for our specific use case.\nBased on the above tutorial, it is possible to fine-tune the Florence-2 model to detect boxes and wheels used in previous labs:\n\nIt is important to note that after fine-tuning, the model can still detect classes that don’t belong to our custom dataset, like cats, dogs, grapes, etc, as seen before).\nThe complete fine-tunning project using a previously annotated dataset in Roboflow and executed on CoLab can be found in the notebook:\n\n30-Finetune_florence_2_on_detection_dataset_box_vs_wheel.ipynb\n\nIn another example, in the post, Fine-tuning Florence-2 - Microsoft’s Cutting-edge Vision Language Models, the authors show an example of fine-tuning Florence on DocVQA. The authors report that Florence 2 can perform visual question answering (VQA), but the released models don’t include VQA capability."
  },
  {
    "objectID": "raspi/vlm/vlm.html#conclusion",
    "href": "raspi/vlm/vlm.html#conclusion",
    "title": "Vision-Language Models at the Edge",
    "section": "Conclusion",
    "text": "Conclusion\nFlorence-2 offers a versatile and powerful approach to vision-language tasks at the edge, providing performance that rivals larger, task-specific models, such as YOLO for object detection, BERT/RoBERTa for text analysis, and specialized OCR models.\nThanks to its multi-modal transformer architecture, Florence-2 is more flexible than YOLO in terms of the tasks it can handle. These include object detection, image captioning, and visual grounding.\nUnlike BERT, which focuses purely on language, Florence-2 integrates vision and language, allowing it to excel in applications that require both modalities, such as image captioning and visual grounding.\nMoreover, while traditional OCR models such as Tesseract and EasyOCR are designed solely for recognizing and extracting text from images, Florence-2’s OCR capabilities are part of a broader framework that includes contextual understanding and visual-text alignment. This makes it particularly useful for scenarios that require both reading text and interpreting its context within images.\nOverall, Florence-2 stands out for its ability to seamlessly integrate various vision-language tasks into a unified model that is efficient enough to run on edge devices like the Raspberry Pi. This makes it a compelling choice for developers and researchers exploring AI applications at the edge.\n\nKey Advantages of Florence-2\n\nUnified Architecture\n\nSingle model handles multiple vision tasks vs. specialized models (YOLO, BERT, Tesseract)\nEliminates the need for multiple model deployments and integrations\nConsistent API and interface across tasks\n\nPerformance Comparison\n\nObject Detection: Comparable to YOLOv8 (~37.5 mAP on COCO vs. YOLOv8’s ~39.7 mAP) despite being general-purpose\nText Recognition: Handles multiple languages effectively like specialized OCR models (Tesseract, EasyOCR)\nLanguage Understanding: Integrates BERT-like capabilities for text processing while adding visual context\n\nResource Efficiency\n\nThe Base model (232M parameters) achieves strong results despite smaller size\nRuns effectively on edge devices (Raspberry Pi)\nSingle model deployment vs. multiple specialized models\n\n\n\n\nTrade-offs\n\nPerformance vs. Specialized Models\n\nYOLO series may offer faster inference for pure object detection\nSpecialized OCR models might handle complex document layouts better\nBERT/RoBERTa provide deeper language understanding for text-only tasks\n\nResource Requirements\n\nHigher latency on edge devices (15-200s depending on task)\nRequires careful memory management on Raspberry Pi\nIt may need optimization for real-time applications\n\nDeployment Considerations\n\nInitial setup is more complex than single-purpose models\nRequires understanding of multiple task types and prompts\nThe learning curve for optimal prompt engineering\n\n\n\n\nBest Use Cases\n\nResource-Constrained Environments\n\nEdge devices requiring multiple vision capabilities\nSystems with limited storage/deployment capacity\nApplications needing flexible vision processing\n\nMulti-modal Applications\n\nContent moderation systems\nAccessibility tools\nDocument analysis workflows\n\nRapid Prototyping\n\nQuick deployment of vision capabilities\nTesting multiple vision tasks without separate models\nProof-of-concept development"
  },
  {
    "objectID": "raspi/vlm/vlm.html#future-implications",
    "href": "raspi/vlm/vlm.html#future-implications",
    "title": "Vision-Language Models at the Edge",
    "section": "Future Implications",
    "text": "Future Implications\nFlorence-2 represents a shift toward unified vision models that could eventually replace task-specific architectures in many applications. While specialized models maintain advantages in specific scenarios, the convenience and efficiency of unified models like Florence-2 make them increasingly attractive for real-world deployments.\nThe lab demonstrates Florence-2’s viability on edge devices, suggesting future IoT, mobile computing, and embedded systems applications where deploying multiple specialized models would be impractical."
  },
  {
    "objectID": "raspi/vlm/vlm.html#resources",
    "href": "raspi/vlm/vlm.html#resources",
    "title": "Vision-Language Models at the Edge",
    "section": "Resources",
    "text": "Resources\n\n10-florence2_test.ipynb\n20-florence_2.ipynb\n30-Finetune_florence_2_on_detection_dataset_box_vs_wheel.ipynb"
  },
  {
    "objectID": "raspi/physical_comp/RPi_Physical_Computing.html#introduction",
    "href": "raspi/physical_comp/RPi_Physical_Computing.html#introduction",
    "title": "Physical Computing with Raspberry Pi",
    "section": "Introduction",
    "text": "Introduction\nPhysical computing creates interactive systems that sense and respond to the analog world. While this field has traditionally focused on direct sensor readings and programmed responses, we’re entering an exciting new era where Large Language Models (LLMs) can add sophisticated decision-making and natural language interaction to physical computing projects.\nIn the Small Language Models (SLM) chapter, we learned how it is possible to run an LLM (or, more precisely, an SLM) in a Single Board Computer (SBC) like the Raspberry Pi. This tutorial will guide us through setting up a Raspberry Pi for physical computing, with an eye toward future AI integration. We’ll cover:\n\nSetting up the Raspberry Pi for physical computing\nWorking with essential sensors and actuators\nUnderstanding GPIO (General Purpose Input/Output) programming\nEstablishing a foundation for integrating LLMs with physical devices\nCreating interactive systems that can respond to both sensor data and natural language commands\n\nWe will also use a Jupyter notebook (programmed in Python) to interact with sensors and actuators—an important and necessary first step toward the goal of integrating the Raspi with an SLM. The combination of Raspberry Pi’s versatility and the power of SLMs opens up exciting possibilities for creating more intelligent and responsive physical computing systems.\nThe diagram below gives us an overview of the project:"
  },
  {
    "objectID": "raspi/physical_comp/RPi_Physical_Computing.html#prerequisites",
    "href": "raspi/physical_comp/RPi_Physical_Computing.html#prerequisites",
    "title": "Physical Computing with Raspberry Pi",
    "section": "Prerequisites",
    "text": "Prerequisites\n\nRaspberry Pi (model 4 or 5)\nDHT22 Temperature and Relative Humidity Sensor\nBMP280 Barometric Pressure, Temperature and Altitude Sensor\nColored LEDs (3x)\nPush Button (1x)\nResistor 4K7 ohm (2x)\nResistor 220 or 330 ohm (3x)"
  },
  {
    "objectID": "raspi/physical_comp/RPi_Physical_Computing.html#install-the-raspi-operating-system",
    "href": "raspi/physical_comp/RPi_Physical_Computing.html#install-the-raspi-operating-system",
    "title": "Physical Computing with Raspberry Pi",
    "section": "Install the Raspi Operating System",
    "text": "Install the Raspi Operating System\nAs described in Setup, we will need an operating system to use the Raspberry Pi. By default, Raspberry Pis check for an operating system on any SD card inserted in the slot, so we should install an operating system using Raspberry Pi Imager.\nRaspberry Pi Imager is a tool for downloading and writing images on macOS, Windows, and Linux. It includes many popular operating system images for Raspberry Pi. We will also use the Imager to preconfigure credentials and remote access settings.\nAfter downloading the Imager and installing it on your computer, use a new and empty SD card. Select the device (RASPBERRY PI Zero, 4 or 5), the Operating System (RASPBERRY PI OS 32 or 64-BIT), and your Storage Device:\n\nWe should also define the options, such as the hostname, username, password, LAN configuration (on GENERAL TAB), and, more importantly, SSH Enable on the SERVICES tab.\n\nUsing the Secure Shell (SSH) protocol, you can access the terminal of a Raspberry Pi remotely from another computer on the same network.\n\n\nAfter burning the OS to the SD card, install it in the Raspi5’s SD slot and plug in the 5V power source.\n\nInteracting with the Raspi via SSH\nThe easiest way to interact with the Raspi is via SSH (“Headless”). We can use a Terminal (MAC/Linux) or PuTTy (Windows).\nOn terminal type &lt;username&gt;@&lt;hostname&gt;.local, for example:\nssh mjrovai@rpi-5.local\n\nYou should replace mjrovai with your username and rpi-5 with the hostname chosen during set-u\n\n\nWhen you see the prompt:\nmjrovai@rpi-5:~ $\nIt means that you are interacting remotely with your Raspi.\n\nNote: ssh &lt;username&gt;@&lt;hostname&gt;.local sometimes does not work. In those cases, try: ssh &lt;username&gt;@&lt;ip address&gt;\n\nIt is a good practice to update the system regularly. For that, you should run:\nsudo apt-get update\nPip is a tool for installing external Python modules on a Raspberry Pi. However, it has not been enabled in recent OS versions. To allow it, you should run the command (only once):\nsudo rm /usr/lib/python3.11/EXTERNALLY-MANAGED\nTo shut down the Rpi-Zero via terminal:\nDo not simply pull the power cord when you want to turn off your Raspberry Pi. The Raspi may still be writing data to the SD card, in which case, merely powering down may result in data loss or, even worse, a corrupted SD card.\nFor safety shut down, use the command line:\nsudo shutdown -h now\n\nTo avoid possible data loss and SD card corruption, wait a few seconds after shutting down for the Raspberry Pi’s LED to stop blinking and go dark before removing the power. Once the LED goes out, it’s safe to power down."
  },
  {
    "objectID": "raspi/physical_comp/RPi_Physical_Computing.html#accessing-the-gpios",
    "href": "raspi/physical_comp/RPi_Physical_Computing.html#accessing-the-gpios",
    "title": "Physical Computing with Raspberry Pi",
    "section": "Accessing the GPIOs",
    "text": "Accessing the GPIOs\nA simple way to reach the GPIO pins on a Raspberry Pi is from the GPIO Zero Library. With a few lines of code in Python, we can control actuators, read sensors, etc. It was created by Ben Nuttall of the Raspberry Pi Foundation, Dave Jones, and other contributors (GitHub).\n\nGPIO Zero is installed by default in the Raspberry Pi OS.\n\n\nPin Numbering\n\nIt is essential to mention that the GPIO Zero Library uses Broadcom (BCM) pin numbering for the GPIO pins, as opposed to physical (board) numbering. Any pin marked “GPIO” in the following diagrams can be used as a PIN. For example, if an LED were attached to “GPIO13,” you would specify the PIN as 18 rather than 33 (the physical one).\n\n\n“Hello World”: Blinking an LED\nTo connect our RPi to the world, let’s first connect:\n\nPhysical Pin 6 (GND) to GND Breadboard Power Grid (Blue -), using a black jumper\nPhysical Pin 1 (3.3V) to +VCC Breadboard Power Grid (Red +), using a red jumper\n\nNow, let’s connect an LED (red) using the physical pin 13 (GPIO13) connected to the LED cathode (longer LED leg). Connect the LED anode to the breadboard GND using a 330 ohms resistor to reduce the current drained from the Raspi, as shown below:\n\nOnce the HW is connected, let’s create a Python script to turn on the LED:\nfrom gpiozero import LED\nled = LED(13)\nled.on()\nWe can use any text editor (such as Nano) to create and run the script. Save the file, for example, as led_test.py, and then execute it using the terminal:\npython led_test.py\nAs we can see, it is elementary to code using the GPIO Zero Library.\nNow, let’s blink the LED (the actual “Hello world”) when talking about physical computing. To do that, we must also import another library, which is time. We need it to define how long the LED will be ON and OFF. In our case below, the LED will blink at a 1-second time.\nfrom gpiozero import LED\nfrom time import sleep\nled = LED(18)\nwhile True:\n    led.on()\n    sleep(1)\n    led.off()\n    sleep(1)\nAlternatively, we can reduce the blink code as below:\nfrom gpiozero import LED\nfrom signal import pause\nred = LED(17)\nred.blink()\npause()\n\n\nInstalling all LEDs (the “actuators”)\nThe LEDs can be used as “actuators”; depending on the condition of a code running on our Pi, we can command one of the LEDs to fire! We will install two more LEDs besides the red one already installed. Follow the diagram and install the yellow (on GPIO 19 ) and the green (on GPIO 26).\n\nFor testing we can run a similar code as the used with the single red led, changing the pin accordantly, for example.\nfrom gpiozero import LED\n\nledRed = LED(13)\nledYlw = LED(19)\nledGrn = LED(26)\n\nledRed.off()\nledYlw.off()\nledGrn.off()\n\nledRed.on()\nledYlw.on()\nledGrn.on()\n\nledRed.off()\nledYlw.off()\nledGrn.off()\n\nRemember that instead of LEDs, we could have relays, motors, etc."
  },
  {
    "objectID": "raspi/physical_comp/RPi_Physical_Computing.html#sensors-installation-and-setup",
    "href": "raspi/physical_comp/RPi_Physical_Computing.html#sensors-installation-and-setup",
    "title": "Physical Computing with Raspberry Pi",
    "section": "Sensors Installation and setup",
    "text": "Sensors Installation and setup\nIn this section, we will setup the Raspberry Pi to capture data from several different sensors:\nSensors and Communication type:\n\nButton (Command via a Push-Button) ==&gt; Digital direct connection\nDHT22 (Temperature and Humidity) ==&gt; Digital communication\nBMP280 (Temperature and Pressure) ==&gt; I2C Protocol\n\n\nButton\nThe simple way to read an external command is by using a push-button, and the GPIO Zero Library provides an easy way to include it in our project. We do not need to think about Pull-up or Pull-down resistors, etc. In terms of HW, the only thing to do is to connect one leg of our push-button to any one of RPi GPIOs and the other one to GND as shown in the diagram:\n\n\nPush-Button leg1 to GPIO 20\nPush-Button leg2 to GND\n\nA simple code for reading the button can be:\nfrom gpiozero import Button\nbutton = Button(20)\nwhile True: \n    if button.is_pressed: \n        print(\"Button is pressed\") \n    else:\n        print(\"Button is not pressed\")\n\n\nInstalling Adafruit CircuitPython\nThe GPIO Zero library is an excellent hardware interfacing library for Raspberry Pi. It’s great for digital in/out, analog inputs, servos, basic sensors, etc. However, it doesn’t cover SPI/I2C sensors or drivers, and by using CircuitPython via adafruit_blinka, we can unlock all of the drivers and example code developed by Adafruit!\n\nNote that we will keep using GPIO Zero for pins, buttons and LEDs.\n\nEnable Interfaces\nRun these commands to enable the various interfaces such as I2C and SPI:\nsudo raspi-config nonint do_i2c 0\nsudo raspi-config nonint do_spi 0\nsudo raspi-config nonint do_serial_hw 0\nsudo raspi-config nonint disable_raspi_config_at_boot 0\nInstall Blinka and Dependencies\nsudo apt-get install -y i2c-tools libgpiod-dev python-libgpiod\npip install --upgrade adafruit-blinka\nCheck I2C and SPI\nThe script will automatically enable I2C and SPI. You can run the following command to verify:\nls /dev/i2c* /dev/spi*\n\nBlinka Test\nCreate a new file called blinka_test.py with nano or your favorite text editor and put the following in:\nimport board\nimport digitalio\nimport busio\n\nprint(\"Hello, blinka!\")\n\n# Try to create a Digital input\npin = digitalio.DigitalInOut(board.D4)\nprint(\"Digital IO ok!\")\n\n# Try to create an I2C device\ni2c = busio.I2C(board.SCL, board.SDA)\nprint(\"I2C ok!\")\n\n# Try to create an SPI device\nspi = busio.SPI(board.SCLK, board.MOSI, board.MISO)\nprint(\"SPI ok!\")\n\nprint(\"done!\")\nSave it and run it at the command line:\npython blinka_test.py\n\n\n\nDHT22 - Temperature & Humidity Sensor\nThe first sensor to be installed will be the DHT22 for capturing air temperature and relative humidity data.\nOverview\nThe low-cost DHT temperature and humidity sensors are elementary and slow but great for logging basic data. They consist of a capacitive humidity sensor and a thermistor. A bare chip inside performs the analog-to-digital conversion and spits out a digital signal with the temperature and humidity. The digital signal is relatively easy to read using any microcontroller.\nDHT22 Main characteristics:\n\nSuitable for 0-100% humidity readings with 2-5% accuracy\nSuitable for -40 to 125°C temperature readings ±0.5°C accuracy\nNo more than 0.5 Hz sampling rate (once every 2 seconds)\nLow cost\n3 to 5V power and I/O\n2.5mA max current use during conversion (while requesting data)\nBody size 15.1mm x 25mm x 7.7mm\n4 pins with 0.1” spacing\n\nOnce we use the sensor at distances less than 20m, a 4K7 ohm resistor should be connected between the Data and VCC pins. The DHT22 output data pin will be connected to Raspberry GPIO 16. Check the electrical diagram, connecting the sensor to RPi pins as below:\n\nPin 1 - Vcc ==&gt; 3.3V\nPin 2 - Data ==&gt; GPIO 16\nPin 3 - Not Connect\nPin 4 - Gnd ==&gt; Gnd\n\n\nDo not forget to Install the 4K7 ohm resistor between the VCC and Data pins.\n\n\nOnce the sensor is connected, we must install its library on our Raspberry Pi. First, we should install the Adafruit CircuitPython library, which we have already done, and the Adafruit_CircuitPython_DHT.\npip install adafruit-circuitpython-dht\nOn your Raspberry, starting at home, go to Documents.\ncd Documents\nCreate a directory to install the library and move to there:\nmkdir sensors\ncd sensors\nCreate a new Python script as below and name it, for example, dht_test.py:\nimport time\nimport board\nimport adafruit_dht\ndhtDevice = adafruit_dht.DHT22(board.D16)\n\nwhile True:\n    try:\n        # Print the values to the serial port\n        temperature_c = dhtDevice.temperature\n        temperature_f = temperature_c * (9 / 5) + 32\n        humidity = dhtDevice.humidity\n        print(\n            \"Temp: {:.1f} F / {:.1f} C    Humidity: {}% \".format(\n                temperature_f, temperature_c, humidity\n            )\n        )\n\n    except RuntimeError as error:\n        # Errors happen fairly often, DHT's are hard to read, \n        # just keep going\n        print(error.args[0])\n        time.sleep(2.0)\n        continue\n    except Exception as error:\n        dhtDevice.exit()\n        raise error\n\n\n\n\nInstalling the BMP280: Barometric Pressure & Altitude Sensor\nSensor Overview:\nEnvironmental sensing has become increasingly important in various industries, from weather forecasting to indoor navigation and consumer electronics. At the forefront of this technological advancement are sensors like the BMP280 and BMP180 (deprected), which excel in measuring temperature and barometric pressure with exceptional precision and reliability.\nAs its predecessor, the BMP180, the BMP280 is an absolute barometric pressure sensor, which is especially feasible for mobile applications. Its diminutive dimensions and low power consumption allow for its implementation in battery-powered devices such as mobile phones, GPS modules, or watches. The BMP280 is based on Bosch’s proven piezo-resistive pressure sensor technology featuring high accuracy and linearity as well as long-term stability and high EMC robustness. Numerous device operation options guarantee the highest flexibility. The device is optimized for power consumption, resolution, and filter performance.\nTechnical data\n\n\n\n\n\n\n\nParameter\nTechnical data\n\n\n\n\nOperation range\nPressure: 300…1100 hPa Temp.: -40…85°C\n\n\nAbsolute accuracy (950…1050 hPa, 0…+40°C)\n~ ±1 hPa\n\n\nRelative accuracy p = 700…900hPa (Temp. @ 25°C)\n± 0.12 hPa (typical) equivalent to ±1 m\n\n\nAverage typical current consumption (1 Hz dt/rate)\n3.4 μA @ 1 Hz\n\n\nAverage current consumption (1 Hz dt refresh rate)\n2.74 μA, typical (ultra-low power mode)\n\n\nAverage current consumption in sleep mode\n0.1 μA\n\n\nAverage measurement time\n5.5 msec (ultra-low power preset)\n\n\nSupply voltage VDDIO\n1.2 … 3.6 V\n\n\nSupply voltage VDD\n1.71 … 3.6 V\n\n\nResolution of data\nPressure: 0.01 hPa ( &lt; 10 cm) Temp.: 0.01° C\n\n\nTemperature coefficient offset (+25°…+40°C @ 900hPa)\n1.5 Pa/K, equiv. to 12.6 cm/K\n\n\nInterface\nI²C and SPI\n\n\n\nBMP280 Sensor Installation\nFollow the diagram and make the connections:\n\nVin ==&gt; 3.3V\nGND ==&gt; GND\nSCL ==&gt; GPIO 3\nSDA ==&gt; GPIO 2\n\n\nEnabling I2C Interface\nGo to RPi Configuration and confirm that the I2C interface is enabled. If not, enable it.\nsudo raspi-config nonint do_i2c 0\nUsing the BMP280\nIf everything has been installed and connected correctly, you can turn on your Rapspi and start interpreting the BMP180’s information about the environment.\nThe first thing to do is to check if the Raspi sees your BMP280. Try the following in a terminal:\nsudo i2cdetect -y 1\nWe should confirm that the BMP280 is on channel 77 (default) or 76.\n\nIn my case, the bus address is 0x76, so we should define it during the library installation.\nInstalling the BMP 280 Library:\nOnce the sensor is connected, we must install its library on our Raspi. For that, we should install the Adafruit_CircuitPython_BMP280.\npip install adafruit-circuitpython-bmp280\nCreate a new Python script as below and name it, for example, bmp280_test.py:\nimport time\nimport board\n\nimport adafruit_bmp280\n\ni2c = board.I2C()\nbmp280 = adafruit_bmp280.Adafruit_BMP280_I2C(i2c, address = 0x76)\nbmp280.sea_level_pressure = 1013.25\n\nwhile True:\n    print(\"\\nTemperature: %0.1f C\" % bmp280.temperature)\n    print(\"Pressure: %0.1f hPa\" % bmp280.pressure)\n    print(\"Altitude = %0.2f meters\" % bmp280.altitude)\n    time.sleep(2)\nExecute the script:\npython bmp280Test.py\nThe Terminal shows the result.\n\n\nNote that that pressure is presented in hPa. See the next section to understand this unit better.\n\n\n\n\nMeasuring Weather and Altitude With BMP280\n\nLet’s take some time to understand more about what we will get with the BMP readings.\n\nYou can skip this part of the tutorial, or return later.\n\nThe BMP280 (and its predecessor, the BMP180) was designed to measure atmospheric pressure accurately. Atmospheric pressure varies with both weather and altitude.\nWhat is Atmospheric Pressure?\nAtmospheric pressure is a force that the air around you exerts on everything. The weight of the gasses in the atmosphere creates atmospheric pressure. A standard unit of pressure is “pounds per square inch” or psi. We will use the international notation, newtons per square meter, called pascals (Pa).\n\nIf you took 1 cm wide column of air would weigh about 1 kg\n\nThis weight, pressing down on the footprint of that column, creates the atmospheric pressure that we can measure with sensors like the BMP280. Because that cm-wide column of air weighs about 1 kg, the average sea level pressure is about 101,325 pascals, or better, 1013.25 hPa (1 hPa is also known as milibar - mbar). This will drop about 4% for every 300 meters you ascend. The higher you get, the less pressure you’ll see because the column to the top of the atmosphere is much shorter and weighs less. This is useful because you can determine your altitude by measuring the pressure and doing math.\n\nThe air pressure at 3,810 meters is only half that at sea level.\n\nThe BMP280 outputs absolute pressure in hPa (mbar). One pascal is a minimal amount of pressure, approximately the amount that a sheet of paper will exert resting on a table. You will often see measurements in hectopascals (1 hPa = 100 Pa). The library here provides outputs of floating-point values in hPa, equaling one millibar (mbar).\nHere are some conversions to other pressure units:\n\n1 hPa = 100 Pa = 1 mbar = 0.001 bar\n1 hPa = 0.75006168 Torr\n1 hPa = 0.01450377 psi (pounds per square inch)\n1 hPa = 0.02953337 inHg (inches of mercury)\n1 hPa = 0.00098692 atm (standard atmospheres)\n\nTemperature Effects\nBecause temperature affects the density of a gas, density affects the mass of a gas, and mass affects the pressure (whew), atmospheric pressure will change dramatically with temperature. Pilots know this as “density altitude”, which makes it easier to take off on a cold day than a hot one because the air is denser and has a more significant aerodynamic effect. To compensate for temperature, the BMP280 includes a rather good temperature sensor and a pressure sensor.\nTo perform a pressure reading, you first take a temperature reading, then combine that with a raw pressure reading to come up with a final temperature-compensated pressure measurement. (The library makes all of this very easy.)\nMeasuring Absolute Pressure\nIf your application requires measuring absolute pressure, all you have to do is get a temperature reading, then perform a pressure reading (see the test script for details). The final pressure reading will be in hPa = mbar. You can convert this to a different unit using the above conversion factors.\n\nNote that the absolute pressure of the atmosphere will vary with both your altitude and the current weather patterns, both of which are useful things to measure.\n\nWeather Observations\nThe atmospheric pressure at any given location on Earth (or anywhere with an atmosphere) isn’t constant. The complex interaction between the earth’s spin, axis tilt, and many other factors result in moving areas of higher and lower pressure, which in turn cause the variations in weather we see every day. By watching for changes in pressure, you can predict short-term changes in the weather. For example, dropping pressure usually means wet weather or a storm is approaching (a low-pressure system is moving in). Rising pressure usually means clear weather is coming (a high-pressure system is moving through). But remember that atmospheric pressure also varies with altitude. The absolute pressure in my home, Lo Barnechea, in Chile (altitude 960m), will always be lower than that in San Francisco (less than 2 meters, almost sea level). If weather stations just reported their absolute pressure, it would be challenging to compare pressure measurements from one location to another (and large-scale weather predictions depend on measurements from as many stations as possible).\nTo solve this problem, weather stations continuously remove the effects of altitude from their reported pressure readings by mathematically adding the equivalent fixed pressure to make it appear that the reading was taken at sea level. When you do this, a higher reading in San Francisco than in Lo Barnechea will always be because of weather patterns and not because of altitude.\nSea Level Pressure Calculation\nThe See Level Pressure can be calculated with the formula:\n\nWhere,\npo = SeaLevel Pressure \np = Atmospheric Pressure \nL = Temperature Lapse Rate \nh = Altitude \nTo = Sea Level Standard Temperature \ng = Earth Surface Gravitational Acceleration \nM = Molar Mass Of Dry Air \nR = Universal Gas Constant\n\nHaving the absolute pressure in Pa, you check the sea level pressure using the Calculator.\n\nOr calculating in Python, where the altitude is the real altitude in meters where the sensor is located.\npresSeaLevel = pres / pow(1.0 - altitude/44330.0, 5.255) \nDetermining Altitude\nSince pressure varies with altitude, you can use a pressure sensor to measure altitude (with a few caveats). The average pressure of the atmosphere at sea level is 1013.25 hPa (or mbar). This drops off to zero as you climb towards the vacuum of space. Because the curve of this drop-off is well understood, you can compute the altitude difference between two pressure measurements (p and p0) by using a specific equation. The BMP280 gives the measured altitude using bmp280Sensor.altitude.\n\nThe above explanation was based on the BMP 180 Sparkfun tutorial."
  },
  {
    "objectID": "raspi/physical_comp/RPi_Physical_Computing.html#playing-with-sensors-and-actuators",
    "href": "raspi/physical_comp/RPi_Physical_Computing.html#playing-with-sensors-and-actuators",
    "title": "Physical Computing with Raspberry Pi",
    "section": "Playing with Sensors and Actuators",
    "text": "Playing with Sensors and Actuators\n\nInstalling Jupyter Notebook\nWe all know that Jupyter Notebook is a fantastic tool—or, better yet, an open-source web application that allows you to create and share documents containing live code, equations, visualizations, and narrative text. Jupyter Notebook is used mainly in Data Science, cleaning and transforming data, numerical simulation, statistical modeling, data visualization, machine learning, and much more!\n\nHow about using Jupyter Notebooks to control Raspberry Pi GPIOs?\n\nIn this section, we will learn how to install Jupyter Notebook on a Raspberry Pi. Then, we will read sensors and act on actuators directly on the Pi.\nTo install Jupyter on your Raspberry (that will run with Python 3), open Terminal and enter the following commands:\npip install jupyter\nsudo reboot\njupyter notebook --generate-config\nEdit the config file:\nnano ~/.jupyter/jupyter_notebook_config.py\nAdd/modify these lines:\nc.NotebookApp.ip = '0.0.0.0'        # Listen on all interfaces\nc.NotebookApp.open_browser = False  # Disable browser auto-launch\nc.NotebookApp.port = 8888           # Default port (change if needed)\nNow, on the Raspi terminal, start the Jupyter notebook server with the command:\njupyter notebook --no-browser\n\n\nYou will need the Token; you can copy it from the terminal as shown above.\n\nOn your Desktop, set up SSH tunneling:\nssh -N -L 8888:localhost:8888 username@raspberry_pi_ip\nThe Jupyter Notebook will be running as a server on:\nhttp:localhost:8888\n\nThe first time you connect, you’ll need the token that appears in the Pi terminal when you start the notebook server.\n\n\n\nWhen you start your Pi and want to use Jupyter Notebook, type the “Jupyter Notebook” command on your terminal and keep it running. This is very important! If you need to use the terminal for another task, such as running a program, open a new Terminal window.\n\nTo stop the server and close the “kernels” (the Jupyter notebooks), press [Ctrl] + [C].\n\n\nTesting the Notebook setup\nLet’s create a new notebook (Kernel: Python 3). Open dht_test.py, copy the code, and paste it into the notebook. That’s it. We can see the temperature and humidity values appearing on the cell. To interrupt the execution, go to the [stop] button at the top menu.\n\nOK, this means we can access the physical world from our notebook! Let’s create a more structured code for dealing with sensors and actuators.\n\n\nInitialization\nImport libraries, instantiate and initialize sensors/actuators\n# time library \nimport time\nimport datetime\n\n# Adafruit DHT library (Temperature/Humidity)\nimport board\nimport adafruit_dht\nDHT22Sensor = adafruit_dht.DHT22(board.D16)\n\n# BMP library (Pressure/Temperature)\nimport adafruit_bmp280\ni2c = board.I2C()\nbmp280Sensor = adafruit_bmp280.Adafruit_BMP280_I2C(i2c, address = 0x76)\nbmp280Sensor.sea_level_pressure = 1013.25\n\n# LEDs\nfrom gpiozero import LED\n\nledRed = LED(13)\nledYlw = LED(19)\nledGrn = LED(26)\n\nledRed.off()\nledYlw.off()\nledGrn.off()\n\n# Push-Button\nfrom gpiozero import Button\nbutton = Button(20)\n\n\nGPIO Input and Output\nCreate a function to get GPIO status:\n# Get GPIO status data \ndef getGpioStatus():\n    global timeString\n    global buttonSts\n    global ledRedSts\n    global ledYlwSts\n    global ledGrnSts\n\n    # Get time of reading\n    now = datetime.datetime.now()\n    timeString = now.strftime(\"%Y-%m-%d %H:%M\")\n    \n    # Read GPIO Status\n    buttonSts = button.is_pressed\n    ledRedSts = ledRed.is_lit\n    ledYlwSts = ledYlw.is_lit\n    ledGrnSts = ledGrn.is_lit \nAnd another to print the status:\n# Print GPIO status data \ndef PrintGpioStatus():\n    print (\"Local Station Time: \", timeString)\n    print (\"Led Red Status:     \", ledRedSts)\n    print (\"Led Yellow Status:  \", ledYlwSts)\n    print (\"Led Green Status:   \", ledGrnSts)\n    print (\"Push-Button Status: \", buttonSts)\nNow, we can, for example, turn on the LEDs:\nledRed.on()\nledYlw.on()\nledGrn.on()\n\nAnd see their status:\n\nIf you press the push-button, its status will also be shown:\n\nAnd turning off the LEDS:\nledRed.off()\nledYlw.off()\nledGrn.off()\n\nWe can create a function to simplify turning LEDs on and off:\n# Acting on GPIOs and printing Status\ndef controlLeds(r, y, g):\n    if (r):\n        ledRed.on()\n    else:\n        ledRed.off()        \n    if (y):\n        ledYlw.on()\n    else:\n        ledYlw.off() \n    if (g):\n        ledGrn.on()\n    else:\n        ledGrn.off() \n    \n    getGpioStatus()\n    PrintGpioStatus()\nFor example, turning on the Yellow LED:\n\n\n\nGetting and displaying Sensor Data\nFirst, we should create a function to read the BMP280 and calculate the pressure value at sea level, once the sensor only gives us the absolute pressure based on the actual altitude:\n# Read data from BMP280\ndef bmp280GetData(real_altitude):\n    \n    temp = bmp280Sensor.temperature\n    pres = bmp280Sensor.pressure\n    alt =  bmp280Sensor.altitude\n    presSeaLevel = pres / pow(1.0 - real_altitude/44330.0, 5.255) \n    \n    temp = round (temp, 1)\n    pres = round (pres, 2) # absolute pressure in mbar\n    alt = round (alt)\n    presSeaLevel = round (presSeaLevel, 2) # absolute pressure in mbar\n    \n    return temp, pres, alt, presSeaLevel\nEntering the BMP280 real altitude where it is located, run the code:\nbmp280GetData(960)\nAs a result, we will get (26.9, 906.73, 927, 1017.29)which means:\n\nTemperature of 26.9 oC\nAbsolute Pressure of 906.73 hPa\nMeasured Altitude (from Pressure) of 927 m\nSea Level converted Pressure: 1,017.29 hPa\n\nNow, we will generate a unique function to get the BMP280 and the DHT data, including a timestamp:\n# Get data (from local sensors)\ndef getSensorData(altReal=0):\n    global timeString\n    global humExt\n    global tempLab\n    global tempExt\n    global presSL\n    global altLab\n    global presAbs\n    global buttonSts\n    \n    # Get time of reading\n    now = datetime.datetime.now()\n    timeString = now.strftime(\"%Y-%m-%d %H:%M\")\n    \n    tempLab, presAbs, altLab, presSL = bmp280GetData(altReal) \n    \n    tempDHT =  DHT22Sensor.temperature\n    humDHT =  DHT22Sensor.humidity\n    \n    if humDHT is not None and tempDHT is not None:\n        tempExt = round (tempDHT)\n        humExt = round (humDHT)\nAnd another function to print the values:\n# Display important data on-screen\ndef printData():\n    print (\"Local Station Time:             \", timeString)\n    print (\"External Air Temperature (DHT): \", tempExt, \"oC\")\n    print (\"External Air Humidity    (DHT): \", humExt, \"%\")\n    print (\"Station Air Temperature  (BMP): \", tempLab, \"oC\")\n    print (\"Sea Level Air Pressure:         \", presSL, \"mBar\")\n    print (\"Absolute Station Air Pressure:  \", presAbs, \"mBar\")\n    print (\"Station Measured Altitude:      \", altLab, \"m\")\nRuning them:\nreal_altitude = 960 # real altitude of where the BMP280 is installed\ngetSensorData(real_altitude)\nprintData()\nResults:\n\n\nUsing Python, we can command the actuators (LEDs) and read the sensors and GIPOs status at this stage. This is important, for example, to generate a data log to be read by an SLM in the future."
  },
  {
    "objectID": "raspi/physical_comp/RPi_Physical_Computing.html#widgets",
    "href": "raspi/physical_comp/RPi_Physical_Computing.html#widgets",
    "title": "Physical Computing with Raspberry Pi",
    "section": "Widgets",
    "text": "Widgets\npywidgets, or jupyter-widgets orwidgets, are interactive HTML widgets for Jupyter notebooks and the IPython kernel. Notebooks come alive when interactive widgets are used. We can gain control of our data and visualize changes in them.\nWidgets are eventful Python objects that have a representation in the browser, often as a control like a slider, text box, etc. We can use widgets to build interactive GUIs for our project.\nIn this lab, for example, we will use a slide bar to control the state of actuators in real time, such as by turning on or off the LEDs. Widgets are great for adding more dynamic behavior to Jupyter Notebooks.\nInstallation\nTo use Widgets, we must install the Ipywidgets library using the commands:\npip install ipywidgets\nAfter installation, we should call the library:\n# widget library\nfrom ipywidgets import interactive\nimport ipywidgets as widgetsfrom \nIPython.display import display\nAnd running the below line, we can control the LEDs in real-time:\nf = interactive(controlLeds, r=(0,1,1), y=(0,1,1), g=(0,1,1))\ndisplay(f)\n\n\nThis interactive widget is very easy to implement and very powerful. You can learn more about Interactive on this link: Interactive Widget."
  },
  {
    "objectID": "raspi/physical_comp/RPi_Physical_Computing.html#interacting-an-slm-with-the-physical-world",
    "href": "raspi/physical_comp/RPi_Physical_Computing.html#interacting-an-slm-with-the-physical-world",
    "title": "Physical Computing with Raspberry Pi",
    "section": "Interacting an SLM with the Physical world",
    "text": "Interacting an SLM with the Physical world\nThis section demonstrates in a simple way how to integrate a Small Language Model (SLM) with the sensors and LEDs we have set up. The diagram below shows how data flows from sensors through processing and AI analysis to control the actuators and ultimately provide user feedback.\n\nWe will use the Transformers library from Hugging Face for model loading and inference. This library provides the architecture for working with pre-trained language models, helping interact with the model, processing input prompts, and obtaining outputs.\nInstallation\npip install transformers torch\nLet’s create a simple SLM test in the Jupyter Notebook that checks if the model loads and measures inference time. The model used here is the TinyLLama 1.1B. We will ask a straightforward question:\n\"The weather today is\"\nAs a result, besides the SLM answer, we will also measure the latency.\nRun this script:\nimport time\nfrom transformers import pipeline\nimport torch\n\n# Check if CUDA is available (it won't be on our case, Raspberry Pi)\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(f\"Using device: {device}\")\n\n# Load the model and measure loading time\nstart_time = time.time()\n\nmodel='TinyLlama/TinyLlama-1.1B-intermediate-step-1431k-3T'\ngenerator = pipeline('text-generation', \n                    model=model,\n                    device=device)\nload_time = time.time() - start_time\nprint(f\"Model loading time: {load_time:.2f} seconds\")\n\n# Test prompt\ntest_prompt = \"The weather today is\"\n\n# Measure inference time\nstart_time = time.time()\nresponse = generator(test_prompt, \n                    max_length=50,\n                    num_return_sequences=1,\n                    temperature=0.7)\ninference_time = time.time() - start_time\n\nprint(f\"\\nTest prompt: {test_prompt}\")\nprint(f\"Generated response: {response[0]['generated_text']}\")\nprint(f\"Inference time: {inference_time:.2f} seconds\")\nAs we can see, the SLM works, but the latency is very high (+3 minutes). It is OK because this particular test is on a Raspberry Pi 4. With a Raspberry Pi 5, the result would be better.:\n\nThe Raspi uses around 1GB of memory (model + process) and all four cores to process the answer. The model alone needs around 800MB.\n\nNow, let us create a code showing a basic interaction pattern where the SLM can respond to sensor data and interact with the LEDs.\nInstall the Libraries:\nimport time\nimport datetime\nimport board\nimport adafruit_dht\nimport adafruit_bmp280\nfrom gpiozero import LED, Button\nfrom transformers import pipeline\nInitialize sensors\nDHT22Sensor = adafruit_dht.DHT22(board.D16)\ni2c = board.I2C()\nbmp280Sensor = adafruit_bmp280.Adafruit_BMP280_I2C(i2c, address=0x76)\nbmp280Sensor.sea_level_pressure = 1013.25\nInitialize LEDs and Button\nledRed = LED(13)\nledYlw = LED(19)\nledGrn = LED(26)\nbutton = Button(20)\nInitialize the SLM pipeline\n# We're using a small model suitable for Raspberry Pi\n\nmodel='TinyLlama/TinyLlama-1.1B-intermediate-step-1431k-3T'\ngenerator = pipeline('text-generation', \n                    model=model,\n                    device='cpu')\nSupport Functions\nNow, let’s create support functions for readings from all sensors and control the LEDs:\ndef get_sensor_data():\n    \"\"\"Get current readings from all sensors\"\"\"\n    try:\n        temp_dht = DHT22Sensor.temperature\n        humidity = DHT22Sensor.humidity\n        temp_bmp = bmp280Sensor.temperature\n        pressure = bmp280Sensor.pressure\n        \n        return {\n            'temperature_dht': round(temp_dht, 1) if temp_dht else None,\n            'humidity': round(humidity, 1) if humidity else None,\n            'temperature_bmp': round(temp_bmp, 1),\n            'pressure': round(pressure, 1)\n        }\n    except RuntimeError:\n        return None\n\n\ndef control_leds(red=False, yellow=False, green=False):\n    \"\"\"Control LED states\"\"\"\n    ledRed.value = red\n    ledYlw.value = yellow\n    ledGrn.value = green\n    \n\ndef process_conditions(sensor_data):\n    \"\"\"Process sensor data and control LEDs based on conditions\"\"\"\n    if not sensor_data:\n        control_leds(red=True)  # Error condition\n        return\n    \n    temp = sensor_data['temperature_dht']\n    humidity = sensor_data['humidity']\n    \n    # Example conditions for LED control\n    if temp &gt; 30:  # Hot\n        control_leds(red=True)\n    elif humidity &gt; 70:  # Humid\n        control_leds(yellow=True)\n    else:  # Normal conditions\n        control_leds(green=True)\nGenerating an SLM’s response\nSo far, the LEDs reaction is only based on logic, but let’s also use the SLM to “analyse” the sensors condition, generating a response based on that:\ndef generate_response(sensor_data):\n    \"\"\"Generate response based on sensor data using SLM\"\"\"\n    if not sensor_data:\n        return \"Unable to read sensor data\"\n    \n    prompt = f\"\"\"Based on these sensor readings:\n    Temperature: {sensor_data['temperature_dht']}°C\n    Humidity: {sensor_data['humidity']}%\n    Pressure: {sensor_data['pressure']} hPa\n    \n    Provide a brief status and recommendation in 2 sentences.\n    \"\"\"\n    \n    # Generate response from SLM\n    response = generator(prompt, \n                       max_length=100,\n                       num_return_sequences=1,\n                       temperature=0.7)[0]['generated_text']\n    \n    return response\nMain Function\nAnd now, let’s create a main() function to wait for the user to, for example, press a button and, capture the data generated by the sensors, delivering some observation or recommendation from the SLM:\ndef main_loop():\n    \"\"\"Main program loop\"\"\"\n    print(\"Starting Physical Computing with SLM Integration...\")\n    print(\"Press the button to get a reading and SLM response.\")\n    \n    try:\n        while True:\n            if button.is_pressed:\n                # Get sensor readings\n                sensor_data = get_sensor_data()\n                \n                # Process conditions and control LEDs\n                process_conditions(sensor_data)\n                \n                if sensor_data:\n                    # Get SLM response\n                    response = generate_response(sensor_data)\n                    \n                    # Print current status\n                    print(\"\\nCurrent Readings:\")\n                    print(f\"Temperature: {sensor_data['temperature_dht']}°C\")\n                    print(f\"Humidity: {sensor_data['humidity']}%\")\n                    print(f\"Pressure: {sensor_data['pressure']} hPa\")\n                    print(\"\\nSLM Response:\")\n                    print(response)\n                    \n                time.sleep(2)  # Debounce and allow time to read\n            \n            time.sleep(0.1)  # Reduce CPU usage\n            \n    except KeyboardInterrupt:\n        print(\"\\nShutting down...\")\n        control_leds(False, False, False)  # Turn off all LEDs\nTest Result\nThe sensors are read after the user presses the button to trigger a reading, and LEDs are controlled based on conditions. Sensor data is formatted into a prompt for the SLM to generate a response analyzing the current conditions. The results are displayed in the terminal, and the LED indicators are shown.\n\nRed: High temperature (&gt;30°C) or error condition\nYellow: High humidity (&gt;70%)\nGreen: Normal conditions\n\nThis simple code integrates a Small Language Model (TinyLlama model (1.1B parameters) with our physical computing setup, providing raw sensor data and intelligent responses from the SLM about the environmental conditions.\n\nWe can extend this first test to more sophisticated and valuable uses of the SLM integration, for example: adding:\n\nStarting the process from a User Prompt.\nReceive commands from the User to switch LEDs ON or OFF\nProvide the status of LEDS, Button, or specific sensor data from the user prompt\nLog data and responses to a file. Provide historical information by user request\nImplement different types of prompts for various use cases"
  },
  {
    "objectID": "raspi/physical_comp/RPi_Physical_Computing.html#other-models",
    "href": "raspi/physical_comp/RPi_Physical_Computing.html#other-models",
    "title": "Physical Computing with Raspberry Pi",
    "section": "Other Models",
    "text": "Other Models\nWe can use other SLMs in a Raspberry Pi that have distinct ways of handling them. For example, many modern models use GGUF formats, and to use them, we need to install llama-cpp-python, which is designed to work with GGUF models.\nAlso, as we saw in a previous lab, Ollama is a great way to download and test SLMs on the Raspberry Pi."
  },
  {
    "objectID": "raspi/physical_comp/RPi_Physical_Computing.html#conclusion",
    "href": "raspi/physical_comp/RPi_Physical_Computing.html#conclusion",
    "title": "Physical Computing with Raspberry Pi",
    "section": "Conclusion",
    "text": "Conclusion\n\nKey Achievements\nThroughout this tutorial, we’ve successfully: - Set up a complete physical computing environment using Raspberry Pi - Integrated multiple environmental sensors (DHT22 and BMP280) - Implemented visual feedback through LED actuators - Created interactive controls using push buttons - Integrated a Small Language Model (TinyLLama 1.1B) for intelligent analysis - Developed a foundation for AI-enhanced environmental monitoring\n\n\nTechnical Insights\n\nHardware Integration\nThe combination of digital (DHT22) and I2C (BMP280) sensors demonstrated different communication protocols and their implementations. This multi-sensor approach provides redundancy and comprehensive environmental monitoring capabilities. The LED actuators and push-button interface created a responsive and interactive system that bridges the digital and physical worlds.\n\n\nSoftware Architecture\nThe layered software architecture we developed supports: 1. Low-level sensor communication and actuator control 2. Data preprocessing and validation 3. SLM integration for intelligent analysis 4. Interactive user interfaces through both hardware and software\n\n\nAI Integration Learnings\nThe integration of TinyLLama 1.1B revealed several important insights: - Small Language Models can effectively run on edge devices like Raspberry Pi - Natural language processing can enhance sensor data interpretation - Real-time analysis is possible, though with some latency considerations - The system can provide human-readable insights from complex sensor data\n\n\n\nPractical Applications\nThis project serves as a foundation for numerous real-world applications: - Environmental monitoring systems - Smart home automation - Industrial sensor networks - Educational platforms for IoT and AI integration - Prototyping platforms for larger-scale deployments\n\n\nChallenges and Solutions\nThroughout the development, we encountered and addressed several challenges: 1. Resource Constraints: - Optimized SLM inference for Raspberry Pi capabilities - Implemented efficient sensor reading strategies - Managed memory usage for stable operation\n\nData Integration:\n\nDeveloped robust sensor data validation\nCreated effective data preprocessing pipelines\nImplemented error handling for sensor failures\n\nAI Integration:\n\nDesigned effective prompting strategies\nManaged inference latency\nBalanced accuracy with response time\n\n\n\n\nFuture Enhancements\nThe system can be extended in several directions: 1. Hardware Expansions: - Additional sensor types (air quality, light, motion) - Camera for IA applications - More complex actuators (displays, motors, relays) - Wireless connectivity options as WiFI, BLE, or LoRa 2. Software Improvements: - Advanced data logging and analysis - Web-based monitoring interface - Real-time visualization tools 3. AI Capabilities: 1. Models for detecting and counting objects 2. RAG or Fine-tuning SLM for specific applications 3. Multi-modal AI integration via sensor integration 4. Automated decision-making systems 5. Predictive maintenance capabilities\n\n\nFinal Thoughts\nThis tutorial demonstrates that integrating physical computing with AI is feasible and practical on accessible hardware like the Raspberry Pi. Combining sensors, actuators, and AI creates a powerful platform for developing intelligent environmental monitoring and control systems.\nWhile the current implementation focuses on environmental monitoring, the principles and techniques can be adapted to various applications. The modular nature of hardware and software components allows for customization and expansion based on specific needs.\nIntegrating Small Language Models in physical computing opens new possibilities for creating more intuitive and intelligent IoT devices. As edge AI capabilities evolve, projects like this will become increasingly important in developing the next generation of smart devices and systems.\nRemember that this is just the beginning. Our foundation can be extended in countless ways to create more sophisticated and capable systems. The key is building upon these basics while balancing functionality, reliability, and resource usage."
  },
  {
    "objectID": "raspi/physical_comp/RPi_Physical_Computing.html#resources",
    "href": "raspi/physical_comp/RPi_Physical_Computing.html#resources",
    "title": "Physical Computing with Raspberry Pi",
    "section": "Resources",
    "text": "Resources\n\nGPIOs - Scripts\nSensors - Scripts\nNotebooks"
  },
  {
    "objectID": "raspi/iot/slm_iot.html",
    "href": "raspi/iot/slm_iot.html",
    "title": "Experimenting with SLMs for IoT Control",
    "section": "",
    "text": "Conclusion\nThis lab has demonstrated the progressive evolution of an IoT system from basic sensor integration to an intelligent, interactive platform powered by Small Language Models. Through our journey, we’ve explored several key aspects of combining edge AI with physical computing:"
  },
  {
    "objectID": "raspi/iot/slm_iot.html#introduction",
    "href": "raspi/iot/slm_iot.html#introduction",
    "title": "Experimenting with SLMs for IoT Control",
    "section": "Introduction",
    "text": "Introduction\nThis lab explores the implementation of Small Language Models (SLMs) in IoT control systems, demonstrating the possibility of creating a monitoring and control system using edge AI. We’ll integrate these models with physical sensors and actuators, creating an intelligent IoT system capable of natural language interaction. While this implementation shows the potential of integrating AI with physical systems, it also highlights current limitations and areas for improvement.\n\nThis project builds upon the concepts introduced in “Small Language Models (SLMs)” and “Physical Computing with Raspberry Pi.”\n\nThe Physical Computing lab laid the groundwork for interfacing with hardware components using the Raspberry Pi’s GPIO pins. We’ll revisit these concepts, focusing on connecting and interacting with sensors (DHT22 for temperature and humidity, BMP280 for temperature and pressure, and a push-button for digital inputs), besides controlling actuators (LEDs) in a more sophisticated setup.\nWe will progress from a simple IoT system to a more advanced platform that combines real-time monitoring, historical data analysis, and natural language processing (NLP).\n\nThis lab demonstrates a progressive evolution through several key stages:\n\nBasic Sensor Integration\n\nHardware interface with DHT22 (temperature/humidity) and BMP280 (temperature/pressure) sensors\nDigital input through a push-button\nOutput control via RGB LEDs\nFoundational data collection and device control\n\nSLM Basic Analysis\n\nInitial integration with small language models\nSimple observation and reporting of system state\nDemonstration of SLM’s ability to interpret sensor data\n\nActive Control Implementation\n\nDirect LED control based on SLM decisions\nTemperature threshold monitoring\nEmergency state detection via button input\nReal-time system state analysis\n\nNatural Language Interaction\n\nFree-form command interpretation\nContext-aware responses\nMultiple SLM model support\nFlexible query handling\n\nData Logging and Analysis\n\nContinuous system state recording\nTrend analysis and pattern detection\nHistorical data querying\nPerformance monitoring\n\n\nLet’s begin by setting up our hardware and software environment, building upon the foundation established in our previous labs."
  },
  {
    "objectID": "raspi/iot/slm_iot.html#setup",
    "href": "raspi/iot/slm_iot.html#setup",
    "title": "Experimenting with SLMs for IoT Control",
    "section": "Setup",
    "text": "Setup\n\nHardware Setup\n\nConnection Diagram\n\n\n\nComponent\nGPIO Pin\n\n\n\n\nDHT22\nGPIO16\n\n\nBMP280 - SCL\nGPIO03\n\n\nBMP280 - SDA\nGPIO02\n\n\nRed LED\nGPIO13\n\n\nYellow LED\nGPIO19\n\n\nGreen LED\nGPIO26\n\n\nButton\nGPIO20\n\n\n\n\n\nRaspberry Pi 5 (with an OS installed, as detailed in previous labs)\nDHT22 temperature and humidity sensor\nBMP280 temperature and pressure sensor\n3 LEDs (red, yellow, green)\nPush button\n330Ω resistors (3)\nJumper wires and breadboard\n\n\n\n\nSoftware Prerequisites\n\nInstall required libraries:\n\npip install adafruit-circuitpython-dht \npip install adafruit-circuitpython-bmp280"
  },
  {
    "objectID": "raspi/iot/slm_iot.html#basic-sensor-integration",
    "href": "raspi/iot/slm_iot.html#basic-sensor-integration",
    "title": "Experimenting with SLMs for IoT Control",
    "section": "Basic Sensor Integration",
    "text": "Basic Sensor Integration\nLet’s create a Python script (monitor.py) to handle the sensors and actuators. This script will contain functions to be called from other scripts later:\nimport time\nimport board\nimport adafruit_dht\nimport adafruit_bmp280\nfrom gpiozero import LED, Button\n\nDHT22Sensor = adafruit_dht.DHT22(board.D16)\ni2c = board.I2C()\nbmp280Sensor = adafruit_bmp280.Adafruit_BMP280_I2C(i2c, address=0x76)\nbmp280Sensor.sea_level_pressure = 1013.25\n\nledRed = LED(13)\nledYlw = LED(19)\nledGrn = LED(26)\nbutton = Button(20)\n\ndef collect_data():\n    try:\n        temperature_dht = DHT22Sensor.temperature\n        humidity = DHT22Sensor.humidity\n        temperature_bmp = bmp280Sensor.temperature\n        pressure = bmp280Sensor.pressure\n        button_pressed = button.is_pressed\n        return temperature_dht, humidity, temperature_bmp, pressure, button_pressed\n    except RuntimeError:\n        return None, None, None, None, None\n\ndef led_status():\n    ledRedSts = ledRed.is_lit\n    ledYlwSts = ledYlw.is_lit\n    ledGrnSts = ledGrn.is_lit \n    return ledRedSts, ledYlwSts, ledGrnSts\n\n\ndef control_leds(red, yellow, green):\n    ledRed.on() if red else ledRed.off()\n    ledYlw.on() if yellow else ledYlw.off()\n    ledGrn.on() if green else ledGrn.off()\nWe can test the functions using:\nwhile True:\n    ledRedSts, ledYlwSts, ledGrnSts  = led_status()\n    temp_dht, hum, temp_bmp, press, button_state  = collect_data()\n\n    #control_leds(True, True, True)\n\n    if all(v is not None for v in [temp_dht, hum, temp_bmp, press]):\n        print(f\"DHT22 Temp: {temp_dht:.1f}°C, Humidity: {hum:.1f}%\")\n        print(f\"BMP280 Temp: {temp_bmp:.1f}°C, Pressure: {press:.2f}hPa\")\n        print(f\"Button {'pressed' if button_state else 'not pressed'}\")\n        print(f\"Red LED {'is on' if ledRedSts else 'is off'}\")\n        print(f\"Yellow LED {'is on' if ledYlwSts else 'is off'}\")\n        print(f\"Green LED {'is on' if ledGrnSts else 'is off'}\")\n\n\n    time.sleep(2)\n\nInstall Ollama on your Raspberry Pi (follow Ollama’s official documentation or the SLM lab)"
  },
  {
    "objectID": "raspi/iot/slm_iot.html#slm-basic-analysis",
    "href": "raspi/iot/slm_iot.html#slm-basic-analysis",
    "title": "Experimenting with SLMs for IoT Control",
    "section": "SLM Basic Analysis",
    "text": "SLM Basic Analysis\nNow, let’s create a new script, slm_basic_analysis.py, which will be responsible for analysing the hardware components’ status, according to the following diagram:\n\nThe diagram shows the basic analysis system, which consists of:\n\nHardware Layer:\n\nSensors: DHT22 (temperature/humidity), BMP280 (temperature/pressure)\nInput: Emergency button\nOutput: Three LEDs (Red, Yellow, Green)\n\nmonitor.py:\n\nHandles all hardware interactions\nProvides two main functions:\n\ncollect_data(): Reads all sensor values\nled_status(): Checks current LED states\n\n\nslm_basic_analysis.py:\n\nCreates a descriptive prompt using sensor data\nSends prompt to SLM (for example, the Llama 3.2 1B)\nDisplays analysis results\nIn this step we will not control the LEDs (observation only)\n\n\nOkay, let’s implement the code. First, if you haven’t already, install Ollama on your Raspberry Pi (follow Ollama’s official documentation or the SLM lab).\nLet’s import the Ollama library and the functions to monitor the HW (from the previous script):\nimport ollama\nfrom monitor import collect_data, led_status\nCalling the monitor functions, we will get all data:\nledRedSts, ledYlwSts, ledGrnSts  = led_status()\ntemp_dht, hum, temp_bmp, press, button_state  = collect_data()\nNow, the heart of out code, we will generate the Prompt, using the data captured on the previous variables:\nprompt = f\"\"\"\n        You are an experienced environmental scientist. \n        Analyze the information received from an IoT system:\n\n        DHT22 Temp: {temp_dht:.1f}°C and Humidity: {hum:.1f}%\n        BMP280 Temp: {temp_bmp:.1f}°C and Pressure: {press:.2f}hPa\n        Button {\"pressed\" if button_state else \"not pressed\"}\n        Red LED {\"is on\" if ledRedSts else \"is off\"}\n        Yellow LED {\"is on\" if ledYlwSts else \"is off\"}\n        Green LED {\"is on\" if ledGrnSts else \"is off\"}\n\n        Where,\n        - The button, not pressed, shows a normal operation\n        - The button, when pressed, shows an emergency\n        - Red LED when is on, indicates a problem/emergency.\n        - Yellow LED when is on indicates a warning situation.\n        - Green LED when is on, indicates system is OK.\n\n        If the temperature is over 20°C, mean a warning situation\n\n        You should answer only with: \"Activate Red LED\" or \n        \"Activate Yellow LED\" or \"Activate Green LED\"\n\n\"\"\"\nNow, the Prompt will be passed to the SLM, which will generate a response:\nMODEL = 'llama3.2:1b'\nPROMPT = prompt\nresponse = ollama.generate(\n    model=MODEL, \n    prompt=PROMPT\n    )\nThe last stage will be show the real monitored data and the SLM’s response:\nprint(f\"\\nSmart IoT Analyser using {MODEL} model\\n\")\n\nprint(f\"SYSTEM REAL DATA\")\nprint(f\" - DHT22 ==&gt; Temp: {temp_dht:.1f}°C, Humidity: {hum:.1f}%\")\nprint(f\" - BMP280 =&gt; Temp: {temp_bmp:.1f}°C, Pressure: {press:.2f}hPa\")\nprint(f\" - Button {'pressed' if button_state else 'not pressed'}\")\nprint(f\" - Red LED {'is on' if ledRedSts else 'is off'}\")\nprint(f\" - Yellow LED {'is on' if ledYlwSts else 'is off'}\")\nprint(f\" - Green LED {'is on' if ledGrnSts else 'is off'}\")\n\nprint(f\"\\n&gt;&gt; {MODEL} Response: {response['response']}\")\nRuning the Python script, we got:\n\nIn this initial experiment, the system successfully collected sensor data (temperatures of 26.3°C and 26.1°C from DHT22 and BMP280, respectively, 40.2% humidity, and 908.84hPa pressure) and processed this information through the SLM, which produced a coherent response recommending the activation of the yellow LED due to elevated temperature conditions.\nThe model’s ability to interpret sensor data and provide logical, rule-based decisions shows promise. Still, the simplistic nature of the current implementation (using basic thresholds and binary LED outputs) suggests room for significant enhancement through more sophisticated prompting strategies, historical data integration, and the implementation of safety mechanisms. Also, the result is probabilistic, meaning it should change after execution."
  },
  {
    "objectID": "raspi/iot/slm_iot.html#active-control-implementation",
    "href": "raspi/iot/slm_iot.html#active-control-implementation",
    "title": "Experimenting with SLMs for IoT Control",
    "section": "Active Control Implementation",
    "text": "Active Control Implementation\nOK, let’s get a usable output from the SLM by activating one of the LEDs. For that, we will create an action system flow diagram to understand the code implementation better:\n\nThe diagram shows the new action-based system, which adds a User Interface where a user will choose which Model to use based on the SLMs pulled by Ollama. The user will also select the temperature threshold for the test (For example, the actual temperature over this threshold should be configured as a “warning”).\nThe SLM will proceed with a decision-making process regarding what the active LED should be based on the data captured by the system.\nThe key differences for this new code are:\n\nThe basic analysis version only observes and reports\nThe action version actively controls the LEDs\nThe action version includes user configuration\nThe action version implements a continuous monitoring loop\n\nOk, let’s implement the code. Go to the GitHub and download the script slm_basic_analysis_action.py\nThe script implementation consists of several key components:\n\nModel Selection System:\n\nMODELS = {\n    1: ('deepseek-r1:1.5b', 'DeepSeek R1 1.5B'),\n    2: ('llama3.2:1b', 'Llama 3.2 1B'),\n    3: ('llama3.2:3b', 'Llama 3.2 3B'),\n    4: ('phi3:latest', 'Phi-3'),\n    5: ('gemma:2b', 'Gemma 2B'),\n}\n\nProvides multiple SLM options\nEach model offers different capabilities and performance characteristics\nUsers can select based on their needs (speed vs. accuracy)\n\n\nUser Interface Functions:\n\ndef get_user_input():\n    \"\"\"Get user input for model selection and temperature threshold\"\"\"\n    print(\"\\nAvailable Models:\")\n    for num, (_, name) in MODELS.items():\n        print(f\"{num}. {name}\")\n    \n    # Get model selection\n    while True:\n        try:\n            model_num = int(input(\"\\nSelect model (1-4): \"))\n            if model_num in MODELS:\n                break\n            print(\"Please select a number between 1 and 4.\")\n        except ValueError:\n            print(\"Please enter a valid number.\")\n    \n    # Get temperature threshold\n    while True:\n        try:\n            temp_threshold = float(input(\"Enter temperature threshold (°C): \"))\n            break\n        except ValueError:\n            print(\"Please enter a valid number for temperature threshold.\")\n    \n    return MODELS[model_num][0], MODELS[model_num][1], temp_threshold\n\nHandles model selection\nSets temperature threshold\nIncludes input validation\n\n\nResponse Parser:\n\ndef parse_llm_response(response_text):\n    \"\"\"Parse the LLM response to extract LED control instructions.\"\"\"\n    response_lower = response_text.lower()\n    red_led = 'activate red led' in response_lower\n    yellow_led = 'activate yellow led' in response_lower\n    green_led = 'activate green led' in response_lower\n    return (red_led, yellow_led, green_led)\n\nConverts text response to control signals\nSimple but effective parsing strategy\nReturns boolean tuple for LED states\n\n\nMonitoring System:\n\ndef monitor_system(model, model_name, temp_threshold):\n    \"\"\"Monitor system continuously\"\"\"\n    while True:\n        try:\n            # Collect sensor data\n            temp_dht, hum, temp_bmp, press, button_state = collect_data()\n            \n            # Generate prompt and get SLM response\n            response = ollama.generate(\n                model=model,\n                prompt=current_prompt\n            )\n            \n            # Control LEDs based on response\n            red, yellow, green = parse_llm_response(response['response'])\n            control_leds(red, yellow, green)\n            \n            # Print status\n            print_status(...)\n            \n            time.sleep(2)\n            \n        except KeyboardInterrupt:\n            print(\"\\nMonitoring stopped by user\")\n            control_leds(False, False, False)  # Turn off all LEDs\n            break\n\nContinuous monitoring loop\nError handling\nClean shutdown capability\nStatus reporting\n\n\nPrompt Engineering:\n\nprompt = f\"\"\"\n    You are monitoring an IoT system which is showing the \n    following sensor status: \n    - DHT22 Temp: {temp_dht:.1f}°C and Humidity: {hum:.1f}%\n    - BMP280 Temp: {temp_bmp:.1f}°C and Pressure: {press:.2f}hPa\n    - Button {\"pressed\" if button_state else \"not pressed\"}\n\n    Based on the Rules: \n    - If system is working in normal conditions → Activate Green LED \n    - If DHT22 Temp or BMP280 Temp are greater \n      than {temp_threshold}°C → Activate Yellow LED \n    - If Button pressed, it is an emergency → Activate Red LED\n\n    You should provide a brief answer only with: \"Activate Red LED\" \n    or \"Activate Yellow LED\" or \"Activate Green LED\"\n\"\"\"\n\nStructured prompt format\nClear rules and conditions\nConstrained response format\n\nIn the video, we can see how the system works with different models.\nAnd here one screen-shot of the SLM working on the Raspi:\n\nSo, at this point, what we have is something like:\n\n\nWhat we can realize is that the SLM-based system can read and react to the physical world, but with a simple prompt, we cannot guarantee that the result will be correct.\n\nLet’s see how it evolved from the previous code to a new approach, where the SLM should react to a user’s command."
  },
  {
    "objectID": "raspi/iot/slm_iot.html#natural-language-interaction-user-command",
    "href": "raspi/iot/slm_iot.html#natural-language-interaction-user-command",
    "title": "Experimenting with SLMs for IoT Control",
    "section": "Natural Language Interaction (User Command)",
    "text": "Natural Language Interaction (User Command)\nAfter implementing a basic monitoring and automated LED control with slm_basic_analysis_action.py, we can now create a more interactive system that responds to user commands in natural language. This represents an evolution where the SLM makes decisions based on sensor data and understands and responds to user queries and commands.\n\nKey Components and Features\n\nModel Selection\nMODELS = {\n    1: ('deepseek-r1:1.5b', 'DeepSeek R1 1.5B'),\n    2: ('llama3.2:1b', 'Llama 3.2 1B'),\n    3: ('llama3.2:3b', 'Llama 3.2 3B'),\n    4: ('phi3:latest', 'Phi-3'),\n    5: ('gemma:2b', 'Gemma 2B'),\n}\n\nMaintains the same model options as previous versions\nUsers can select their preferred SLM model for interaction\n\nCommand Processing\ndef process_command(model, temp_threshold, user_input):\n    prompt = f\"\"\"\n        You are monitoring an IoT system which is showing the \n        following sensor status: \n        - DHT22 Temp: {temp_dht:.1f}°C and Humidity: {hum:.1f}%\n        - BMP280 Temp: {temp_bmp:.1f}°C and Pressure: {press:.2f}hPa\n        - Button {\"pressed\" if button_state else \"not pressed\"}\n\n        The user command is: \"{user_input}\"\n    \"\"\"\n\nTakes natural language input from users\nCreates context-aware prompts by including current sensor data\nMaintains temperature threshold monitoring\n\nLED Control\ndef parse_llm_response(response_text):\n    \"\"\"Parse the LLM response to extract LED control instructions.\"\"\"\n    response_lower = response_text.lower()\n    red_led = 'activate red led' in response_lower\n    yellow_led = 'activate yellow led' in response_lower\n    green_led = 'activate green led' in response_lower\n    return (red_led, yellow_led, green_led)\n\nUses the same reliable parsing mechanism from previous versions\nMaintains consistency in LED control commands\n\nInteractive Loop\nwhile True:\n    user_input = input(\"Command: \").strip().lower()\n\n    if user_input == 'quit':\n        print(\"\\nShutting down...\")\n        control_leds(False, False, False)\n        break\n\n    process_command(model, temp_threshold, user_input)\n\nProvides continuous interaction through a command prompt\nProcesses one command at a time\nAllows clean system shutdown\n\n\n\n\nSystem Capabilities\nThe system can now: 1. Accept natural language commands and queries 2. Provide information about sensor readings 3. Control LEDs based on user commands 4. Monitor temperature thresholds 5. Display comprehensive system status after each command\n\n\nExample Usage\nSelect model (1-5): 2\nEnter temperature threshold (°C): 25\n\nStarting IoT control system with Llama 3.2 1B\nTemperature threshold: 25°C\nType 'quit' to exit\n\nCommand: what's the current temperature?\n==================================================\nTime: 14:30:45\nDHT22: 22.4°C, 44.8%\nBMP280: 23.2°C, 905.4hPa\nButton: not pressed\nSLM Response: The current temperature is 22.4°C from the DHT22 sensor \nand 23.2°C from the BMP280 sensor.\nLED Status: R=off, Y=off, G=off\n==================================================\n\nCommand: turn on the red led\n[System activates red LED and shows status]\n\nCommand: quit\nShutting down...\nThe previous diagram can be update as:\n\nLet’s see the system runing the above example using the model Llama 3.2 3B:\n\nOr, for example, asking for the SLM to turn on the red LED in case the push-button is activated:\n\nOr the green LED, in case the push-button is not activated:\n\nThe video shows several examples of how the system works.\nLet’s continue evolving the system, which now includes a log to record what happens with the IoT sensors and actuators every minute."
  },
  {
    "objectID": "raspi/iot/slm_iot.html#data-logging-and-analysis",
    "href": "raspi/iot/slm_iot.html#data-logging-and-analysis",
    "title": "Experimenting with SLMs for IoT Control",
    "section": "Data Logging and Analysis",
    "text": "Data Logging and Analysis\nIn this step, we enhance our IoT system by adding data logging, analysis capabilities, and more sophisticated interaction. We split the functionality into two files: monitor_log.py for logging and data analysis and slm_basic_interaction_log.py for user interaction.\n\nThe Logging System (monitor_log.py)\nThis module handles all data logging and analysis functions. Let’s break down its key components:\n# Core functionality\ndef setup_log_file():\n    \"\"\"Create or verify log file with headers\"\"\"\n    headers = ['timestamp', 'temp_dht', 'humidity', 'temp_bmp', 'pressure', \n              'button_state', 'led_red', 'led_yellow', 'led_green', 'command']\nThe system creates a CSV file with headers for all sensor data, LED states, and user commands.\ndef log_data(timestamp, sensors, leds, command=\"\"):\n    \"\"\"Log system data to CSV file\"\"\"\n    temp_dht, hum, temp_bmp, press, button = sensors\n    red, yellow, green = leds\n    \n    row = [\n        timestamp,\n        f\"{temp_dht:.1f}\" if temp_dht is not None else \"NA\",\n        f\"{hum:.1f}\" if hum is not None else \"NA\",\n        # ... other sensor and state data\n    ]\nThis function formats and logs each data point with proper error handling.\ndef automatic_logging():\n    \"\"\"Background thread for automatic logging every minute\"\"\"\n    while not stop_logging.is_set():\n        try:\n            sensors = collect_data()\n            leds = led_status()\n            # ... log data every minute\nA background thread that automatically logs system state every minute.\ndef count_state_changes(series):\n    \"\"\"Count actual state changes in a binary series\"\"\"\n    series = series.astype(int)\n    changes = 0\n    last_state = series.iloc[0]\n    \n    for state in series[1:]:\n        if state != last_state:\n            changes += 1\n            last_state = state\nAccurately counts state changes for LEDs and button presses.\ndef analyze_log_data():\n    \"\"\"Analyze log data and return statistics\"\"\"\n    # Calculates:\n    # - Temperature, humidity, and pressure trends\n    # - Averages for all sensor readings\n    # - LED and button state changes\ndef get_log_summary():\n    \"\"\"Get a formatted summary of log data for SLM prompts\"\"\"\n    # Formats all statistics into a readable summary\nHere is an example of the log summary generated, which will be sent to the SLM per request:\n\n\n\n2. The Interaction System (slm_basic_interaction_log.py)\nThis module handles user interaction and SLM integration:\nMODELS = {\n    1: ('deepseek-r1:1.5b', 'DeepSeek R1 1.5B'),\n    2: ('llama3.2:1b', 'Llama 3.2 1B'),\n    # ... other models\n}\nAvailable SLM models for interaction.\ndef process_command(model, temp_threshold, user_input):\n    \"\"\"Process a single user command\"\"\"\n    # Handles:\n    # 1. Log queries\n    # 2. LED control commands\n    # 3. Sensor data queries\ndef query_log(query, model):\n    \"\"\"Query the log data using SLM\"\"\"\n    # Gets log summary\n    # Creates context-aware prompt\n    # Returns SLM analysis\n\n\nKey Features and Improvements:\n\nData Logging\n\nAutomatic background logging every minute\nComprehensive data storage in CSV format\nCommand history tracking\n\nData Analysis\n\nTemperature and humidity trends\nLED and button state change tracking\nStatistical analysis of sensor data\n\nNatural Language Interaction\n\nLog querying using natural language\nTrend analysis and reporting\nHistorical data access\n\nImproved Error Handling\n\nRobust sensor reading protection\nData validation\nGraceful error recovery\n\n\nThe below flow diagram shows how, in a simplified way, the modules interact and their internal processes.\n\nAnd in this, with more details:\n\nNow, we can run the slm_basic_interaction_log.py, which will call the other two modules.\npython slm_basic_interaction_log.py\nWe can try queries like:\n\"what's the temperature trend?\"\n\"show me button press history\"\n\"turn on the red LED\"\n\"how many times was the button pressed?\"\nExamples:\n\n\nThis modular design separates concerns between data logging/analysis and user interaction, making the system more maintainable and extensible. The SLM integration allows for natural language interaction with current and historical data.\nBelow is an example of the log created."
  },
  {
    "objectID": "raspi/iot/slm_iot.html#evolution-to-structured-command-processing",
    "href": "raspi/iot/slm_iot.html#evolution-to-structured-command-processing",
    "title": "Experimenting with SLMs for IoT Control",
    "section": "Evolution to Structured Command Processing",
    "text": "Evolution to Structured Command Processing\nIn our journey to improve the IoT control system, we should explore a more robust approach to handling commands and responses using structured data models, as we saw in the “Calculating Distance project” section of the SLM Chapter, where the Pydantic python library was used for type checking. This evolution can significantly improve code reliability, maintainability, and extensibility.\nOur original implementation in slm_basic_interaction.py used simple string parsing and direct command processing. While functional, this approach had several limitations:\n\nInconsistent Responses: The SLM could return responses in varying formats, requiring complex parsing logic\nLimited Validation: No built-in validation for command structures or responses\nError-Prone: String parsing could break with slight variations in model outputs\nDifficult Maintenance: Adding new features or command types required modifying multiple code sections\n\n\nStructured Data Models\nAs we did in the SLM chapter, we can use Pydantic to create structured data models that define exactly what our commands and responses should look like. Here’s an example of how we can define our core data structures:\nfrom pydantic import BaseModel, Field\nfrom typing import Optional\n\nclass LEDCommand(BaseModel):\n    red: bool = Field(..., description=\"Whether to turn on red LED\")\n    yellow: bool = Field(..., description=\"Whether to turn on yellow LED\")\n    green: bool = Field(..., description=\"Whether to turn on green LED\")\n    reason: str = Field(..., description=\"Reasoning behind LED state changes\")\n\nclass SensorQuery(BaseModel):\n    temperature_dht: Optional[bool] = Field(False, \n        description=\"Whether to include DHT22 temperature\")\n    temperature_bmp: Optional[bool] = Field(False, \n        description=\"Whether to include BMP280 temperature\")\n    humidity: Optional[bool] = Field(False, \n        description=\"Whether to include humidity\")\n    pressure: Optional[bool] = Field(False, \n        description=\"Whether to include pressure\")\n    button: Optional[bool] = Field(False, \n        description=\"Whether to include button state\")\n\nclass CommandResponse(BaseModel):\n    command_type: str = Field(..., \n        description=\"Type of command (led_control, sensor_query, or system_status)\")\n    led_command: Optional[LEDCommand] = Field(None, \n        description=\"LED control instructions if applicable\")\n    sensor_query: Optional[SensorQuery] = Field(None, \n        description=\"Sensor query specifications if applicable\")\n    response_text: str = Field(..., \n        description=\"Human-readable response to the command\")\nThese models provide several benefits:\n\nType Safety: Automatic validation of data types and structures\nSelf-Documenting: Field descriptions provide built-in documentation\nClear Interface: Explicit definition of what data is expected and provided\nError Handling: Automatic validation with clear error messages\n\n\n\nImproved Command Processing\nThe command processing system should be changed to use these models, ensuring consistent handling:\ndef process_command(model: str, \n                    temp_threshold: float, \n                    user_input: str) -&gt; CommandResponse:\n    \"\"\"Process user command and return structured response\"\"\"\n    # Get current system state\n    temp_dht, hum, temp_bmp, press, button_state = collect_data()\n    \n    # Create structured prompt\n    prompt = f\"\"\"\n    You are an IoT system interface. Current system state:\n    - DHT22: Temperature {temp_dht:.1f}°C, Humidity {hum:.1f}%\n    - BMP280: Temperature {temp_bmp:.1f}°C, Pressure {press:.2f}hPa\n    - Button: {\"pressed\" if button_state else \"not pressed\"}\n    - Temperature threshold: {temp_threshold}°C\n\n    User command: \"{user_input}\"\n\n    Provide a response in this exact JSON format:\n    {{\n        \"command_type\": \"led_control/sensor_query/system_status\",\n        \"led_command\": {{\n            \"red\": boolean,\n            \"yellow\": boolean,\n            \"green\": boolean,\n            \"reason\": \"string\"\n        }},\n        \"sensor_query\": {{\n            \"temperature_dht\": boolean,\n            \"temperature_bmp\": boolean,\n            \"humidity\": boolean,\n            \"pressure\": boolean,\n            \"button\": boolean\n        }},\n        \"response_text\": \"human readable response\"\n    }}\n    \"\"\"\n\n    # Get and parse response\n    response = client.chat.completions.create(\n        model=model,\n        messages=[{\"role\": \"user\", \"content\": prompt}],\n        response_model=CommandResponse,\n        max_retries=3,\n        temperature=0,\n    )\n    \n    # Execute LED commands if present\n    if response.led_command:\n        control_leds(\n            response.led_command.red,\n            response.led_command.yellow,\n            response.led_command.green\n        )\n    \n    return response\n\n\nBenefits of the New Approach\n\nReliability:\n\nStructured responses ensure consistent data format\nAutomatic validation catches errors early\nClear error messages for debugging\n\nMaintainability:\n\nModels separate data structure from logic\nEasy to add new fields or command types\nSelf-documenting code with clear interfaces\n\nExtensibility:\n\nNew command types can be added by extending models\nEasy to add validation rules\nSimple to integrate with other systems\n\nBetter Error Handling:\ntry:\n    response = process_command(model, temp_threshold, user_input)\n    print_status(response, collect_data())\nexcept ValueError as e:\n    print(f\"Invalid command or response: {e}\")\nexcept Exception as e:\n    print(f\"Error processing command: {e}\")\n\n\n\nHandling Different Model Capabilities\nDifferent SLM models may have varying capabilities in generating structured responses. This new approach handles this through:\n\nClear Prompting: Explicit examples and format specifications\nFallback Mechanisms: Graceful degradation for simpler models\nError Recovery: Ability to extract partial information from responses"
  },
  {
    "objectID": "raspi/iot/slm_iot.html#example-usage-1",
    "href": "raspi/iot/slm_iot.html#example-usage-1",
    "title": "Experimenting with SLMs for IoT Control",
    "section": "Example Usage",
    "text": "Example Usage\n# Example command: \"What's the temperature?\"\nResponse:\n{\n    \"command_type\": \"sensor_query\",\n    \"sensor_query\": {\n        \"temperature_dht\": true,\n        \"temperature_bmp\": true,\n        \"humidity\": false,\n        \"pressure\": false,\n        \"button\": false\n    },\n    \"response_text\": \"Current temperature readings: DHT22: 22.4°C, BMP280: 22.8°C\"\n}\n\n# Example command: \"Turn on red LED\"\nResponse:\n{\n    \"command_type\": \"led_control\",\n    \"led_command\": {\n        \"red\": true,\n        \"yellow\": false,\n        \"green\": false,\n        \"reason\": \"User requested red LED activation\"\n    },\n    \"response_text\": \"Activating red LED as requested\"\n}\nThe evolution to structured command processing may significantly improve our IoT control system, providing a more robust and maintainable foundation for future enhancements.\n\nWhile the structured approach adds some overhead, the benefits in reliability and maintainability outweigh the minimal performance impact.\n\nI did not play extensively with this approach, but a first attempt can be found in the GitHub repo."
  },
  {
    "objectID": "raspi/iot/slm_iot.html#next-steps",
    "href": "raspi/iot/slm_iot.html#next-steps",
    "title": "Experimenting with SLMs for IoT Control",
    "section": "Next Steps",
    "text": "Next Steps\nThis lab involved experimenting with simple applications and verifying the feasibility of using an SLM to control IoT devices. The final result is far from something usable in the real world, but it can give the start point for more interesting applications. Below are some observations and suggestions for improvement:\n\nSLM responses can be probabilistic and inconsistent. To increase reliability, consider implementing a confidence threshold or voting system using multiple prompts/responses.\nTry to add data validation and sanity checks for sensor readings before passing them to the SLM.\nApply Structured Response Parsing as discussed early. Future improvements in this approuch could include:\n\nAdd more sophisticated validation rules\nImplement command history tracking\nAdd support for compound commands\nIntegrate with the logging system\nAdd user permission levels\nImplement command templates for common operations\n\nConsider implementing a fallback mechanism when SLM responses are ambiguous or inconsistent.\nStudy using RAG and fine-tuning to increase the system’s reliability when using very small models.\nConsider adding input validation for user commands to prevent potential issues.\nThe current implementation queries the SLM for every command. We did it to study how SLMs would behave. We should consider implementing a caching mechanism for common queries.\nSome simple commands could be handled without SLM intervention. We can do it programmatically.\nConsider implementing a proper state machine for LED control to ensure consistent behavior.\nImplement more sophisticated trend analysis using statistical methods.\nAdd support for more complex queries combining multiple data points."
  },
  {
    "objectID": "raspi/iot/slm_iot.html#resourses",
    "href": "raspi/iot/slm_iot.html#resourses",
    "title": "Experimenting with SLMs for IoT Control",
    "section": "Resourses",
    "text": "Resourses\n\nPython Scripts"
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "TinyML4D\nTinyML Made Easy, an eBook collection of a series of Hands-On tutorials, is part of the TinyML4D, an initiative to make Embedded Machine Learning (TinyML) education available to everyone, explicitly enabling innovative solutions for the unique challenges Developing Countries face."
  },
  {
    "objectID": "references.html#to-learn-more",
    "href": "references.html#to-learn-more",
    "title": "References",
    "section": "To learn more:",
    "text": "To learn more:\n\nOnline Courses\n\nHarvard School of Engineering and Applied Sciences - CS249r: Tiny Machine Learning\nProfessional Certificate in Tiny Machine Learning (TinyML) – edX/Harvard\nIntroduction to Embedded Machine Learning - Coursera/Edge Impulse\nComputer Vision with Embedded Machine Learning - Coursera/Edge Impulse\nUNIFEI-IESTI01 TinyML: “Machine Learning for Embedding Devices”\n\n\n\nBooks\n\n“Python for Data Analysis” by Wes McKinney\n“Deep Learning with Python” by François Chollet - GitHub Notebooks\n“TinyML” by Pete Warden and Daniel Situnayake\n“TinyML Cookbook 2nd Edition” by Gian Marco Iodice\n“Technical Strategy for AI Engineers, In the Era of Deep Learning” by Andrew Ng\n“AI at the Edge” book by Daniel Situnayake and Jenny Plunkett\n“XIAO: Big Power, Small Board” by Lei Feng and Marcelo Rovai\n“Machine Learning Systems” by Vijay Janapa Reddi\n\n\n\nProjects Repository\n\nEdge Impulse Expert Network"
  },
  {
    "objectID": "about_the_author.html",
    "href": "about_the_author.html",
    "title": "About the author",
    "section": "",
    "text": "Marcelo Rovai, a Brazilian living in Chile, is a recognized engineering and technology education figure. He holds the title of Professor Honoris Causa from the Federal University of Itajubá (UNIFEI), Brazil. His educational background includes an Engineering degree from UNIFEI and a specialization from the Polytechnic School of São Paulo University (POLI/USP). Further enhancing his expertise, he earned an MBA from IBMEC (INSPER) and a Master’s in Data Science from the Universidad del Desarrollo (UDD) in Chile.\nWith a career spanning several high-profile technology companies such as AVIBRAS Airspace, AT&T, NCR, and IGT, where he served as Vice President for Latin America, he brings industry experience to his academic endeavors. He is a prolific writer on electronics-related topics and shares his knowledge through open platforms like Hackster.io.\nIn addition to his professional pursuits, he is dedicated to educational outreach, serving as a volunteer professor at UNIFEI and engaging with the TinyML4D group and the EDGE AIP– the Academia-Industry Partnership of EDGEAI Foundation as a Co-Chair, promoting TinyML education in developing countries. His work underscores a commitment to leveraging technology for societal advancement.\nLinkedIn profile: https://www.linkedin.com/in/marcelo-jose-rovai-brazil-chile/\nLectures, books, papers, and tutorials: https://github.com/Mjrovai/TinyML4D"
  }
]