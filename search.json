[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Edge AI Engineering",
    "section": "",
    "text": "Preface\nIn the rapidly evolving technology landscape, the convergence of artificial intelligence and edge computing is one of the most exciting frontiers. This intersection promises to revolutionize how we interact with the world around us, bringing intelligence and decision-making capabilities directly to the devices we use every day. At the heart of this revolution lies the Raspberry Pi, a powerful yet accessible single-board computer (SBC) that has democratized computing and now stands poised to do the same for edge AI.\nThis book, which serves as the official textbook for IESTI05 Edge AI Engineering at the Federal University of Itajubá (UNIFEI) in Brazil, embodies both a passion for technology and a conviction in its capacity to address real-world problems. While developed to support UNIFEI’s engineering curriculum, the content is valuable for all learners, whether in academic settings or pursuing independent study.\n“Edge AI Engineering: Hands-on with the Raspberry Pi” is not just about theory or abstract concepts. It’s about getting your hands dirty, writing code, training models, and seeing your creations come to life. Each chapter blends foundational knowledge with practical application, focusing on what’s possible with the Raspberry Pi platform.\nFrom the compact Raspberry Pi Zero to the more powerful Pi 5, we explore how these incredible devices can become the brains of intelligent systems—recognizing images, understanding speech, detecting objects, and even running small language models. Each project serves as a stepping stone, building your skills and confidence as you progress.\nBeyond the technical skills, this book aims to instill something more valuable – a sense of curiosity and possibility. The field of edge AI is still in its infancy, with new applications and techniques emerging daily. By mastering the fundamentals presented here, you’ll be well-equipped to explore these frontiers, perhaps even pushing the boundaries of what’s possible on edge devices.\nWhether you’re a student seeking to understand AI’s practical applications, a professional looking to expand your skill set, or an enthusiast eager to add intelligence to your projects, we hope this book serves as both a guide and an inspiration.\nAs you embark on this journey, remember that every expert was once a beginner. The learning path is filled with challenges and moments of joy and discovery. Embrace both, and let your creativity guide you.\nThank you for joining us on this exciting adventure into edge machine learning. Let’s begin exploring what’s possible when we bring AI to the edge, one Raspberry Pi at a time.\nHappy coding, and may your models always converge!\nProf. Marcelo Rovai\nJanuary, 2026"
  },
  {
    "objectID": "Acknowledgements.html",
    "href": "Acknowledgements.html",
    "title": "Acknowledgments",
    "section": "",
    "text": "I extend my deepest gratitude to the entire TinyML4D Academic Network, comprised of distinguished professors, researchers, and professionals. Notable contributions from Marco Zennaro, Ermanno Petrosemoli, Brian Plancher, José Alberto Ferreira, Jesus Lopez, Diego Mendez, Shawn Hymel, Dan Situnayake, Pete Warden, and Laurence Moroney have been instrumental in advancing our understanding of Embedded Machine Learning (TinyML) and Edge AI.\nSpecial commendation is reserved for Professor Vijay Janapa Reddi of Harvard University. His steadfast belief in the transformative potential of open-source communities, coupled with his invaluable guidance and teachings, has been a beacon and a cornerstone of our efforts from the beginning.\nAcknowledging these individuals, we pay tribute to the collective wisdom and dedication that have enriched this field and our work.\n\n\nGoogle Nano Banana and OpenAI’s GPT were used to generate some of the images in the book. Claude Sonnet and Perplexity helped with code and text reviews."
  },
  {
    "objectID": "intro.html#edge-ai-engineering",
    "href": "intro.html#edge-ai-engineering",
    "title": "Introduction",
    "section": "Edge AI Engineering",
    "text": "Edge AI Engineering\nIn today’s rapidly evolving technological landscape, the convergence of artificial intelligence and edge computing represents one of the most promising frontiers of innovation. Edge AI—the practice of running AI algorithms locally on hardware devices rather than in the cloud—transforms how we interact with technology daily, enabling more responsive, private, and efficient intelligent systems.\nThis book, “Edge AI Engineering: Hands-on with the Raspberry Pi,” is your practical guide to this exciting field. We’ll explore fixed-function AI (reactive systems that process specific inputs) and generative AI (proactive systems that create new content) through hands-on projects using the versatile and accessible Raspberry Pi platform.\n\nWhy Edge AI Matters\nTraditional AI deployment often relies on cloud infrastructure, which requires constant connectivity and introduces latency. Edge AI addresses these limitations by bringing intelligence directly to where data is generated and actions occur. This approach offers several compelling advantages:\n\nReduced latency: Process data locally for near-instantaneous responses\nEnhanced privacy: Keep sensitive information on your device rather than sending it to remote servers\nNetwork independence: Maintain functionality even without internet connectivity\nLower bandwidth usage: Process data locally, sending only relevant results when needed\nEnergy efficiency: Optimize processing for resource-constrained environments\n\n\n\nThe Raspberry Pi Advantage\nThe Raspberry Pi, with its combination of affordability, processing capability, and extensive GPIO options, provides an ideal platform for exploring Edge AI concepts. From the compact Raspberry Pi Zero 2W to the more powerful Pi 5, these devices offer:\n\nSufficient computational power for running optimized AI models\nA complete Linux-based operating system for straightforward development\nExtensive connectivity options for integrating with sensors and actuators\nA vibrant community and ecosystem of libraries and tools\nAn accessible entry point for students, hobbyists, and professionals alike\n\n\n\nWhat You’ll Learn\nThis book takes a progressive approach to Edge AI engineering, starting with foundational concepts and building toward more advanced applications:\n\nEssential setup and configuration: Prepare your Raspberry Pi for Edge AI development\nComputer vision applications: Implement image classification and object detection systems\nSmall Language Models (SLMs): Run and optimize language models directly on your Raspberry Pi\nVision-Language Models: Explore multimodal AI with Florence-2\nPhysical computing integration: Connect AI systems with sensors and actuators\nAdvanced optimization techniques: Enhance model performance through methods like RAG, agents, and function calling\n\nEach chapter includes detailed explanations, step-by-step instructions, and practical projects demonstrating real-world applications of Edge AI concepts.\n\n\nWho This Book Is For\nWhether you’re a student exploring AI for the first time, an educator developing a curriculum, a maker building innovative projects, or a professional seeking to expand your skills, this book provides the knowledge and hands-on experience needed to successfully implement Edge AI solutions on the Raspberry Pi platform.\nJoin us on this journey to the edge of AI innovation, where we’ll bridge theory and practice through engaging, accessible projects that demonstrate the transformative potential of intelligent edge computing."
  },
  {
    "objectID": "about_book.html#key-features",
    "href": "about_book.html#key-features",
    "title": "About this Book",
    "section": "Key Features",
    "text": "Key Features\n\nProgressive Learning Path: The book structure follows a natural progression from basic to advanced concepts, beginning with foundational computer vision applications and advancing to generative AI techniques.\nModel-Specific Optimizations: Each chapter provides targeted guidance for specific Raspberry Pi models, helping you maximize performance whether you’re using a Pi Zero 2W or a Pi 5.\nOpen-Source Foundation: We emphasize accessible tools and frameworks, including Edge Impulse Studio, TensorFlow Lite, PyTorch, Transformers, and Ollama, ensuring you can continue your learning journey with widely available resources.\nPractical Problem-Solving: Rather than abstract exercises, each project addresses real-world challenges that demonstrate Edge AI’s practical value.\nResource Optimization Techniques: Learn essential strategies for deploying AI on resource-constrained devices, balancing performance needs with hardware limitations.\nCross-Domain Applications: Explore implementations spanning computer vision, natural language processing, and physical computing, showcasing the versatility of Edge AI."
  },
  {
    "objectID": "about_book.html#structure-and-organization",
    "href": "about_book.html#structure-and-organization",
    "title": "About this Book",
    "section": "Structure and Organization",
    "text": "Structure and Organization\nThe book is organized into two main sections:\n\nFixed Function AI (Computer Vision): Chapters covering image classification, object detection, and specialized applications like object counting. We also explore the section “Hardware Acceleration for Fixed-Function AI.”\nGenerative AI (Language and Vision Models): Chapters exploring Small Language Models, Vision-Language Models, physical computing integration, and advanced optimization techniques.\n\nEach chapter follows a consistent format that includes:\n\nConceptual background and theory\nStep-by-step implementation guides\nPractical projects with complete code\nPerformance optimization strategies\nIdeas for further exploration"
  },
  {
    "objectID": "about_book.html#prerequisites",
    "href": "about_book.html#prerequisites",
    "title": "About this Book",
    "section": "Prerequisites",
    "text": "Prerequisites\nWhile designed to be accessible, readers will benefit from:\n\nBasic Python programming knowledge\nFamiliarity with Linux command-line basics\nElementary understanding of machine learning concepts\nPrevious experience with Raspberry Pi (helpful but not required)\n\nBy completing this book, you’ll possess the skills to design, implement, and optimize Edge AI applications across a wide range of use cases, leveraging the unique capabilities of the Raspberry Pi platform to bring intelligence to the edge."
  },
  {
    "objectID": "Classification_of_AI_Applications.html#fixed-function-ai-vs.-generative-ai",
    "href": "Classification_of_AI_Applications.html#fixed-function-ai-vs.-generative-ai",
    "title": "Classification of AI Applications",
    "section": "Fixed Function AI vs. Generative AI",
    "text": "Fixed Function AI vs. Generative AI\nAI applications can be broadly categorized into two approaches that represent different capabilities, interaction models, and implementation strategies:\n\nFixed Function AI (Reactive)\nFixed Function AI, or Reactive AI, operates by analyzing specific inputs according to predetermined patterns and rules and then producing consistent outputs for given scenarios. These systems:\n\nRespond to specific triggers: They activate only when presented with particular inputs.\nFollow defined patterns: Their behavior is predictable and consistent.\nExcel at structured tasks: They perform exceptionally well at classification, detection, and pattern recognition\nOperate within boundaries: Their capabilities are limited to their specific programming.\n\nIn the first part of this book (Chapters 2-4), we explore fixed-function AI through computer vision applications:\n\nImage classification for identifying objects in images\nObject detection for locating and labeling multiple objects\nSpecialized detection applications like counting objects\n\nThese applications demonstrate how edge devices can deliver reliable, efficient AI in constrained environments, focusing on specific, well-defined tasks.\n\n\nGenerative AI (Proactive)\nGenerative AI, also known as Proactive AI, represents a fundamental shift in capability. These systems can:\n\nCreate new content: They generate novel text, images, or solutions.\nUnderstand context: They interpret and respond to nuanced situations.\nEngage in dialogue: They maintain contextual awareness across interactions.\nAdapt to novel scenarios: They apply knowledge to situations beyond their explicit training.\n\nThe second part of this book (Chapters 5-9) explores Generative AI at the edge:\n\nSmall Language Models that bring conversational AI to edge devices\nVision-Language Models that combine visual and textual understanding\nPhysical computing integration that connects AI to the real world\nAdvanced techniques to enhance edge AI capabilities\n\nThis progression from Fixed Function to Generative AI mirrors the evolution of artificial intelligence itself—from specialized systems designed for specific tasks to more flexible, creative systems capable of addressing a broader range of challenges.\n\n\nSummary Table\n\n\n\n\n\n\n\n\n\nAI Type\nCore Focus\nExample Applications\nTypical Use Cases\n\n\n\n\nFixed Function (Reactive)\nData analysis, assessment, automation\nFraud detection, spam filters, image recognition\nBanking, healthcare diagnostics, security systems\n\n\nGenerative (Proactive)\nContent creation, anticipation, dialogue\nChatGPT, DALL·E, predictive maintenance, smart assistants\nContent creation, customer support, design, automation\n\n\n\n\n\nConclusion\n\nFixed Function (Reactive) AI is ideal for applications requiring efficiency, predictability, and low resource use, where tasks are well-defined and do not require creative output or adaptation.\nGenerative (Proactive) AI is suited for scenarios demanding creativity, personalization, and anticipatory actions. It enables richer, more human-like interactions and innovative solutions across industries."
  },
  {
    "objectID": "Classification_of_AI_Applications.html#the-edge-ai-advantage",
    "href": "Classification_of_AI_Applications.html#the-edge-ai-advantage",
    "title": "Classification of AI Applications",
    "section": "The Edge AI Advantage",
    "text": "The Edge AI Advantage\nBoth Fixed Function and Generative AI gain unique benefits when deployed at the edge:\n\nReduced latency: Processing happens locally, eliminating network delays\nEnhanced privacy: Sensitive data remains on-device\nOperational reliability: Systems function regardless of network connectivity\nResource efficiency: Optimized models utilize limited hardware effectively\n\nBy understanding these fundamental classifications, you’ll realize how different AI approaches serve distinct purposes and how each can be effectively implemented on edge devices like the Raspberry Pi.\nAs we progress through this book, this classification framework will help contextualize each project and technique, connecting individual implementations to broader AI concepts and applications."
  },
  {
    "objectID": "raspi/setup/setup.html#introduction",
    "href": "raspi/setup/setup.html#introduction",
    "title": "Setup",
    "section": "Introduction",
    "text": "Introduction\nThe Raspberry Pi is a powerful and versatile single-board computer that has become an essential tool for engineers across various disciplines. Developed by the Raspberry Pi Foundation, these compact devices offer a unique combination of affordability, computational power, and extensive GPIO (General Purpose Input/Output) capabilities, making them ideal for prototyping, embedded systems development, and advanced engineering projects.\n\nKey Features\n\nComputational Power: Despite their small size, Raspberry Pis offer significant processing capabilities, with the latest models featuring multi-core ARM processors and up to 8GB of RAM.\nGPIO Interface: The 40-pin GPIO header enables direct interaction with sensors, actuators, and other electronic components, facilitating hardware-software integration.\nExtensive Connectivity: Built-in Wi-Fi, Bluetooth, Ethernet, and multiple USB ports enable a wide range of communication and networking projects.\nLow-Level Hardware Access: Raspberry Pis provide access to interfaces such as I2C, SPI, and UART, enabling detailed control and communication with external devices.\nReal-Time Capabilities: With proper configuration, Raspberry Pis can be used for soft real-time applications, making them suitable for control systems and signal processing tasks.\nPower Efficiency: Low power consumption enables battery-powered and energy-efficient designs, especially in models like the Pi Zero.\n\n\n\nRaspberry Pi Models (covered in this book)\n\nRaspberry Pi Zero 2 W (Raspi-Zero):\n\nIdeal for: Compact embedded systems\nKey specs: 1GHz single-core CPU (ARM Cortex-A53), 512MB RAM, minimal power consumption (usually, around 600mW in Idle. Also, the power-off (“zombie current”) is very low, at around 45 mA (225 mW), significantly lower than on full-size Raspberry Pi models.\n\nRaspberry Pi 5 (Raspi-5):\n\nIdeal for more demanding applications, such as edge computing, computer vision, and edgeAI applications, including LLMs.\nKey specs: 2.4GHz quad-core CPU (ARM Cortex A-76), up to 8GB RAM, PCIe interface for expansions, higher power consumption (usually, around 3-3.5W in Idle)\n\n\n\n\nEngineering Applications\n\nEmbedded Systems Design: Develop and prototype embedded systems for real-world applications.\nIoT and Networked Devices: Create interconnected devices and explore protocols like MQTT, CoAP, and HTTP/HTTPS.\nControl Systems: Implement feedback control loops, PID controllers, and interface with actuators.\nComputer Vision and AI: Utilize libraries like OpenCV and TensorFlow Lite for image processing and machine learning at the edge.\nData Acquisition and Analysis: Collect sensor data, perform real-time analysis, and create data logging systems.\nRobotics: Build robot controllers, implement motion planning algorithms, and interface with motor drivers.\nSignal Processing: Perform real-time signal analysis, filtering, and DSP applications.\nNetwork Security: Set up VPNs, firewalls, and explore network penetration testing.\n\nThis lab will guide you through setting up the most common Raspberry Pi models, so you can get started on your machine learning project quickly. We’ll cover hardware setup, operating system installation, and initial configuration, focusing on preparing your Pi for Machine Learning applications."
  },
  {
    "objectID": "raspi/setup/setup.html#hardware-overview",
    "href": "raspi/setup/setup.html#hardware-overview",
    "title": "Setup",
    "section": "Hardware Overview",
    "text": "Hardware Overview\n\nRaspberry Pi Zero 2W\n\n\nProcessor: 1GHz quad-core 64-bit Arm Cortex-A53 CPU\nRAM: 512MB SDRAM\nWireless: 2.4GHz 802.11 b/g/n wireless LAN, Bluetooth 4.2, BLE\nPorts: Mini HDMI, micro USB OTG, CSI-2 camera connector\nPower: 5V/2.5A via micro USB port 12.5W Micro USB power supply\n\n\n\nRaspberry Pi 5\n\n\nProcessor:\n\nPi 5: Quad-core 64-bit Arm Cortex-A76 CPU @ 2.4GHz\nPi 4: Quad-core Cortex-A72 (ARM v8) 64-bit SoC @ 1.5GHz\n\nRAM: 2GB, 4GB, or 8GB options (8GB recommended for AI tasks)\nWireless: Dual-band 802.11ac wireless (2.4 GHz and 5 GHz), Bluetooth 5.0\nPorts: 2 × micro HDMI ports, 2 × USB 3.0 ports, 2 × USB 2.0 ports, CSI camera port, DSI display port\nPower: 5V/5A, 5V/3A limits peripherals to 600mA, via USB-C connector 27W USB-C power supply\n\n\nIn the labs, we will use different names to address the Raspberry Pi: Raspi, Raspi-5, Raspi-Zero, etc. Usually, “Raspi” or “Raspberry Pi” is used when the instructions or comments apply to all models."
  },
  {
    "objectID": "raspi/setup/setup.html#installing-the-operating-system",
    "href": "raspi/setup/setup.html#installing-the-operating-system",
    "title": "Setup",
    "section": "Installing the Operating System",
    "text": "Installing the Operating System\n\nThe Operating System (OS)\nAn operating system (OS) is essential software that manages computer hardware and software resources, providing standard services for computer programs. It is the core software that runs on a computer, serving as an intermediary between hardware and application software. The OS oversees the computer’s memory, processes, device drivers, files, and security protocols.\n\nKey functions:\n\nProcess management: Allocating CPU time to different programs\nMemory management: Allocating and freeing up memory as needed\nFile system management: Organizing and keeping track of files and directories\nDevice management: Communicating with connected hardware devices\nUser interface: Providing a way for users to interact with the computer\n\nComponents:\n\nKernel: The core of the OS that manages hardware resources\nShell: The user interface for interacting with the OS\nFile system: Organizes and manages data storage\nDevice drivers: Software that allows the OS to communicate with hardware\n\n\nThe Raspberry Pi runs a specialized version of Linux designed for embedded systems. This operating system, typically a variant of Debian called Raspberry Pi OS (formerly Raspbian), is optimized for the Pi’s ARM-based architecture and limited resources.\n\nThe latest version of Raspberry Pi OS (Dec 25) is based on Debian Trixie.\n\nKey features:\n\nLightweight: Tailored to run efficiently on the Pi’s hardware.\nVersatile: Supports a wide range of applications and programming languages.\nOpen-source: Allows for customization and community-driven improvements.\nGPIO support: Enables interaction with sensors and other hardware through the Pi’s pins.\nRegular updates: Continuously improved for performance and security.\n\nEmbedded Linux on the Raspberry Pi provides a full-featured operating system in a compact package, making it ideal for projects ranging from simple IoT devices to more complex edge machine-learning applications. Its compatibility with standard Linux tools and libraries makes it a powerful platform for development and experimentation.\n\n\nInstallation\nTo use the Raspberry Pi, we will need an operating system. By default, Raspberry Pis check for an operating system on any SD card inserted in the slot, so we should install an operating system using Raspberry Pi Imager.\n\nIn November 2025, the Raspberry Pi Imager 2.0 was launched. It brings a new wizard interface, the opportunity to pre-configure Raspberry Pi Connect, and improved accessibility for screen readers and other assistive technologies.\n\nRaspberry Pi Imager is a tool for downloading and writing images on macOS, Windows, and Linux. It includes many popular operating system images for Raspberry Pi. We will also use the Imager to preconfigure credentials and remote access settings.\nFollow the steps to install the OS on your Raspberry Pi.\n\nDownload and install the Raspberry Pi Imager on your computer.\nInsert a microSD card into your computer (a 32GB SD card is recommended) .\nOpen Raspberry Pi Imager and select your Raspberry Pi model.\n\n\n\nChoose the appropriate operating system:\n\nFor Raspi-Zero: For example, you can select under Raspberry Pi OS (Other), Raspberry Pi OS Lite (64-bit).\n\n\n\nDue to the Raspberry Pi Zero’s limited SDRAM (512 MB), the recommended OS is the 32-bit version. However, to run some machine learning models, such as the YOLO from Ultralitics, we should use the 64-bit version. Although the Raspi-Zero can run a desktop, we will choose the LITE version (no Desktop) to reduce the RAM needed for regular operation.\n\n\nFor Raspi-5: We can select the full 64-bit version, which includes a desktop: Raspberry Pi OS (64-bit)\n\n\nSelect your microSD card as the storage device.\nClick Next, then go to the Customization tab. The imager will guide you through setting the hostname, the Raspberry Pi username and password, configuring WiFi, and enabling SSH (Very important!).\nWrite the image to the microSD card.\n\n\nIn the examples here, we will use different hostnames depending on the device used: raspi, raspi-5, raspi-Zero, etc. Please replace it with the one you’re currently using.\n\n\n\nInitial Configuration\n\nInsert the microSD card into your Raspberry Pi.\nConnect the power to boot up the Raspberry Pi. For the Rasp-5, use a 5V/3A (or 5A) external Power supply. For the Rasp-Zero, use a 5V/2.5A external Power supply (Optionally, for light use, you can use the computer’s USB port to power the Rasp-Zero).\n\n\n\nPlease wait for the initial boot process to complete (it may take a few minutes).\n\n\nYou can find the most common Linux commands for the Raspberry Pi here or here."
  },
  {
    "objectID": "raspi/setup/setup.html#remote-access",
    "href": "raspi/setup/setup.html#remote-access",
    "title": "Setup",
    "section": "Remote Access",
    "text": "Remote Access\n\nSSH Access\nThe easiest way to interact with the Raspi-Zero is via SSH (“Headless”). You can use a Terminal (MAC/Linux), PuTTy (Windows), or any other.\n\nThe Raspberry Pi and the notebook should be on the same WiFi network. Note that the Raspberry Pi 5 supports dual-band 802.11ac Wi-Fi (2.4 GHz and 5 GHz), whereas the Raspberry Pi Zero 2W supports only 2.4 GHz.\n\n\nFind your Raspberry Pi’s IP address (for example, check your router).\nOn your computer, open a terminal and connect via SSH:\nssh username@[raspberry_pi_ip_address]   \nAlternatively, if you do not have the IP address, you can try, for example, ssh mjrovai@rpi-5.local , ssh mjrovai@raspi.local , etc.: bash ssh username@hostname.local When you see the prompt:\nmjrovai@rpi-5:~ $\nIt means you are interacting with your Raspberry Pi remotely.\nIt is a good practice to regularly update/upgrade the system. For that, you should run:\nsudo apt-get update\nsudo apt upgrade\nsudo reboot  # Reboot to ensure all updates take effect\nYou should confirm the Raspberry Pi IP address. On the terminal, you can use:\nhostname -I\n\n\nAlso, check the current Python version:\npython3 --version\nOnce we use the latest Raspberry Pi OS (based on Debian Trixie), it should be: 3.13:\n\nAs of today (January 2026), some packages, such as ExecuTorch, officially support only Python versions 3.10-3.12. Python 3.13.5 is too new and will likely cause compatibility issues. Since Debian Trixie ships with Python 3.13 by default, we’ll need to install a compatible Python version alongside it.\nOne solution is to install Pyenv, so that we can easily manage multiple Python versions for different projects without affecting the system Python. We will do it in the appropriate Lab. For now, we will keep the system Python.\n\nIf the Raspberry Pi OS is the legacy, the Python version should be 3.11, and it is not necessary to install Pyenv.\n\n\n\nTo shut down the Raspi via terminal:\nWhen you want to turn off your Raspberry Pi, there are better ideas than just pulling the power cord. This is because the Raspi may still be writing data to the SD card, in which case merely powering down may result in data loss or, even worse, a corrupted SD card.\nFor a safety shutdown, use the command line:\nsudo shutdown -h now\n\nTo avoid possible data loss and SD card corruption, before removing power, wait a few seconds after shutdown for the Raspberry Pi’s LED to stop blinking and go dark. Once the LED goes out, it’s safe to power down.\n\n\n\nTransfer Files between the Raspberry Pi and a computer\nTransferring files between the Raspberry Pi and our main computer can be done using a USB drive, via the terminal (scp), or an FTP program over the network.\n\nUsing Secure Copy Protocol (scp):\n\nCopy files to your Raspberry Pi\nLet’s create a text file on our computer, for example, test.txt.\n\n\nYou can use any text editor. In the same terminal, an option is nano.\n\nTo copy the file named test.txt from your personal computer to a user’s home folder on your Raspberry Pi, run the following command from the directory containing test.txt, replacing the &lt;username&gt; placeholder with the username you use to log in to your Raspberry Pi and the &lt;pi_ip_address&gt; placeholder with your Raspberry Pi’s IP address:\n$ scp test.txt &lt;username&gt;@&lt;pi_ip_address&gt;:~/\n\nNote that ~/ means we will move the file to the ROOT of our Raspberry Pi. You can choose any folder in your Raspberry Pi. But you should create the folder before you run scp, since scp won’t create folders automatically.\n\nFor example, let’s transfer the file test.txt to the ROOT of my Raspberry Pi Zero, which has an IP of 192.168.4.210:\nscp test.txt mjrovai@192.168.4.210:~/\n\nI use a different profile to differentiate the terminals. The action above occurs on your computer. Now, let’s go to our Raspi (using SSH) and check if the file is there:\n\n\n\nCopy files from your Raspberry Pi\nTo copy a file named test.txt from a user’s home directory on a Raspberry Pi to the current directory on another computer, run the following command on your Host Computer:\n$ scp &lt;username&gt;@&lt;pi_ip_address&gt;:myfile.txt .\nFor example:\nOn the Raspi, let’s create a copy of the file with another name:\ncp test.txt test_2.txt\nAnd on the Host Computer (in my case, a Mac)\nscp mjrovai@192.168.4.210:test_2.txt .\n\n\n\n\nTransferring files using FTP\nTransferring files via FTP, such as FileZilla FTP Client, is also possible and much easier to use. Follow the instructions to install the program on your Desktop, then use the Raspberry Pi’s IP address as the Host. For example:\nsftp://192.168.4.210\nEnter your Raspberry Pi username and password. Pressing Quickconnect opens two windows, one for your host computer desktop (right) and another for the Raspberry Pi (left)."
  },
  {
    "objectID": "raspi/setup/setup.html#increasing-swap-memory",
    "href": "raspi/setup/setup.html#increasing-swap-memory",
    "title": "Setup",
    "section": "Increasing SWAP Memory",
    "text": "Increasing SWAP Memory\nUsing htop, a cross-platform interactive process viewer, we can easily monitor resources on our Raspberry Pi in real time, including the list of processes, the running CPUs, and the memory usage. To lunch hop, enter with the command on the terminal:\nhtop\n\nRegarding memory, among the Raspberry Pi family, the Raspberry Pi Zero has the least SRAM (500 MB), compared to 2GB to 16GB on the Raspberry Pi 4 or 5.\nOn any Raspberry Pi, it is possible to increase/modify the system’s available memory using “Swap.” Swap memory, also known as swap space, is a technique used in computer operating systems to temporarily store data from RAM (Random Access Memory) on the SD card when the physical RAM is fully utilized. This allows the operating system (OS) to continue running even when RAM is full, which can prevent system crashes or slowdowns.\nSwap memory benefits devices with limited RAM, such as the Raspberry Pi Zero. Increasing swap can help run more demanding applications or processes, but it’s essential to balance this with the potential performance impact of frequent disk access.\nWe can check the swap memory using htopor by the command:\nsudo swapon --show\n\n\nOn the Debian Trixie, 2 MB of swap memory is configured by default on the Rasp-5 and 512MB on the Raspi-Zero\n\nBy default, the Rapi-Zero’s SWAP (zram) memory is 512MB, which may be insufficient for running more complex and demanding Machine Learning applications (for example, YOLO). So, let’s increase it to 2MB:\nOn Zero 2 W with Raspberry Pi OS Trixie, you configure rpi-swap by dropping a small config file into /etc/rpi/swap.conf.d/. The same mechanism works on Pi 5 and Zero‑2.\nBelow are two common patterns: fixed swap file (on SD/SSD) and fixed zram size (in RAM). ### Fixed‑size swap file (e.g., 2 GB on SD)\n\nCreate the override directory (if not already there):\nsudo mkdir -p /etc/rpi/swap.conf.d\nCreate an override file:\nsudo nano /etc/rpi/swap.conf.d/fixedswapsize.conf\nPut this in it (example: 2 GB = 2048 MiB):\n[Main]\nMechanism=swapfile\n\n[File]\nFixedSizeMiB=2048\n\nMechanism=swapfile tells rpi-swap to use a classic swap file.\n\nFixedSizeMiB is the exact size we want, in MiB.\n\nApply:\nsudo systemctl restart rpi-swap\nswapon --show\n\nWe should now see /var/swap with the size we set.\n\nFixed‑size zram (better for Zero‑2’s SD card)\nIf we want to keep everything in compressed RAM (no SD wear), we can set a fixed zram size instead:\n\nCreate the same drop‑in file:\nsudo nano /etc/rpi/swap.conf.d/fixedzram.conf\nSet the zram swap:\n[Main]\nMechanism=zram\n\n[Zram]\nFixedSizeMiB=2048\nApply and check:\nsudo systemctl restart rpi-swap\nswapon --show\n\nWe should see /dev/zram0 with the new size.\n\n\nNotes specific to Zero 2 W Trixie\n\nDefault Trixie on Zero‑2 uses rpi-swap with zram sized roughly to RAM×1, capped by MaxSizeMiB (usually 2048), and no swap file unless you add it.\nIf we create multiple drop‑ins in /etc/rpi/swap.conf.d/, they are merged; keep it simple with one clear override file per board/use‑case.\n\n\nTo keep htop running, open another terminal window to continuously interact with the Raspberry Pi."
  },
  {
    "objectID": "raspi/setup/setup.html#installing-a-camera",
    "href": "raspi/setup/setup.html#installing-a-camera",
    "title": "Setup",
    "section": "Installing a Camera",
    "text": "Installing a Camera\nThe Raspberry Pi is an excellent device for computer vision applications that require a camera. We can install a camera module connected to the Raspberry Pi CSI (Camera Serial Interface) port, or a standard USB webcam on the micro-USB port using a USB OTG adapter (Raspi-Zero) or directly on the USB port on the Raspi-5.\n\nUSB Webcams generally have inferior image quality compared to camera modules that connect to the CSI port. They can not be controlled using the raspistill and rasivid commands in the terminal, or the picamera recording package in Python. Nevertheless, there may be reasons you want to connect a USB camera to your Raspberry Pi, such as setting up multiple cameras with a single Raspberry Pi, avoiding long cables, or simply because you have such a camera on hand.\n\n\nInstalling a Camera Module on the CSI port\nThere are now several Raspberry Pi camera modules. The original 5-megapixel model was releasedin 2013, followed by the 8-megapixel Camera Module 2, released in 2016. The latest camera model is the 12-megapixel Camera Module 3, released in 2023.\nThe original 5MP camera (Arducam OV5647) is no longer available from Raspberry Pi but can be found from several alternative suppliers. Below is an example of such a camera on a Raspberry Pi Zero.\n\nHere is another example of a v2 Camera Module, which has a Sony IMX219 8-megapixel sensor:\n\nFirst, try to list the installed cameras:\nrpicam-hello --list-cameras\nIf the command is not recognized, install\nsudo apt install libcamera-apps\nTry to list the installed camera again. If Ok, you should see something like:\n\nAny camera module will work on the Raspberry Pi, but for that, it is possible that the config.txt file must be updated:\nsudo nano /boot/firmware/config.txt\nAt the bottom of the file, for example, to use the 5MP Arducam OV5647 camera, add the line:\ndtoverlay=ov5647,cam0\nOr for the v2 module, wich has the 8MP Sony IMX219 camera:\ndtoverlay=imx219,cam0\nSave the file (CTRL+O [ENTER] CRTL+X) and reboot the Raspi:\nSudo reboot\nAfter the boot, you can see if the camera is listed:\nrpicam-hello --list-cameras\n\n\n\nlibcamerais an open-source software library that supports camera systems directly on Linux for Arm processors. It minimizes the amount of proprietary code running on the Broadcom GPU.\n\nLet’s capture a JPEG image with a resolution of 640 x 480 for testing and save it to a file named test_cli_camera.jpg\nrpicam-jpeg --output test_cli_camera.jpg --width 640 --height 480\nTo view the saved file, we should use ls, which lists the contents of the current directory:\n\nAs before, we can use scp to view the image:\n\nAlternatively, you can transfer it to your desktop using FileZilla.\n\n\nInstalling a USB WebCam\n\nPower off the Raspi:\n\nsudo shutdown -h no\n\nConnect the USB Webcam (USB Camera Module 30 fps, 1280x720) to your Raspberry Pi (in this example, I am using a Raspberry Pi Zero, but the instructions work for all Raspberry Pis).\n\n\n\nPower on again and run the SSH\nTo check if your USB camera is recognized, run:\n\nlsusb\nYou should see your camera listed in the output.\n\n\nTo take a test picture with your USB camera, use:\n\nfswebcam test_image.jpg\nThis will save an image named “test_image.jpg” in your current directory.\n\n\nSince we are using SSH to connect to our Rapsi, we must transfer the image to our main computer so we can view it. We can use FileZilla or SCP for this:\n\nOpen a terminal on your host computer and run:\nscp mjrovai@raspi-zero.local:~/test_image.jpg .\n\nReplace “mjrovai” with your username and “raspi-zero” with Pi’s hostname.\n\n\n\nIf the image quality isn’t satisfactory, you can adjust various settings; for example, define a resolution that is suitable for YOLO (640x640):\n\nfswebcam -r 640x640 --no-banner test_image_yolo.jpg\nThis captures a higher-resolution image without the default banner.\n\nAn ordinary USB Webcam can also be used:\n\nAnd verified using lsusb\n\n\nVideo Streaming\nFor stream video (which is more resource-intensive), we can install and use mjpg-streamer:\nFirst, install Git:\nsudo apt install git\nNow, we should install the necessary dependencies for mjpg-streamer, clone the repository, and proceed with the installation:\nsudo apt install cmake libjpeg62-turbo-dev\ngit clone https://github.com/jacksonliam/mjpg-streamer.git\ncd mjpg-streamer/mjpg-streamer-experimental\nmake\nsudo make install\nThen start the stream with:\nmjpg_streamer -i \"input_uvc.so\" -o \"output_http.so -w ./www\"\nWe can then access the stream by opening a web browser and navigating to:\nhttp://&lt;your_pi_ip_address&gt;:8080. In my case: http://192.168.4.210:8080\nWe should see a webpage with options to view the stream. Click on the link that says “Stream” or try accessing:\nhttp://&lt;raspberry_pi_ip_address&gt;:8080/?action=stream"
  },
  {
    "objectID": "raspi/setup/setup.html#running-the-raspi-desktop-remotely",
    "href": "raspi/setup/setup.html#running-the-raspi-desktop-remotely",
    "title": "Setup",
    "section": "Running the Raspi Desktop remotely",
    "text": "Running the Raspi Desktop remotely\nWhile we’ve primarily interacted with the Raspberry Pi via SSH terminal commands, we can access the full graphical desktop environment remotely if we have installed the complete Raspberry Pi OS (for example, Raspberry Pi OS (64-bit). This can be particularly useful for tasks that benefit from a visual interface. To enable this functionality, we must set up a VNC (Virtual Network Computing) server on the Raspberry Pi. Here’s how to do it:\n\nEnable the VNC Server:\n\nConnect to your Raspberry Pi via SSH.\nRun the Raspberry Pi configuration tool by entering:\nsudo raspi-config\nNavigate to Interface Options using the arrow keys.\n\n\n\nSelect VNC and Yes to enable the VNC server.\n\n\n\nExit the configuration tool (use [Tab]), saving changes when prompted.\n\n\nInstall a VNC Viewer on Your Computer:\n\nDownload and install a VNC viewer application on your main computer. Popular options include RealVNC Viewer, TightVNC, or VNC Viewer by RealVNC. We will install VNC Viewer by RealVNC.\n\nOnce installed, confirm the Raspberry Pi’s IP address. For example, on the terminal, you can use:\nhostname -I\n\nConnect to Your Raspberry Pi:\n\nOpen your VNC viewer application.\n\n\n\nEnter your Raspberry Pi’s IP address and hostname.\nWhen prompted, enter your Raspberry Pi’s username and password.\n\n\nThe Raspberry Pi 5 Desktop should appear on your computer monitor.\n\nAdjust Display Settings (if needed):\n\nOnce connected, adjust the display resolution for optimal viewing. This can be done through the Raspberry Pi’s desktop settings or by modifying the config.txt file.\nLet’s do it using the desktop settings. Reach the menu (the Raspberry Icon at the left upper corner) and select the best screen definition for your monitor:"
  },
  {
    "objectID": "raspi/setup/setup.html#updating-and-installing-software",
    "href": "raspi/setup/setup.html#updating-and-installing-software",
    "title": "Setup",
    "section": "Updating and Installing Software",
    "text": "Updating and Installing Software\n\nUpdate your system:\nsudo apt update && sudo apt upgrade -y\nInstall essential software:\nsudo apt install python3-pip -y\nEnable pip for Python projects:\nsudo rm /usr/lib/python3.11/EXTERNALLY-MANAGED\n\n\nAVOID using PIP for System-Level dependencies libraries\nUse sudo apt install (outside of a virtual environment (venv) for:\n\nSystem-level dependencies and libraries\nHardware interface libraries (as a camera)\nDevelopment headers and build tools\nAnything that needs to interface directly with hardware\n\n\nRule of thumb: Use sudo apt install only for system dependencies and hardware interfaces. Use pip install (without sudo) inside an activated virtual environment for everything else. Inside the vent, PIP or PIP3 are the same."
  },
  {
    "objectID": "raspi/setup/setup.html#model-specific-considerations",
    "href": "raspi/setup/setup.html#model-specific-considerations",
    "title": "Setup",
    "section": "Model-Specific Considerations",
    "text": "Model-Specific Considerations\n\nRaspberry Pi Zero (Raspi-Zero)\n\nLimited processing power, best for lightweight projects\nIt is better to use a headless setup (SSH) to conserve resources.\nConsider increasing swap space for memory-intensive tasks.\nIt can be used for Image Classification and Object Detection Labs, but not for the LLM (SLM).\n\n\n\nRaspberry Pi 4 or 5 (Raspi-4 or Raspi-5)\n\nSuitable for more demanding projects, including AI and machine learning.\nIt can run the whole desktop environment smoothly.\nThe Raspberry Pi 4 can be used for Image Classification and Object Detection Labs, but it will not work well with LLMs (SLM) due to high latency.\nFor the Raspberry Pi 5, consider using an active cooler to manage temperature during intensive tasks, as in the LLMs (SLMs) lab.\n\nRemember to adjust your project requirements based on the specific Raspberry Pi model you’re using. The Raspi-Zero is great for low-power, space-constrained projects, while the Raspi-4 or 5 models are better suited for more computationally intensive tasks.\nHere’s a concise, copy‑pasteable tutorial you can share or publish."
  },
  {
    "objectID": "raspi/setup/setup.html#measuring-temperature-and-power",
    "href": "raspi/setup/setup.html#measuring-temperature-and-power",
    "title": "Setup",
    "section": "Measuring Temperature and Power",
    "text": "Measuring Temperature and Power\nIn this section, we will explore how to measure (and monitor) CPU temperature and power consumption on a Raspberry Pi 5 using only built‑in tools and a small shell script.\n\n\nCheck CPU Temperature\n\nQuick one‑shot reading\nvcgencmd measure_temp\nExample:\ntemp=47.2'C\nThis is the SoC (CPU/GPU) temperature in degrees Celsius.\n\n\nContinuous temperature monitor\nwatch -n 2 vcgencmd measure_temp\n\nUpdates every 2 seconds.\n\nPress Ctrl+C to stop.\n\n\n\nReading from sysfs (for scripts)\ncat /sys/class/thermal/thermal_zone0/temp\nThe result is in millidegrees Celsius (e.g., 47200 = 47.2 °C).\nWe should convert to human‑readable:\necho $(($(cat /sys/class/thermal/thermal_zone0/temp) / 1000))'°C'\n\n\n\n\nUnderstand Power Measurement on Pi 5\nRaspberry Pi 5 exposes internal PMIC rails via vcgencmd pmic_read_adc.\nvcgencmd pmic_read_adc\nExample (truncated):\n3V3_SYS_A   current(1)=0.06245952A\n1V8_SYS_A   current(2)=0.16102850A\n...\n3V3_SYS_V   volt(13)=3.29687500V\n1V8_SYS_V   volt(14)=1.79687500V\n...\nTo get power, we need to sum \\[P_i = V_i \\times I_i\\] over all rails. This gives board core power, not exact wall power (USB devices and PSU losses are not fully included).\n\n\n\nScript: Average Temperature and Power\nLet’s create a script that:\n\nSamples once per second for N seconds.\n\nComputes the average CPU temperature.\n\nSums all PMIC rails to get power.\n\nApplies a linear correction to estimate real board power, based on the RPi5‑power calibration.\n\n\nCreate the script\nnano avg_temp_power.sh\nPaste:\n#!/bin/bash\n# avg_temp_power.sh\n# Measure average CPU temperature and power on Raspberry Pi 5\n\nSECONDS=50   # number of samples (1 per second)\n\nTEMP_FILE=$(mktemp)\nPMIC_FILE=$(mktemp)\n\necho \"Sampling for ${SECONDS} seconds...\"\n\nfor i in $(seq 1 ${SECONDS}); do\n    # --- Temperature (°C) ---\n    temp_c=$(vcgencmd measure_temp | cut -d= -f2 | cut -d\"'\" -f1)\n    echo \"$temp_c\" &gt;&gt; \"${TEMP_FILE}\"\n\n    # --- PMIC power sum (W) ---\n    pmic_line=$(vcgencmd pmic_read_adc)\n    volts=($(echo \"$pmic_line\" | grep -o \"volt([^)]*)=[0-9.]*V\" | sed 's/.*=//;s/V//'))\n    currents=($(echo \"$pmic_line\" | grep -o \"current([^)]*)=[0-9.]*A\" | sed 's/.*=//;s/A//'))\n\n    sum_pmic=0\n    for idx in \"${!volts[@]}\"; do\n        v=${volts[$idx]}\n        i_amp=${currents[$idx]}\n        sum_pmic=$(awk -v a=\"$sum_pmic\" -v b=\"$v\" -v c=\"$i_amp\" \\\n                   'BEGIN{printf \"%.8f\", a + b*c}')\n    done\n\n    echo \"$sum_pmic\" &gt;&gt; \"${PMIC_FILE}\"\n    sleep 1\ndone\n\n# --- Compute statistics ---\nawk -v corr_a=1.1451 -v corr_b=0.5879 '\nfunction stats(file,   x, n, sum, sum2, mean, std) {\n    while ((getline x &lt; file) &gt; 0) {\n        n++\n        sum  += x\n        sum2 += x*x\n    }\n    close(file)\n    if (n &gt; 1) {\n        mean = sum / n\n        std  = sqrt( (sum2 - (sum*sum)/n) / (n-1) )\n    } else {\n        mean = sum\n        std  = 0\n    }\n    return mean \"|\" std\n}\nBEGIN{\n    split(stats(\"'\"${TEMP_FILE}\"'\"), t, \"|\")\n    t_mean = t[1]\n    t_std  = t[2]\n\n    split(stats(\"'\"${PMIC_FILE}\"'\"), p, \"|\")\n    p_pmic_mean = p[1]\n    p_pmic_std  = p[2]\n\n    # Apply linear correction for real power estimate (RPi5-power calibration)\n    p_real_mean = corr_a * p_pmic_mean + corr_b\n    p_real_std  = corr_a * p_pmic_std\n\n    printf \"Average_temperature = %.2f +/- %.2f °C\\n\", t_mean, t_std\n    printf \"Average_pmic_power  = %.3f +/- %.3f W\\n\", p_pmic_mean, p_pmic_std\n    printf \"Estimated_real_power = %.3f +/- %.3f W\\n\", p_real_mean, p_real_std\n}\n' /dev/null\n\nrm -f \"${TEMP_FILE}\" \"${PMIC_FILE}\"\nSave and exit.\n\n\nMake it executable\nchmod +x avg_temp_power.sh\n\n\n\nRun a measurement\nIdle baseline:\n./avg_temp_power.sh\nExample output (30–50 s window, active fan):\nSampling for 50 seconds...\nAverage_temperature = 47.25 +/- 0.40 °C\nAverage_pmic_power  = 2.312 +/- 0.298 W\nEstimated_real_power = 3.235 +/- 0.341 W\nThen start a heavy workload (e.g., Llama 3.2:3B) in another terminal and run the script again during inference. You should see higher temperature and power, often around 70–75 °C and 10–11 W with active cooling.\n\n\n\n\nInterpreting the Results\nTypical Raspberry Pi 5 ranges with active cooling:\n\n\n\nState\nTemp (°C)\nEstimated real power (W)\n\n\n\n\nIdle CLI\n40–50\n3.0–3.5\n\n\nLight desktop\n50–60\n4–5\n\n\nModerate CPU load\n55–70\n5–7\n\n\nHeavy CPU/GPU\n70–80\n7–10\n\n\n\n\nThrottling usually starts around 80 °C, with a hard limit at 85 °C.[5]\nIf we are well below that under full load, your cooling solution is working properly."
  },
  {
    "objectID": "raspi/image_classification/image_classification_fund.html#introduction",
    "href": "raspi/image_classification/image_classification_fund.html#introduction",
    "title": "Image Classification Fundamentals",
    "section": "Introduction",
    "text": "Introduction\nImage classification is a fundamental task in computer vision that involves categorizing an image into one of several predefined classes. It’s a cornerstone of artificial intelligence, enabling machines to interpret and understand visual information in ways that mimic human perception.\nImage classification is the assignment of a label or category to an entire image based on its visual content. This task is crucial in computer vision and has numerous applications across various industries. Image classification’s importance lies in its ability to automate visual understanding tasks that would otherwise require human intervention.\n\nApplications in Real-World Scenarios\nImage classification has found its way into numerous real-world applications, revolutionizing various sectors:\n\nHealthcare: Assisting in medical image analysis, such as identifying abnormalities in X-rays or MRIs.\nAgriculture: Monitoring crop health and detecting plant diseases through aerial imagery.\nAutomotive: Enabling advanced driver assistance systems and autonomous vehicles to recognize road signs, pedestrians, and other vehicles.\nRetail: Powering visual search capabilities and automated inventory management systems.\nSecurity and Surveillance: Enhancing threat detection and facial recognition systems.\nEnvironmental Monitoring: Analyzing satellite imagery for deforestation, urban planning, and climate change studies.\n\n\n\nAdvantages of Running Classification on Edge Devices like Raspberry Pi\nImplementing image classification on edge devices such as the Raspberry Pi offers several compelling advantages:\n\nLow Latency: Processing images locally eliminates the need to send data to cloud servers, significantly reducing response times.\nOffline Functionality: Classification can be performed without an internet connection, making it suitable for remote or connectivity-challenged environments.\nPrivacy and Security: Sensitive image data remains on the local device, addressing data privacy concerns and compliance requirements.\nCost-Effectiveness: Eliminates the need for expensive cloud computing resources, especially for continuous or high-volume classification tasks.\nScalability: Enables distributed computing architectures in which multiple devices can operate independently or in a network.\nEnergy Efficiency: Optimized models on dedicated hardware can be more energy-efficient than cloud-based solutions, which is crucial for battery-powered or remote applications.\nCustomization: Deploying specialized or frequently updated models tailored to specific use cases is more manageable.\n\nWe can create more responsive, secure, and efficient computer vision solutions by leveraging the power of edge devices such as the Raspberry Pi for image classification. This approach opens new possibilities for integrating intelligent visual processing across diverse applications and environments.\nIn the following sections, we’ll explore how to implement and optimize image classification on the Raspberry Pi, leveraging these advantages to build powerful, efficient computer vision systems."
  },
  {
    "objectID": "raspi/image_classification/image_classification_fund.html#setting-up-the-environment",
    "href": "raspi/image_classification/image_classification_fund.html#setting-up-the-environment",
    "title": "Image Classification Fundamentals",
    "section": "Setting Up the Environment",
    "text": "Setting Up the Environment\n\nUpdating the Raspberry Pi\nFirst, ensure your Raspberry Pi is up to date:\nsudo apt update\nsudo apt upgrade -y\nsudo reboot # Reboot to ensure all updates take effect\n\n\nInstalling Required System Level Libraries\nInstall Python tools and camera libraries\nsudo apt install -y python3-pip python3-venv python3-picamera2\nsudo apt install -y libcamera-dev libcamera-tools libcamera-apps\nPicamera2](https://github.com/raspberrypi/picamera2), a Python library for interacting with Raspberry Pi’s camera, is based on the libcamera camera stack, and the Raspberry Pi Foundation maintains it. The Picamera2 library is supported on all Raspberry Pi models, from the Pi Zero to the Pi 5.\nTesting picamera2 Instalation\n\n\n\nSetting up a Virtual Environment\nCreate a virtual environment with access to system packages to manage dependencies:\npython3 -m venv ~/tflite_env --system-site-packages\nActivate the environment:\nsource ~/tflite_env/bin/activate\nTo exit the virtual environment, use:\ndeactivate\n\n\nInstall Python Packages (inside Virtual Environment)\n\nEnsure you’re in the virtual environment (venv)\n\npip install numpy pillow  # Image processing\npip install matplotlib  # For displaying images\npip install opencv-python  # Computer vision\nVerify installation\npip list | grep -E \"(numpy|pillow|opencv|picamera)\"\n\n\nSystem vs pip Package Installation Rule\nUse sudo apt install (outside of venv) for:\n\nSystem-level dependencies and libraries\nHardware interface libraries (as a camera)\nDevelopment headers and build tools\nAnything that needs to interface directly with hardware\n\nUse pip install (inside venv) for:\n\nPure Python packages\nApplication-specific libraries\nPackages that don’t need system-level access\n\n\nRule of thumb: Use sudo apt install only for system dependencies and hardware interfaces. Use pip install (without sudo) inside an activated virtual environment for everything else. Inside the vent, PIP or PIP3 are the same.\nThe virtual environment will automatically include both system packages and pip-installed packages thanks to the --system-site-packages flag.\n\n\n\n\nSetting up Jupyter Notebook\nLet’s set up Jupyter Notebook optimized for headless Raspberry Pi camera work and development:\npip install jupyter jupyterlab notebook\njupyter notebook --generate-config\nTo run Jupyter Notebook, run the command (change the IP address for yours):\njupyter notebook --ip=192.168.4.210 --no-browser\nOn the terminal, you can see the local URL address to open the notebook:\n\nYou can access it from another device by entering the Raspberry Pi’s IP address and the provided token in a web browser (you can copy the token from the terminal).\n\nDefine the working directory in the Raspi and create a new Python 3 notebook. For example:\ncd Documents\nmkdir Python\n\nIt is possible to create folders directly in the Jupyter Notebook\n\nCreate a new Notebook and test the code below:\nImport Libraries\nimport time\nimport numpy as np\nfrom PIL import Image\nimport matplotlib.pyplot as plt\nfrom picamera2 import Picamera2\nLoad an image from the internet, for example (note that it is possible to run a command line from the Notebook, using ! before the command:\n!wget https://upload.wikimedia.org/wikipedia/commons/3/3a/Cat03.jpg\nAn image (Cat03.jpg will be downloaded to the current directory.\nLoad and show the image:\nimg_path = \"Cat03.jpg\"\nimg = Image.open(img_path)\n\n# Display the image\nplt.figure(figsize=(6, 6))\nplt.imshow(img)\nplt.title(\"Original Image\")\nplt.show()\nWe can see the image displayed on the Notebook:\n\nNow, let’s use the camera to capture a local image:\nfrom picamera2 import Picamera2\nimport time\n\n# Initialize camera\npicam2 = Picamera2()\npicam2.start()\n\n# Wait for camera to warm up\ntime.sleep(2)\n\n# Capture image\npicam2.capture_file(\"class3_test.jpg\")\nprint(\"Image captured: class3_test.jpg\")\n\n# Stop camera\npicam2.stop()\npicam2.close()\nAnd use a similar code as before to show it (adapting the img_path and title):"
  },
  {
    "objectID": "raspi/image_classification/image_classification_fund.html#installing-litert",
    "href": "raspi/image_classification/image_classification_fund.html#installing-litert",
    "title": "Image Classification Fundamentals",
    "section": "Installing LiteRT",
    "text": "Installing LiteRT\nWe are interested in inference, which involves running trained models on a device to make predictions from input data. To perform an inference with a model, we must run it through an interpreter. For that, we will use LiteRT, Google’s on-device framework for high-performance ML & GenAI deployment on edge platforms, via efficient conversion, runtime, and optimization.\n\nLiteRT continues TensorFlow Lite’s legacy as the trusted, high-performance runtime for on-device AI.\n\nLiteRT features advanced GPU/NPU acceleration, delivers superior ML & GenAI performance, making on-device ML inference easier than ever.\nFor installation on the Raspi, let’s use the command:\npip install ai-edge-litert\nVerifying the instalation:\npip show ai-edge-litert\n\n\nCreating a working directory:\nIf you are working on the Raspi-Zero with the minimum OS (No Desktop), you do not have a user-pre-defined directory tree (you can check it with ls. So, let’s create one:\nmkdir Documents\ncd Documents/\nmkdir TFLITE\ncd TFLITE/\nmkdir IMG_CLASS\ncd IMG_CLASS\nmkdir models\ncd models\n\nOn the Raspi-5, the /Documents should be there.\n\nGet a pre-trained Image Classification model:\nAn appropriate pre-trained model is crucial for successful image classification on resource-constrained devices like the Raspberry Pi. MobileNet is designed for mobile and embedded vision applications with a good balance between accuracy and speed. Versions: MobileNetV1, MobileNetV2, MobileNetV3. Let’s download the V2:\nwget https://storage.googleapis.com/download.tensorflow.org/models/tflite_11_05_08/mobilenet_v2_1.0_224_quant.tgz\n\ntar xzf mobilenet_v2_1.0_224_quant.tgz\nGet its labels:\nwget https://raw.githubusercontent.com/Mjrovai/EdgeML-with-Raspberry-Pi/refs/heads/main/IMG_CLASS/models/labels.txt\nIn the end, you should have the models in its directory:\n\n\nWe will only need the mobilenet_v2_1.0_224_quant.tflite model and the labels.txt. We can delete the other files.\n\n\n\nVerifying the Setup\nLet’s test our setup by running a simple Python script on TFLITE/IMG_CLASS folder:\nfrom ai_edge_litert.interpreter import Interpreter\nimport numpy as np\nfrom PIL import Image\n\nprint(\"NumPy:\", np.__version__)\nprint(\"Pillow:\", Image.__version__)\n\n# Try to create a LiteRT Interpreter\nmodel_path = \"./models/mobilenet_v2_1.0_224_quant.tflite\"\ninterpreter = Interpreter(model_path=model_path)\ninterpreter.allocate_tensors()\nprint(\"LiteRT Interpreter created successfully!\")\nWe can create the Python script using nano on the terminal, saving it with CTRL+0 + ENTER + CTRL+X\n\nAnd run it with the command:\npython setup_test.py\n\nOr you can run it directly on the Notebook:"
  },
  {
    "objectID": "raspi/image_classification/image_classification_fund.html#making-inferences-with-mobilenet-v2",
    "href": "raspi/image_classification/image_classification_fund.html#making-inferences-with-mobilenet-v2",
    "title": "Image Classification Fundamentals",
    "section": "Making inferences with Mobilenet V2",
    "text": "Making inferences with Mobilenet V2\nIn the last section, we set up the environment, including downloading a popular pre-trained model, Mobilenet V2, trained on ImageNet’s 224x224 images (1.2 million) for 1,001 classes (1,000 object categories plus 1 background). The model was converted to a compact 3.5MB tflite format, making it suitable for the limited storage and memory of a Raspberry Pi.\n\nIn the IMG_CLASS working directory, let’s start a new notebook to follow all the steps to classify one image:\nImport the needed libraries:\nimport time\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom PIL import Image\nfrom ai_edge_litert.interpreter import Interpreter\nLoad the TFLite model and allocate tensors:\nmodel_path = \"./models/mobilenet_v2_1.0_224_quant.tflite\"\ninterpreter = Interpreter(model_path=model_path)\ninterpreter.allocate_tensors()\n\nINFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n\nThe message means LiteRTsuccessfully enabled an optimized CPU backend (XNNPACK) for our model, which is good and expected.\nWhat XNNPACK is\n\nXNNPACK is a library of highly optimized operators (conv, FC, etc.) for running neural networks on CPUs, especially ARM and x86.\nLiteRT can “delegate” supported ops to XNNPACK so they run using these faster kernels instead of the default reference CPU implementation.\n\nSo, it means the interpreter has attached the XNNPACK delegate and will run all compatible parts of the graph on the CPU using it.\n\nOn devices like the Raspberry Pi, this usually results in lower inference latency at the cost of slightly longer delivery time and a bit more RAM for packed weights.\nWe are currently using CPU acceleration, not GPU, which is the standard/optimal path for many TFLite/LiteRT models on Pi-class hardware.\n\nGet input and output tensors.\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\nInput details provide information on how the model should be fed an image. The shape of (1, 224, 224, 3) informs us that an image with dimensions (224x224x3) should be input one by one (Batch Dimension: 1).\n\nThe output details indicate that the inference will produce an array of 1,001 integer values. Those values result from image classification, where each value is the probability that the corresponding label is associated with the image.\n\nLet’s also inspect the dtype of the input details of the model\ninput_dtype = input_details[0]['dtype']\ninput_dtype\ndtype('uint8')\nThis shows that the input image should be represented as raw pixels (0-255).\nLet’s get a test image. We can either transfer it from our computer or download one for testing, as we did before. Let’s first create a folder under our working directory:\nmkdir images\ncd images\nwget https://upload.wikimedia.org/wikipedia/commons/3/3a/Cat03.jpg\nLet’s load and display the image:\n# Load the image\nimg_path = \"./images/Cat03.jpg\"\nimg = Image.open(img_path)\n\n# Display the image\nplt.figure(figsize=(8, 8))\nplt.imshow(img)\nplt.title(\"Original Image\")\nplt.show()\n\nWe can see the image size by running the command:\nwidth, height = img.size\nThat shows that the image is an RGB image with a width and height of 1600 pixels each. To use our model, we should reshape it to (224, 224, 3) and add a batch dimension of 1, as defined in the input details: (1, 224, 224, 3). The inference result, as shown in the output details, will be an array of size 1001, as shown below:\n\nSo, let’s reshape the image, add the batch dimension, and see the result:\nimg = img.resize((input_details[0]['shape'][1], input_details[0]['shape'][2]))\ninput_data = np.expand_dims(img, axis=0)\ninput_data.shape\nThe input_data shape is as expected: (1, 224, 224, 3)\nLet’s confirm the dtype of the input data:\ninput_data.dtype\ndtype('uint8')\nThe input data dtype is ‘uint8’, which is compatible with the dtype expected for the model.\nUsing the input_data, let’s run the interpreter and get the predictions (output):\ninterpreter.set_tensor(input_details[0]['index'], input_data)\ninterpreter.invoke()\npredictions = interpreter.get_tensor(output_details[0]['index'])[0]\nThe prediction is an array with 1001 elements. Let’s get the Top-5 indices where their elements have high values:\ntop_k_results = 5\ntop_k_indices = np.argsort(predictions)[::-1][:top_k_results]\ntop_k_indices \nThe top_k_indices is an array with 5 elements: array([283, 286, 282])\nSo, 283, 286, 282, 288, and 479 are the image’s most probable classes. Having the index, we must find to which class it belongs (such as car, cat, or dog). The text file downloaded with the model includes a label for each index from 0 to 1,000. Let’s use a function to load the .txt file as a list:\ndef load_labels(filename):\n    with open(filename, 'r') as f:\n        return [line.strip() for line in f.readlines()]\nAnd get the list, printing the labels associated with the indexes:\nlabels_path = \"./models/labels.txt\"\nlabels = load_labels(labels_path)\n\nprint(labels[286])\nprint(labels[283])\nprint(labels[282])\nprint(labels[288])\nprint(labels[479])\nAs a result, we have:\nEgyptian cat\ntiger cat\ntabby\nlynx\ncarton\nAt least four of the top indices are related to felines. The prediction content is the probability associated with each one of the labels. As we saw in the output details, those values are quantized and should be dequantized:\nscale, zero_point = output_details[0]['quantization']\ndequantized_output = (predictions.astype(np.float32) - zero_point) * scale\ndequantized_output\narray([1.8662813e-06, 3.0599874e-06, 1.8146475e-05, ..., 6.9421253e-07,\n       2.0032754e-05, 4.2967865e-04], shape=(1001,), dtype=float32)\nThe output (positive and negative numbers) shows that the output probably does not have a Softmax. Checking the model documentation (https://arxiv.org/abs/1801.04381v4): MobileNet V2 typically doesn’t include a softmax layer at the output. It usually ends with a 1x1 convolution followed by average pooling and a fully connected layer. So, for getting the probabilities (0 to 1), we should apply Softmax:\nexp_output = np.exp(dequantized_output - np.max(dequantized_output))\nprobabilities = exp_output / np.sum(exp_output)\nLet’s print the top-5 probabilities:\nprint (probabilities[286])\nprint (probabilities[283])\nprint (probabilities[282])\nprint (probabilities[288])\nprint (probabilities[479])\n0.265947\n0.39499295\n0.17906114\n0.08961108\n0.022443123\nFor clarity, let’s create a function to relate the labels to the probabilities:\nfor i in range(top_k_results):\n    print(\"\\t{:20}: {}%\".format(\n        labels[top_k_indices[i]],\n        (int(probabilities[top_k_indices[i]]*100))))\ntiger cat           : 39%\nEgyptian cat        : 26%\ntabby               : 17%\nlynx                : 8%\ncarton              : 2%\n\nDefine a general Image Classification function\nLet’s create a general function to give an image as input, and we get the Top-5 possible classes:\ndef image_classification(img_path, model_path, labels, top_k_results=5):\n    # load the image\n    img = Image.open(img_path)\n    plt.figure(figsize=(4, 4))\n    plt.imshow(img)\n    plt.axis('off')\n\n    # Load the LiteRT model\n    interpreter = Interpreter(model_path=model_path)\n    interpreter.allocate_tensors()\n    \n    # Get input and output tensors\n    input_details = interpreter.get_input_details()\n    output_details = interpreter.get_output_details()\n    \n    # Preprocess\n    img = img.resize((input_details[0]['shape'][1], \n                      input_details[0]['shape'][2]))\n    input_data = np.expand_dims(img, axis=0)\n    \n    # Inference on Raspi\n    interpreter.set_tensor(input_details[0]['index'], input_data)\n    interpreter.invoke()\n    \n    # Obtain results and map them to the classes\n    predictions = interpreter.get_tensor(output_details[0]['index'])[0]\n    \n    # Get indices of the top k results\n    top_k_indices = np.argsort(predictions)[::-1][:top_k_results]\n    \n    # Get quantization parameters\n    scale, zero_point = output_details[0]['quantization']\n    \n    # Dequantize the output and apply softmax\n    dequantized_output = (predictions.astype(np.float32) - zero_point) * scale\n    exp_output = np.exp(dequantized_output - np.max(dequantized_output))\n    probabilities = exp_output / np.sum(exp_output)\n    \n    print(\"\\n\\t[PREDICTION]        [Prob]\\n\")\n    for i in range(top_k_results):\n        print(\"\\t{:20}: {}%\".format(\n            labels[top_k_indices[i]],\n            (int(probabilities[top_k_indices[i]]*100))))\nAnd loading some images for testing, we have:\n\n\n\nTesting the classification with the Camera\nLet’s modify the Python script used before to capture an image from the camera (size: 224x224), saving it in the images folder:\nfrom picamera2 import Picamera2\nimport time\n\ndef capture_image(image_path):\n    \n  # Initialize camera\n  picam2 = Picamera2() # default is index 0\n\n  # Configure the camera\n  config = picam2.create_still_configuration(main={\"size\": (224, 224)})\n  picam2.configure(config)\n  picam2.start()\n\n  # Wait for camera to warm up\n  time.sleep(2)\n\n  # Capture image\n  picam2.capture_file(image_path)\n  print(\"Image captured: \"+\"image_path\")\n\n  # Stop camera\n  picam2.stop()\n  picam2.close()\nNow, let’s capture an image and sent it for inference:\nimg_path = './images/cam_img_test.jpg'\nmodel_path = \"./models/mobilenet_v2_1.0_224_quant.tflite\"\nlabels = load_labels(\"./models/labels.txt\")\ncapture_image(img_path)\nimage_classification(img_path, model_path, labels, top_k_results=5)\n\n\n\nExploring a Model Trained from Zero\nLet’s get a TFLite model trained from scratch. For that, we can follow the Notebook:\nCNN to classify Cifar-10 dataset\nIn the notebook, we trained a model using the CIFAR10 dataset, which contains 60,000 images from 10 classes of CIFAR (airplane, automobile, bird, cat, deer, dog, frog, horse, ship, and truck). CIFAR has 32x32 color images (3 color channels) where the objects are not centered and can have the object with a background, such as airplanes that might have a cloudy sky behind them! In short, small but real images.\nThe CNN trained model (cifar10_model.keras) had a size of 2.0MB. Using the TFLite Converter, the model cifar10.tflite became with 674MB (around 1/3 of the original size).\n\nRuning the notebook 20_Cifar_10_Image_Classification.ipynb with the trained model: cifar10.tflite, and following the same steps as we did with the MobileNet model, we can see that the inference result is inferior in terms of probability when compared with the MobileNetV2.\nBelow are examples of images using the General Function for Image Classification on a Raspberry Pi Zero, as shown in the last section."
  },
  {
    "objectID": "raspi/image_classification/image_classification_fund.html#conclusion",
    "href": "raspi/image_classification/image_classification_fund.html#conclusion",
    "title": "Image Classification Fundamentals",
    "section": "Conclusion:",
    "text": "Conclusion:\nThis chapter has established a solid foundation for understanding and implementing image classification on Raspberry Pi devices using Python and LiteRT. Throughout this journey, we have explored the essential components that make edge-based computer vision both practical and powerful.\nWe began by understanding the theoretical foundations of image classification and its real-world applications across diverse sectors, from healthcare to environmental monitoring. The advantages of running classification on edge devices like the Raspberry Pi—including low latency, offline functionality, enhanced privacy, and cost-effectiveness—make it an attractive solution for many practical applications.\nThe hands-on experience of setting up the development environment provided crucial insights into the requirements and constraints of embedded systems. We successfully configured LiteRT, installed essential Python libraries, and established a working directory structure that serves as the foundation for computer vision projects.\nWorking with the pre-trained MobileNet V2 model demonstrated several key concepts:\n\nModel Architecture Understanding: We explored how pre-trained models like MobileNet V2 are optimized for mobile and embedded applications, achieving an excellent balance between accuracy and computational efficiency.\nQuantization Benefits: The 3.5MB quantized model showed how compression techniques make sophisticated neural networks feasible on resource-constrained devices without significant accuracy loss.\nInference Pipeline: We implemented the complete inference workflow, from image preprocessing (resizing to 224×224, handling data types) to post-processing (dequantization, softmax application, and top-k prediction extraction).\nPerformance Considerations: The chapter highlighted the importance of understanding model input/output specifications, memory management, and the trade-offs between accuracy and speed on edge devices.\n\nThe practical implementation using cameras connected to a Raspberry Pi demonstrated the seamless integration between hardware and software components. The ability to capture images directly from the device and perform real-time classification showcases the potential for autonomous and IoT applications.\nKey technical achievements include:\n\nSuccessfully setting up LiteRT and dependencies\nImplementing proper image preprocessing pipelines\nUnderstanding quantized model operations and dequantization processes\nCreating reusable functions for image classification tasks\nIntegrating camera capture with inference workflows\n\nThis foundational knowledge prepares us for more advanced topics, including custom model training and deployment. The skills developed here—understanding model architectures, implementing inference pipelines, and working with embedded Python environments—are transferable to a wide range of computer vision applications.\nThe chapter serves as a stepping stone toward building more sophisticated AI systems on edge devices, demonstrating that powerful computer vision capabilities are accessible even on modest hardware platforms when properly optimized and implemented."
  },
  {
    "objectID": "raspi/image_classification/image_classification_fund.html#resources",
    "href": "raspi/image_classification/image_classification_fund.html#resources",
    "title": "Image Classification Fundamentals",
    "section": "Resources",
    "text": "Resources\n\nDataset Example\nSetup Test Notebook on a Raspi\nImage Classification Notebook on a Raspi\nCNN to classify Cifar-10 dataset at CoLab\nCifar 10 - Image Classification on a Raspi\nPython Scripts"
  },
  {
    "objectID": "raspi/image_classification/custom_image_classification_project.html#sec-image-classification-image-classification-project-d575",
    "href": "raspi/image_classification/custom_image_classification_project.html#sec-image-classification-image-classification-project-d575",
    "title": "Custom Image Classification Project",
    "section": "Image Classification Project",
    "text": "Image Classification Project\nIn this chapter, we will develop a complete Image Classification project using the Edge Impulse Studio. As we did with the MobiliNet V2, the trained and converted TFLite model will be used for inference using a Python script.\nHere is a typical ML workflow that we will use in our project:\n\n\n\n\n\n\nThe Goal\nThe first step in any ML project is to define its goal. In this case, it is to detect and classify two specific objects present in one image. For this project, we will use two small toys: a robot and a small Brazilian parrot (named Periquito). We will also collect images of a background where those two objects are absent.\n \n\n\nData Collection\nOnce we have defined our Machine Learning project goal, the next and most crucial step is collecting the dataset. We can use a phone for the image capture, but we will use the Raspi here. Let’s set up a simple web server on our Raspberry Pi to view the QVGA (320 x 240) captured images in a browser.\n\nFirst, let’s install Flask, a lightweight web framework for Python:\npip install flask\nGo to the working folder (IMG_CLASS) and create a new Python script combining image capture with a web server. We’ll call it get_img_data.py:\n\n\nfrom flask import Flask, Response, render_template_string,\n                  request, redirect, url_for\nfrom picamera2 import Picamera2\nimport io\nimport threading\nimport time\nimport os\nimport signal\n\napp = Flask(__name__)\n\n# Global variables\nbase_dir = \"dataset\"\npicam2 = None\nframe = None\nframe_lock = threading.Lock()\ncapture_counts = {}\ncurrent_label = None\nshutdown_event = threading.Event()\n\ndef initialize_camera():\n    global picam2\n    picam2 = Picamera2()\n    config = picam2.create_preview_configuration(\n             main={\"size\": (320, 240)}\n    )\n    picam2.configure(config)\n    picam2.start()\n    time.sleep(2)  # Wait for camera to warm up\n\ndef get_frame():\n    global frame\n    while not shutdown_event.is_set():\n        stream = io.BytesIO()\n        picam2.capture_file(stream, format='jpeg')\n        with frame_lock:\n            frame = stream.getvalue()\n        time.sleep(0.1)  # Adjust as needed for smooth preview\n\ndef generate_frames():\n    while not shutdown_event.is_set():\n        with frame_lock:\n            if frame is not None:\n                yield (b'--frame\\r\\n'\n                       b'Content-Type: image/jpeg\\r\\n\\r\\n' +\n                                       frame + b'\\r\\n')\n        time.sleep(0.1)  # Adjust as needed for smooth streaming\n\ndef shutdown_server():\n    shutdown_event.set()\n    if picam2:\n        picam2.stop()\n    # Give some time for other threads to finish\n    time.sleep(2)\n    # Send SIGINT to the main process\n    os.kill(os.getpid(), signal.SIGINT)\n\n@app.route('/', methods=['GET', 'POST'])\ndef index():\n    global current_label\n    if request.method == 'POST':\n        current_label = request.form['label']\n        if current_label not in capture_counts:\n            capture_counts[current_label] = 0\n        os.makedirs(os.path.join(base_dir, current_label),\n                                 exist_ok=True)\n        return redirect(url_for('capture_page'))\n    return render_template_string('''\n        &lt;!DOCTYPE html&gt;\n        &lt;html&gt;\n        &lt;head&gt;\n            &lt;title&gt;Dataset Capture - Label Entry&lt;/title&gt;\n        &lt;/head&gt;\n        &lt;body&gt;\n            &lt;h1&gt;Enter Label for Dataset&lt;/h1&gt;\n            &lt;form method=\"post\"&gt;\n                &lt;input type=\"text\" name=\"label\" required&gt;\n                &lt;input type=\"submit\" value=\"Start Capture\"&gt;\n            &lt;/form&gt;\n        &lt;/body&gt;\n        &lt;/html&gt;\n    ''')\n\n@app.route('/capture')\ndef capture_page():\n    return render_template_string('''\n        &lt;!DOCTYPE html&gt;\n        &lt;html&gt;\n        &lt;head&gt;\n            &lt;title&gt;Dataset Capture&lt;/title&gt;\n            &lt;script&gt;\n                var shutdownInitiated = false;\n                function checkShutdown() {\n                    if (!shutdownInitiated) {\n                        fetch('/check_shutdown')\n                            .then(response =&gt; response.json())\n                            .then(data =&gt; {\n                                if (data.shutdown) {\n                                    shutdownInitiated = true;\n                                    document.getElementById(\n                                          'video-feed').src = '';\n                                    document.getElementById(\n                                          'shutdown-message')\n                                    .style.display = 'block';\n                                }\n                            });\n                    }\n                }\n                setInterval(checkShutdown, 1000); // Check\n                                                     every second\n            &lt;/script&gt;\n        &lt;/head&gt;\n        &lt;body&gt;\n            &lt;h1&gt;Dataset Capture&lt;/h1&gt;\n            &lt;p&gt;Current Label: {{ label }}&lt;/p&gt;\n            &lt;p&gt;Images captured for this label: {{ capture_count\n                                                  }}&lt;/p&gt;\n            &lt;img id=\"video-feed\" src=\"{{ url_for('video_feed')\n                                         }}\" width=\"640\"\n            height=\"480\" /&gt;\n            &lt;div id=\"shutdown-message\" style=\"display: none;\n                                              color: red;\"&gt;\n                Capture process has been stopped.\n                You can close this window.\n            &lt;/div&gt;\n            &lt;form action=\"/capture_image\" method=\"post\"&gt;\n                &lt;input type=\"submit\" value=\"Capture Image\"&gt;\n            &lt;/form&gt;\n            &lt;form action=\"/stop\" method=\"post\"&gt;\n                &lt;input type=\"submit\" value=\"Stop Capture\"\n                style=\"background-color: #ff6666;\"&gt;\n            &lt;/form&gt;\n            &lt;form action=\"/\" method=\"get\"&gt;\n                &lt;input type=\"submit\" value=\"Change Label\"\n                style=\"background-color: #ffff66;\"&gt;\n            &lt;/form&gt;\n        &lt;/body&gt;\n        &lt;/html&gt;\n    ''', label=current_label, capture_count=capture_counts.get(\n                                            current_label, 0))\n\n@app.route('/video_feed')\ndef video_feed():\n    return Response(generate_frames(),\n                    mimetype='multipart/x-mixed-replace;\n                    boundary=frame')\n\n@app.route('/capture_image', methods=['POST'])\ndef capture_image():\n    global capture_counts\n    if current_label and not shutdown_event.is_set():\n        capture_counts[current_label] += 1\n        timestamp = time.strftime(\"%Y%m%d-%H%M%S\")\n        filename = f\"image_{timestamp}.jpg\"\n        full_path = os.path.join(base_dir, current_label,\n                                 filename)\n\n        picam2.capture_file(full_path)\n\n    return redirect(url_for('capture_page'))\n\n@app.route('/stop', methods=['POST'])\ndef stop():\n    summary = render_template_string('''\n        &lt;!DOCTYPE html&gt;\n        &lt;html&gt;\n        &lt;head&gt;\n            &lt;title&gt;Dataset Capture - Stopped&lt;/title&gt;\n        &lt;/head&gt;\n        &lt;body&gt;\n            &lt;h1&gt;Dataset Capture Stopped&lt;/h1&gt;\n            &lt;p&gt;The capture process has been stopped.\n               You can close this window.&lt;/p&gt;\n            &lt;p&gt;Summary of captures:&lt;/p&gt;\n            &lt;ul&gt;\n            {% for label, count in capture_counts.items() %}\n                &lt;li&gt;{{ label }}: {{ count }} images&lt;/li&gt;\n            {% endfor %}\n            &lt;/ul&gt;\n        &lt;/body&gt;\n        &lt;/html&gt;\n    ''', capture_counts=capture_counts)\n\n    # Start a new thread to shutdown the server\n    threading.Thread(target=shutdown_server).start()\n\n    return summary\n\n@app.route('/check_shutdown')\ndef check_shutdown():\n    return {'shutdown': shutdown_event.is_set()}\n\nif __name__ == '__main__':\n    initialize_camera()\n    threading.Thread(target=get_frame, daemon=True).start()\n    app.run(host='0.0.0.0', port=5000, threaded=True)\n\n\nRun this script:\n\n    python get_img_data.py\n\nAccess the web interface:\n\nOn the Raspberry Pi itself (if you have a GUI): Open a web browser and go to http://localhost:5000\nFrom another device on the same network: Open a web browser and go to http://&lt;raspberry_pi_ip&gt;:5000 (Replace &lt;raspberry_pi_ip&gt; with your Raspberry Pi’s IP address). For example: http://192.168.4.210:5000/\n\n\nThis Python script creates a web-based interface for capturing and organizing image datasets using a Raspberry Pi and its camera. It’s handy for machine learning projects that require labeled image data.\n\nKey Features:\n\nWeb Interface: Accessible from any device on the same network as the Raspberry Pi.\nLive Camera Preview: This shows a real-time feed from the camera.\nLabeling System: Allows users to input labels for different categories of images.\nOrganized Storage: Automatically saves images in label-specific subdirectories.\nPer-Label Counters: Keeps track of how many images are captured for each label.\nSummary Statistics: Provides a summary of captured images when stopping the capture process.\n\n\n\nMain Components:\n\nFlask Web Application: Handles routing and serves the web interface.\nPicamera2 Integration: Controls the Raspberry Pi camera.\nThreaded Frame Capture: Ensures smooth live preview.\nFile Management: Organizes captured images into labeled directories.\n\n\n\nKey Functions:\n\ninitialize_camera(): Sets up the Picamera2 instance.\nget_frame(): Continuously captures frames for the live preview.\ngenerate_frames(): Yields frames for the live video feed.\nshutdown_server(): Sets the shutdown event, stops the camera, and shuts down the Flask server\nindex(): Handles the label input page.\ncapture_page(): Displays the main capture interface.\nvideo_feed(): Shows a live preview to position the camera\ncapture_image(): Saves an image with the current label.\nstop(): Stops the capture process and displays a summary.\n\n\n\nUsage Flow:\n\nStart the script on your Raspberry Pi.\nAccess the web interface from a browser.\nEnter a label for the images you want to capture and press Start Capture.\n\n \n\nUse the live preview to position the camera.\nClick Capture Image to save images under the current label.\n\n \n\nChange labels as needed for different categories, selecting Change Label.\nClick Stop Capture when finished to see a summary.\n\n \n\n\nTechnical Notes:\n\nThe script uses threading to handle concurrent frame capture and web serving.\nImages are saved with timestamps in their filenames for uniqueness.\nThe web interface is responsive and can be accessed from mobile devices.\n\n\n\nCustomization Possibilities:\n\nAdjust image resolution in the initialize_camera() function. Here we used QVGA \\((320\\times 240)\\).\nModify the HTML templates for a different look and feel.\nAdd additional image processing or analysis steps in the capture_image() function.\n\n\n\nNumber of samples on Dataset:\nGet around 60 images from each category (periquito, robot and background). Try to capture different angles, backgrounds, and light conditions.\nOn the Raspi, we will end with a folder named dataset, which contains three sub-folders: periquito, robot, and background, one for each class of images.\nYou can use Filezilla to transfer the created dataset to your main computer."
  },
  {
    "objectID": "raspi/image_classification/custom_image_classification_project.html#sec-image-classification-training-model-edge-impulse-studio-671f",
    "href": "raspi/image_classification/custom_image_classification_project.html#sec-image-classification-training-model-edge-impulse-studio-671f",
    "title": "Custom Image Classification Project",
    "section": "Training the model with Edge Impulse Studio",
    "text": "Training the model with Edge Impulse Studio\nWe will use the Edge Impulse Studio to train our model. Go to the Edge Impulse Page, enter your account credentials, and create a new project:\n \n\nHere, you can clone a similar project: Raspi - Img Class.\n\n\nDataset\nWe will walk through four main steps using the EI Studio (or Studio). These steps are crucial in preparing our model for use on the Raspi: Dataset, Impulse, Tests, and Deploy (on the Edge Device, in this case, the Raspi).\n\nRegarding the Dataset, it is essential to point out that our Original Dataset, captured with the Raspi, will be split into Training, Validation, and Test. The Test Set will be separated from the beginning and reserved for use only in the Test phase after training. The Validation Set will be used during training.\n\nOn Studio, follow the steps to upload the captured data:\n\nGo to the Data acquisition tab, and in the UPLOAD DATA section, upload the files from your computer in the chosen categories.\nLeave to the Studio the splitting of the original dataset into train and test and choose the label about\nRepeat the procedure for all three classes. At the end, you should see your “raw data” in the Studio:\n\n \nThe Studio allows you to explore your data, showing a complete view of all the data in your project. You can clear, inspect, or change labels by clicking on individual data items. In our case, a straightforward project, the data seems OK."
  },
  {
    "objectID": "raspi/image_classification/custom_image_classification_project.html#sec-image-classification-impulse-design-cfb1",
    "href": "raspi/image_classification/custom_image_classification_project.html#sec-image-classification-impulse-design-cfb1",
    "title": "Custom Image Classification Project",
    "section": "The Impulse Design",
    "text": "The Impulse Design\nIn this phase, we should define how to:\n\nPre-process our data, which consists of resizing the individual images and determining the color depth to use (be it RGB or Grayscale) and\nSpecify a Model. In this case, it will be the Transfer Learning (Images) to fine-tune a pre-trained MobileNet V2 image classification model on our data. This method performs well even with relatively small image datasets (around 180 images in our case).\n\nTransfer Learning with MobileNet offers a streamlined approach to model training, which is especially beneficial for resource-constrained environments and projects with limited labeled data. MobileNet, known for its lightweight architecture, is a pre-trained model that has already learned valuable features from a large dataset (ImageNet).\n \nBy leveraging these learned features, we can train a new model for your specific task with fewer data and computational resources and achieve competitive accuracy.\n \nThis approach significantly reduces training time and computational cost, making it ideal for quick prototyping and deployment on embedded devices where efficiency is paramount.\nGo to the Impulse Design Tab and create the impulse, defining an image size of \\(160\\times 160\\) and squashing them (squared form, without cropping). Select Image and Transfer Learning blocks. Save the Impulse.\n \n\nImage Pre-Processing\nAll the input QVGA/RGB565 images will be converted to 76,800 features \\((160\\times 160\\times 3)\\).\n\n\n\n\n\nPress Save parameters and select Generate features in the next tab.\n\n\nModel Design\nMobileNet is a family of efficient convolutional neural networks designed for mobile and embedded vision applications. The key features of MobileNet are:\n\nLightweight: Optimized for mobile devices and embedded systems with limited computational resources.\nSpeed: Fast inference times, suitable for real-time applications.\nAccuracy: Maintains good accuracy despite its compact size.\n\nMobileNetV2, introduced in 2018, improves the original MobileNet architecture. Key features include:\n\nInverted Residuals: Inverted residual structures are used where shortcut connections are made between thin bottleneck layers.\nLinear Bottlenecks: Removes non-linearities in the narrow layers to prevent the destruction of information.\nDepth-wise Separable Convolutions: Continues to use this efficient operation from MobileNetV1.\n\nIn our project, we will do a Transfer Learning with the MobileNetV2 160x160 1.0, which means that the images used for training (and future inference) should have an input Size of \\(160\\times 160\\) pixels and a Width Multiplier of 1.0 (full width, not reduced). This configuration balances between model size, speed, and accuracy.\n\n\nModel Training\nAnother valuable deep learning technique is Data Augmentation. Data augmentation improves the accuracy of machine learning models by creating additional artificial data. A data augmentation system makes small, random changes to the training data during the training process (such as flipping, cropping, or rotating the images).\nLooking under the hood, here you can see how Edge Impulse implements a data Augmentation policy on your data:\n# Implements the data augmentation policy\ndef augment_image(image, label):\n    # Flips the image randomly\n    image = tf.image.random_flip_left_right(image)\n\n    # Increase the image size, then randomly crop it down to\n    # the original dimensions\n    resize_factor = random.uniform(1, 1.2)\n    new_height = math.floor(resize_factor * INPUT_SHAPE[0])\n    new_width = math.floor(resize_factor * INPUT_SHAPE[1])\n    image = tf.image.resize_with_crop_or_pad(image, new_height,\n                                             new_width)\n    image = tf.image.random_crop(image, size=INPUT_SHAPE)\n\n    # Vary the brightness of the image\n    image = tf.image.random_brightness(image, max_delta=0.2)\n\n    return image, label\nExposure to these variations during training can help prevent your model from taking shortcuts by “memorizing” superficial clues in your training data, meaning it may better reflect the deep underlying patterns in your dataset.\nThe final dense layer of our model will have 0 neurons with a 10% dropout for overfitting prevention. Here is the Training result:\n \nThe result is excellent, with a reasonable 35 ms of latency (for a Raspi-4), which should result in around 30 fps (frames per second) during inference. A Raspi-Zero should be slower, and the Raspi-5, faster.\n\n\nTrading off: Accuracy versus speed\nIf faster inference is needed, we should train the model using smaller alphas (0.35, 0.5, and 0.75) or even reduce the image input size, trading with accuracy. However, reducing the input image size and decreasing the alpha (width multiplier) can speed up inference for MobileNet V2, but they have different trade-offs. Let’s compare:\n\nReducing Image Input Size:\n\nPros:\n\nSignificantly reduces the computational cost across all layers.\nDecreases memory usage.\nIt often provides a substantial speed boost.\n\nCons:\n\nIt may reduce the model’s ability to detect small features or fine details.\nIt can significantly impact accuracy, especially for tasks requiring fine-grained recognition.\n\n\nReducing Alpha (Width Multiplier):\n\nPros:\n\nReduces the number of parameters and computations in the model.\nMaintains the original input resolution, potentially preserving more detail.\nIt can provide a good balance between speed and accuracy.\n\nCons:\n\nIt may not speed up inference as dramatically as reducing input size.\nIt can reduce the model’s capacity to learn complex features.\n\nComparison:\n\nSpeed Impact:\n\nReducing input size often provides a more substantial speed boost because it reduces computations quadratically (halving both width and height reduces computations by about 75%).\nReducing alpha provides a more linear reduction in computations.\n\nAccuracy Impact:\n\nReducing input size can severely impact accuracy, especially when detecting small objects or fine details.\nReducing alpha tends to have a more gradual impact on accuracy.\n\nModel Architecture:\n\nChanging input size doesn’t alter the model’s architecture.\nChanging alpha modifies the model’s structure by reducing the number of channels in each layer.\n\n\nRecommendation:\n\nIf our application doesn’t require detecting tiny details and can tolerate some loss in accuracy, reducing the input size is often the most effective way to speed up inference.\nReducing alpha might be preferable if maintaining the ability to detect fine details is crucial or if you need a more balanced trade-off between speed and accuracy.\nFor best results, you might want to experiment with both:\n\nTry MobileNet V2 with input sizes like \\(160\\times 160\\) or \\(92\\times 92\\)\nExperiment with alpha values like 1.0, 0.75, 0.5 or 0.35.\n\nAlways benchmark the different configurations on your specific hardware and with your particular dataset to find the optimal balance for your use case.\n\n\nRemember, the best choice depends on your specific requirements for accuracy, speed, and the nature of the images you’re working with. It’s often worth experimenting with combinations to find the optimal configuration for your particular use case.\n\n\n\nModel Testing\nNow, you should take the data set aside at the start of the project and run the trained model using it as input. Again, the result is excellent (92.22%).\n\n\nDeploying the model\nAs we did in the previous section, we can deploy the trained model as .tflite and use Raspi to run it using Python.\nOn the Dashboard tab, go to Transfer learning model (int8 quantized) and click on the download icon:\n \n\nLet’s also download the float32 version for comparison\n\nTransfer the models from your computer to the Raspi (./models), for example, using FileZilla. Also, capture some images for inference and save them in (./images), or use the images in the ./dataset folder.\nLet’s remember what we did in the last chapter:\nActivate the environment:\nsource ~/tflite_env/bin/activate\nRun a Jupyter Notebook, using the command:\njupyter notebook --ip=192.168.4.210 --no-browser\n\nChange the IP address for yours\n\nOpen a new notebook and enter with the code below:\nImport the needed libraries:\nimport time\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom PIL import Image\nfrom ai_edge_litert.interpreter import Interpreter\nDefine the paths and labels:\nimg_path = \"./images/robot.jpg\"\nmodel_path = \"./models/ei-raspi-img-class-int8-quantized-\\\n                model.tflite\"\nlabels = ['background', 'periquito', 'robot']\n\nNote that the models trained on the Edge Impulse Studio will output values with index 0, 1, 2, etc., where the actual labels will follow an alphabetic order.\n\nLoad the model, allocate the tensors, and get the input and output tensor details:\n# Load the LiteRT model\ninterpreter = Interpreter(model_path=model_path)\ninterpreter.allocate_tensors()\n\n# Get input and output tensors\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\nOne important difference to note is that the dtype of the input details of the model is now int8, which means that the input values go from –128 to +127, while each pixel of our image goes from 0 to 255. This means that we should pre-process the image to match it. We can check here:\ninput_dtype = input_details[0]['dtype']\ninput_dtype\nnumpy.int8\nSo, let’s open the image and show it:\nimg = Image.open(img_path)\nplt.figure(figsize=(4, 4))\nplt.imshow(img)\nplt.axis('off')\nplt.show()\n \nAnd perform the pre-processing:\nscale, zero_point = input_details[0]['quantization']\nimg = img.resize((input_details[0]['shape'][1],\n                  input_details[0]['shape'][2]))\nimg_array = np.array(img, dtype=np.float32) / 255.0\nimg_array = (\n    (img_array / scale + zero_point)\n    .clip(-128, 127)\n    .astype(np.int8)\n)\ninput_data = np.expand_dims(img_array, axis=0)\nChecking the input data, we can verify that the input tensor is compatible with what is expected by the model:\ninput_data.shape, input_data.dtype\n((1, 160, 160, 3), dtype('int8'))\nNow, it is time to perform the inference. Let’s also calculate the latency of the model:\n# Inference on Raspi-Zero\nstart_time = time.time()\ninterpreter.set_tensor(input_details[0]['index'], input_data)\ninterpreter.invoke()\nend_time = time.time()\ninference_time = (end_time - start_time) * 1000 # Convert\n                                                # to milliseconds\nprint (\"Inference time: {:.1f}ms\".format(inference_time))\nThe model will take around 125ms to perform the inference in the Raspi-Zero, which is 3 to 4 times longer than a Raspi-5.\nNow, we can get the output labels and probabilities. It is also important to note that the model trained on the Edge Impulse Studio has a softmax activation function in its output (different from the original Movilenet V2), and we can use the model’s raw output as the “probabilities.”\n# Obtain results and map them to the classes\npredictions = interpreter.get_tensor(output_details[0]\n                                     ['index'])[0]\n\n# Get indices of the top k results\ntop_k_results=3\ntop_k_indices = np.argsort(predictions)[::-1][:top_k_results]\n\n# Get quantization parameters\nscale, zero_point = output_details[0]['quantization']\n\n# Dequantize the output\ndequantized_output = (predictions.astype(np.float32) -\n                      zero_point) * scale\nprobabilities = dequantized_output\n\nprint(\"\\n\\t[PREDICTION]        [Prob]\\n\")\nfor i in range(top_k_results):\n    print(\"\\t{:20}: {:.2f}%\".format(\n        labels[top_k_indices[i]],\n        probabilities[top_k_indices[i]] * 100))\n \nLet’s modify the function created before so that we can handle different type of models:\n\ndef image_classification(img_path, model_path, labels,\n                         top_k_results=3, apply_softmax=False):\n    # Load the image\n    img = Image.open(img_path)\n    plt.figure(figsize=(4, 4))\n    plt.imshow(img)\n    plt.axis('off')\n\n    # Load the LiteRT model\n    interpreter = Interpreter(model_path=model_path)\n    interpreter.allocate_tensors()\n    \n    # Get input and output tensors\n    input_details = interpreter.get_input_details()\n    output_details = interpreter.get_output_details()\n    \n    # Preprocess\n    img = img.resize((input_details[0]['shape'][1],\n                      input_details[0]['shape'][2]))\n    \n    input_dtype = input_details[0]['dtype']\n    \n    if input_dtype == np.uint8:\n        input_data = np.expand_dims(np.array(img), axis=0)\n    elif input_dtype == np.int8:\n        scale, zero_point = input_details[0]['quantization']\n        img_array = np.array(img, dtype=np.float32) / 255.0\n        img_array = (\n            img_array / scale\n            + zero_point\n        ).clip(-128, 127).astype(np.int8)\n        input_data = np.expand_dims(img_array, axis=0)\n    else:  # float32\n        input_data = np.expand_dims(\n            np.array(img, dtype=np.float32),\n            axis=0\n        ) / 255.0\n    \n    # Inference on Raspi-Zero\n    start_time = time.time()\n    interpreter.set_tensor(input_details[0]['index'], input_data)\n    interpreter.invoke()\n    end_time = time.time()\n    inference_time = (end_time -\n                        start_time\n                      ) * 1000 # Convert to milliseconds\n    \n    # Obtain results\n    predictions = interpreter.get_tensor(output_details[0]\n                                         ['index'])[0]\n    \n    # Get indices of the top k results\n    top_k_indices = np.argsort(predictions)[::-1][:top_k_results]\n    \n    # Handle output based on type\n    output_dtype = output_details[0]['dtype']\n    if output_dtype in [np.int8, np.uint8]:\n        # Dequantize the output\n        scale, zero_point = output_details[0]['quantization']\n        predictions = (predictions.astype(np.float32) -\n                       zero_point) * scale\n    \n    if apply_softmax:\n        # Apply softmax\n        exp_preds = np.exp(predictions - np.max(predictions))\n        probabilities = exp_preds / np.sum(exp_preds)\n    else:\n        probabilities = predictions\n    \n    print(\"\\n\\t[PREDICTION]        [Prob]\\n\")\n    for i in range(top_k_results):\n        print(\"\\t{:20}: {:.1f}%\".format(\n            labels[top_k_indices[i]],\n            probabilities[top_k_indices[i]] * 100))\n    print (\"\\n\\tInference time: {:.1f}ms\".format(inference_time))\n\nAnd test it with different images and the int8 quantized model (160x160 alpha =1.0).\n \nLet’s download a smaller model, such as the one trained for the Nicla Vision Lab (int8 quantized model, 96x96, alpha = 0.1), as a test. We can use the same function:\n \nThe model lost some accuracy, but it is still OK once our model does not look for many details. Regarding latency, we are aboutt ten times faster on the Raspi-Zero.\n\nWith the Raspberry Pi 5, the latencies are respectively 8 ms and 0.8 ms"
  },
  {
    "objectID": "raspi/image_classification/custom_image_classification_project.html#sec-image-classification-live-image-classification-f123",
    "href": "raspi/image_classification/custom_image_classification_project.html#sec-image-classification-live-image-classification-f123",
    "title": "Custom Image Classification Project",
    "section": "Live Image Classification",
    "text": "Live Image Classification\nLet’s develop an app that captures images with the camera in real-time and displays their classification.\nUsing the nano on the terminal, save the code below, such as img_class_live_infer.py.\n\nfrom flask import Flask, Response, render_template_string,\n                  request, jsonify\nfrom picamera2 import Picamera2\nimport io\nimport threading\nimport time\nimport numpy as np\nfrom PIL import Image\nfrom ai_edge_litert.interpreter import Interpreter\nfrom queue import Queue\n\napp = Flask(__name__)\n\n# Global variables\npicam2 = None\nframe = None\nframe_lock = threading.Lock()\nis_classifying = False\nconfidence_threshold = 0.8\nmodel_path = \"./models/ei-raspi-img-class-int8-quantized-\\\n                model.tflite\"\nlabels = ['background', 'periquito', 'robot']\ninterpreter = None\nclassification_queue = Queue(maxsize=1)\n\ndef initialize_camera():\n    global picam2\n    picam2 = Picamera2()\n    config = picam2.create_preview_configuration(\n        main={\"size\": (320, 240)}\n    )\n    picam2.configure(config)\n    picam2.start()\n    time.sleep(2)  # Wait for camera to warm up\n\ndef get_frame():\n    global frame\n    while True:\n        stream = io.BytesIO()\n        picam2.capture_file(stream, format='jpeg')\n        with frame_lock:\n            frame = stream.getvalue()\n        time.sleep(0.1)  # Capture frames more frequently\n\ndef generate_frames():\n    while True:\n        with frame_lock:\n            if frame is not None:\n                yield (\n                   b'--frame\\r\\n'\n                   b'Content-Type: image/jpeg\\r\\n\\r\\n'\n                   + frame + b'\\r\\n'\n                )\n        time.sleep(0.1)\n\ndef load_model():\n    global interpreter\n    if interpreter is None:\n        interpreter = Interpreter(model_path=model_path)\n        interpreter.allocate_tensors()\n    return interpreter\n\ndef classify_image(img, interpreter):\n    input_details = interpreter.get_input_details()\n    output_details = interpreter.get_output_details()\n\n    img = img.resize((input_details[0]['shape'][1],\n                      input_details[0]['shape'][2]))\n    input_data = np.expand_dims(np.array(img), axis=0)\\\n                             .astype(input_details[0]['dtype'])\n    \n    interpreter.set_tensor(input_details[0]['index'], input_data)\n    interpreter.invoke()\n    \n    predictions = interpreter.get_tensor(output_details[0]\n                                         ['index'])[0]\n    # Handle output based on type\n    output_dtype = output_details[0]['dtype']\n    if output_dtype in [np.int8, np.uint8]:\n        # Dequantize the output\n        scale, zero_point = output_details[0]['quantization']\n        predictions = (predictions.astype(np.float32) -\n                       zero_point) * scale\n    return predictions\n\ndef classification_worker():\n    interpreter = load_model()\n    while True:\n        if is_classifying:\n            with frame_lock:\n                if frame is not None:\n                    img = Image.open(io.BytesIO(frame))\n            predictions = classify_image(img, interpreter)\n            max_prob = np.max(predictions)\n            if max_prob &gt;= confidence_threshold:\n                label = labels[np.argmax(predictions)]\n            else:\n                label = 'Uncertain'\n            classification_queue.put({\n                 'label': label,\n                 'probability': float(max_prob)\n            })\n        time.sleep(0.1)  # Adjust based on your needs\n\n@app.route('/')\ndef index():\n   return render_template_string('''\n      &lt;!DOCTYPE html&gt;\n      &lt;html&gt;\n      &lt;head&gt;\n          &lt;title&gt;Image Classification&lt;/title&gt;\n          &lt;script\n            src=\"https://code.jquery.com/jquery-3.6.0.min.js\"&gt;\n          &lt;/script&gt;\n          &lt;script&gt;\n              function startClassification() {\n                  $.post('/start');\n                  $('#startBtn').prop('disabled', true);\n                  $('#stopBtn').prop('disabled', false);\n              }\n              function stopClassification() {\n                  $.post('/stop');\n                  $('#startBtn').prop('disabled', false);\n                  $('#stopBtn').prop('disabled', true);\n              }\n              function updateConfidence() {\n                  var confidence = $('#confidence').val();\n                  $.post('/update_confidence',\n                         {confidence: confidence}\n                        );\n              }\n              function updateClassification() {\n                  $.get('/get_classification', function(data) {\n                    $('#classification').text(data.label + ': '\n                    + data.probability.toFixed(2));\n                  });\n              }\n              $(document).ready(function() {\n                  setInterval(updateClassification, 100);\n                  // Update every 100ms\n              });\n          &lt;/script&gt;\n      &lt;/head&gt;\n      &lt;body&gt;\n          &lt;h1&gt;Image Classification&lt;/h1&gt;\n          &lt;img src=\"{{ url_for('video_feed') }}\"\n               width=\"640\"\n               height=\"480\" /&gt;\n\n          &lt;br&gt;\n          &lt;button id=\"startBtn\"\n                  onclick=\"startClassification()\"&gt;\n            Start Classification\n          &lt;/button&gt;\n    \n          &lt;button id=\"stopBtn\"\n                  onclick=\"stopClassification()\"\n                  disabled&gt;\n            Stop Classification\n          &lt;/button&gt;\n    \n          &lt;br&gt;\n          &lt;label for=\"confidence\"&gt;Confidence Threshold:&lt;/label&gt;\n          &lt;input type=\"number\"\n                 id=\"confidence\"\n                 name=\"confidence\"\n                 min=\"0\" max=\"1\"\n                 step=\"0.1\"\n                 value=\"0.8\"\n                 onchange=\"updateConfidence()\" /&gt;\n    \n          &lt;br&gt;\n          &lt;div id=\"classification\"&gt;\n             Waiting for classification...\n          &lt;/div&gt;\n    \n      &lt;/body&gt;\n      &lt;/html&gt;\n   ''')\n\n@app.route('/video_feed')\ndef video_feed():\n    return Response(\n       generate_frames(),\n       mimetype='multipart/x-mixed-replace; boundary=frame'\n    )\n\n@app.route('/start', methods=['POST'])\ndef start_classification():\n    global is_classifying\n    is_classifying = True\n    return '', 204\n\n@app.route('/stop', methods=['POST'])\ndef stop_classification():\n    global is_classifying\n    is_classifying = False\n    return '', 204\n\n@app.route('/update_confidence', methods=['POST'])\ndef update_confidence():\n    global confidence_threshold\n    confidence_threshold = float(request.form['confidence'])\n    return '', 204\n\n@app.route('/get_classification')\ndef get_classification():\n    if not is_classifying:\n        return jsonify({'label': 'Not classifying',\n                       'probability': 0})\n    try:\n        result = classification_queue.get_nowait()\n    except Queue.Empty:\n        result = {'label': 'Processing', 'probability': 0}\n    return jsonify(result)\n\nif __name__ == '__main__':\n    initialize_camera()\n    threading.Thread(target=get_frame, daemon=True).start()\n    threading.Thread(target=classification_worker,\n                     daemon=True).start()\n    app.run(host='0.0.0.0', port=5000, threaded=True)\n\nOn the terminal, run:\npython img_class_live_infer.py\nAnd access the web interface:\n\nOn the Raspberry Pi itself (if you have a GUI): Open a web browser and go to http://localhost:5000\nFrom another device on the same network: Open a web browser and go to http://&lt;raspberry_pi_ip&gt;:5000 (Replace &lt;raspberry_pi_ip&gt; with your Raspberry Pi’s IP address). For example: http://192.168.4.210:5000/\n\nHere are some screenshots of the app running on an external desktop\n \nHere, you can see the app running on YouTube.\nThe code creates a web application for real-time image classification using a Raspberry Pi, its camera module, and a TensorFlow Lite model. The application uses Flask to serve a web interface where is possible to view the camera feed and see live classification results.\n\nKey Components:\n\nFlask Web Application: Serves the user interface and handles requests.\nPiCamera2: Captures images from the Raspberry Pi camera module.\nLiteRT: Runs the image classification model.\nThreading: Manages concurrent operations for smooth performance.\n\n\n\nMain Features:\n\nLive camera feed display\nReal-time image classification\nAdjustable confidence threshold\nStart/Stop classification on demand\n\n\n\nCode Structure:\n\nImports and Setup:\n\nFlask for web application\nPiCamera2 for camera control\nTensorFlow Lite for inference\nThreading and Queue for concurrent operations\n\nGlobal Variables:\n\nCamera and frame management\nClassification control\nModel and label information\n\nCamera Functions:\n\ninitialize_camera(): Sets up the PiCamera2\nget_frame(): Continuously captures frames\ngenerate_frames(): Yields frames for the web feed\n\nModel Functions:\n\nload_model(): Loads the TFLite model\nclassify_image(): Performs inference on a single image\n\nClassification Worker:\n\nRuns in a separate thread\nContinuously classifies frames when active\nUpdates a queue with the latest results\n\nFlask Routes:\n\n/: Serves the main HTML page\n/video_feed: Streams the camera feed\n/start and /stop: Controls classification\n/update_confidence: Adjusts the confidence threshold\n/get_classification: Returns the latest classification result\n\nHTML Template:\n\nDisplays camera feed and classification results\nProvides controls for starting/stopping and adjusting settings\n\nMain Execution:\n\nInitializes camera and starts necessary threads\nRuns the Flask application\n\n\n\n\nKey Concepts:\n\nConcurrent Operations: Using threads to handle camera capture and classification separately from the web server.\nReal-time Updates: Frequent updates to the classification results without page reloads.\nModel Reuse: Loading the TFLite model once and reusing it for efficiency.\nFlexible Configuration: Allowing users to adjust the confidence threshold on the fly.\n\n\n\nUsage:\n\nEnsure all dependencies are installed.\nRun the script on a Raspberry Pi with a camera module.\nAccess the web interface from a browser using the Raspberry Pi’s IP address.\nStart classification and adjust settings as needed."
  },
  {
    "objectID": "raspi/image_classification/custom_image_classification_project.html#sec-image-classification-summary-9796",
    "href": "raspi/image_classification/custom_image_classification_project.html#sec-image-classification-summary-9796",
    "title": "Custom Image Classification Project",
    "section": "Summary:",
    "text": "Summary:\nImage classification has emerged as a powerful and versatile application of machine learning, with significant implications for various fields, from healthcare to environmental monitoring. This chapter has demonstrated how to implement a robust image classification system on edge devices like the Raspi-Zero and Raspi-5, showcasing the potential for real-time, on-device intelligence.\nWe’ve explored the entire pipeline of an image classification project, from data collection and model training using Edge Impulse Studio to deploying and running inferences on a Raspi. The process highlighted several key points:\n\nThe importance of proper data collection and preprocessing for training effective models.\nThe power of transfer learning, allowing us to leverage pre-trained models like MobileNet V2 for efficient training with limited data.\nThe trade-offs between model accuracy and inference speed, especially crucial for edge devices.\nThe implementation of real-time classification using a web-based interface, demonstrating practical applications.\n\nThe ability to run these models on edge devices like the Raspi opens up numerous possibilities for IoT applications, autonomous systems, and real-time monitoring solutions. It allows for reduced latency, improved privacy, and operation in environments with limited connectivity.\nAs we’ve seen, even with the computational constraints of edge devices, it’s possible to achieve impressive results in terms of both accuracy and speed. The flexibility to adjust model parameters, such as input size and alpha values, allows for fine-tuning to meet specific project requirements.\nLooking forward, the field of edge AI and image classification continues to evolve rapidly. Advances in model compression techniques, hardware acceleration, and more efficient neural network architectures promise to further expand the capabilities of edge devices in computer vision tasks.\nThis project serves as a foundation for more complex computer vision applications and encourages further exploration into the exciting world of edge AI and IoT. Whether it’s for industrial automation, smart home applications, or environmental monitoring, the skills and concepts covered here provide a solid starting point for a wide range of innovative projects."
  },
  {
    "objectID": "raspi/image_classification/custom_image_classification_project.html#sec-image-classification-resources-1d0e",
    "href": "raspi/image_classification/custom_image_classification_project.html#sec-image-classification-resources-1d0e",
    "title": "Custom Image Classification Project",
    "section": "Resources",
    "text": "Resources\n\nDataset Example\nPython Scripts\nEdge Impulse Project\nImage Classification Project - Edge Impulse Notebook"
  },
  {
    "objectID": "raspi/object_detection/object_detection_fundamentals.html#introduction",
    "href": "raspi/object_detection/object_detection_fundamentals.html#introduction",
    "title": "Object Detection: Fundamentals",
    "section": "Introduction",
    "text": "Introduction\nBuilding on our exploration of image classification, we now turn to a more advanced computer vision task: object detection. While image classification assigns a single label to an entire image, object detection goes further by identifying and locating multiple objects within a single image. This capability opens up many new applications and challenges, particularly in edge computing and IoT devices like the Raspberry Pi.\nObject detection combines classification and localization. It not only determines which objects are present in an image but also pinpoints their locations, for example, by drawing bounding boxes around them. This added complexity makes object detection a more powerful tool for understanding visual scenes, but it also requires more sophisticated models and training techniques.\nIn edge AI, where computational resources are constrained, implementing efficient object detection models is crucial. The challenges we faced with image classification—balancing model size, inference speed, and accuracy—are even more pronounced in object detection. However, the rewards are also more significant, as object detection enables more nuanced and detailed analysis of visual data.\nSome applications of object detection on edge devices include:\n\nSurveillance and security systems\nAutonomous vehicles and drones\nIndustrial quality control\nWildlife monitoring\nAugmented reality applications\n\nAs we put our hands into object detection, we’ll build on the concepts and techniques we explored in image classification. We’ll examine popular object detection architectures designed for efficiency, such as:\n\nSingle Stage Detectors, such as MobileNet and EfficientDet,\nFOMO (Faster Objects, More Objects), and\nYOLO (You Only Look Once).\n\n\nTo learn more about object detection models, follow the tutorial A Gentle Introduction to Object Recognition With Deep Learning.\n\nWe will explore those object detection models using:\n\nTensorFlow Lite Runtime (now changed to LiteRT),\nEdge Impulse Linux Python SDK and\nUltralitics\n\n\nThroughout this lab, we’ll cover the fundamentals of object detection and how it differs from image classification. We’ll also learn how to train, fine-tune, test, optimize, and deploy popular object detection architectures using a dataset created from scratch.\n\nObject Detection Fundamentals\nObject detection builds upon the foundations of image classification but extends its capabilities significantly. To understand object detection, it’s crucial first to recognize its key differences from image classification:\n\nImage Classification vs. Object Detection\nImage Classification:\n\nAssigns a single label to an entire image\nAnswers the question: “What is this image’s primary object or scene?”\nOutputs a single class prediction for the whole image\n\nObject Detection:\n\nIdentifies and locates multiple objects within an image\nAnswers the questions: “What objects are in this image, and where are they located?”\nOutputs multiple predictions, each consisting of a class label and a bounding box\n\nTo visualize this difference, let’s consider an example:\n\nThis diagram illustrates the critical difference: image classification provides a single label for the entire image, while object detection identifies multiple objects, their classes, and their locations within the image.\n\n\nKey Components of Object Detection\nObject detection systems typically consist of two main components:\n\nObject Localization: This component identifies the location of objects within the image. It typically outputs bounding boxes, rectangular regions encompassing each detected object.\nObject Classification: This component determines the class or category of each detected object, similar to image classification but applied to each localized region.\n\n\n\nChallenges in Object Detection\nObject detection presents several challenges beyond those of image classification:\n\nMultiple objects: An image may contain multiple objects of various classes, sizes, and positions.\nVarying scales: Objects can appear at different sizes within the image.\nOcclusion: Objects may be partially hidden or overlapping.\nBackground clutter: Distinguishing objects from complex backgrounds can be challenging.\nReal-time performance: Many applications require fast inference times, especially on edge devices.\n\n\n\nApproaches to Object Detection\nThere are two main approaches to object detection:\n\nTwo-stage detectors: These first propose regions of interest and then classify each region. Examples include R-CNN and its variants (Fast R-CNN, Faster R-CNN).\nSingle-stage detectors: These predict bounding boxes (or centroids) and class probabilities in a single forward pass through the network. Examples include YOLO (You Only Look Once), EfficientDet, SSD (Single Shot Detector), and FOMO (Faster Objects, More Objects). These are often faster and better suited to edge devices, such as the Raspberry Pi.\n\n\n\nEvaluation Metrics\nObject detection uses different metrics compared to image classification:\n\nIntersection over Union (IoU) is a metric used to evaluate the accuracy of an object detector. It measures the overlap between two bounding boxes: the Ground Truth box (the manually labeled correct box) and the Predicted box (the box generated by the object detection model). The IoU value is calculated by dividing the area of the Intersection (the overlapping area) by the area of the Union (the total area covered by both boxes). A higher IoU value indicates a better prediction.\n\n\n\nMean Average Precision (mAP) is a widely used metric for evaluating the performance of object detection models. It provides a single number that reflects a model’s ability to accurately both classify and localize objects. The “mean” in mAP refers to the average taken over all object classes in the dataset. The “average precision” (AP) is calculated for each class, and then these AP values are averaged to get the final mAP score. A high mAP score indicates that the model is excellent at identifying all objects and placing a tight-fitting, accurate bounding box around them.\n\n\n\nFrames Per Second (FPS): Measures detection speed, crucial for real-time applications on edge devices."
  },
  {
    "objectID": "raspi/object_detection/object_detection_fundamentals.html#pre-trained-object-detection-models-overview",
    "href": "raspi/object_detection/object_detection_fundamentals.html#pre-trained-object-detection-models-overview",
    "title": "Object Detection: Fundamentals",
    "section": "Pre-Trained Object Detection Models Overview",
    "text": "Pre-Trained Object Detection Models Overview\nAs we saw in the introduction, given an image or a video stream, an object detection model can identify which of a known set of objects might be present and provide information about their positions within the image.\n\nYou can test some common models online by visiting Object Detection - MediaPipe Studio\n\nOn Kaggle, we can find the most common pre-trained TFLite models to use with the Raspberry Pi, ssd_mobilenet_v1, and efficiendet. Those models were trained on the COCO (Common Objects in Context) dataset, which contains over 200,000 labeled images across 91 categories.\nDownload the models and upload them to the ./models folder on the Raspberry Pi.\n\nAlternatively, you can find the models and the COCO labels on GitHub.\n\nFor the first part of this lab, we will focus on a pre-trained 300x300 SSD-Mobilenet V1 model and compare it with the 320x320 EfficientDet-lite0, also trained using the COCO 2017 dataset. Both models were converted to a TensorFlow Lite format (4.2MB for the SSD Mobilenet and 4.6MB for the EfficientDet).\n\nSSD-Mobilenet V2 or V3 is recommended for transfer learning projects, but once the V1 TFLite model is publicly available, we will use it for this overview.\n\n\n\nThe model outputs up to ten detections per image, including bounding boxes, class IDs, and confidence scores.\n\n\nSetting Up the TFLite Environment\nWe should confirm the steps done on the last Hands-On Lab, Image Classification, as follows:\n\nUpdating the Raspberry Pi\nInstalling Required Libraries\nSetting up a Virtual Environment (Optional but Recommended)\n\nsource ~/tflite/bin/activate\n\nInstalling TensorFlow Lite Runtime\nInstalling Additional Python Libraries (inside the environment)\n\n\n\nCreating a Working Directory:\nConsidering that we have created the Documents/TFLITE folder in the last Lab, let’s now create the specific folders for this object detection lab:\ncd Documents/TFLITE/\nmkdir OBJ_DETECT\ncd OBJ_DETECT\nmkdir images\nmkdir models\ncd models\n\n\nInference and Post-Processing\n\nLet’s start a new notebook to follow all the steps to detect objects in an image:\nImport the needed libraries:\nimport time\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom PIL import Image\nfrom ai_edge_litert.interpreter import Interpreter\nDownload the model and labels from the folder models and save them in the models folder under OBJ_DETECT.\nLoad the model and allocate tensors:\nmodel_path = \"./models/ssd-mobilenet-v1-tflite-default-v1.tflite\"\ninterpreter = Interpreter(model_path=model_path)\ninterpreter.allocate_tensors()\nGet input and output tensors.\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\nInput details will inform us how the model should be fed with an image. The shape of (1, 300, 300, 3) with a dtype of uint8 tells us that a non-normalized (pixel value range from 0 to 255) image with dimensions (300x300x3) should be input one by one (Batch Dimension: 1).\n\nThe output details include not only the labels (“classes”) and probabilities (“scores”), but also the relative window positions of the bounding boxes (“boxes”), indicating where the object is located in the image, and the number of detected objects (“num_detections”). The output details also indicate that the model can detect up to 10 objects in the image.\n\nSo, for the above example, using the same cat image used with the Image Classification Lab, looking for the output, we have a 76% probability of having found an object with a class ID of 16 on an area delimited by a bounding box of [0.028011084, 0.020121813, 0.9886069, 0.802299]. Those four numbers are related to ymin, xmin, ymax, and xmax, the box coordinates.\n\nConsidering that y ranges from the top (ymin) to the bottom (ymax) and x ranges from left (xmin) to right (xmax), we have, in fact, the coordinates of the top-left corner and the bottom-right one. With both edges and knowing the shape of the picture, it is possible to draw a rectangle around the object, as shown in the figure below:\n\nNext, we should find what class ID 16 means. Opening the file coco_labels.txt, we see that each element has an associated index; inspecting index 16, we get, as expected, cat. The probability is the value returned from the score.\nLet’s now upload some images with multiple objects on them for testing.\nimg_path = \"./images/cat_dog.jpeg\"\norig_img = Image.open(img_path)\n\n# Display the image\nplt.figure(figsize=(8, 8))\nplt.imshow(orig_img)\nplt.title(\"Original Image\")\nplt.show()\n\nBased on the input details, let’s pre-process the image, changing its shape and expanding its dimensions:\nimg = orig_img.resize((input_details[0]['shape'][1], \n                  input_details[0]['shape'][2]))\ninput_data = np.expand_dims(img, axis=0)\ninput_data.shape, input_data.dtype \nThe new input_data shape is(1, 300, 300, 3) with a dtype of uint8, which is compatible with what the model expects.\nUsing the input_data, let’s run the interpreter, measure the latency, and get the output:\nstart_time = time.time()\ninterpreter.set_tensor(input_details[0]['index'], input_data)\ninterpreter.invoke()\nend_time = time.time()\ninference_time = (end_time - start_time) * 1000  # Convert to milliseconds\nprint (\"Inference time: {:.1f}ms\".format(inference_time))\nWith a latency of around 800 ms on the Raspi-Zero and 100 ms n the Raspi-5 , we can get four distinct outputs:\nboxes = interpreter.get_tensor(output_details[0]['index'])[0] \nclasses = interpreter.get_tensor(output_details[1]['index'])[0]  \nscores = interpreter.get_tensor(output_details[2]['index'])[0]   \nnum_detections = int(interpreter.get_tensor(output_details[3]['index'])[0])\nOn a quick inspection, we can see that the model detected two objects with a score over 0.5:\nfor i in range(num_detections):\n    if scores[i] &gt; 0.5:  # Confidence threshold\n        print(f\"Object {i}:\")\n        print(f\"  Bounding Box: {boxes[i]}\")\n        print(f\"  Confidence: {scores[i]}\")\n        print(f\"  Class: {classes[i]}\")\n\nAnd we can also visualize the results:\nplt.figure(figsize=(12, 8))\nplt.imshow(orig_img)\nfor i in range(num_detections):\n    if scores[i] &gt; 0.5:  # Adjust threshold as needed\n        ymin, xmin, ymax, xmax = boxes[i]\n        (left, right, top, bottom) = (xmin * orig_img.width, \n                                      xmax * orig_img.width, \n                                      ymin * orig_img.height, \n                                      ymax * orig_img.height)\n        rect = plt.Rectangle((left, top), right-left, bottom-top, \n                             fill=False, color='red', linewidth=2)\n        plt.gca().add_patch(rect)\n        class_id = int(classes[i])\n        class_name = labels[class_id]\n        plt.text(left, top-10, f'{class_name}: {scores[i]:.2f}', \n                 color='red', fontsize=12, backgroundcolor='white')\n\n\nThe choice of the confidence threshold is crucial. For example, setting it to 0.2 will show false positives. A proper code should handle it.\n\n\n\nEfficientDet\nEfficientDet is not technically an SSD (Single Shot Detector) model, but it shares some similarities and builds upon ideas from SSD and other object detection architectures:\n\nEfficientDet:\n\nDeveloped by Google researchers in 2019\nUses EfficientNet as the backbone network\nEmploys a novel bi-directional feature pyramid network (BiFPN)\nIt uses compound scaling to efficiently scale the backbone network and object detection components.\n\nSimilarities to SSD:\n\nBoth are single-stage detectors, meaning they perform object localization and classification in a single forward pass.\nBoth use multi-scale feature maps to detect objects at different scales.\n\nKey differences:\n\nBackbone: SSD typically uses VGG or MobileNet, while EfficientDet uses EfficientNet.\nFeature fusion: SSD uses a simple feature pyramid, while EfficientDet uses the more advanced BiFPN.\nScaling method: EfficientDet introduces compound scaling for all components of the network\n\nAdvantages of EfficientDet:\n\nGenerally achieves better trade-offs between accuracy and efficiency than SSD and many other object detection models.\nMore flexible scaling enables a family of models with varying size-performance trade-offs.\n\n\nWhile EfficientDet is not an SSD model, it can be seen as an evolution of single-stage detection architectures, incorporating more advanced techniques to improve efficiency and accuracy. When using EfficientDet, we can expect outputs similar to those of SSD (e.g., bounding boxes and class scores).\n\nOn GitHub, you can find another notebook exploring the EfficientDet model that we did with SSD MobileNet."
  },
  {
    "objectID": "raspi/object_detection/object_detection_fundamentals.html#object-detection-on-a-live-stream",
    "href": "raspi/object_detection/object_detection_fundamentals.html#object-detection-on-a-live-stream",
    "title": "Object Detection: Fundamentals",
    "section": "Object Detection on a live stream",
    "text": "Object Detection on a live stream\nObject detection models can also detect objects in real-time using a camera. The captured image should be the input for the trained and converted model. On the Raspberry Pi 4 or 5, OpenCV can capture frames and display inference results on a desktop.\nHowever, even without a desktop, it’s possible to create a live stream with a webcam to detect objects in real time. For example, let’s start with the script developed for the Image Classification app and adapt it for a Real-Time Object Detection Web Application Using TensorFlow Lite and Flask.\nDownload the Python script object_detection_app.py from GitHub.\nThis app version should work for any TFLite/LiteRT models.\n\nVerify if the model is in its correct folder, for example:\n\nmodel_path = \"./models/ssd-mobilenet-v1-tflite-default-v1.tflite\"\nCheck the model and labels:\n\nAnd on the terminal, run:\npython object_detection_app.py\nAfter starting, you should receive the message on the terminal (the IP is from my Raspberry):\n\n* Running on http://192.168.4.210:5000\nPress CTRL+C to quit\n\nAnd access the web interface:\n\nOn the Raspberry Pi itself (if you have a GUI): Open a web browser and go to http://localhost:5000\nFrom another device on the same network: Open a web browser and go to http://&lt;raspberry_pi_ip&gt;:5000 (Replace &lt;raspberry_pi_ip&gt; with your Raspberry Pi’s IP address). For example: http://192.168.4.210:5000/\n\nHere is a screenshot of the app running on an external desktop\n\nLet’s see a technical description of the key modules used in the object detection application:\n\nLiteRT:\n\nPurpose: Efficient inference of machine learning models on edge devices.\nWhy: LiteRT offers a smaller model size and optimized performance compared to full TensorFlow, which is crucial for resource-constrained devices like the Raspberry Pi. It supports hardware acceleration and quantization, further improving efficiency.\nKey functions: Interpreter for loading and running the model, get_input_details(), and get_output_details() for interfacing with the model.\n\nFlask:\n\nPurpose: Lightweight web framework for building backend servers.\nWhy: Flask’s simplicity and flexibility make it ideal for rapidly developing and deploying web applications. It’s less resource-intensive than larger frameworks suitable for edge devices.\nKey components: route decorators for defining API endpoints, Response objects for streaming video, render_template_string for serving dynamic HTML.\n\nPicamera2:\n\nPurpose: Interface with the Raspberry Pi camera module.\nWhy: Picamera2 is the latest library for controlling Raspberry Pi cameras, offering improved performance and features over the original Picamera library.\nKey functions: create_preview_configuration() for setting up the camera, capture_file() for capturing frames.\n\nPIL (Python Imaging Library):\n\nPurpose: Image processing and manipulation.\nWhy: PIL provides a wide range of image processing capabilities. It’s used here to resize images, draw bounding boxes, and convert between image formats.\nKey classes: Image for loading and manipulating images, ImageDraw for drawing shapes and text on images.\n\nNumPy:\n\nPurpose: Efficient array operations and numerical computing.\nWhy: NumPy’s array operations are much faster than pure Python lists, which is crucial for efficiently processing image data and model inputs/outputs.\nKey functions: array() for creating arrays, expand_dims() for adding dimensions to arrays.\n\nThreading:\n\nPurpose: Concurrent execution of tasks.\nWhy: Threading enables simultaneous frame capture, object detection, and web server operation, which is crucial for maintaining real-time performance.\nKey components: Thread class creates separate execution threads, and Lock is used for thread synchronization.\n\nio.BytesIO:\n\nPurpose: In-memory binary streams.\nWhy: Allows efficient handling of image data in memory without needing temporary files, improving speed and reducing I/O operations.\n\ntime:\n\nPurpose: Time-related functions.\nWhy: Used for adding delays (time.sleep()) to control frame rate and for performance measurements.\n\njQuery (client-side):\n\nPurpose: Simplified DOM manipulation and AJAX requests.\nWhy: It makes it easy to update the web interface dynamically and communicate with the server without page reloads.\nKey functions: .get() and .post() for AJAX requests, DOM manipulation methods for updating the UI.\n\n\nRegarding the main app system architecture:\n\nMain Thread: Runs the Flask server, handling HTTP requests and serving the web interface.\nCamera Thread: Continuously captures frames from the camera.\nDetection Thread: Processes frames using the LiteRT object detection model.\nFrame Buffer: Shared memory space (protected by locks) storing the latest frame and detection results.\n\nAnd the app data flow, we can describe in short:\n\nCamera captures frame → Frame Buffer\nDetection thread reads from Frame Buffer → Processes through TFLite/LiteRT model → Updates detection results in Frame Buffer\nFlask routes access the Frame Buffer to serve the latest frame and detection results\nWeb client receives updates via AJAX and updates the UI\n\nThis architecture enables efficient, real-time object detection while maintaining a responsive web interface on a resource-constrained edge device, such as a Raspberry Pi. Threading and efficient libraries, such as LiteRT and PIL, enable the system to process video frames in real-time, while Flask and jQuery provide a user-friendly way to interact with them.\nYou can test the app with another pre-processed model, such as the EfficientDet, by changing the app line:\nmodel_path = \"./models/lite-model_efficientdet_lite0_detection_metadata_1.tflite\"\n\nIf we want to use the app with the SSD-MobileNetV2 model, trained in Edge Impulse Studio with the “Box versus Wheel” dataset, the code should also be adapted to the input details, as we explored in its notebook."
  },
  {
    "objectID": "raspi/object_detection/object_detection_fundamentals.html#conclusion",
    "href": "raspi/object_detection/object_detection_fundamentals.html#conclusion",
    "title": "Object Detection: Fundamentals",
    "section": "Conclusion",
    "text": "Conclusion\nThis lab has explored implementing object detection on edge devices such as the Raspberry Pi, demonstrating the power and potential of running advanced computer vision tasks on resource-constrained hardware. We examined the object detection models SSD-MobileNet and EfficientDet, comparing their performance and trade-offs on edge devices.\nThe lab demonstrated a real-time object-detection web application, showing how these models can be integrated into practical, interactive systems.\nThe ability to perform object detection on edge devices opens up numerous possibilities across domains such as precision agriculture, industrial automation, quality control, smart home applications, and environmental monitoring. By processing data locally, these systems can offer reduced latency, improved privacy, and operation in environments with limited connectivity.\nLooking ahead, potential areas for further exploration include: - Using a custom dataset (labeled on Roboflow), walking through the process of training models using Edge Impulse Studio and Ultralytics, and deploying them on Raspberry Pi. - To improve inference speed on edge devices, explore various optimization methods, such as model quantization (TFLite int8) and format conversion (e.g., to NCNN). - Implementing multi-model pipelines for more complex tasks - Exploring hardware acceleration options for Raspberry Pi - Integrating object detection with other sensors for more comprehensive edge AI systems - Developing edge-to-cloud solutions that leverage both local processing and cloud resources\nObject detection on edge devices can create intelligent, responsive systems that bring the power of AI directly into the physical world, opening up new frontiers in how we interact with and understand our environment."
  },
  {
    "objectID": "raspi/object_detection/object_detection_fundamentals.html#resources",
    "href": "raspi/object_detection/object_detection_fundamentals.html#resources",
    "title": "Object Detection: Fundamentals",
    "section": "Resources",
    "text": "Resources\n\nSSD-MobileNet Notebook on a Raspberry Pi\nEfficientDet Notebook on a Raspberry Pi\nPython Scripts\nModels\n​"
  },
  {
    "objectID": "raspi/object_detection/custom_object_detection.html#object-detection-project",
    "href": "raspi/object_detection/custom_object_detection.html#object-detection-project",
    "title": "Custom Object Detection Project",
    "section": "Object Detection Project",
    "text": "Object Detection Project\nIn this chapter, we will develop a complete Object Detection project from data collection, labelling, training, and deployment. As we did with the Image Classification project, the trained and converted model will be used for inference.\n\nWe will use the same dataset to train 3 models: SSD-MobileNet V2, FOMO, and YOLO.\n\nThe Goal\nAll Machine Learning projects need to start with a goal. Let’s assume we are in an industrial facility and must sort and count wheels and special boxes.\n\nIn other words, we should perform a multi-label classification, where each image can have three classes:\n\nBackground (no objects)\nBox\nWheel\n\n\n\nRaw Data Collection\nOnce we have defined our Machine Learning project goal, the next and most crucial step is collecting the dataset. We can use a phone, the Raspi, or a mix to create the raw dataset (with no labels). Let’s use the simple web app on our Raspberry Pi to view the QVGA (320 x 240) captured images in a browser.\nFrom GitHub, get the Python script get_img_data.py and open it in the terminal:\npython3 get_img_data.py \nAccess the web interface:\n\nOn the Raspberry Pi itself (if you have a GUI): Open a web browser and go to http://localhost:5000\nFrom another device on the same network: Open a web browser and go to http://&lt;raspberry_pi_ip&gt;:5000 (Replace &lt;raspberry_pi_ip&gt; with your Raspberry Pi’s IP address). For example: http://192.168.4.210:5000/\n\nThe Python script creates a web-based interface for capturing and organizing image datasets using a Raspberry Pi and its camera. It’s handy for machine learning projects that require labeled image data, or not, as in our case here.\nAccess the web interface from a browser, enter a generic label for the images you want to capture, and press Start Capture.\n\n\nNote that the images to be captured will have multiple labels that should be defined later.\n\nUse the live preview to position the camera, then click Capture Image to save the images under the current label (in this case, box-wheel).\n\nWhen we have enough images, we can press Stop Capture. The captured images are saved in the folder dataset/box-wheel:\n\n\nGet around 60 images. Try to capture different angles, backgrounds, and light conditions. FileZilla can transfer the raw dataset you created to your main computer.\n\n\n\nLabeling Data\nThe next step in an Object Detect project is to create a labeled dataset. We should label the raw dataset images, creating bounding boxes around each picture’s objects (box and wheel). We can use labeling tools such as LabelImg, CVAT, Roboflow, or even the Edge Impulse Studio. Once we have explored the Edge Impulse tool in other labs, let’s use Roboflow here.\n\nWe are using Roboflow (free version) here for two main reasons. 1) We can have an auto-labeler, and 2) The annotated dataset is available in several formats and can be used both on Edge Impulse Studio (we will use it for MobileNet V2 and FOMO train) and on CoLab (YOLOv8 or YOLOv11 train), for example. An annotated dataset created on Edge Impulse (Free account) cannot be used for training on other platforms.\n\nWe should upload the raw dataset to Roboflow. Create a free account there and start a new project, for example, (“box-versus-wheel”).\n\n\nWe will not go into great detail about the Roboflow process, as many tutorials are already available.\n\n\nAnnotate\nOnce the project is created and the dataset is uploaded, you can use the “Auto-Label” tool to generate annotations, or do it manually.\n\nThe Label Assist tool can be handy for the labeling process.\n\nNote that you should also upload images with only a background, which should be saved w/o any annotations using the Null Tool option.\n\nOnce all images are annotated, split them into training, validation, and test sets.\n\n\n\nData Pre-Processing\nThe last step in the dataset is preprocessing to generate a final training version. Let’s resize all images to 320x320 and generate augmented versions of each image (augmentation) to create new training examples from which our model can learn.\nFor augmentation, we will rotate the images (+/-15o), crop, and vary the brightness and exposure.\n\nAt the end of the process, we will have 153 images.\n\nNow, you should export the annotated dataset in a format that Edge Impulse, Ultralitics, and other frameworks/tools understand, for example, YOLOv8 (or v11). Let’s download a zipped version of the dataset to our desktop.\n\nHere, it is possible to review how the dataset was structured\n\nThere are 3 separate folders, one for each split (train/test/valid). For each of them, there are 2 subfolders, images, and labels. The pictures are stored as image_id.jpg and images_id.txt, where “image_id” is unique for every picture.\nThe labels file format will be class_id bounding box coordinates, where in our case, class_id will be 0 for box and 1 for wheel. The numerical id (o, 1, 2…) will follow the alphabetical order of the class name.\nThe data.yaml file contains information about the dataset, such as the classes’ names (names: ['box', 'wheel']) following the YOLO format.\nAnd that’s it! We are ready to start training using Edge Impulse Studio (as we will in the next step), Ultralytics (as we will when discussing YOLO), or even training from scratch on CoLab (as we did with the Cifar-10 dataset in the Image Classification lab).\n\nThe pre-processed dataset can be found at the Roboflow site, or here:"
  },
  {
    "objectID": "raspi/object_detection/custom_object_detection.html#training-an-ssd-mobilenet-model-on-edge-impulse-studio",
    "href": "raspi/object_detection/custom_object_detection.html#training-an-ssd-mobilenet-model-on-edge-impulse-studio",
    "title": "Custom Object Detection Project",
    "section": "Training an SSD MobileNet Model on Edge Impulse Studio",
    "text": "Training an SSD MobileNet Model on Edge Impulse Studio\nGo to Edge Impulse Studio, enter your credentials at Login (or create an account), and start a new project.\n\nHere, you can clone the project developed for this hands-on lab: Raspi - Object Detection.\n\nOn the Project Dashboard tab, go down to Project info, and for Labeling method select Bounding boxes (object detection)\n\nUploading the annotated data\nIn Studio, go to the Data acquisition tab, and in the UPLOAD DATA section, upload the raw dataset from your computer.\nWe can use the Select a folder option, choosing, for example, the train folder on your computer, which contains two sub-folders: images and labels. Select the Image label format, “YOLO TXT”, upload it into the category Training, and press Upload data.\n\nRepeat the process for the test data (upload both folders, test, and validation). At the end of the upload process, you should end with the annotated dataset of 153 images split in the train/test (84%/16%).\n\nNote that labels will be stored at the labels files 0 and 1 , which are equivalent to box and wheel.\n\n\n\n\nThe Impulse Design\nThe first thing to define when we enter the Create impulse step is to describe the target device for deployment. A pop-up window will appear. We will select Raspberry 4, an intermediary device between the Raspi-Zero and the Raspi-5.\n\nThis choice will not interfere with the training; it will only give us an idea about the latency of the model on that specific target.\n\n\nIn this phase, you should define how to:\n\nPre-processing consists of resizing the individual images. In our case, the images were pre-processed on Roboflow, to 320x320 , so let’s keep it. The resize will not matter here because the images are already squared. If you upload a rectangular image, squash it (squared form, without cropping). Afterward, you could define if the images are converted from RGB to Grayscale or not.\nDesign a Model, in this case, “Object Detection.”\n\n\n\n\nPreprocessing all dataset\nIn the section Image, select Color depth as RGB, and press Save parameters.\n\nThe Studio automatically moves to the next section, Generate features, where all samples will be preprocessed, resulting in 480 objects: 207 boxes and 273 wheels.\n\nThe feature explorer shows that all samples exhibit a good separation after the feature generation.\n\n\nModel Design, Training, and Test\nFor training, we should select a pre-trained model. Let’s use the MobileNetV2 SSD FPN-Lite (320x320 only).\n\nBase Network (MobileNetV2)\nDetection Network (Single Shot Detector or SSD)\nFeature Extractor (FPN-Lite)\n\nIt is a pre-trained object detection model that locates up to 10 objects in an image and outputs a bounding box for each. The model is approximately 3.7 MB. It supports an RGB input at 320x320px.\n\nRegarding the training hyper-parameters, the model will be trained with:\n\nEpochs: 25\nBatch size: 32\nLearning Rate: 0.15.\n\nFor validation during training, 20% of the dataset (validation_dataset) will be spared.\n\nAs a result, the model achieves an overall precision score (based on COCO mAP) of 88.8%, higher than the score on the test data (83.3%).\n\n\nDeploying the model\nWe have two ways to deploy our model:\n\nTFLite model, which lets deploy the trained model as .tflite for the Raspberry Pi to run it using Python.\nLinux (AARCH64), a binary for Linux (AARCH64), implements the Edge Impulse Linux protocol, which lets us run our models on any Linux-based development board, with SDKs such as Python. See the documentation for more information and setup instructions.\n\nLet’s deploy the TFLite model. On the Dashboard tab, go to Transfer learning model (int8 quantized) and click on the download icon:\n\n\nTransfer the model from your computer to the Raspi folder./models and capture or get some images for inference and save them in the folder ./images.\n\n\nInference and Post-Processing\nThe inference can be made as discussed in the Pre-Trained Object Detection Models Overview. Let’s start a new notebook to follow all the steps to detect cubes and wheels in an image.\nImport the needed libraries:\nimport time\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as patches\nfrom PIL import Image\nfrom ai_edge_litert.interpreter import Interpreter\nDefine the model path and labels:\nmodel_path = \"./models/ei-raspi-object-detection-SSD-MobileNetv2-320x0320-\\\nint8.lite\"\nlabels = ['box', 'wheel']\n\nRemember that the model will output the class ID as values (0 and 1), following an alphabetic order regarding the class names.\n\nLoad the model, allocate the tensors, and get the input and output tensor details:\n# Create a LiteRT Interpreter\ninterpreter = Interpreter(model_path=model_path)\ninterpreter.allocate_tensors()\n\n# Get input and output tensors\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\nOne crucial difference to note is that the dtype of the input details of the model is now int8, which means that the input values go from -128 to +127, while each pixel of our raw image goes from 0 to 256. This means that we should pre-process the image to match it. We can check here:\ninput_dtype = input_details[0]['dtype']\ninput_dtype\nnumpy.int8\nSo, let’s open the image and show it:\n# Load the image\nimg_path = \"./images/box_2_wheel_2.jpg\"\norig_img = Image.open(img_path)\n\n# Display the image\nplt.figure(figsize=(6, 6))\nplt.imshow(orig_img)\nplt.title(\"Original Image\")\nplt.show()\n\nAnd perform the pre-processing:\nscale, zero_point = input_details[0]['quantization']\nimg = orig_img.resize((input_details[0]['shape'][1], \n                  input_details[0]['shape'][2]))\nimg_array = np.array(img, dtype=np.float32) / 255.0\nimg_array = (img_array / scale + zero_point).clip(-128, 127).astype(np.int8)\ninput_data = np.expand_dims(img_array, axis=0)\nChecking the input data, we can verify that the input tensor is compatible with what is expected by the model:\ninput_data.shape, input_data.dtype\n((1, 320, 320, 3), dtype('int8'))\nNow, it is time to perform the inference. Let’s also calculate the latency of the model:\n# Inference on Raspi-Zero\nstart_time = time.time()\ninterpreter.set_tensor(input_details[0]['index'], input_data)\ninterpreter.invoke()\nend_time = time.time()\ninference_time = (end_time - start_time) * 1000  # Convert to milliseconds\nprint (\"Inference time: {:.1f}ms\".format(inference_time))\nThe model will take around 600ms to perform the inference in the Raspi-Zero, which is around 5 times longer than a Raspi-5.\nNow, we can get the output classes of objects detected, its bounding boxes coordinates, and probabilities.\nboxes = interpreter.get_tensor(output_details[1]['index'])[0]  \nclasses = interpreter.get_tensor(output_details[3]['index'])[0]  \nscores = interpreter.get_tensor(output_details[0]['index'])[0]        \nnum_detections = int(interpreter.get_tensor(output_details[2]['index'])[0])\nfor i in range(num_detections):\n    if scores[i] &gt; 0.5:  # Confidence threshold\n        print(f\"Object {i}:\")\n        print(f\"  Bounding Box: {boxes[i]}\")\n        print(f\"  Confidence: {scores[i]}\")\n        print(f\"  Class: {classes[i]}\")\n\nFrom the results, we can see that 4 objects were detected: two with class ID 0 (box)and two with class ID 1 (wheel), what is correct!\nLet’s visualize the result for a threshold of 0.5\nthreshold = 0.5\nplt.figure(figsize=(6,6))\nplt.imshow(orig_img)\nfor i in range(num_detections):\n    if scores[i] &gt; threshold:  \n        ymin, xmin, ymax, xmax = boxes[i]\n        (left, right, top, bottom) = (xmin * orig_img.width, \n                                      xmax * orig_img.width, \n                                      ymin * orig_img.height, \n                                      ymax * orig_img.height)\n        rect = plt.Rectangle((left, top), right-left, bottom-top, \n                             fill=False, color='red', linewidth=2)\n        plt.gca().add_patch(rect)\n        class_id = int(classes[i])\n        class_name = labels[class_id]\n        plt.text(left, top-10, f'{class_name}: {scores[i]:.2f}', \n                 color='red', fontsize=12, backgroundcolor='white')\n\nBut what happens if we reduce the threshold to 0.3, for example?\n\nWe start to see false positives and multiple detections, where the model detects the same object multiple times with different confidence levels and slightly different bounding boxes.\nCommonly, sometimes, we need to adjust the threshold to smaller values to capture all objects, avoiding false negatives, which would lead to multiple detections.\nTo improve the detection results, we should implement Non-Maximum Suppression (NMS), which helps eliminate overlapping bounding boxes and keeps only the most confident detection.\nFor that, let’s create a general function named non_max_suppression(), with the role of refining object detection results by eliminating redundant and overlapping bounding boxes. It achieves this by iteratively selecting the detection with the highest confidence score and removing other significantly overlapping detections based on an Intersection over Union (IoU) threshold.\ndef non_max_suppression(boxes, scores, threshold):\n    # Convert to corner coordinates\n    x1 = boxes[:, 0]\n    y1 = boxes[:, 1]\n    x2 = boxes[:, 2]\n    y2 = boxes[:, 3]\n\n    areas = (x2 - x1 + 1) * (y2 - y1 + 1)\n    order = scores.argsort()[::-1]\n\n    keep = []\n    while order.size &gt; 0:\n        i = order[0]\n        keep.append(i)\n        xx1 = np.maximum(x1[i], x1[order[1:]])\n        yy1 = np.maximum(y1[i], y1[order[1:]])\n        xx2 = np.minimum(x2[i], x2[order[1:]])\n        yy2 = np.minimum(y2[i], y2[order[1:]])\n\n        w = np.maximum(0.0, xx2 - xx1 + 1)\n        h = np.maximum(0.0, yy2 - yy1 + 1)\n        inter = w * h\n        ovr = inter / (areas[i] + areas[order[1:]] - inter)\n\n        inds = np.where(ovr &lt;= threshold)[0]\n        order = order[inds + 1]\n\n    return keep\nHow it works:\n\nSorting: It starts by sorting all detections by their confidence scores, highest to lowest.\nSelection: It selects the highest-scoring box and adds it to the final list of detections.\nComparison: This selected box is compared with all remaining lower-scoring boxes.\nElimination: Any box that overlaps significantly (above the IoU threshold) with the selected box is eliminated.\nIteration: This process repeats with the next highest-scoring box until all boxes are processed.\n\nNow, we can define a more precise visualization function that will take into consideration an IoU threshold, detecting only the objects that were selected by the non_max_suppression function:\ndef visualize_detections(image, boxes, classes, scores, \n                         labels, threshold, iou_threshold):\n    if isinstance(image, Image.Image):\n        image_np = np.array(image)\n    else:\n        image_np = image\n\n    height, width = image_np.shape[:2]\n    \n    # Convert normalized coordinates to pixel coordinates\n    boxes_pixel = boxes * np.array([height, width, height, width])\n    \n    # Apply NMS\n    keep = non_max_suppression(boxes_pixel, scores, iou_threshold)\n    \n    # Set the figure size to 12x8 inches\n    fig, ax = plt.subplots(1, figsize=(12, 8))\n\n    ax.imshow(image_np)\n    \n    for i in keep:\n        if scores[i] &gt; threshold:\n            ymin, xmin, ymax, xmax = boxes[i]\n            rect = patches.Rectangle((xmin * width, ymin * height),\n                                     (xmax - xmin) * width,\n                                     (ymax - ymin) * height,\n                                     linewidth=2, edgecolor='r', facecolor='none')\n            ax.add_patch(rect)\n            class_name = labels[int(classes[i])]\n            ax.text(xmin * width, ymin * height - 10,\n                    f'{class_name}: {scores[i]:.2f}', color='red',\n                    fontsize=12, backgroundcolor='white')\n\n    plt.show()\nNow we can create a function that will call the others, performing inference on any image:\ndef detect_objects(img_path, conf=0.5, iou=0.5):\n    orig_img = Image.open(img_path)\n    scale, zero_point = input_details[0]['quantization']\n    img = orig_img.resize((input_details[0]['shape'][1], \n                      input_details[0]['shape'][2]))\n    img_array = np.array(img, dtype=np.float32) / 255.0\n    img_array = (img_array / scale + zero_point).clip(-128, 127).\\\n    astype(np.int8)\n    input_data = np.expand_dims(img_array, axis=0)\n    \n    # Inference on Raspi-Zero\n    start_time = time.time()\n    interpreter.set_tensor(input_details[0]['index'], input_data)\n    interpreter.invoke()\n    end_time = time.time()\n    inference_time = (end_time - start_time) * 1000  # Convert to ms\n    print (\"Inference time: {:.1f}ms\".format(inference_time))\n    \n    # Extract the outputs\n    boxes = interpreter.get_tensor(output_details[1]['index'])[0]  \n    classes = interpreter.get_tensor(output_details[3]['index'])[0]  \n    scores = interpreter.get_tensor(output_details[0]['index'])[0]        \n    num_detections = int(interpreter.get_tensor(output_details[2]['index'])[0])\n\n    visualize_detections(orig_img, boxes, classes, scores, labels, \n                         threshold=conf, \n                         iou_threshold=iou)\nNow, running the code, having the same image again with a confidence threshold of 0.3, but with a small IoU:\nimg_path = \"./images/box_2_wheel_2.jpg\"\ndetect_objects(img_path, conf=0.3,iou=0.05)"
  },
  {
    "objectID": "raspi/object_detection/custom_object_detection.html#training-a-fomo-model-at-edge-impulse-studio",
    "href": "raspi/object_detection/custom_object_detection.html#training-a-fomo-model-at-edge-impulse-studio",
    "title": "Custom Object Detection Project",
    "section": "Training a FOMO Model at Edge Impulse Studio",
    "text": "Training a FOMO Model at Edge Impulse Studio\nThe inference with the SSD MobileNet model worked well, but the latency was significantly high. The inference varied from 0.5 to 1.3 seconds on a Raspi-Zero, which means around or less than 1 FPS (1 frame per second). One alternative to speed up the process is to use FOMO (Faster Objects, More Objects).\nThis novel machine learning algorithm lets us count multiple objects and find their location in an image in real-time using up to 30x less processing power and memory than MobileNet SSD or YOLO. The main reason this is possible is that while other models calculate the object’s size by drawing a square around it (bounding box), FOMO ignores the size of the image, providing only the information about where the object is located in the image through its centroid coordinates.\n\nHow FOMO works?\nIn a typical object detection pipeline, the first stage is extracting features from the input image. FOMO leverages MobileNetV2 to perform this task. MobileNetV2 processes the input image to produce a feature map that captures essential characteristics, such as textures, shapes, and object edges, in a computationally efficient way.\n\nOnce these features are extracted, FOMO’s simpler architecture, focused on center-point detection, interprets the feature map to determine where objects are located in the image. The output is a grid of cells, where each cell represents whether or not an object center is detected. The model outputs one or more confidence scores for each cell, indicating the likelihood of an object being present.\nLet’s see how it works on an image.\nFOMO divides the image into blocks of pixels using a factor of 8. For the input of 96x96, the grid would be 12x12 (96/8=12). For a 160x160, the grid will be 20x20, and so on. Next, FOMO will run a classifier through each pixel block to calculate the probability that there is a box or a wheel in each of them and, subsequently, determine the regions that have the highest probability of containing the object (If a pixel block has no objects, it will be classified as background). From the overlap of the final region, the FOMO provides the coordinates (related to the image dimensions) of the centroid of this region.\n\nTrade-off Between Speed and Precision:\n\nGrid Resolution: FOMO uses a grid of fixed resolution, meaning each cell can detect if an object is present in that part of the image. While it doesn’t provide high localization accuracy, it makes a trade-off by being fast and computationally light, which is crucial for edge devices.\nMulti-Object Detection: Since each cell is independent, FOMO can detect multiple objects simultaneously in an image by identifying multiple centers.\n\n\n\nImpulse Design, new Training and Testing\nReturn to Edge Impulse Studio, and in the Experiments tab, create another impulse. Now, the input images should be 160x160 (this is the expected input size for MobilenetV2).\n\nOn the Image tab, generate the features and go to the Object detection tab.\nWe should select a pre-trained model for training. Let’s use the FOMO (Faster Objects, More Objects) MobileNetV2 0.35.\n\nRegarding the training hyper-parameters, the model will be trained with:\n\nEpochs: 30\nBatch size: 32\nLearning Rate: 0.001.\n\nFor validation during training, 20% of the dataset (validation_dataset) will be spared. We will not apply Data Augmentation for the remaining 80% (train_dataset) because our dataset was already augmented during the labeling phase at Roboflow.\nAs a result, the model ends with an overall F1 score of 93.3% with an impressive latency of 8ms (Raspi-4), around 60X less than we got with the SSD MovileNetV2.\n\n\nNote that FOMO automatically added a third label background to the two previously defined boxes (0) and wheels (1).\n\nOn the Model testing tab, we can see that the accuracy was 94%. Here is one of the test sample results:\n\n\nIn object detection tasks, accuracy is generally not the primary evaluation metric. Object detection involves classifying objects and providing bounding boxes around them, making it a more complex problem than simple classification. The issue is that we do not have the bounding box, only the centroids. In short, using accuracy as a metric could be misleading and may not provide a complete understanding of how well the model is performing.\n\n\n\nDeploying the model\nAs we did in the previous section, we can deploy the trained model as TFLite or Linux (AARCH64). Let’s do it now as Linux (AARCH64), a binary that implements the Edge Impulse Linux protocol.\nEdge Impulse for Linux models is delivered in .eim format. This executable contains our “full impulse” created in Edge Impulse Studio. The impulse consists of the signal processing block(s) and any learning and anomaly block(s) we added and trained. It is compiled with optimizations for our processor or GPU (e.g., NEON instructions on ARM cores), plus a straightforward IPC layer (over a Unix socket).\nAt the Deploy tab, select the option Linux (AARCH64), the int8model and press Build.\n\nThe model will be automatically downloaded to your computer.\nOn our Raspi, let’s create a new working area:\ncd ~\ncd Documents\nmkdir EI_Linux\ncd EI_Linux\nmkdir models\nmkdir images\nRename the model for easy identification:\nFor example, raspi-object-detection-linux-aarch64-FOMO-int8.eim and transfer it to the new Raspi folder./models and capture or get some images for inference and save them in the folder ./images.\n\n\nInference and Post-Processing\nThe inference will be made using the Linux Python SDK. This library lets us run machine learning models and collect sensor data on Linux machines using Python. The SDK is open source and available on GitHub at edgeimpulse/linux-sdk-python.\nLet’s set up a Virtual Environment for working with the Linux Python SDK\npython3 -m venv ~/eilinux\nsource ~/eilinux/bin/activate\nAnd Install the all the libraries needed:\nsudo apt-get update\nsudo apt-get install libatlas-base-dev libportaudio0 libportaudio2\nsudo apt-get install libportaudiocpp0 portaudio19-dev\n\npip3 install edge_impulse_linux -i https://pypi.python.org/simple\npip3 install Pillow matplotlib pyaudio opencv-contrib-python\n\nsudo apt-get install portaudio19-dev\npip3 install pyaudio \npip3 install opencv-contrib-python\nPermit our model to be executable.\nchmod +x raspi-object-detection-linux-aarch64-FOMO-int8.eim\nInstall the Jupiter Notebook on the new environment\npip3 install jupyter\nRun a notebook locally (on the Raspi-4 or 5 with desktop)\njupyter notebook\nor on the browser on your computer:\njupyter notebook --ip=192.168.4.210 --no-browser\nLet’s start a new notebook by following all the steps to detect cubes and wheels on an image using the FOMO model and the Edge Impulse Linux Python SDK.\nImport the needed libraries:\nimport sys, time\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as patches\nfrom PIL import Image\nimport cv2\nfrom edge_impulse_linux.image import ImageImpulseRunner\nDefine the model path and labels:\nmodel_file = \"raspi-object-detection-linux-aarch64-int8.eim\"\nmodel_path = \"models/\"+ model_file # Trained ML model from Edge Impulse\nlabels = ['box', 'wheel']\n\nRemember that the model will output the class ID as values (0 and 1), following an alphabetic order regarding the class names.\n\nLoad and initialize the model:\n# Load the model file\nrunner = ImageImpulseRunner(model_path)\n\n# Initialize model\nmodel_info = runner.init()\nThe model_info will contain critical information about our model. However, unlike the TFLite interpreter, the EI Linux Python SDK library will now prepare the model for inference.\nSo, let’s open the image and show it (Now, for compatibility, we will use OpenCV, the CV Library used internally by EI. OpenCV reads the image as BGR, so we will need to convert it to RGB :\n# Load the image\nimg_path = \"./images/1_box_1_wheel.jpg\"\norig_img = cv2.imread(img_path)\nimg_rgb = cv2.cvtColor(orig_img, cv2.COLOR_BGR2RGB)\n\n# Display the image\nplt.imshow(img_rgb)\nplt.title(\"Original Image\")\nplt.show()\n\nNow we will get the features and the preprocessed image (cropped) using the runner:\nfeatures, cropped = runner.get_features_from_image_auto_studio_setings(img_rgb)\nAnd perform the inference. Let’s also calculate the latency of the model:\nres = runner.classify(features)\nLet’s get the output classes of objects detected, their bounding boxes centroids, and probabilities.\nprint('Found %d bounding boxes (%d ms.)' % (\n  len(res[\"result\"][\"bounding_boxes\"]), \n  res['timing']['dsp'] + res['timing']['classification']))\nfor bb in res[\"result\"][\"bounding_boxes\"]:\n    print('\\t%s (%.2f): x=%d y=%d w=%d h=%d' % (\n      bb['label'], bb['value'], bb['x'], \n      bb['y'], bb['width'], bb['height']))\nFound 2 bounding boxes (29 ms.)\n    1 (0.91): x=112 y=40 w=16 h=16\n    0 (0.75): x=48 y=56 w=8 h=8\nThe results show that two objects were detected: one with class ID 0 (box) and one with class ID 1 (wheel), which is correct!\nLet’s visualize the result (The threshold is 0.5, the default value set during the model testing on the Edge Impulse Studio).\nprint('\\tFound %d bounding boxes (latency: %d ms)' % (\n  len(res[\"result\"][\"bounding_boxes\"]), \n  res['timing']['dsp'] + res['timing']['classification']))\nplt.figure(figsize=(5,5))\nplt.imshow(cropped)\n\n# Go through each of the returned bounding boxes\nbboxes = res['result']['bounding_boxes']\nfor bbox in bboxes:\n\n    # Get the corners of the bounding box\n    left = bbox['x']\n    top = bbox['y']\n    width = bbox['width']\n    height = bbox['height']\n    \n    # Draw a circle centered on the detection\n    circ = plt.Circle((left+width//2, top+height//2), 5, \n                     fill=False, color='red', linewidth=3)\n    plt.gca().add_patch(circ)\n    class_id = int(bbox['label'])\n    class_name = labels[class_id]\n    plt.text(left, top-10, f'{class_name}: {bbox[\"value\"]:.2f}', \n              color='red', fontsize=12, backgroundcolor='white')\nplt.show()"
  },
  {
    "objectID": "raspi/object_detection/custom_object_detection.html#conclusion",
    "href": "raspi/object_detection/custom_object_detection.html#conclusion",
    "title": "Custom Object Detection Project",
    "section": "Conclusion",
    "text": "Conclusion\nThis chapter has explored the implementation of a custom object detector on edge devices, such as the Raspberry Pi, demonstrating the power and potential of running advanced computer vision tasks on resource-constrained hardware. We’ve covered several vital aspects:\n\nModel Comparison: We examined object detection models, including SSD-MobileNet and FOMO, and compared their performance and trade-offs on edge devices.\nTraining and Deployment: Using a custom dataset of boxes and wheels (labeled on Roboflow), we walked through the process of training models with Edge Impulse Studio and Ultralytics and deploying them on a Raspberry Pi.\nOptimization Techniques: To improve inference speed on edge devices, we explored various optimization methods, such as model quantization (int8).\nPerformance Considerations: Throughout the lab, we discussed the balance between model accuracy and inference speed, a critical consideration for edge AI applications.\n\nAs discussed earlier, the ability to perform object detection on edge devices opens up numerous possibilities across domains, such as precision agriculture, industrial automation, quality control, smart home applications, and environmental monitoring. By processing data locally, these systems can offer reduced latency, improved privacy, and operation in environments with limited connectivity.\nLooking ahead, potential areas for further exploration include: - Implementing multi-model pipelines for more complex tasks - Exploring hardware acceleration options for Raspberry Pi - Integrating object detection with other sensors for more comprehensive edge AI systems - Developing edge-to-cloud solutions that leverage both local processing and cloud resources\nObject detection on edge devices can create intelligent, responsive systems that bring the power of AI directly into the physical world, opening up new frontiers in how we interact with and understand our environment."
  },
  {
    "objectID": "raspi/object_detection/custom_object_detection.html#resources",
    "href": "raspi/object_detection/custom_object_detection.html#resources",
    "title": "Custom Object Detection Project",
    "section": "Resources",
    "text": "Resources\n\nRoboflow Annotated Dataset (“Box versus Wheel”)\nEdge Impulse Project - SSD MobileNet and FOMO\nNotebooksi\nPython Scripts\nModels"
  },
  {
    "objectID": "raspi/object_detection/cv_yolo.html#exploring-a-yolo-models-using-ultralitics",
    "href": "raspi/object_detection/cv_yolo.html#exploring-a-yolo-models-using-ultralitics",
    "title": "Computer Vision Applications with YOLO",
    "section": "Exploring a YOLO Models using Ultralitics",
    "text": "Exploring a YOLO Models using Ultralitics\nIn this chapter, we will explore YOLOv8 and v11. Ultralytics YOLO (v8 and v11) are versions of the acclaimed real-time object detection and image segmentation model, YOLO. YOLOv8 and v11 are built on cutting-edge advances in deep learning and computer vision, offering unparalleled speed and accuracy. Its streamlined design makes it suitable for a wide range of applications and easily adaptable across hardware platforms, from edge devices to cloud APIs.\n\nTalking about the YOLO Model\nThe YOLO (You Only Look Once) model is a highly efficient, widely used object detection algorithm known for its real-time performance. Unlike traditional object detection systems that repurpose classifiers or localizers to perform detection, YOLO frames the detection problem as a single regression task. This innovative approach enables YOLO to simultaneously predict multiple bounding boxes and their class probabilities from full images during a single evaluation, significantly boosting its speed.\n\nKey Features:\n\nSingle Network Architecture:\n\nYOLO employs a single neural network to process the entire image. This network divides the image into a grid and, for each grid cell, directly predicts bounding boxes and associated class probabilities. This end-to-end training improves speed and simplifies the model architecture.\n\nReal-Time Processing:\n\nOne of YOLO’s standout features is its ability to perform object detection in real-time. Depending on the version and hardware, YOLO can process images at high frames per second (FPS). This makes it ideal for applications requiring quick and accurate object detection, such as video surveillance, autonomous driving, and live sports analysis.\n\nEvolution of Versions:\n\nOver the years, YOLO has undergone significant improvements, from YOLOv1 to the latest YOLOv12. Each iteration has introduced enhancements in accuracy, speed, and efficiency. YOLOv8, for instance, incorporates advancements in network architecture, improved training methodologies, and better support for various hardware, ensuring a more robust performance.\nYOLOv11 offers substantial improvements in accuracy, speed, and parameter efficiency compared to prior versions such as YOLOv8 and YOLOv10, making it one of the most versatile and powerful real-time object detection models available as of 2025\n\n\nAccuracy and Efficiency:\n\nWhile early versions of YOLO traded off some accuracy for speed, recent versions have made substantial strides in balancing both. The newer models are faster and more accurate, detecting small objects (such as bees) and performing well on complex datasets.\n\nWide Range of Applications:\n\nYOLO’s versatility has led to its adoption in numerous fields. It is used in traffic monitoring systems to detect and count vehicles, security applications to identify potential threats and agricultural technology to monitor crops and livestock. Its application extends to any domain requiring efficient and accurate object detection.\n\nCommunity and Development:\n\nYOLO continues to evolve and is supported by a strong community of developers and researchers (with YOLOv8 being very strong). Open-source implementations and extensive documentation have made it accessible for customization and integration into various projects. Popular deep learning frameworks like Darknet, TensorFlow, and PyTorch support YOLO, further broadening its applicability.\n\nModel Capabilities\nYOLO models support multiple computer vision tasks:\n\nObject Detection: Identifying and localizing objects with bounding boxes\nInstance Segmentation: Pixel-level object segmentation\nPose Estimation: Human pose keypoint detection\nClassification: Image classification tasks\n\n\nUltralitics YOLO Detect, Segment, and Pose models pre-trained on the COCO dataset, and Classify on the ImageNet dataset.\n\nTrack mode is available for all Detect, Segment, and Pose models. The latest versions of YOLO can also perform OBB, which stands for Oriented Bounding Box, a rectangular box in computer vision that can rotate to match the orientation of an object within an image, providing a much tighter and more precise fit than traditional axis-aligned bounding boxes.\n\n\n\n\n\nAvailable Model Sizes\nYOLO offers several model variants optimized for different use cases, for example. The YOLOv8:\n\nYOLOv8n (Nano): Smallest model, fastest inference, lowest accuracy\nYOLOv8s (Small): Balanced performance for edge devices\nYOLOv8m (Medium): Higher accuracy, moderate computational requirements\nYOLOv8l (Large): High accuracy, requires more computational resources\nYOLOv8x (Extra Large): Highest accuracy, most computational intensive\n\n\nBy 2025, for Raspberry Pi applications, YOLOv8n or YOLO11n are typically the best choice due to their optimized size and speed. In 2026, Ultralytics launched YOLO 26, not covered in this lab."
  },
  {
    "objectID": "raspi/object_detection/cv_yolo.html#installation",
    "href": "raspi/object_detection/cv_yolo.html#installation",
    "title": "Computer Vision Applications with YOLO",
    "section": "Installation",
    "text": "Installation\nFirst, let’s confirm the System Python version:\npython --version\nIf we use the latest Raspberry Pi OS (based on Debian Trixie), it should be:\n3.13.5\nAs of today (January 2026), Ultralytics officially supports only Python 3.9-3.12; Python 3.13.5 is too new and will likely cause compatibility issues. Since Debian Trixie ships with Python 3.13 by default, we’ll need to install a compatible Python version alongside it.\nOne solution is to install Pyenv, so that we can easily manage multiple Python versions for different projects without affecting the system Python.\n\nIf the Raspberry Pi OS is the legacy, the Python version should be 3.11, and it is not necessary to install Pyenv.\n\n\nInstall pyenv Dependencies\nsudo apt update\nsudo apt install -y build-essential libssl-dev zlib1g-dev \\\n    libbz2-dev libreadline-dev libsqlite3-dev curl git \\\n    libncursesw5-dev xz-utils tk-dev libxml2-dev \\\n    libxmlsec1-dev libffi-dev liblzma-dev \\\n    libopenblas-dev libjpeg-dev libpng-dev cmake\n\n\nInstall pyenv\n# Download and install pyenv\ncurl https://pyenv.run | bash\n\n\nConfigure Shell\nAdd pyenv to ~/.bashrc:\ncat &gt;&gt; ~/.bashrc &lt;&lt; 'EOF'\n\n# pyenv configuration\nexport PYENV_ROOT=\"$HOME/.pyenv\"\n[[ -d $PYENV_ROOT/bin ]] && export PATH=\"$PYENV_ROOT/bin:$PATH\"\neval \"$(pyenv init -)\"\nEOF\nReload the shell:\nsource ~/.bashrc\nVerify if pyenv is installed:\npyenv --version\n\n\nInstall Python 3.11 (or 3.12)\n# See available versions\npyenv install --list | grep \" 3.11\"\n\n# Install Python 3.11.14 (latest 3.11 stable)\npyenv install 3.11.14\n\n# Or install Python 3.12.3 if you prefer\n# pyenv install 3.12.12\n\nThis will take a few minutes to compile.\n\n\n\nCreate YOLO Workspace\ncd Documents    \nmkdir YOLO\ncd YOLO\n\n# Set Python 3.11.14 for this directory\npyenv local 3.11.14\n\n# Verify\npython --version  # Should show Python 3.11.14\n\n\nCreate Virtual Environment\npython -m venv yolo-env\nsource yolo-env/bin/activate\n\n# Verify if we're using the correct Python\nwhich python\npython --version\nTo exit the virtual environment later:\ndeactivate\n\n\nInstalling Ultralytics/Yolo\nOn our Raspi, let’s create new folders on the working area:\ncd Documents/\ncd YOLO\nmkdir models\nmkdir images\nAnd install the Ultralytics packages for local inference on the Raspberry Pi (inside the env)\n\nUpdate the packages list, install and/or upgrade PIP to the latest:\n\nsudo apt update\npip install -U pip\n\nInstall the ultralytics pip package with optional dependencies:\n\npip install ultralytics[export]\n\nReboot the device:\n\nsudo reboot\n\n\nTesting the YOLO\nAfter the Raspi booting, let’s activate the yolo env, go to the working directory,\ncd /Documents/YOLO\nsource ~/yolo_env/bin/activate\nAnd run inference on an image that will be downloaded from the Ultralytics website, using, for example, the YOLOV8n model (the smallest in the family) at the Terminal (CLI):\nyolo predict model='yolov8n' source='https://ultralytics.com/images/bus.jpg'\n\nNote that the first time we invoke a model, it will automatically be downloaded to the current directory.\n\nThe inference result will appear in the terminal. In the image (bus.jpg), 4 persons, 1 bus, and 1 stop signal were detected:\n\nAlso, we got a message that Results saved to runs/detect/predict. Inspecting that directory, we can see a new image saved (bus.jpg). Let’s download it from the Raspi to our desktop for inspection:\n\nSo, the Ultrayitics YOLO is correctly installed on our Raspberry Pi. Note that on the Raspberry Pi Zero, an issue is the high latency for this inference, which takes several seconds, even with the most compact model in the family (YOLOv8n).\n\nTesting with the YOLOv11\nThe procedure is the same as we did with version v8. As a comparison, we can see that the YOLOv11 is faster than the v8, but seems a little less precise, as it does not detect the “stop sign” as the v8."
  },
  {
    "objectID": "raspi/object_detection/cv_yolo.html#export-models-to-ncnn-format",
    "href": "raspi/object_detection/cv_yolo.html#export-models-to-ncnn-format",
    "title": "Computer Vision Applications with YOLO",
    "section": "Export Models to NCNN format",
    "text": "Export Models to NCNN format\nDeploying computer vision models on edge devices with limited computational power, such as the Raspberry Pi Zero, can cause latency issues. One alternative is to use a format optimized for optimal performance. This ensures that even devices with limited processing power can handle advanced computer vision tasks well.\nOf all the model export formats supported by Ultralytics, the NCNN is a high-performance neural network inference computing framework optimized for mobile platforms. From the beginning of the design, NCNN was deeply considerate of deployment and use on mobile phones, and it did not have third-party dependencies. It is cross-platform and runs faster than all known open-source frameworks (such as TFLite).\nNCNN delivers the best inference performance when working with Raspberry Pi devices. NCNN is highly optimized for mobile embedded platforms (such as ARM architecture).\nLet’s move the downloaded YOLO models to the ./models folder and thebus.jpg to ./images.\nAnd convert our models and rerun the inferences:\n\nExport the YOLO PyTorch models to NCNN format, creating: yolov8n_ncnn_model and yolo11n_ncnn_model\n\nyolo export model=./models/yolov8n.pt format=ncnn \nyolo export model=./models/yolo11n.pt format=ncnn\n\nRun inference with the exported models:\n\nyolo predict task=detect model='./models/yolov8n_ncnn_model' source='./images/bus.jpg'\nyolo predict task=detect model='./models/yolo11n_ncnn_model' source='./images/bus.jpg'\n\nThe first inference, when the model is loaded, typically has a high latency; however, from the second inference, it is possible to note that the inference time decreases.\n\nWe can now realize that neither model detects the “Stop Signal”, with YOLOv11 being the fastest. The optimized models are more rapid but also less accurate."
  },
  {
    "objectID": "raspi/object_detection/cv_yolo.html#exploring-yolo-with-python",
    "href": "raspi/object_detection/cv_yolo.html#exploring-yolo-with-python",
    "title": "Computer Vision Applications with YOLO",
    "section": "Exploring YOLO with Python",
    "text": "Exploring YOLO with Python\nTo start, let’s call the Python Interpreter so we can explore how the YOLO model works, line by line:\npython\nNow, we should call the YOLO library from Ultralitics and load the model:\nfrom ultralytics import YOLO\nmodel = YOLO('./models/yolov8n_ncnn_model')\nRun inference over an image (let’s use again bus.jpg):\nimg = './images/bus.jpg'\nresult = model.predict(img, save=True, imgsz=640, conf=0.5, iou=0.3)\n\nWe can verify that the result is almost identical to the one we get running the inference at the terminal level (CLI), except that the bus stop was not detected with the reduced NCNN model. Note that the latency was reduced.\nLet’s analyze the “result” content.\nFor example, we can see result[0].boxes.data, showing us the main inference result, which is a tensor with a shape of (4, 6). Each line is one of the objects detected, being the first four columns, the bounding boxes coordinates, the 5th, the confidence, and the 6th, the class (in this case, 0: person and 5: bus):\n\nWe can access several inference results separately, as the inference time, and have it printed in a better format:\ninference_time = int(result[0].speed['inference'])\nprint(f\"Inference Time: {inference_time} ms\")\nOr we can have the total number of objects detected:\nprint(f'Number of objects: {len (result[0].boxes.cls)}')\n\nWith Python, we can create a detailed output that meets our needs (See Model Prediction with Ultralytics YOLO for more details). Let’s run a Python script instead of manually entering it line by line in the interpreter, as shown below. Let’s use nano as our text editor. First, we should create an empty Python script named, for example, yolov8_tests.py:\nnano yolov8_tests.py\nEnter the code lines:\nfrom ultralytics import YOLO\n\n# Load the YOLOv8 model\nmodel = YOLO('./models/yolov8n_ncnn_model')\n\n# Run inference\nimg = './images/bus.jpg'\nresult = model.predict(img, save=False, imgsz=640, conf=0.5, iou=0.3)\n\n# print the results\ninference_time = int(result[0].speed['inference'])\nprint(f\"Inference Time: {inference_time} ms\")\nprint(f'Number of objects: {len (result[0].boxes.cls)}')\n\nAnd enter with the commands: [CTRL+O] + [ENTER] +[CTRL+X] to save the Python script.\nRun the script:\npython yolov8_tests.py\nThe result is the same as running the inference at the terminal level (CLI) and with the built-in Python interpreter.\n\nCalling the YOLO library and loading the model for inference for the first time takes a long time, but the inferences after that will be much faster. For example, the first single inference can take several seconds, but after that, the inference time should be reduced to less than 1 second.\n\n\nInference Arguments\nmodel.predict() accepts multiple arguments that can be passed at inference time to override defaults:\nInference arguments:\n\n\n\nArgument\nType\nDefault\nDescription\n\n\n\n\nsource\nstr\n'ultralytics/assets'\nSpecifies the data source for inference. Can be an image path, video file, directory, URL, or device ID for live feeds. Supports a wide range of formats and sources, enabling flexible application across different types of input.\n\n\nconf\nfloat\n0.25\nSets the minimum confidence threshold for detections. Objects detected with confidence below this threshold will be disregarded. Adjusting this value can help reduce false positives.\n\n\niou\nfloat\n0.7\nIntersection Over Union (IoU) threshold for Non-Maximum Suppression (NMS). Lower values result in fewer detections by eliminating overlapping boxes, useful for reducing duplicates.\n\n\nimgsz\nint or tuple\n640\nDefines the image size for inference. Can be a single integer 640 for square resizing or a (height, width) tuple. Proper sizing can improve detection accuracy and processing speed.\n\n\nrect\nbool\nTrue\nIf enabled, minimally pads the shorter side of the image until it’s divisible by stride to improve inference speed. If disabled, pads the image to a square during inference.\n\n\nhalf\nbool\nFalse\nEnables half-precision (FP16) inference, which can speed up model inference on supported GPUs with minimal impact on accuracy.\n\n\ndevice\nstr\nNone\nSpecifies the device for inference (e.g., cpu, cuda:0 or 0). Allows users to select between CPU, a specific GPU, or other compute devices for model execution.\n\n\nbatch\nint\n1\nSpecifies the batch size for inference (only works when the source is a directory, video file or .txt file). A larger batch size can provide higher throughput, shortening the total amount of time required for inference.\n\n\nmax_det\nint\n300\nMaximum number of detections allowed per image. Limits the total number of objects the model can detect in a single inference, preventing excessive outputs in dense scenes.\n\n\nvid_stride\nint\n1\nFrame stride for video inputs. Allows skipping frames in videos to speed up processing at the cost of temporal resolution. A value of 1 processes every frame, higher values skip frames.\n\n\nstream_buffer\nbool\nFalse\nDetermines whether to queue incoming frames for video streams. If False, old frames get dropped to accommodate new frames (optimized for real-time applications). If True, queues new frames in a buffer, ensuring no frames get skipped, but will cause latency if inference FPS is lower than stream FPS.\n\n\nvisualize\nbool\nFalse\nActivates visualization of model features during inference, providing insights into what the model is “seeing”. Useful for debugging and model interpretation.\n\n\naugment\nbool\nFalse\nEnables test-time augmentation (TTA) for predictions, potentially improving detection robustness at the cost of inference speed.\n\n\nagnostic_nms\nbool\nFalse\nEnables class-agnostic Non-Maximum Suppression (NMS), which merges overlapping boxes of different classes. Useful in multi-class detection scenarios where class overlap is common.\n\n\nclasses\nlist[int]\nNone\nFilters predictions to a set of class IDs. Only detections belonging to the specified classes will be returned. Useful for focusing on relevant objects in multi-class detection tasks.\n\n\nretina_masks\nbool\nFalse\nReturns high-resolution segmentation masks. The returned masks (masks.data) will match the original image size if enabled. If disabled, they have the image size used during inference.\n\n\nembed\nlist[int]\nNone\nSpecifies the layers from which to extract feature vectors or embeddings. Useful for downstream tasks like clustering or similarity search.\n\n\nproject\nstr\nNone\nName of the project directory where prediction outputs are saved if save is enabled.\n\n\nname\nstr\nNone\nName of the prediction run. Used for creating a subdirectory within the project folder, where prediction outputs are stored if save is enabled.\n\n\nstream\nbool\nFalse\nEnables memory-efficient processing for long videos or numerous images by returning a generator of Results objects instead of loading all frames into memory at once.\n\n\nverbose\nbool\nTrue\nControls whether to display detailed inference logs in the terminal, providing real-time feedback on the prediction process.\n\n\n\nVisualization arguments:\n\n\n\n\n\n\n\n\n\nArgument\nType\nDefault\nDescription\n\n\n\n\nshow\nbool\nFalse\nIf True, displays the annotated images or videos in a window. Useful for immediate visual feedback during development or testing.\n\n\nsave\nbool\nFalse or True\nEnables saving of the annotated images or videos to file. Useful for documentation, further analysis, or sharing results. Defaults to True when using CLI & False when used in Python.\n\n\nsave_frames\nbool\nFalse\nWhen processing videos, saves individual frames as images. Useful for extracting specific frames or for detailed frame-by-frame analysis.\n\n\nsave_txt\nbool\nFalse\nSaves detection results in a text file, following the format [class] [x_center] [y_center] [width] [height] [confidence]. Useful for integration with other analysis tools.\n\n\nsave_conf\nbool\nFalse\nIncludes confidence scores in the saved text files. Enhances the detail available for post-processing and analysis.\n\n\nsave_crop\nbool\nFalse\nSaves cropped images of detections. Useful for dataset augmentation, analysis, or creating focused datasets for specific objects.\n\n\nshow_labels\nbool\nTrue\nDisplays labels for each detection in the visual output. Provides immediate understanding of detected objects.\n\n\nshow_conf\nbool\nTrue\nDisplays the confidence score for each detection alongside the label. Gives insight into the model’s certainty for each detection.\n\n\nshow_boxes\nbool\nTrue\nDraws bounding boxes around detected objects. Essential for visual identification and location of objects in images or video frames.\n\n\nline_width\nNone or int\nNone\nSpecifies the line width of bounding boxes. If None, the line width is automatically adjusted based on the image size. Provides visual customization for clarity."
  },
  {
    "objectID": "raspi/object_detection/cv_yolo.html#exploring-other-computer-vision-applications",
    "href": "raspi/object_detection/cv_yolo.html#exploring-other-computer-vision-applications",
    "title": "Computer Vision Applications with YOLO",
    "section": "Exploring other Computer Vision Applications",
    "text": "Exploring other Computer Vision Applications\nLet’s set up Jupyter Notebook optimized for headless Raspberry Pi camera work and development:\npip install jupyter jupyterlab notebook\njupyter notebook --generate-config\nTo run Jupyter Notebook, run the command (change the IP address for yours):\njupyter notebook --ip=192.168.4.210 --no-browser\nOn the terminal, you can see the local URL address and its Token to open the notebook. Copy and paste it into the Browser.\n\nEnvironment Setup and Dependencies\nimport time\nimport numpy as np\nfrom PIL import Image\nfrom ultralytics import YOLO\nimport matplotlib.pyplot as plt\nHere we have all the necessary libraries, which we installed automatically when we installed Ultralytics.\n\nTime: Performance measurement and benchmarking\nNumPy: Numerical computations and array operations\nPIL (Python Imaging Library): Image loading and manipulation\nUltralytics YOLO: Core YOLO functionality\nMatplotlib: Visualization and plotting results\n\n\n\nModel Configuration and Loading\nmodel_path= \"./models/yolo11n.pt\"\ntask = \"detect\" \nverbose = False\n\nmodel = YOLO(model_path, task, verbose)\n\nModel Selection: YOLOv11n (nano) is chosen for its balance of speed and accuracy\nTask Specification: We will select detect, which in fact is the default for the model. But remember that YOLO supports multiple computer vision tasks, which will be explored later.\nVerbose Control: output model information during model initialization\n\n\n\nPerformance Characteristics\nLet’s open the previous bus image using PIL\nsource = Image.open(\"./images/bus.jpg\")\nAnd run an inference in the source:\nresults = model.predict(source, save=False, imgsz=640, conf=0.5, iou=0.3)\nFrom the inference results info, we can see that the first time an inference is run, the latency is greater.\n# First inference\n0: 640x480 4 persons, 1 bus, 7528.3ms\n\n# Second inference  \n0: 640x480 4 persons, 1 bus, 2822.1ms\nThe dramatic difference between the first inference (7.5s) and subsequent inferences (2.8s) illustrates:\n\nModel Loading Overhead: Initial inference includes model loading time\nOptimization Effects: Subsequent inferences benefit from cached optimizations\n\n\n\nResults Object Structure\nLet’s explore the YOLO’s output structure:\nresult = results[0]\n# - boxes, keypoints, masks, names\n# - orig_img, orig_shape, path\n# - speed metrics\n\n\nBounding Box Analysis\nresult.boxes.cls  # Class IDs: tensor([5., 0., 0., 0., 0.])\nresult.boxes.conf # Confidence scores\nresult.boxes.xyxy # Bounding box coordinates\n\nCoordinate Systems: On Result.boxes, we can get different bounding box formats (xyxy, xywh, normalized):\n\nxywh: Tensor with bounding box coordinates in center_x, center_y, width, height format, in pixels.\nxywhn: Normalized center_x, center_y, width, height, scaled to the image dimensions, values in .\nxyxy: Tensor of boxes as x1, y1, x2, y2 in pixels, representing the top-left and bottom-right corners.\nxyxyn: Normalized x1, y1, x2, y2, scaled by image width and height, values in .\n\n\n\n\n8. Visualization and Customization\nThe Ultralytics plot() can be customized to show as the detection result, for example, only the bounding boxes:\nim_bgr = result.plot(boxes=True, labels=False, conf=False)\nimg = Image.fromarray(im_bgr[..., ::-1])\n\nplt.figure(figsize=(6, 6))\nplt.imshow(img)\n#plt.axis('off')  # This turns off the axis numbers\nplt.title(\"YOLO Result\")\nplt.show()\n\nCustomization Options:\nThe plot() method in Ultralytics YOLO Results object accepts several arguments to control what is visualized on the image, including boxes, masks, keypoints, confidences, labels, and more. Common Arguments for plot()\n\nboxes (bool): Show/hide bounding boxes. Default is True.\nconf (bool): Show/hide confidence scores. Default is True.\nlabels (bool): Show/hide class labels. Default is True.\nmasks (bool): Show/hide segmentation masks (when available, e.g. in segment tasks).\nkpt_line (bool): Draw lines connecting pose keypoints (skeleton diagram). Default is True in pose tasks.\nline_width (int): Set annotation line thickness.\nfont_size (int): Set font size for text annotations.\nshow (bool): If True, immediately display the image (interactive environments)."
  },
  {
    "objectID": "raspi/object_detection/cv_yolo.html#exploring-other-computer-vision-tasks",
    "href": "raspi/object_detection/cv_yolo.html#exploring-other-computer-vision-tasks",
    "title": "Computer Vision Applications with YOLO",
    "section": "Exploring Other Computer Vision Tasks",
    "text": "Exploring Other Computer Vision Tasks\n\nImage Classification\nAs explored in previous chapters, the output of an image classifier is a single class label and a confidence score. Image classification is useful when we need to know only what class an image belongs to and don’t need to know where objects of that class are located or what their exact shape is.\nmodel_path= \"./models/yolo11n-cls.pt\"\ntask = \"clasification\"\n\nmodel = YOLO(model_path, task, verbose)\nNote that a specific variation of the model, for image classification, will be downloaded. Now, let’s do an inference, using the same bus image:\nresults = model.predict(source, save=False)\nresult = results[0]\nThe model inference will return:\n0: 224x224 minibus 0.57, police_van 0.34, trolleybus 0.04, recreational_vehicle 0.01, streetcar 0.01, 3355.1ms\n\nSpeed: 5233.9ms preprocess, 3355.1ms inference, 28.2ms postprocess per image at shape (1, 3, 224, 224)\nWe can check the top5 inference results using Python:\nclasses = result.probs.top5\nclasses\nAnd we will get:\n[654, 734, 874, 757, 829]\nWhat is related to:\nfor id in classes:\n    print(result.names[id])\nminibus\npolice_van\ntrolleybus\nrecreational_vehicle\nstreetcar\nAnd the top 5 probabilities:\nprobs = result.probs.top5conf.tolist()\nprobs\n[0.5710113048553467,\n 0.33745330572128296,\n 0.04209813103079796,\n 0.014150412753224373,\n 0.005880324635654688]\nThe Top 1 result can be extract directly from the result:\nprint(result.names[result.probs.top1], \n      round(result.probs.top1conf.tolist(), 2))\nminibus 0.57\n\n\nInstance Segmentation\nmodel_path= \"./models/yolo11n-seg.pt\"\ntask = \"segment\"\n\nmodel = YOLO(model_path, task, verbose)\nNote that a specific variation of the model, for instance segmentation, will be downloaded. Now, lt’s use another image for testing:\nsource = Image.open(\"./images/beatles.jpg\")\nDisplay the image\nplt.figure(figsize=(6, 6))\nplt.imshow(source)\n#plt.axis('off')  # This turns off the axis numbers\nplt.title(\"Original Image\")\nplt.show()\n\nAnd run the inference:\nresults = model.predict(source, save=False)\nresult = results[0]\nDisplay the result:\nim_bgr = result.plot(boxes=False, conf=False, masks=True)\nimg = Image.fromarray(im_bgr[..., ::-1])\nplt.figure(figsize=(6, 6))\nplt.imshow(img)\nplt.axis('off')  # This turns off the axis numbers\nplt.title(\"YOLO Segmentation Result\")\nplt.show()\n\n\nPose Estimation\nDownload the model:\nmodel_path= \"./models/yolo11n-pose.pt\"\ntask = \"pose\"\n\nmodel = YOLO(model_path, task, verbose)\nRunning the Inference on the beatles image:\nsource = Image.open(\"./images/beatles.jpg\")\nresults = model.predict(source, save=False)\nresult = results[0]\nShowing the human pose keypoint detection and skeleton visualization.\nim_bgr = result.plot(boxes=False, conf=False, kpt_line=True)\nimg = Image.fromarray(im_bgr[..., ::-1])\nplt.figure(figsize=(6, 6))\nplt.imshow(img)\nplt.axis('off')  # This turns off the axis numbers\nplt.title(\"YOLO Pose Estimation Result\")\nplt.show()"
  },
  {
    "objectID": "raspi/object_detection/cv_yolo.html#training-yolo-on-a-customized-dataset",
    "href": "raspi/object_detection/cv_yolo.html#training-yolo-on-a-customized-dataset",
    "title": "Computer Vision Applications with YOLO",
    "section": "Training YOLO on a Customized Dataset",
    "text": "Training YOLO on a Customized Dataset\n\nObject Detection Project\nWe will now develop a customized object detection project from the data collected and labelled with Roboflow. The training and deployment will be done in Python using a CoLab and Ultralytics functions.\n\n\nWe will use with YOLO, the same dataset previously used to train the SSD-MobileNet V2 and FOMO models.\n\nAs a reminder, we are assuming we are in an industrial facility that must sort and count wheels and special boxes.\n\nEach image can have three classes:\n\nBackground (no objects)\nBox\nWheel\n\n\n\nThe Dataset\nReturn to our “Boxe versus Wheel” dataset, labeled on Roboflow. On the Download Dataset, instead of Download a zip to computer option done for training on Edge Impulse Studio, we will opt for Show download code. This option will open a pop-up window with a code snippet that should be pasted into our training notebook.\n\nFor training, let’s choose one model (let’s say YOLOv8) and adapt one of the publicly available examples from Ultralytics, then run it on Google Colab. Below, you can find my adaptation:\n\nYOLOv8 Box versus Wheel Dataset Training [Open In Colab]\n\n\nCritical points on the Notebook:\n\nRun it with GPU (the NVidia T4 is free)\nInstall Ultralytics using PIP.\n\nNow, you can import the YOLO and upload your dataset to the CoLab, pasting the Download code that we get from Roboflow. Note that our dataset will be mounted under /content/datasets/:\n\n\n\nIt is essential to verify and change the file data.yaml with the correct path for the images (copy the path on each images folder).\n\nnames:\n- box\n- wheel\nnc: 2\nroboflow:\n  license: CC BY 4.0\n  project: box-versus-wheel-auto-dataset\n  url: https://universe.roboflow.com/marcelo-rovai-riila/box-versus-wheel-auto-dataset/dataset/5\n  version: 5\n  workspace: marcelo-rovai-riila\ntest: /content/datasets/Box-versus-Wheel-auto-dataset-5/test/images\ntrain: /content/datasets/Box-versus-Wheel-auto-dataset-5/train/images\nval: /content/datasets/Box-versus-Wheel-auto-dataset-5/valid/images\n\nDefine the main hyperparameters that you want to change from default, for example:\nMODEL = 'yolov8n.pt'\nIMG_SIZE = 640\nEPOCHS = 25 # For a final project, you should consider at least 100 epochs \nRun the training (using CLI):\n!yolo task=detect mode=train model={MODEL} data={dataset.location}/data.yaml epochs={EPOCHS} imgsz={IMG_SIZE} plots=True \n\n\n​ The model took a few minutes to be trained and has an excellent result (mAP50 of 0.995). At the end of the training, all results are saved in the folder listed, for example: /runs/detect/train/. There, you can find, for example, the confusion matrix.\n\n\nNote that the trained model (best.pt) is saved in the folder /runs/detect/train/weights/. Now, you should validate the trained model with the valid/images.\n\n!yolo task=detect mode=val model={HOME}/runs/detect/train/weights/best.pt data={dataset.location}/data.yaml\n​ The results were similar to training.\n\nNow, we should perform inference on the images left aside for testing\n\n!yolo task=detect mode=predict model={HOME}/runs/detect/train/weights/best.pt conf=0.25 source={dataset.location}/test/images save=True\nThe inference results are saved in the folder runs/detect/predict. Let’s see some of them:\n\n\nIt is advised to export the train, validation, and test results for a Drive at Google. To do so, we should mount the drive.\nfrom google.colab import drive\ndrive.mount('/content/gdrive')\nand copy the content of /runs folder to a folder that you should create in your Drive, for example:\n!scp -r /content/runs '/content/gdrive/MyDrive/10_UNIFEI/Box_vs_Wheel_Project'\n\n\n\n\nInference with the trained model, using the Raspi\nDownload the trained model /runs/detect/train/weights/best.pt to your computer. Using the FileZilla FTP, let’s transfer the best.pt to the Raspi models folder (before the transfer, you may change the model name, for example, box_wheel_320_yolo.pt).\nUsing the FileZilla FTP, let’s transfer a few images from the test dataset to .\\YOLO\\images:\nLet’s return to the YOLO folder and use the Python Interpreter:\ncd ..\npython\nWe will import the YOLO library and define the model to use::\n&gt;&gt;&gt; from ultralytics import YOLO\n&gt;&gt;&gt; model = YOLO('./models/box_wheel_320_yolo.pt')\nNow, let’s define an image and call the inference (we will save the image result this time to external verification):\n&gt;&gt;&gt; img = './images/1_box_1_wheel.jpg'\n&gt;&gt;&gt; result = model.predict(img, save=True, imgsz=320, conf=0.5, iou=0.3)\nLet’s repeat for several images. The inference result is saved on the variable result, and the processed image on runs/detect/predict8\n\nUsing FileZilla FTP, we can send the inference result to our Desktop for verification:\n\nWe can see that the inference result is excellent! The model was trained based on the smaller base model of the YOLOv8 family (YOLOv8n). The issue is the latency, around 1 second (or 1 FPS on the Raspi-Zero). We can reduce this latency and convert the model to TFLite or NCNN.\n\nThe model trained with YOLO11 has a latency of around 800 ms, similar to the result of v8 with ncnn.\n\nIn the last section of the notebook, we can find inferences made with the trained YOLO11n model on a Raspberry 5, which took around 400ms:\n\nThe same model, when exported to NCNN, took around 80 ms."
  },
  {
    "objectID": "raspi/object_detection/cv_yolo.html#conclusion",
    "href": "raspi/object_detection/cv_yolo.html#conclusion",
    "title": "Computer Vision Applications with YOLO",
    "section": "Conclusion",
    "text": "Conclusion\nThis chapter has explored the YOLO model and the implementation of a custom object detector on a Raspberry Pi, demonstrating the power and potential of running advanced computer vision tasks on resource-constrained hardware. We’ve covered several vital aspects:\n\nModel Comparison: We examined different object detection models, including SSD-MobileNet, FOMO, and YOLO, comparing their performance and trade-offs on edge devices.\nTraining and Deployment: Using a custom dataset of boxes and wheels (labeled on Roboflow), we walked through the process of training models with Ultralytics and deploying them on a Raspberry Pi.\nOptimization Techniques: To improve inference speed on edge devices, we explored various optimization methods, such as format conversion (e.g., to NCNN).\nPerformance Considerations: Throughout the lab, we discussed the balance between model accuracy and inference speed, a critical consideration for edge AI applications.\n\nAS discussed before, the ability to perform object detection on edge devices opens up numerous possibilities across various domains, including precision agriculture, industrial automation, quality control, smart home applications, and environmental monitoring. By processing data locally, these systems can offer reduced latency, improved privacy, and operation in environments with limited connectivity.\nLooking ahead, potential areas for further exploration include: - Implementing multi-model pipelines for more complex tasks - Exploring hardware acceleration options for Raspberry Pi - Integrating object detection with other sensors for more comprehensive edge AI systems - Developing edge-to-cloud solutions that leverage both local processing and cloud resources\nObject detection on edge devices can create intelligent, responsive systems that bring the power of AI directly into the physical world, opening up new frontiers in how we interact with and understand our environment."
  },
  {
    "objectID": "raspi/object_detection/cv_yolo.html#resources",
    "href": "raspi/object_detection/cv_yolo.html#resources",
    "title": "Computer Vision Applications with YOLO",
    "section": "Resources",
    "text": "Resources\n\nDataset (“Box versus Wheel”)\nYOLOv8 Box versus Wheel Dataset Training on CoLab\nYOLO11 Box versus Wheel Dataset Training on CoLab\nModel Predictions with Ultralytics YOLO\nPython Scripts\nModels"
  },
  {
    "objectID": "raspi/counting_objects_yolo/counting_objects_yolo.html#introduction",
    "href": "raspi/counting_objects_yolo/counting_objects_yolo.html#introduction",
    "title": "Counting objects with YOLO",
    "section": "Introduction",
    "text": "Introduction\nAt the Federal University of Itajuba in Brazil, with the master’s student José Anderson Reis and Professor José Alberto Ferreira Filho, we are exploring a project that delves into the intersection of technology and nature. This tutorial will review our first steps and share our observations on deploying YOLOv8, a cutting-edge machine learning model, on the compact and efficient Raspberry Pi Zero 2W (Raspi-Zero). We aim to estimate the number of bees entering and exiting their hive—a task crucial for beekeeping and ecological studies.\nWhy is this important? Bee populations are vital indicators of environmental health, and their monitoring can provide essential data for ecological research and conservation efforts. However, manual counting is labor-intensive and prone to errors. By leveraging the power of embedded machine learning, or tinyML, we automate this process, enhancing accuracy and efficiency.\n\n\n\nimg\n\n\nThis tutorial will cover setting up the Raspberry Pi, integrating a camera module, optimizing and deploying YOLOv8 for real-time image processing, and analyzing the data gathered."
  },
  {
    "objectID": "raspi/counting_objects_yolo/counting_objects_yolo.html#estimating-the-number-of-bees",
    "href": "raspi/counting_objects_yolo/counting_objects_yolo.html#estimating-the-number-of-bees",
    "title": "Counting objects with YOLO",
    "section": "Estimating the number of Bees",
    "text": "Estimating the number of Bees\nFor our project at the university, we are preparing to collect a dataset of bees at the entrance of a beehive using the same camera connected to the Raspberry Pi. The images should be collected every 10 seconds. With the Arducam OV5647, the horizontal Field of View (FoV) is 53.5o, which means that a camera positioned at the top of a standard Hive (46 cm) will capture all of its entrance (about 47 cm).\n\n\nDataset\nThe dataset collection is the most critical phase of the project and should take several weeks or months. For this tutorial, we will use a public dataset: “Sledevic, Tomyslav (2023), “[Labeled dataset for bee detection and direction estimation on beehive landing boards,” Mendeley Data, V5, doi: 10.17632/8gb9r2yhfc.5”\nThe original dataset contains 6,762 images (1920 x 1080), and around 8% (518) of them have no bees (only background). This is very important in Object Detection, where we should keep around 10% of the dataset with only background (no objects to be detected).\nThe images contain from zero to up to 61 bees:\n\nWe downloaded the dataset (images and annotations) and uploaded it to Roboflow. There, you should create a free account and start a new project, for example, (“Bees_on_Hive_landing_boards”):\n\n\nWe will not enter details about the Roboflow process once many tutorials are available.\n\nOnce the project is created and the dataset is uploaded, you should review the annotations using the “Auto-Label” Tool. Note that all images with only a background should be saved w/o any annotations. At this step, you can also add additional images.\n\nOnce all images are annotated, you should split them into training, validation, and testing.\n\n\n\nPre-Processing\nThe last step with the dataset is preprocessing to generate a final version for training. The Yolov8 model can be trained with 640 x 640 pixels (RGB) images. Let’s resize all images and generate augmented versions of each image (augmentation) to create new training examples from which our model can learn.\nFor augmentation, we will rotate the images (+/-15o) and vary the brightness and exposure.\n\nThis will create a final dataset of 16,228 images.\n\nNow, you should export the annotateddataset in a YOLOv8 format. You can download a zipped version of the dataset to your desktop or get a downloaded code to be used with a Jupyter Notebook:\n\nAnd that is it! We are prepared to start our training using Google Colab.\n\nThe pre-processed dataset can be found at the Roboflow site.\n\n\n\nTraining YOLOv8 on a Customized Dataset\nFor training, let’s adapt one of the public examples available from Ultralitytics and run it on Google Colab:\n\nyolov8_bees_on_hive_landing_board.ipynb [Open In Colab]\n\n\nCritical points on the Notebook:\n\nRun it with GPU (the NVidia T4 is free)\nInstall Ultralytics using PIP.\n\nNow, you can import the YOLO and upload your dataset to the CoLab, pasting the Download code that you get from Roboflow. Note that your dataset will be mounted under /content/datasets/:\n\n\n\nIt is important to verify and change, if needed, the file data.yaml with the correct path for the images:\n\nnames:\n- bee\nnc: 1\nroboflow:\n  license: CC BY 4.0\n  project: bees_on_hive_landing_boards\n  url: https://universe.roboflow.com/marcelo-rovai-riila/bees_on_hive_landing_boards/dataset/1\n  version: 1\n  workspace: marcelo-rovai-riila\ntest: /content/datasets/Bees_on_Hive_landing_boards-1test/images\ntrain: /content/datasets/Bees_on_Hive_landing_boards-1/train/images\nval: /content/datasets/Bees_on_Hive_landing_boards-1/valid/images\n\nDefine the main hyperparameters that you want to change from default, for example:\nMODEL = 'yolov8n.pt'\nIMG_SIZE = 640\nEPOCHS = 25 # For a final project, you should consider at least 100 epochs \nRun the training (using CLI):\n!yolo task=detect mode=train model={MODEL} data={dataset.location}/data.yaml epochs={EPOCHS} imgsz={IMG_SIZE} plots=True \n\n\n​ The model took 2.7 hours to train and has an excellent result (mAP50 of 0.984). At the end of the training, all results are saved in the folder listed, for example: /runs/detect/train3/. There, you can find, for example, the confusion matrix and the metrics curves per epoch.\n\n\nNote that the trained model (best.pt) is saved in the folder /runs/detect/train3/weights/. Now, you should validade the trained model with the valid/images.\n\n!yolo task=detect mode=val model={HOME}/runs/detect/train3/weights/best.pt data={dataset.location}/data.yaml\n​ The results were similar to training.\n\nNow, we should perform inference on the images left aside for testing\n\n!yolo task=detect mode=predict model={HOME}/runs/detect/train3/weights/best.pt conf=0.25 source={dataset.location}/test/images save=True\nThe inference results are saved in the folder runs/detect/predict. Let’s see some of them:\n\nWe can also perform inference with a completely new and complex image from another beehive with a different background (the beehive of Professor Maurilio of our University). The results were great (but not perfect and with a lower confidence score). The model found 41 bees.\n\n\nThe last thing to do is export the train, validation, and test results for your Drive at Google. To do so, you should mount your drive.\nfrom google.colab import drive\ndrive.mount('/content/gdrive')\nand copy the content of /runs folder to a folder that you should create in your Drive, for example:\n!scp -r /content/runs '/content/gdrive/MyDrive/10_UNIFEI/Bee_Project/YOLO/bees_on_hive_landing'"
  },
  {
    "objectID": "raspi/counting_objects_yolo/counting_objects_yolo.html#inference-with-the-trained-model-using-the-rasp-zero",
    "href": "raspi/counting_objects_yolo/counting_objects_yolo.html#inference-with-the-trained-model-using-the-rasp-zero",
    "title": "Counting objects with YOLO",
    "section": "Inference with the trained model, using the Rasp-Zero",
    "text": "Inference with the trained model, using the Rasp-Zero\nUsing the FileZilla FTP, let’s transfer the best.pt to our Rasp-Zero (before the transfer, you may change the model name, for example, bee_landing_640_best.pt).\nThe first thing to do is convert the model to an NCNN format:\nyolo export model=bee_landing_640_best.pt format=ncnn \nAs a result, a new converted model, bee_landing_640_best_ncnn_model is created in the same directory.\nLet’s create a folder to receive some test images (under Documents/YOLO/:\nmkdir test_images\nUsing the FileZilla FTP, let’s transfer a few images from the test dataset to our Rasp-Zero:\n\nLet’s use the Python Interpreter:\npython\nAs before, we will import the YOLO library and define our converted model to detect bees:\nfrom ultralytics import YOLO\nmodel = YOLO('bee_landing_640_best_ncnn_model')\nNow, let’s define an image and call the inference (we will save the image result this time to external verification):\nimg = 'test_images/15_bees.jpg'\nresult = model.predict(img, save=True, imgsz=640, conf=0.2, iou=0.3)\nThe inference result is saved on the variable result, and the processed image on runs/detect/predict9\n\nUsing FileZilla FTP, we can send the inference result to our Desktop for verification:\n\nlet’s go over the other images, analyzing the number of objects (bees) found:\n\nDepending on the confidence level, we may see some false positives or negatives. But in general, with a model trained on a smaller base model in the YOLOv8 family (YOLOv8n) and converted to NCNN, the results are pretty good, running on an Edge device such as the Rasp-Zero. Also, note that the inference latency is around 730ms.\nFor example, by running the inference on Maurilio-bee.jpeg, we can find 40 bees. During the test phase on Colab, 41 bees were found (we only missed one here)."
  },
  {
    "objectID": "raspi/counting_objects_yolo/counting_objects_yolo.html#considerations-about-the-post-processing",
    "href": "raspi/counting_objects_yolo/counting_objects_yolo.html#considerations-about-the-post-processing",
    "title": "Counting objects with YOLO",
    "section": "Considerations about the Post-Processing",
    "text": "Considerations about the Post-Processing\nOur final project should be very simple in terms of code. We will use the camera to capture an image every 10 seconds. As we did in the previous section, the captured image should serve as the input to the trained and converted model. We should count the bees in each image and store the counts in a database (e.g., timestamp: number of bees).\nWe can do it with a single Python script, or use a Linux system timer, such as cron, to periodically capture images every 10 seconds, and have a separate Python script process them as they are saved. This method can be particularly efficient at managing system resources and is more robust against potential delays in image processing.\n\nSetting Up the Image Capture with cron\nFirst, we should set up a cron job to use the rpicam-jpeg command to capture an image every 10 seconds.\n\nEdit the crontab:\n\nOpen the terminal and type crontab -e to edit the cron jobs.\ncron normally doesn’t support sub-minute intervals directly, so we should use a workaround, such as a loop or a file watcher.\n\nCreate a Bash Script (capture.sh):\n\nImage Capture: This bash script captures images every 10 seconds using rpicam-jpeg, a command in the raspijpeg tool. This command lets us control the camera and capture JPEG images directly from the command line. This is especially useful because we are looking for a lightweight, straightforward method to capture images without requiring additional libraries like Picamera or external software. The script also saves the captured image with a timestamp.\n\n#!/bin/bash\n# Script to capture an image every 10 seconds\n\nwhile true\ndo\n  DATE=$(date +\"%Y-%m-%d_%H%M%S\")\n  rpicam-jpeg --output test_images/$DATE.jpg --width 640 --height 640\n  sleep 10\ndone\n\nWe should make the script executable with chmod +x capture.sh.\nThe script must start at boot or use a @reboot entry in cron to start it automatically.\n\n\n\n\nSetting Up the Python Script for Inference\nImage Processing: The Python script continuously monitors the designated directory for new images, processes each new image using the YOLOv8 model, updates the database with the count of detected bees, and optionally deletes the image to conserve disk space.\nDatabase Updates: The results, along with the timestamps, are saved in an SQLite database. For that, a simple option is to use sqlite3.\nIn short, we need to write a script that continuously monitors the directory for new images, processes them using a YOLO model, and then saves the results to a SQLite database. Here’s how we can create and make the script executable:\n#!/usr/bin/env python3\nimport os\nimport time\nimport sqlite3\nfrom datetime import datetime\nfrom ultralytics import YOLO\n\n# Constants and paths\nIMAGES_DIR = 'test_images/'\nMODEL_PATH = 'bee_landing_640_best_ncnn_model'\nDB_PATH = 'bee_count.db'\n\ndef setup_database():\n    \"\"\" \n      Establishes a database connection and creates the table \n      if it doesn't exist. \n    \"\"\"\n    conn = sqlite3.connect(DB_PATH)\n    cursor = conn.cursor()\n    cursor.execute('''\n        CREATE TABLE IF NOT EXISTS bee_counts\n        (timestamp TEXT, count INTEGER)\n    ''')\n    conn.commit()\n    return conn\n\ndef process_image(image_path, model, conn):\n    \"\"\" \n      Processes an image to detect objects and logs \n      the count to the database. \n    \"\"\"\n    result = model.predict(image_path, save=False, imgsz=640, conf=0.2, iou=0.3, verbose=False)\n    num_bees = len(result[0].boxes.cls) \n    timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n    cursor = conn.cursor()\n    cursor.execute(\"INSERT INTO bee_counts (timestamp, count) VALUES (?, ?)\", \n                   (timestamp, num_bees)\n                  )\n    conn.commit()\n    print(f'Processed {image_path}: Number of bees detected = {num_bees}')\n\ndef monitor_directory(model, conn):\n    \"\"\" \n      Monitors the directory for new images and processes \n      them as they appear. \n    \"\"\"\n    processed_files = set()\n    while True:\n        try:\n            files = set(os.listdir(IMAGES_DIR))\n            new_files = files - processed_files\n            for file in new_files:\n                if file.endswith('.jpg'):\n                    full_path = os.path.join(IMAGES_DIR, file)\n                    process_image(full_path, model, conn)\n                    processed_files.add(file)\n            time.sleep(1)  # Check every second\n        except KeyboardInterrupt:\n            print(\"Stopping...\")\n            break\n\ndef main():\n    conn = setup_database()\n    model = YOLO(MODEL_PATH)\n    monitor_directory(model, conn)\n    conn.close()\n\nif __name__ == \"__main__\":\n    main()\nThe Python script must be executable, for that:\n\nSave the script: For example, as process_images.py.\nChange file permissions to make it executable:\nchmod +x process_images.py\nRun the script directly from the command line:\n./process_images.py\n\nWe should consider keeping the script running even after closing the terminal; for that, we can use nohup or screen:\nnohup ./process_images.py &\nor\nscreen -S bee_monitor\n./process_images.py\nNote that we capture images with their own timestamps and log a separate timestamp when the inference results are saved to the database. This approach can be beneficial for the following reasons:\n\nAccuracy in Data Logging:\n\nCapture Timestamp: The timestamp associated with each image capture represents the exact moment the image was taken. This is crucial for applications where precise timing of events (like bee activity) is important for analysis.\nInference Timestamp: This timestamp indicates when the image was processed and the results were recorded in the database. This can differ from the capture time due to processing delays or if the image processing is batched or queued.\n\nPerformance Monitoring:\n\nSeparate timestamps enable us to monitor the performance and efficiency of your image processing pipeline. We can measure the delay between image capture and result logging, which helps optimize the system for real-time processing needs.\n\nTroubleshooting and Audit:\n\nSeparate timestamps provide a better audit trail and troubleshooting data. If there are issues with the image processing or data recording, having distinct timestamps can help isolate whether delays or problems occurred during capture, processing, or logging.\n\n\n\n\nScript For Reading the SQLite Database\nHere is an example of a code to retrieve the data from the database:\n#!/usr/bin/env python3\nimport sqlite3\n\ndef main():\n    db_path = 'bee_count.db'\n    conn = sqlite3.connect(db_path)\n    cursor = conn.cursor()\n    query = \"SELECT * FROM bee_counts\"\n    cursor.execute(query)\n    data = cursor.fetchall()\n    for row in data:\n        print(f\"Timestamp: {row[0]}, Number of bees: {row[1]}\")\n    conn.close()\n\nif __name__ == \"__main__\":\n    main()\n\n\nAdding Environment data\nBesides bee counting, environmental data, such as temperature and humidity, are essential for monitoring the bee-have health. Using a Rasp-Zero, it is straightforward to add a digital sensor such as the DHT-22 to get this data.\n\nEnvironmental data will be part of our final project. If you want to know more about connecting sensors to a Raspberry Pi and, even more, how to save the data to a local database and send it to the web, follow this tutorial: From Data to Graph: A Web Journey With Flask and SQLite."
  },
  {
    "objectID": "raspi/counting_objects_yolo/counting_objects_yolo.html#conclusion",
    "href": "raspi/counting_objects_yolo/counting_objects_yolo.html#conclusion",
    "title": "Counting objects with YOLO",
    "section": "Conclusion",
    "text": "Conclusion\nIn this tutorial, we have thoroughly explored integrating the YOLOv8 model with a Raspberry Pi Zero 2W to address the practical, pressing task of counting (or, better, “estimating”) bees at a beehive entrance. Our project underscores the robust capability of embedding advanced machine learning technologies within compact edge computing devices, highlighting their potential impact on environmental monitoring and ecological studies.\nThis tutorial provides a step-by-step guide to deploying the YOLOv8 model in practice. We demonstrate a tangible real-world application by optimizing it for edge computing, improving efficiency and processing speed (using the NCNN format). This not only serves as a functional solution but also as an instructional tool for similar projects.\nThe technical insights and methodologies shared in this tutorial are the basis for the complete work to be developed at our university in the future. We envision further development, such as integrating additional environmental sensing capabilities and refining the model’s accuracy and processing efficiency. Implementing alternative energy solutions, such as the proposed solar power setup, will enhance the project’s sustainability and applicability in remote or underserved locations."
  },
  {
    "objectID": "raspi/counting_objects_yolo/counting_objects_yolo.html#resources",
    "href": "raspi/counting_objects_yolo/counting_objects_yolo.html#resources",
    "title": "Counting objects with YOLO",
    "section": "Resources",
    "text": "Resources\nThe Dataset paper, Notebooks, and PDF version are in the Project repository."
  },
  {
    "objectID": "raspi/executorch/executorch.html#introduction",
    "href": "raspi/executorch/executorch.html#introduction",
    "title": "Image Classification with EXECUTORCH",
    "section": "Introduction",
    "text": "Introduction\nImage classification is a fundamental computer vision task that powers countless real-world applications—from quality control in manufacturing to wildlife monitoring, medical diagnostics, and smart home devices. In the edge AI landscape, the ability to run these models efficiently on resource-constrained devices has become increasingly critical for privacy-preserving, low-latency applications.\nIn the chapter Image Classification Fundamentals, we explored image classification with TensorFlow Lite and demonstrated how to deploy efficient neural networks on the Raspberry Pi. That tutorial covered the complete workflow from model conversion to real-time camera inference, achieving excellent results with the MobileNet V2 architecture and a real dataset (CIFAR-10).\nThis chapter takes a parallel approach using PyTorch EXECUTORCH—Meta’s modern solution for edge deployment. Rather than replacing our TFLite knowledge, this chapter expands your edge AI toolkit, giving us the flexibility to choose the right framework for our specific needs.\n\nWhat is EXECUTORCH?\nEXECUTORCH is PyTorch’s official solution for deploying machine learning models on edge devices, from smartphones and embedded systems to microcontrollers and IoT devices. Released in 2023, it represents Meta’s commitment to bringing the entire PyTorch ecosystem to edge computing.\nCore Capabilities:\n\nNative PyTorch Integration: Seamless workflow from model training to edge deployment without switching frameworks\nEfficient Execution: Optimized runtime designed specifically for resource-constrained devices\nBroad Portability: Runs on diverse hardware platforms (ARM, x86, specialized accelerators)\nFlexible Backend System: Extensible delegate architecture for hardware-specific optimizations\nQuantization Support: Built-in integration with PyTorch’s quantization tools for model compression\n\n\n\nWhy EXECUTORCH for Edge AI?\nEXECUTORCH offers compelling advantages for edge deployment:\n1. Unified Workflow If we are training models in PyTorch, EXECUTORCH provides a natural deployment path without framework switching. This eliminates conversion errors and maintains model fidelity from training to deployment.\n2. Modern Architecture Built from the ground up for edge computing with contemporary best practices, EXECUTORCH incorporates lessons learned from previous mobile deployment frameworks.\n3. Comprehensive Quantization Native support for various quantization techniques (dynamic, static, quantization-aware training) enables significant model size reduction with minimal accuracy loss.\n4. Extensible Backend System The delegate system allows seamless integration with hardware accelerators (XNNPACK for CPU optimization, QNN for Qualcomm chips, CoreML for Apple devices, and more).\n5. Active Development Backed by Meta with rapid iteration and strong community support, ensuring the framework evolves with edge AI needs.\n6. Growing Model Zoo Access to pretrained models specifically optimized for edge deployment, with consistent performance across devices.\n\n\nFramework Comparison: EXECUTORCH vs TensorFlow Lite\nUnderstanding when to choose each framework is crucial for effective edge deployment:\n\n\n\n\n\n\n\n\nFeature\nEXECUTORCH\nTensorFlow Lite\n\n\n\n\nTraining Framework\nPyTorch\nTensorFlow/Keras\n\n\nMaturity\nNewer (2023+)\nMature (2017+)\n\n\nModel Format\n.pte\n.tflite (.lite)\n\n\nQuantization\nPyTorch native quantization\nTF quantization-aware training\n\n\nBackend Acceleration\nDelegate system (XNNPACK, QNN, CoreML)\nDelegates (GPU, NNAPI, Hexagon)\n\n\nCommunity\nRapidly growing\nLarge, established\n\n\nHardware Support\nExpanding quickly\nExtensive, mature\n\n\nLearning Curve\nEasier for PyTorch users\nEasier for TF/Keras users\n\n\nDocumentation\nGrowing, modern\nComprehensive, mature\n\n\nIndustry Adoption\nIncreasing in research\nWidespread in production\n\n\n\nThe Reality: Both Are Excellent Choices\nIn practice, both frameworks achieve similar goals with different philosophies. Our choice often comes down to:\n\nOur training framework preference\nTeam expertise and existing infrastructure\nSpecific hardware requirements\nProject timeline and maturity needs\n\nThis chapter demonstrates that transitioning between frameworks is straightforward, allowing us to make informed decisions based on project needs rather than framework limitations."
  },
  {
    "objectID": "raspi/executorch/executorch.html#setting-up-the-environment",
    "href": "raspi/executorch/executorch.html#setting-up-the-environment",
    "title": "Image Classification with EXECUTORCH",
    "section": "Setting Up the Environment",
    "text": "Setting Up the Environment\n\nUpdating the Raspberry Pi\nFirst, ensure that the Raspberry Pi is up to date:\nsudo apt update\nsudo apt upgrade -y\nsudo reboot  # Reboot to ensure all updates take effect\n\n\nInstalling Required System-Level Libraries\nInstall Python tools, camera libraries, and build dependencies for PyTorch:\nsudo apt install -y python3-pip python3-venv python3-picamera2\nsudo apt install -y libcamera-dev libcamera-tools libcamera-apps\nsudo apt install -y libopenblas-dev libjpeg-dev zlib1g-dev libpng-dev\nPicamera2 Installation Test\nWe can test the camera with:\nrpicam-hello --list-cameras\n\n\nWe should see that the OV5647 cam is installed.\n\nNow, let’s create a test script to verify everything works:\ncamera_capture.py\nimport numpy as np\nfrom picamera2 import Picamera2\nimport time\n\nprint(f\"NumPy version: {np.__version__}\")\n\n# Initialize camera\npicam2 = Picamera2()\n\nconfig = picam2.create_preview_configuration(main={\"size\":(640,480)}) \npicam2.configure(config)\npicam2.start()\n\n# Wait for camera to warm up\ntime.sleep(2)\n\nprint(\"Camera working in the system!\")\n\n# Capture image\npicam2.capture_file(\"camera_capture.jpg\")\nprint(\"Image captured: cam_test.jpg\")\n\n# Stop camera\npicam2.stop()\npicam2.close()\n\nA test image should be created in the current directory\n\n\n\nSetting up a Virtual Environment\nFirst, let’s confirm the System Python version:\npython --version\nIf we use the latest Raspberry Pi OS (based on Debian Trixie), it should be:\n3.13.5\nAs of today (January 2026), ExecuTorch officially supports only Python 3.10 to 3.12; Python 3.13.5 is too new and will likely cause compatibility issues. Since Debian Trixie ships with Python 3.13 by default, we’ll need to install a compatible Python version alongside it.\nOne solution is to install Pyenv, so that we can easily manage multiple Python versions for different projects without affecting the system Python.\n\nIf the Raspberry Pi OS is the legacy, the Python version should be 3.11, and it is not necessary to install Pyenv.\n\n\nInstall pyenv Dependencies\nsudo apt update\nsudo apt install -y build-essential libssl-dev zlib1g-dev \\\n    libbz2-dev libreadline-dev libsqlite3-dev curl git \\\n    libncursesw5-dev xz-utils tk-dev libxml2-dev \\\n    libxmlsec1-dev libffi-dev liblzma-dev \\\n    libopenblas-dev libjpeg-dev libpng-dev cmake\n\n\nInstall pyenv\n# Download and install pyenv\ncurl https://pyenv.run | bash\n\n\nConfigure Shell\nAdd pyenv to ~/.bashrc:\ncat &gt;&gt; ~/.bashrc &lt;&lt; 'EOF'\n\n# pyenv configuration\nexport PYENV_ROOT=\"$HOME/.pyenv\"\n[[ -d $PYENV_ROOT/bin ]] && export PATH=\"$PYENV_ROOT/bin:$PATH\"\neval \"$(pyenv init -)\"\nEOF\nReload the shell:\nsource ~/.bashrc\nVerify if pyenv is installed:\npyenv --version\n\n\nInstall Python 3.11 (or 3.12)\n# See available versions\npyenv install --list | grep \" 3.11\"\n\n# Install Python 3.11.14 (latest 3.11 stable)\npyenv install 3.11.14\n\n# Or install Python 3.12.3 if you prefer\n# pyenv install 3.12.12\n\nThis will take a few minutes to compile.\n\n\n\nCreate ExecuTorch Workspace\ncd Documents    \nmkdir EXECUTORCH\ncd EXECUTORCH\n\n# Set Python 3.11.14 for this directory\npyenv local 3.11.14\n\n# Verify\npython --version  # Should show Python 3.11.14\n\n\nCreate Virtual Environment\npython -m venv executorch-venv\nsource executorch-venv/bin/activate\n\n# Verify if we're using the correct Python\nwhich python\npython --version\nTo exit the virtual environment later:\ndeactivate\n\n\n\nInstall Python Packages\n\nEnsure we’re in the virtual environment (venv)\n\npip install --upgrade pip\npip install numpy pillow matplotlib opencv-python\nVerify installation:\npip list | grep -E \"(numpy|pillow|opencv)\""
  },
  {
    "objectID": "raspi/executorch/executorch.html#pytorch-and-executorch-installation",
    "href": "raspi/executorch/executorch.html#pytorch-and-executorch-installation",
    "title": "Image Classification with EXECUTORCH",
    "section": "PyTorch and EXECUTORCH Installation",
    "text": "PyTorch and EXECUTORCH Installation\n\nInstalling PyTorch for Raspberry Pi\nPyTorch provides pre-built wheels for ARM64 architecture (Raspberry Pi 3/4/5).\nFor Raspberry Pi 4/5 (aarch64):\n# Install PyTorch (CPU version for ARM64)\npip install torch torchvision --index-url \\\nhttps://download.pytorch.org/whl/cpu\n\nFor the Raspberry Pi Zero 2 W (32-bit ARM), we may need to build from source or use lighter alternatives, which are not covered here.\n\nVerify PyTorch installation:\npython -c \"import torch; print(f'PyTorch version: \\\n{torch.__version__}')\"\nWe will get, for example, PyTorch version: 2.9.1+cpu\n\n\nInstalling EXECUTORCH Runtime\nEXECUTORCH can be installed via pip:\npip install executorch\nBuilding from Source (Optional - for latest features):\nIf we want the absolute latest features or need to customize:\n# Clone the repository\ngit clone https://github.com/pytorch/executorch.git\ncd executorch\n\n# Install dependencies\n./install_requirements.sh\n\n# Install EXECUTORCH in development mode\npip install -e .\n\n\nVerifying the Setup\nLet’s verify our setup with a test script. Create setup_test.py (for example, using nano):\nimport torch\nimport numpy as np\nfrom PIL import Image\nimport executorch\n\nprint(\"=\" * 50)\nprint(\"SETUP VERIFICATION\")\nprint(\"=\" * 50)\n\n# Check versions\nprint(f\"PyTorch version: {torch.__version__}\")\nprint(f\"NumPy version: {np.__version__}\")\nprint(f\"PIL version: {Image.__version__}\")\nprint(f\"EXECUTORCH available: {executorch is not None}\")\n\n# Test basic PyTorch functionality\nx = torch.randn(3, 224, 224)\nprint(f\"\\nCreated test tensor with shape: {x.shape}\")\n\n# Test PIL\ntest_img = Image.new('RGB', (224, 224), color='red')\nprint(f\"Created test PIL image: {test_img.size}\")\n\nprint(\"\\n✓ Setup verification complete!\")\nprint(\"=\" * 50)\nRun it:\npython setup_test.py\nExpected output (the versions can be different):\n==================================================\nSETUP VERIFICATION\n==================================================\nPyTorch version: 2.9.1+cpu\nNumPy version: 2.2.6\nPIL version: 12.1.0\nEXECUTORCH available: True\n\nCreated test tensor with shape: torch.Size([3, 224, 224])\nCreated test PIL image: (224, 224)\n\n✓ Setup verification complete!\n=================================================="
  },
  {
    "objectID": "raspi/executorch/executorch.html#image-classification-using-mobilenet-v2",
    "href": "raspi/executorch/executorch.html#image-classification-using-mobilenet-v2",
    "title": "Image Classification with EXECUTORCH",
    "section": "Image Classification using MobileNet V2",
    "text": "Image Classification using MobileNet V2\n\nWorking directory:\ncd Documents\ncd EXECUTORCH\nmkdir IMG_CLASS\ncd IMG_CLASS\nmkdir MOBILENET\ncd MOBILENET\nmkdir models images notebooks\n\n\nMaking inference with Torch\nLoad an image from the internet, for example, a cat: \"https://upload.wikimedia.org/wikipedia/commons/3/3a/Cat03.jpg\"\nAnd save it in the images folder as “cat.jpg”:\nwget \"https://upload.wikimedia.org/wikipedia/commons/3/3a/Cat03.jpg\" \\\n     -O ./images/cat.jpg\nNow, let’s create a test program where we should take into consideration:\n\nFirst run - Downloads model & labels (and saves them)\nPreprocessing - MobileNetV2 expects 224x224 images with ImageNet normalization\ntorch.no_grad() -Disables gradient calculation for faster inference\nTiming - Measures only inference time, not preprocessing\nSoftmax - Converts raw outputs to probabilities\nTop-5 - Shows the 5 most likely classes\n\nand save it as img_class_test_torch.py:\nimport torch\nimport torchvision.transforms as transforms\nfrom torchvision import models\nfrom PIL import Image\nimport time\nimport json\nimport urllib.request\nimport os\n\n# Paths\nMODEL_PATH = \"models/mobilenet_v2.pth\"\nLABELS_PATH = \"models/imagenet_labels.json\"\nIMAGE_PATH = \"images/cat.jpg\"\n\n# Download and save ImageNet labels (only first time)\nif not os.path.exists(LABELS_PATH):\n    print(\"Downloading ImageNet labels...\")\n    LABELS_URL = \"https://raw.githubusercontent.com/anishathalye/\\\n    imagenet-simple-labels/master/imagenet-simple-labels.json\"\n    with urllib.request.urlopen(LABELS_URL) as url:\n        labels = json.load(url)\n    \n    # Save labels locally\n    with open(LABELS_PATH, 'w') as f:\n        json.dump(labels, f)\n    print(f\"Labels saved to {LABELS_PATH}\")\nelse:\n    print(\"Loading labels from disk...\")\n    with open(LABELS_PATH, 'r') as f:\n        labels = json.load(f)\n\n# Load or download model\nif not os.path.exists(MODEL_PATH):\n    print(\"Downloading MobileNetV2 model...\")\n    model = models.mobilenet_v2(pretrained=True)\n    model.eval()\n    torch.save(model.state_dict(), MODEL_PATH)\n    print(f\"Model saved to {MODEL_PATH}\")\nelse:\n    print(\"Loading model from disk...\")\n    model = models.mobilenet_v2()\n    model.load_state_dict(torch.load(MODEL_PATH, map_location='cpu'))\n    model.eval()\n\n# Define image preprocessing\npreprocess = transforms.Compose([\n    transforms.Resize(256),\n    transforms.CenterCrop(224),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], \n                         std=[0.229, 0.224, 0.225]),\n])\n\n# Load and preprocess image\nprint(f\"\\nLoading image from {IMAGE_PATH}...\")\nimg = Image.open(IMAGE_PATH)\nimg_tensor = preprocess(img)\nbatch = img_tensor.unsqueeze(0)\n\n# Perform inference with timing\nprint(\"Running inference...\")\nstart_time = time.time()\n\nwith torch.no_grad():\n    output = model(batch)\n    \ninference_time = (time.time() - start_time) * 1000\n\n# Get predictions\nprobabilities = torch.nn.functional.softmax(output[0], dim=0)\ntop5_prob, top5_idx = torch.topk(probabilities, 5)\n\n# Display results\nprint(\"\\n\" + \"=\"*50)\nprint(\"CLASSIFICATION RESULTS\")\nprint(\"=\"*50)\nprint(f\"Inference Time: {inference_time:.2f} ms\\n\")\nprint(\"Top 5 Predictions:\")\nprint(\"-\"*50)\n\nfor i in range(5):\n    idx = top5_idx[i].item()\n    prob = top5_prob[i].item()\n    print(f\"{i+1}. {labels[idx]:20s} - {prob*100:.2f}%\")\n\nprint(\"=\"*50)\nThe result:\nLoading image from images/cat.jpg...\nRunning inference...\n\n==================================================\nCLASSIFICATION RESULTS\n==================================================\nInference Time: 86.12 ms\n\nTop 5 Predictions:\n--------------------------------------------------\n1. tiger cat            - 47.44%\n2. Egyptian Mau         - 37.61%\n3. lynx                 - 6.91%\n4. tabby cat            - 6.22%\n5. plastic bag          - 0.47%\n==================================================\nThe inference was OK, taking 86ms (first time). We can also verify the size of the saved Torch model\nls -lh ./models/mobilenet_v2.pth\nWhich has 14Mb."
  },
  {
    "objectID": "raspi/executorch/executorch.html#exporting-models-to-executorch-format",
    "href": "raspi/executorch/executorch.html#exporting-models-to-executorch-format",
    "title": "Image Classification with EXECUTORCH",
    "section": "Exporting Models to EXECUTORCH Format",
    "text": "Exporting Models to EXECUTORCH Format\nUnlike TensorFlow Lite, where we downloaded pre-converted .tflite models, with EXECUTORCH, we typically export PyTorch models to the .pte (PyTorch EXECUTORCH) format ourselves. This gives us full control over the export process.\n\nUnderstanding the Export Process\nThe EXECUTORCH export process involves several steps:\n\nLoad a PyTorch model (pretrained or custom)\nTrace/script the model (convert to TorchScript)\nExport to EXECUTORCH format (.pte file)\n\nOptional optimization steps:\n\nQuantization (before or during export)\nBackend delegation (XNNPACK, QNN, etc.)\nMemory planning optimization\n\nThe complete ExecuTorch pipeline:\n\nexport() → Captures the model graph\nto_edge() → Converts to Edge dialect\nto_executorch() → Lowers to ExecuTorch format\n.buffer → Gets the binary data to save\n\nPyTorch Model (.pt/.pth)\n          ↓\n    torch.export()         # Export to ExportedProgram\n          ↓\n    to_edge()              # Convert to Edge dialect\n          ↓\n    to_executorch()        # Generate EXECUTORCH program\n          ↓\n   .pte file               # Ready for edge deployment\n\n\nExporting MobileNet V2 to ExecuTorch\nLet’s export a MobileNet V2 model to EXECUTORCH basic format. Creating a Python script as convert_mobv2_executorch.py\nimport torch\nfrom torchvision import models\nfrom executorch.exir import to_edge\nfrom torch.export import export\n\n# Paths\nPYTORCH_MODEL_PATH = \"models/mobilenet_v2.pth\"\nEXECUTORCH_MODEL_PATH = \"models/mobilenet_v2.pte\"\n\nprint(\"Loading PyTorch model...\")\n# Load the saved model\nmodel = models.mobilenet_v2()\nmodel.load_state_dict(torch.load(PYTORCH_MODEL_PATH, map_location='cpu'))\nmodel.eval()\n\n# Create example input (batch_size=1, channels=3, height=224, width=224)\nexample_input = (torch.randn(1, 3, 224, 224),)\n\nprint(\"Exporting to ExecuTorch format...\")\n\n# Step 1: Export to EXIR (ExecuTorch Intermediate Representation)\nprint(\"  1. Capturing model with torch.export...\")\nexported_program = export(model, example_input)\n\n# Step 2: Convert to Edge dialect\nprint(\"  2. Converting to Edge dialect...\")\nedge_program = to_edge(exported_program)\n\n# Step 3: Convert to ExecuTorch program\nprint(\"  3. Lowering to ExecuTorch...\")\nexecutorch_program = edge_program.to_executorch()\n\n# Step 4: Save as .pte file\nprint(\"  4. Saving to .pte file...\")\nwith open(EXECUTORCH_MODEL_PATH, \"wb\") as f:\n    f.write(executorch_program.buffer)\n\nprint(f\"\\n? Model successfully exported to {EXECUTORCH_MODEL_PATH}\")\n\n# Display file sizes for comparison\nimport os\npytorch_size = os.path.getsize(PYTORCH_MODEL_PATH)/(1024*1024)\nexecutorch_size = os.path.getsize(EXECUTORCH_MODEL_PATH)/(1024*1024)\n\nprint(\"\\n\" + \"=\"*50)\nprint(\"MODEL SIZE COMPARISON\")\nprint(\"=\"*50)\nprint(f\"PyTorch model:    {pytorch_size:.2f} MB\")\nprint(f\"ExecuTorch model: {executorch_size:.2f} MB\")\nprint(f\"Reduction:        {((pytorch_size - executorch_size) \\\n/pytorch_size * 100):.1f}%\")\nprint(\"=\"*50)\nRuning the export script:\npython export_mobv2_executorch.py\nWe will get:\nLoading PyTorch model...\nExporting to ExecuTorch format...\n  1. Capturing model with torch.export...\n  2. Converting to Edge dialect...\n  3. Lowering to ExecuTorch...\n  4. Saving to .pte file...\n\n? Model successfully exported to models/mobilenet_v2.pte\n\n==================================================\nMODEL SIZE COMPARISON\n==================================================\nPyTorch model:    13.60 MB\nExecuTorch model: 13.58 MB\nReduction:        0.2%\n==================================================\nThe basic ExecuTorch conversion doesn’t compress the model much - it’s mainly for runtime efficiency. To get real size reduction, we need quantization, which we will explore later. But first, let’s do an inference test using the converted model.\nRuning the script mobv2_executorch.py:\nimport torch\nimport torchvision.transforms as transforms\nfrom PIL import Image\nimport time\nimport json\nfrom executorch.extension.pybindings.portable_lib import _load_for_executorch\n\n# Paths\nEXECUTORCH_MODEL_PATH = \"models/mobilenet_v2.pte\"\nLABELS_PATH = \"models/imagenet_labels.json\"\nIMAGE_PATH = \"images/cat.jpg\"\n\n# Load labels\nprint(\"Loading labels...\")\nwith open(LABELS_PATH, 'r') as f:\n    labels = json.load(f)\n\n# Load ExecuTorch model\nprint(f\"Loading ExecuTorch model from {EXECUTORCH_MODEL_PATH}...\")\nmodel = _load_for_executorch(EXECUTORCH_MODEL_PATH)\n\n# Define image preprocessing (same as PyTorch)\npreprocess = transforms.Compose([\n    transforms.Resize(256),\n    transforms.CenterCrop(224),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], \n                         std=[0.229, 0.224, 0.225]),\n])\n\n# Load and preprocess image\nprint(f\"Loading image from {IMAGE_PATH}...\")\nimg = Image.open(IMAGE_PATH)\nimg_tensor = preprocess(img)\nbatch = img_tensor.unsqueeze(0)  # Add batch dimension\n\n# Perform inference with timing\nprint(\"Running ExecuTorch inference...\")\nstart_time = time.time()\n\n# ExecuTorch expects a tuple of inputs\noutput = model.forward((batch,))\n\ninference_time = (time.time() - start_time) * 1000  # Convert to ms\n\n# Get predictions\noutput_tensor = output[0]  # ExecuTorch returns a list\nprobabilities = torch.nn.functional.softmax(output_tensor[0], dim=0)\ntop5_prob, top5_idx = torch.topk(probabilities, 5)\n\n# Display results\nprint(\"\\n\" + \"=\"*50)\nprint(\"EXECUTORCH CLASSIFICATION RESULTS\")\nprint(\"=\"*50)\nprint(f\"Inference Time: {inference_time:.2f} ms\\n\")\nprint(\"Top 5 Predictions:\")\nprint(\"-\"*50)\n\nfor i in range(5):\n    idx = top5_idx[i].item()\n    prob = top5_prob[i].item()\n    print(f\"{i+1}. {labels[idx]:20s} - {prob*100:.2f}%\")\n\nprint(\"=\"*50)\nAs a result, we got a similar inference result, but a much higher latency (almost 2.5 seconds), which was unexpected.\nLoading labels...\nLoading ExecuTorch model from models/mobilenet_v2.pte...\nLoading image from images/cat.jpg...\nRunning ExecuTorch inference...\n\n==================================================\nEXECUTORCH CLASSIFICATION RESULTS\n==================================================\nInference Time: 2445.78 ms\n\nTop 5 Predictions:\n--------------------------------------------------\n1. tiger cat            - 47.44%\n2. Egyptian Mau         - 37.61%\n3. lynx                 - 6.91%\n4. tabby cat            - 6.22%\n5. plastic bag          - 0.47%\n==================================================\nThat export path produces a generic ExecuTorch CPU graph with reference kernels and no backend optimizations or fusions, so significantly higher latency than PyTorch is expected for MobileNet_v2 on a Pi 5.\nExecuTorch is designed to shine when delegated to a backend (XNNPACK, OpenVINO, etc.), where large subgraphs are lowered into highly optimized kernels. Without a delegate, most of the graph runs on the generic portable path, which is known to be significantly slower than PyTorch for many models.\nSo, let’s export the .pth model again with a CPU‑optimized backend (e.g., XNNPACK) and run with that backend enabled; this alone should reduce latency when compared with the naïve interpreter path.\nHere’s the corrected conversion script with XNNPACK delegation (convert_mobv2_xnnpack.py):\nimport torch\nfrom torchvision import models\nfrom executorch.exir import to_edge\nfrom torch.export import export\nfrom executorch.backends.xnnpack.partition.xnnpack_partitioner \\\n     import XnnpackPartitioner\n\n# Paths\nPYTORCH_MODEL_PATH = \"models/mobilenet_v2.pth\"\nEXECUTORCH_MODEL_PATH = \"models/mobilenet_v2_xnnpack.pte\"\n\nprint(\"Loading PyTorch model...\")\nmodel = models.mobilenet_v2()\nmodel.load_state_dict(torch.load(PYTORCH_MODEL_PATH, map_location='cpu'))\nmodel.eval()\n\n# Create example input\nexample_input = (torch.randn(1, 3, 224, 224),)\n\nprint(\"Exporting to ExecuTorch with XNNPACK backend...\")\n\n# Step 1: Export to EXIR\nprint(\"  1. Capturing model with torch.export...\")\nexported_program = export(model, example_input)\n\n# Step 2: Convert to Edge dialect with XNNPACK partitioner\nprint(\"  2. Converting to Edge dialect with XNNPACK delegation...\")\nedge_program = to_edge(exported_program)\n\n# Step 3: Partition for XNNPACK backend\nprint(\"  3. Delegating to XNNPACK backend...\")\nedge_program = edge_program.to_backend(XnnpackPartitioner())\n\n# Step 4: Convert to ExecuTorch program\nprint(\"  4. Lowering to ExecuTorch...\")\nexecutorch_program = edge_program.to_executorch()\n\n# Step 5: Save as .pte file\nprint(\"  5. Saving to .pte file...\")\nwith open(EXECUTORCH_MODEL_PATH, \"wb\") as f:\n    f.write(executorch_program.buffer)\n\nprint(f\"\\n? Model successfully exported to {EXECUTORCH_MODEL_PATH}\")\n\n# Display file size\nimport os\npytorch_size = os.path.getsize(PYTORCH_MODEL_PATH) / (1024 * 1024)\nexecutorch_size = os.path.getsize(EXECUTORCH_MODEL_PATH) / (1024 * 1024)\n\nprint(\"\\n\" + \"=\"*50)\nprint(\"MODEL SIZE COMPARISON\")\nprint(\"=\"*50)\nprint(f\"PyTorch model:           {pytorch_size:.2f} MB\")\nprint(f\"ExecuTorch+XNNPACK:      {executorch_size:.2f} MB\")\nprint(\"=\"*50)\nRuning it we get:\nLoading PyTorch model...\nExporting to ExecuTorch with XNNPACK backend...\n  1. Capturing model with torch.export...\n  2. Lowering to Edge with XNNPACK delegation...\n  3. Converting to ExecuTorch...\n  4. Saving to .pte file...\n\n? Model successfully exported to models/mobilenet_v2_xnnpack.pte\n\n==================================================\nMODEL SIZE COMPARISON\n==================================================\nPyTorch model:           13.60 MB\nExecuTorch+XNNPACK:      13.35 MB\n==================================================\nWe did not gain in terms of size, but let’s run the same inference script as before, with this new converted model, to inspect the latency:\nthe result:\nLoading labels...\nLoading ExecuTorch model from models/mobilenet_v2_xnnpack.pte...\nLoading image from images/cat.jpg...\nRunning ExecuTorch inference...\n\n==================================================\nEXECUTORCH CLASSIFICATION RESULTS\n==================================================\nInference Time: 19.95 ms\n\nTop 5 Predictions:\n--------------------------------------------------\n1. tiger cat            - 47.44%\n2. Egyptian Mau         - 37.61%\n3. lynx                 - 6.91%\n4. tabby cat            - 6.22%\n5. plastic bag          - 0.47%\n==================================================\nNow, the ExecuTorch runtime detects the backend automatically from the .pte file metadata. We have achieved much faster inference: 20ms instead of 2445ms. This latency is, in fact, several times faster than PyTorch.\nWhy XNNPACK is so fast:\n\n✅ ARM NEON SIMD optimizations\n✅ Multi-threading on Raspberry Pi’s 4 cores\n✅ Operator fusion and memory optimization\n✅ Cache-friendly memory access patterns\n\nThis demonstrates:\n\nExecuTorch (basic) without a backend = don’t use in production\nExecuTorch + XNNPACK = production-ready edge AI\nRaspberry Pi 5 can do 50+ inferences/second at this speed!\n\nNow we can add quantization to get an even smaller model size while maintaining (or even increasing) this speed!\n\n\nModel Quantization\nQuantization reduces model size and can further improve inference speed. EXECUTORCH supports PyTorch’s native quantization.\nQuantization Overview\nQuantization is a technique that reduces the precision of numbers used in a model’s computations and stored weights—typically from 32-bit floats to 8-bit integers. This reduces the model’s memory footprint, speeds up inference, and lowers power consumption, often with minimal loss in accuracy.\nQuantization is especially important for deploying models on edge devices such as wearables, embedded systems, and microcontrollers, which often have limited compute, memory, and battery capacity. By quantizing models, we can make them significantly more efficient and better suited to these resource-constrained environments.\nQuantization in ExecuTorch\nExecuTorch uses torchao as its quantization library. This integration allows ExecuTorch to leverage PyTorch-native tools for preparing, calibrating, and converting quantized models.\nQuantization in ExecuTorch is backend-specific. Each backend defines how models should be quantized based on its hardware capabilities. Most ExecuTorch backends use the torchao PT2E quantization flow, which works with models exported with torch.export and enables tailored quantization for each backend.\nFor a quantized XNNPACK .pte we need a different pipeline: PT2E quantization (with XNNPACKQuantizer), then lowering with XnnpackPartitioner before to_executorch(). Otherwise, we will hit errors or get an undelegated model.\nFor the conversion, we need: (1) calibrate with real, preprocessed images, and (2) compute the quantized .pte size after you actually write the file.\nFirst, let us create a small calib_images/ folder (e.g., 50–100 natural images across a few classes). A simple way is to reuse an existing dataset (e.g., CIFAR‑10) and save 50–100 images into calib_images/ with an ImageNet‑style folder layout.\nThe script gen_calibr_images.py will: • Download CIFAR‑10. • Pick 10 classes × 10 images each = 100 images. • Save them under calib_images/&lt;class_name&gt;/img_XXX.jpg.\nimport os\nfrom pathlib import Path\n\nimport torch\nfrom torchvision import datasets, transforms\nfrom torchvision.utils import save_image\n\n# Where to store calibration images\nOUT_ROOT = Path(\"calib_images\")\nOUT_ROOT.mkdir(parents=True, exist_ok=True)\n\n# 1) Load a small, natural-image dataset (CIFAR-10)\ntransform = transforms.ToTensor()  # we will NOT normalize here\ndataset = datasets.CIFAR10(\n    root=\"data\",\n    train=True,\n    download=True,\n    transform=transform,\n)\n\n# 2) Map label index -&gt; class name (CIFAR-10 has 10 classes)\nclasses = dataset.classes  # ['airplane', 'automobile', ..., 'truck']\n\n# 3) Choose how many classes and images per class\nnum_classes = 10\nimages_per_class = 10   # 10 x 10 = 100 images\n\n# 4) Collect and save images\ncounts = {cls: 0 for cls in classes[:num_classes]}\n\nfor img, label in dataset:\n    cls_name = classes[label]\n    if cls_name not in counts:\n        continue\n    if counts[cls_name] &gt;= images_per_class:\n        continue\n\n    # Make class subdir\n    class_dir = OUT_ROOT / cls_name\n    class_dir.mkdir(parents=True, exist_ok=True)\n\n    idx = counts[cls_name]\n    out_path = class_dir / f\"img_{idx:04d}.jpg\"\n    save_image(img, out_path)\n\n    counts[cls_name] += 1\n\n    # Stop when we have enough\n    if all(counts[c] &gt;= images_per_class for c in counts):\n        break\n\nprint(\"Saved calibration images:\")\nfor cls_name, n in counts.items():\n    print(f\"  {cls_name}: {n} images\")\nprint(f\"\\nRoot folder: {OUT_ROOT.resolve()}\")\nLet’s use the inference script convert_mobv2_xnnpack_int8.py, which is the same inference script as before, with this new int8 converted model to inspect the latency:\nimport os\nimport torch\nimport torchvision.models as models\nimport torchvision.transforms as transforms\nimport torchvision.datasets as datasets\n\nfrom torch.export import export\nfrom torchao.quantization.pt2e.quantize_pt2e import (\n    prepare_pt2e,\n    convert_pt2e,\n)\nfrom executorch.backends.xnnpack.quantizer.xnnpack_quantizer import (\n    get_symmetric_quantization_config,\n    XNNPACKQuantizer,\n)\nfrom executorch.backends.xnnpack.partition.xnnpack_partitioner import (\n    XnnpackPartitioner,\n)\nfrom executorch.exir import to_edge_transform_and_lower\n\nPYTORCH_MODEL_PATH = \"models/mobilenet_v2.pth\"\nEXECUTORCH_QUANTIZED_PATH = \"models/mobilenet_v2_quantized_xnnpack.pte\"\nCALIB_IMAGES_DIR = \"calib_images\"   # &lt;-- put some natural images here\n\n# 1) Load FP32 model\nmodel = models.mobilenet_v2()\nmodel.load_state_dict(torch.load(PYTORCH_MODEL_PATH, map_location=\"cpu\"))\nmodel.eval()\n\n# Example input only defines shapes for export\nexample_inputs = (torch.randn(1, 3, 224, 224),)\n\n# 2) Configure XNNPACK quantizer (global symmetric config)\nqparams = get_symmetric_quantization_config(is_per_channel=True)\nquantizer = XNNPACKQuantizer()\nquantizer.set_global(qparams)\n\n# 3) Export float model for PT2E and prepare for quantization\nexported = torch.export.export(model, example_inputs)\ntraining_ep = exported.module()\nprepared = prepare_pt2e(training_ep, quantizer)\n\n# 4) Calibration with REAL images using SAME preprocessing as inference\ncalib_transform = transforms.Compose([\n    transforms.Resize(256),\n    transforms.CenterCrop(224),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                         std=[0.229, 0.224, 0.225]),\n])\n\ncalib_dataset = datasets.ImageFolder(CALIB_IMAGES_DIR, \n                                     transform=calib_transform)\ncalib_loader = torch.utils.data.DataLoader(\n    calib_dataset, batch_size=1, shuffle=True\n)\n\nprint(f\"Calibrating on {len(calib_dataset)} images from {CALIB_IMAGES_DIR}...\")\n\nnum_calib = min(100, len(calib_dataset))  # or adjust\nwith torch.no_grad():\n    for i, (calib_img, _) in enumerate(calib_loader):\n        if i &gt;= num_calib:\n            break\n        prepared(calib_img)\n\n# 5) Convert calibrated model to quantized model\nquantized_model = convert_pt2e(prepared)\n\n# 6) Export quantized model and lower to XNNPACK, then to ExecuTorch\nexported_quant = export(quantized_model, example_inputs)\n\net_program = to_edge_transform_and_lower(\n    exported_quant,\n    partitioner=[XnnpackPartitioner()],\n).to_executorch()\n\n# 7) Save .pte and compute sizes\nwith open(EXECUTORCH_QUANTIZED_PATH, \"wb\") as f:\n    et_program.write_to_file(f)\n\npytorch_size = os.path.getsize(PYTORCH_MODEL_PATH)/(1024*1024)\nquantized_size = os.path.getsize(EXECUTORCH_QUANTIZED_PATH)/(1024*1024)\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"MODEL SIZE COMPARISON\")\nprint(\"=\"*60)\nprint(f\"PyTorch (FP32):                  {pytorch_size:6.2f} MB\")\nprint(f\"ExecuTorch Quantized (INT8):     {quantized_size:6.2f} MB\")\nprint(f\"Size reduction:                  {((pytorch_size - quantized_size) \\\n/ pytorch_size * 100):5.1f}%\")\nprint(f\"Savings:                         {pytorch_size - quantized_size:6.2f} MB\")\nprint(\"=\"*60)\n\nRuning the script, we get:\nCalibrating on 100 images from calib_images...\n\n============================================================\nMODEL SIZE COMPARISON\n============================================================\nPyTorch (FP32):                   13.60 MB\nExecuTorch Quantized (INT8):       3.59 MB\nSize reduction:                   73.6%\nSavings:                          10.01 MB\n============================================================\nThe quantized (int8) model achieved 74% size reduction: ~3.5 MB (similar to TFLite). Let’s see about the inference latency, runing mobv2_xnnpack_int8.py.\nLoading labels...\nLoading ExecuTorch model from models/mobilenet_v2_quantized_xnnpack.pte...\nLoading image from images/cat.jpg...\nRunning ExecuTorch inference (Quantized INT8)...\n\n==================================================\nEXECUTORCH QUANTIZED INT8 RESULTS\n==================================================\nInference Time: 13.56 ms\nOutput dtype:   torch.float32\n\nTop 5 Predictions:\n--------------------------------------------------\n1. tiger cat            - 51.01%\n2. Egyptian Mau         - 34.11%\n3. lynx                 - 7.54%\n4. tabby cat            - 6.17%\n5. plastic bag          - 0.37%\n==================================================\n\nSlightly higher top‑1 probabilities in the INT8 model are normal and do not indicate a problem by themselves. Quantization slightly changes the logits, and softmax can become a bit “sharper” or “flatter” even when top‑1 remains correct.\n\n\n\nModel Size/Performance Comparison\n\n\n\nModel Configuration\nFile Size\nSize Reduction\nLatency\n\n\n\n\nFloat32 (basic export)\n13.58 MB\nBaseline\n2.5 s\n\n\nFloat32 + XNNPACK\n13.35 MB\n~0%\n20 ms\n\n\nINT8 + XNNPACK\n3.59 MB\n~75%\n14 ms\n\n\n\nNOTE\n\nLooking at Htop, we can see that only one of the Pi’s cores is at 100%. This indicates that the shipped Python runtime currently runs our ExecuTorch/XNNPACK model effectively single‑threaded on Pi.\nTo exploit all four cores, the next step would be to move inference into a small C++ wrapper that sets the ExecuTorch threadpool size before executing the graph. With the pure‑Python path, there is no clean public knob to change it yet. We will not explore it here."
  },
  {
    "objectID": "raspi/executorch/executorch.html#making-inferences-with-executorch",
    "href": "raspi/executorch/executorch.html#making-inferences-with-executorch",
    "title": "Image Classification with EXECUTORCH",
    "section": "Making Inferences with EXECUTORCH",
    "text": "Making Inferences with EXECUTORCH\nNow that we have our EXECUTORCH models, let’s explore them in more detail for image classification using a Jupyter Notebook!\n\nSetting up Jupyter Notebook\nSet up Jupyter Notebook for interactive development:\npip install jupyter jupyterlab notebook\njupyter notebook --generate-config\nTo run the Jupyter notebook on the Raspberry Pi desktop, run:\njupyter notebook\nand open the URL with the token\nTo run Jupyter Notebook on your computer (headless), run the command below, replacing with your Raspberry Pi’s IP address:\nTo get the IP Address, we can use the command: hostname -I\njupyter notebook --ip=192.168.4.42 --no-browser\n\nAccess it from another device using the provided token in your web browser.\n\n\n\nThe Project folder\nWe must be sure that we have this project folder structure:\nEXECUTORCH/MOBILENET/\n├── convert_mobv2_executorch.py\n├── convert_mobv2_xnnpack.py\n├── convert_mobv2_xnnpack_int8.py        \n├── mobv2_executorch.py\n├── mobv2_xnnpack.py     \n├── mobv2_xnnpack_int8.py \n├── calib_images/\n├── data/         \n├── models/\n│   ├── mobilenet_v2.pth                        # Float32 pytorch model\n│   ├── mobilenet_v2.pte                        # Float32 conv model\n│   ├── mobilenet_v2_xnnpack.pte                # Float32 conv model\n│   ├── mobilenet_v2_quantized_xnnpack.pte      # Quantized conv model\n│   └── imagenet_labels.json                    # Labels\n├── images/                                     # Test images\n│   ├── cat.jpg\n│   └── camera_capture.jpg\n└── notebooks/                                   \n    └── image_classification_executorch.ipynb\n\n\nLoading and Running a Model\nInside the folder ‘notebooks’, on the project space IMAGE_CLASS/MOBILENET, create a new notebook: image_classification_executorch.ipynb.\n\n\nSetup and Verification\n# Import required libraries\nimport os\nimport time\nimport json\nimport urllib.request\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom PIL import Image\nimport torch\nfrom torchvision import transforms\nimport executorch\nfrom executorch.extension.pybindings.portable_lib import _load_for_executorch\n\nprint(\"=\" * 50)\nprint(\"SETUP VERIFICATION\")\nprint(\"=\" * 50)\n\n# Check versions\nprint(f\"PyTorch version: {torch.__version__}\")\nprint(f\"NumPy version: {np.__version__}\")\nprint(f\"PIL version: {Image.__version__}\")\nprint(f\"EXECUTORCH available: {executorch is not None}\")\n\n# Test basic PyTorch functionality\nx = torch.randn(3, 224, 224)\nprint(f\"\\nCreated test tensor with shape: {x.shape}\")\n\n# Test PIL\ntest_img = Image.new('RGB', (224, 224), color='red')\nprint(f\"Created test PIL image: {test_img.size}\")\n\nprint(\"\\n✓ Setup verification complete!\")\nprint(\"=\" * 50)\nWe get:\n==================================================\nSETUP VERIFICATION\n==================================================\nPyTorch version: 2.9.1+cpu\nNumPy version: 2.2.6\nPIL version: 12.1.0\nEXECUTORCH available: True\n\nCreated test tensor with shape: torch.Size([3, 224, 224])\nCreated test PIL image: (224, 224)\n\n✓ Setup verification complete!\n==================================================\n\n\nDownload Test Image\n\nDownload test image for example from:\n\n“https://upload.wikimedia.org/wikipedia/commons/3/3a/Cat03.jpg”\nAnd save it on the ../images folder as “cat.jpg”\n\n\nimg_path = \"../images/cat.jpg\"\n\n# Load and display\nimg = Image.open(img_path)\nplt.figure(figsize=(6, 6))\nplt.imshow(img)\nplt.title(\"Original Image\")\n#plt.axis('off')\nplt.show()\n\nprint(f\"Image size: {img.size}\")\nprint(f\"Image mode: {img.mode}\")\n\n\n\n\n\nImage size: (1600, 1598)\nImage mode: RGB"
  },
  {
    "objectID": "raspi/executorch/executorch.html#load-executorch-model",
    "href": "raspi/executorch/executorch.html#load-executorch-model",
    "title": "Image Classification with EXECUTORCH",
    "section": "Load EXECUTORCH Model",
    "text": "Load EXECUTORCH Model\n\nNote: You need to export a model first using the export_mobv2_executorch.py script.\nIf you don’t have a model yet, run the export script first:\n\npython export_mobv2_executorch.py\n\n\nLet’s verify what the models in the folder ../models:\nimagenet_labels.json  mobilenet_v2_quantized_xnnpack.pte\nmobilenet_v2.pte      mobilenet_v2_xnnpack.pte\nmobilenet_v2.pth\n\nThe conversions were performed using the Python scripts in the previous sections.\n\n# Load the EXECUTORCH model\nmodel_path = \"../models/mobilenet_v2.pte\"\n\ntry:\n    model = _load_for_executorch(model_path)\n    print(f\"Model loaded successfully from: {model_path}\")\n    #print(f\"  Available methods: {model.method_names}\")\n    \n    # Check file size\n    file_size = os.path.getsize(model_path) / (1024 * 1024)  # MB\n    print(f\"Model size: {file_size:.2f} MB\")\n    \nexcept FileNotFoundError:\n    print(f\"✗ Model not found: {model_path}\")\n    print(\"\\nPlease run the export script first:\")\n    print(\"  python export_mobilenet.py\")\nModel loaded successfully from: ../models/mobilenet_v2.pte\nModel size: 13.58 MB"
  },
  {
    "objectID": "raspi/executorch/executorch.html#download-imagenet-labels",
    "href": "raspi/executorch/executorch.html#download-imagenet-labels",
    "title": "Image Classification with EXECUTORCH",
    "section": "Download ImageNet Labels",
    "text": "Download ImageNet Labels\n# Download and save ImageNet labels (if you do not have it)\nLABELS_PATH = \"../models/imagenet_labels.json\"\n\nif not os.path.exists(LABELS_PATH):\n    print(\"Downloading ImageNet labels...\")\n    LABELS_URL = \"https://raw.githubusercontent.com/anishathalye/\\\n    imagenet-simple-labels/master/imagenet-simple-labels.json\"\n    with urllib.request.urlopen(LABELS_URL) as url:\n        labels = json.load(url)\n    \n    # Save labels locally\n    with open(LABELS_PATH, 'w') as f:\n        json.dump(labels, f)\n    print(f\"Labels saved to {LABELS_PATH}\")\nelse:\n    print(\"Loading labels from disk...\")\n    with open(LABELS_PATH, 'r') as f:\n        labels = json.load(f)      \nCheck the labels:\nprint(f\"\\nTotal classes: {len(labels)}\")\nprint(f\"Sample labels: {labels[280:285]}\")  \nTotal classes: 1000\nSample labels: ['grey fox', 'tabby cat', 'tiger cat', 'Persian cat', 'Siamese cat']"
  },
  {
    "objectID": "raspi/executorch/executorch.html#image-preprocessing",
    "href": "raspi/executorch/executorch.html#image-preprocessing",
    "title": "Image Classification with EXECUTORCH",
    "section": "Image Preprocessing",
    "text": "Image Preprocessing\nA preprocessing pipeline is needed because ExecuTorch only runs the exported core network; it does not include the input normalization logic that MobileNet v2 expects, and the model will give incorrect predictions if the input tensor is not in the exact format it was trained on.\nWhat MobileNet v2 expects For typical PyTorch MobileNet v2 models (ImageNet‑pretrained): • Input shape: 3‑channel RGB tensor of size. • Value range: floating-point values, usually in float32 after dividing by 255. • Normalization: per‑channel mean/std (ImageNet) normalization, e.g., mean=0.485, 0.456, 0.406, std=0.229, 0.224, 0.225.\nThese steps (resize, convert to tensor, normalize) are not “optional decorations”; they are part of the functional definition of the model’s expected input distribution.\n\nDefine preprocessing pipeline\npreprocess = transforms.Compose([\n    transforms.Resize(256),              # Resize to 256\n    transforms.CenterCrop(224),          # Center crop to 224x224\n    transforms.ToTensor(),               # Convert to tensor [0, 1]\n    transforms.Normalize(                # Normalize with ImageNet stats\n        mean=[0.485, 0.456, 0.406],\n        std=[0.229, 0.224, 0.225]\n    ),\n])\n\n\nApply preprocessing\ninput_tensor = preprocess(img)\nprint(f\"  Input shape: {input_tensor.shape}\")\nprint(f\"  Input dtype: {input_tensor.dtype}\")\n  Input shape: torch.Size([3, 224, 224])\n  Input dtype: torch.float32\n\n\nAdd batch dimension: [1, 3, 224, 224]\ninput_batch = input_tensor.unsqueeze(0)  \n\nprint(f\"  Input shape: {input_batch.shape}\")\nprint(f\"  Input dtype: {input_batch.dtype}\")\nprint(f\"  Value range: [{input_batch.min():.3f}, {input_batch.max():.3f}]\")\n  Input shape: torch.Size([1, 3, 224, 224])\n  Input dtype: torch.float32\n  Value range: [-2.084, 2.309]\nThe Preprocessing is complete!"
  },
  {
    "objectID": "raspi/executorch/executorch.html#run-inference",
    "href": "raspi/executorch/executorch.html#run-inference",
    "title": "Image Classification with EXECUTORCH",
    "section": "Run Inference",
    "text": "Run Inference\nFor inference, we should run a forward pass of the model in inference mode (torch.no_grad()), measure the time, and print basic information about the outputs.\ntorch.no_grad() is a context manager that disables gradient calculation inside its block. During inference, we do not need gradients, so disabling them:\n\nSaves memory (no computation graph is stored).\nCan speed up computation slightly.\nEverything computed inside this block will have requires_grad=False, so we cannot call .backward() on it.\n\n# Run inference\nwith torch.no_grad():\n    start_time = time.time()\n    outputs = model.forward((input_batch,))\n    inference_time = time.time() - start_time\n\nprint(f\"Inference completed in {inference_time*1000:.2f} ms\")\nprint(f\"Output type: {type(outputs)}\")\nprint(f\"Output shape: {outputs[0].shape}\")\nInference completed in 2478.74 ms\nOutput type: &lt;class 'list'&gt;\nOutput shape: torch.Size([1, 1000])\ntype(outputs) tells us what container the model returned. Often this is a tuple or list when working with exported/ExecuTorch‑style models, e.g., &lt;class 'tuple'&gt;.\nThat container may hold one or more tensors (e.g., logits, auxiliary outputs).\n\noutputs[0] accesses the first element of that container (usually the main output tensor), and .shape prints its dimensions (For image classification, this is often batch_size, num_classes)."
  },
  {
    "objectID": "raspi/executorch/executorch.html#process-and-display-results",
    "href": "raspi/executorch/executorch.html#process-and-display-results",
    "title": "Image Classification with EXECUTORCH",
    "section": "Process and Display Results",
    "text": "Process and Display Results\nNow we should take the model’s raw scores (logits) for a single image, convert them into probabilities with softmax, select the top‑5 most likely classes, and print them nicely formatted.\n\noutputs[0][0] selects the first element in the batch, giving a 1D tensor of logits of length num_classes.\ntorch.nn.functional.softmax(..., dim=0) applies the softmax function along that 1D dimension, turning logits into probabilities that sum to 1.\n\n# Apply softmax to get probabilities\nprobabilities = torch.nn.functional.softmax(outputs[0][0], dim=0)\n\n# Get top 5 predictions\ntop5_prob, top5_indices = torch.topk(probabilities, 5)\n\n# Display results\nprint(\"\\n\" + \"=\"*60)\nprint(\"TOP 5 PREDICTIONS\")\nprint(\"=\"*60)\nprint(f\"{'Class':&lt;35} {'Probability':&gt;10}\")\nprint(\"-\"*60)\n\nfor i in range(5):\n    label = labels[top5_indices[i]]\n    prob = top5_prob[i].item() * 100\n    print(f\"{label:&lt;35} {prob:&gt;9.2f}%\")\n\nprint(\"=\"*60)\n============================================================\nTOP 5 PREDICTIONS\n============================================================\nClass                               Probability\n------------------------------------------------------------\ntiger cat                               12.85%\nEgyptian cat                             9.75%\ntabby                                    6.09%\nlynx                                     1.70%\ncarton                                   0.84%\n============================================================"
  },
  {
    "objectID": "raspi/executorch/executorch.html#create-reusable-classification-function",
    "href": "raspi/executorch/executorch.html#create-reusable-classification-function",
    "title": "Image Classification with EXECUTORCH",
    "section": "Create Reusable Classification Function",
    "text": "Create Reusable Classification Function\nFor simplicity and reuse across other tests, let’s create a reusable function that builds on what was done so far.\ndef classify_image_executorch(img_path, model_path, labels_path, \n                              top_k=5, show_image=True):\n    \"\"\"\n    Classify an image using EXECUTORCH model\n    \n    Args:\n        img_path: Path to input image\n        model_path: Path to .pte model file\n        labels_path: Path to labels text file\n        top_k: Number of top predictions to return\n        show_image: Whether to display the image\n    \n    Returns:\n        inference_time: Inference time in ms\n        top_indices: Indices of top k predictions\n        top_probs: Probabilities of top k predictions\n    \"\"\"\n    # Load image\n    img = Image.open(img_path).convert('RGB')\n    \n    # Display image\n    if show_image:\n        plt.figure(figsize=(4, 4))\n        plt.imshow(img)\n        plt.axis('off')\n        plt.title('Input Image')\n        plt.show()\n\n    print(f\"Image Path: {img_path}\")\n    \n    # Load model\n    print(f\"Model Path {model_path}\") \n    model_size = os.path.getsize(model_path) / (1024 * 1024)\n    print(f\"Model size: {model_size:6.2f} MB\")\n    \n    model = _load_for_executorch(model_path)\n\n    # Preprocess\n    preprocess = transforms.Compose([\n        transforms.Resize(256),\n        transforms.CenterCrop(224),\n        transforms.ToTensor(),\n        transforms.Normalize(\n            mean=[0.485, 0.456, 0.406],\n            std=[0.229, 0.224, 0.225]\n        ),\n    ])\n    \n    input_tensor = preprocess(img)\n    input_batch = input_tensor.unsqueeze(0)\n    \n    # Inference\n    with torch.no_grad():\n        start_time = time.time()\n        outputs = model.forward((input_batch,))\n        inference_time = (time.time() - start_time)*1000\n        \n    \n    # Process results\n    probabilities = torch.nn.functional.softmax(outputs[0][0], dim=0)\n    top_prob, top_indices = torch.topk(probabilities, top_k)\n    \n    # Load labels\n    with open(labels_path, 'r') as f:\n        labels = json.load(f)\n    \n    # Display results\n    print(f\"\\nInference time: {inference_time:.2f} ms\")\n    print(\"\\n\" + \"=\"*60)\n    print(f\"{'[PREDICTION]':&lt;35} {'[Probability]':&gt;15}\")\n    print(\"-\"*60)\n    \n    for i in range(top_k):\n        label = labels[top_indices[i]]\n        prob = top_prob[i].item() * 100\n        print(f\"{label:&lt;35} {prob:&gt;14.2f}%\")\n    \n    print(\"=\"*60)\n    \n    return inference_time, top_indices, top_prob\n\nprint(\"✓ Classification function defined!\")\n✓ Classification function defined!"
  },
  {
    "objectID": "raspi/executorch/executorch.html#classification-function-test",
    "href": "raspi/executorch/executorch.html#classification-function-test",
    "title": "Image Classification with EXECUTORCH",
    "section": "Classification Function Test",
    "text": "Classification Function Test\n# Test with the cat image\ninf_time, indices, probs = classify_image_executorch(\n    img_path=\"../images/cat.jpg\",\n    model_path=\"../models/mobilenet_v2.pte\",\n    labels_path=\"../models/imagenet_labels.json\",\n    top_k=5\n)\n\n\n\n\n\nWe can also check what is retrurned fron the function\ninf_time, indices, probs\n(2445.200204849243,\n tensor([282, 285, 287, 281, 728]),\n tensor([0.4744, 0.3761, 0.0691, 0.0622, 0.0047]))"
  },
  {
    "objectID": "raspi/executorch/executorch.html#using-the-xnnpack-accelerated-backend",
    "href": "raspi/executorch/executorch.html#using-the-xnnpack-accelerated-backend",
    "title": "Image Classification with EXECUTORCH",
    "section": "Using the XNNPACK accelerated backend",
    "text": "Using the XNNPACK accelerated backend\nNote: We need to export a model using the convert_mobv2_xnnpack.py script first.\n# Test with the cat image\ninf_time, indices, probs = classify_image_executorch(\n    img_path=\"../images/cat.jpg\",\n    model_path=\"../models/mobilenet_v2_xnnpack.pte\",\n    labels_path=\"../models/imagenet_labels.json\",\n    top_k=5\n)\n\n\n\n\n\nThe inference time was reduced from +2.5s to around -20ms"
  },
  {
    "objectID": "raspi/executorch/executorch.html#quantized-model---xnnpack-accelerated-backend",
    "href": "raspi/executorch/executorch.html#quantized-model---xnnpack-accelerated-backend",
    "title": "Image Classification with EXECUTORCH",
    "section": "Quantized model - XNNPACK accelerated backend",
    "text": "Quantized model - XNNPACK accelerated backend\nNote: We need to export a model first using the convert_mobv2_xnnpack_int8.py script.\n# Test with the cat image\ninf_time, indices, probs = classify_image_executorch(\n    img_path=\"../images/cat.jpg\",\n    model_path=\"../models/mobilenet_v2_quantized_xnnpack.pte\",\n    labels_path=\"../models/imagenet_labels.json\",\n    top_k=5\n)\n\n\n\n\n\n==&gt; Even faster inference with a lower model in size\n\nSlightly higher probabilities in the INT8 model are normal and do not indicate a problem by themselves. Quantization slightly changes the logits, and softmax can become a bit “sharper” or “flatter” even when top‑1 remains correct."
  },
  {
    "objectID": "raspi/executorch/executorch.html#camera-integration",
    "href": "raspi/executorch/executorch.html#camera-integration",
    "title": "Image Classification with EXECUTORCH",
    "section": "Camera Integration",
    "text": "Camera Integration\nWe essentially have two different Python worlds: system Python 3.13 (where the camera stack is wired up) and our 3.11 virtual env (where ExecuTorch is installed). To run ExecuTorch on live frames from the Pi camera, we need to bridge those worlds.\n\nWhy the camera “only works” in 3.13\n\nRecent Raspberry Pi OS uses Picamera2 on top of libcamera as the recommended interface.\nThe Picamera2/libcamera Python bindings are usually installed into the system Python and are not trivially pip‑installable into arbitrary venvs or other Python versions.\nOnce we create a separate 3.11 environment, it will not automatically see the Picamera2/libcamera bindings under 3.13, so imports fail or the camera device is not accessible from that environment.\n\n\nWe will use a two‑process solution: capture in 3.13, infer in 3.11. For that, we should run a small capture service under Python 3.13 that:\n\nGrabs frames from the Pi camera (Picamera2 / libcamera).\nSends frames to your ExecuTorch process (3.11) over a local channel (e.g., ZeroMQ, TCP/UDP socket, shared memory, filesystem (write JPEG/PNG to a temp directory and signal), or a simple HTTP server.\n\nThe 3.11 process (under venev) receives the frame, decodes it, runs the preprocessing pipeline (resize, normalize), then calls ExecuTorch for inference..\n\nImage Capture\nOutside of the ExecuTorch env and folder, we will create a folder (CAMERA).\nDocuments/\n├── EXECUTORCH/MOBILENET/     # Python 3.11\n├── CAMERA/                   # Python 3.13\n       ├── camera_capture.py\n       ├── camera_capture.jpg\nThere we will run the script camera_capture.py):\nimport numpy as np\nfrom picamera2 import Picamera2\nimport time\n\nprint(f\"NumPy version: {np.__version__}\")\n\n# Initialize camera\n\npicam2 = Picamera2()\n\nconfig = picam2.create_preview_configuration(main={\"size\":(640,480)}) \npicam2.configure(config)\npicam2.start()\n\n# Wait for camera to warm up\n\ntime.sleep(2)\n\nprint(\"Camera working in isolated venv!\")\n\n# Capture image\n\npicam2.capture_file(\"camera_capture.jpg\")\nprint(\"Image captured: camera_capture.jpg\")\n\n# Stop camera\n\npicam2.stop()\npicam2.close()\nRuning the script, we um get an image that will be stored on:\n\n/Documents/CAMERA/camera_capture.jpg\n\nLooking from the notebook folder, the image path will be:\n../../../../CAMERA/camera_capture.jpg\nLet’s run the same function used with the test image:\n# Test the quantized model with the captured image\ninf_time, indices, probs = classify_image_executorch(\n    img_path=\"../../../../CAMERA/camera_capture.jpg\",\n    model_path=\"../models/mobilenet_v2_quantized_xnnpack.pte\",\n    labels_path=\"../models/imagenet_labels.json\",\n    top_k=5\n)"
  },
  {
    "objectID": "raspi/executorch/executorch.html#performance-benchmarking",
    "href": "raspi/executorch/executorch.html#performance-benchmarking",
    "title": "Image Classification with EXECUTORCH",
    "section": "Performance Benchmarking",
    "text": "Performance Benchmarking\nLet’s now define a function to run inference several times for each model and compare their performance.\ndef benchmark_inference(model_path, num_runs=50):\n    \"\"\"\n    Benchmark model inference speed\n    \"\"\"\n    print(f\"Benchmarking model: {model_path}\")\n    print(f\"Number of runs: {num_runs}\\n\")\n    \n    # Load model\n    model = _load_for_executorch(model_path)\n    \n    # Create dummy input\n    dummy_input = torch.randn(1, 3, 224, 224)\n    \n    # Warmup (10 runs)\n    print(\"Warming up...\")\n    for _ in range(10):\n        with torch.no_grad():\n            _ = model.forward((dummy_input,))\n    \n    # Benchmark\n    print(f\"Running benchmark...\")\n    times = []\n    for i in range(num_runs):\n        start = time.time()\n        with torch.no_grad():\n            _ = model.forward((dummy_input,))\n        times.append(time.time() - start)\n    \n    times = np.array(times) * 1000  # Convert to ms\n    \n    # Print statistics\n    print(\"\\n\" + \"=\"*50)\n    print(\"BENCHMARK RESULTS\")\n    print(\"=\"*50)\n    print(f\"  Mean:   {times.mean():.2f} ms\")\n    print(f\"  Median: {np.median(times):.2f} ms\")\n    print(f\"  Std:    {times.std():.2f} ms\")\n    print(f\"  Min:    {times.min():.2f} ms\")\n    print(f\"  Max:    {times.max():.2f} ms\")\n    print(\"=\"*50)\n    \n    # Plot distribution\n    plt.figure(figsize=(12, 4))\n    \n    # Histogram\n    plt.subplot(1, 2, 1)\n    plt.hist(times, bins=20, edgecolor='black', alpha=0.7)\n    plt.axvline(times.mean(), color='red', linestyle='--', \n                label=f'Mean: {times.mean():.2f} ms')\n    plt.xlabel('Inference Time (ms)')\n    plt.ylabel('Frequency')\n    plt.title('Inference Time Distribution')\n    plt.legend()\n    plt.grid(alpha=0.3)\n    \n    # Time series\n    plt.subplot(1, 2, 2)\n    plt.plot(times, marker='o', markersize=3, alpha=0.6)\n    plt.axhline(times.mean(), color='red', linestyle='--', \n                label=f'Mean: {times.mean():.2f} ms')\n    plt.xlabel('Run Number')\n    plt.ylabel('Inference Time (ms)')\n    plt.title('Inference Time Over Runs')\n    plt.legend()\n    plt.grid(alpha=0.3)\n    \n    plt.tight_layout()\n    plt.show()\n    \n    return times\nTo recal, we have the folowing converted models:\nmobilenet_v2.pte     \nmobilenet_v2_xnnpack.pte\nmobilenet_v2_quantized_xnnpack.pte\n\nBasic (Float32): mobilenet_v2.pte\n# Run benchmark\nbenchmark_times = benchmark_inference(\n    model_path=\"../models/mobilenet_v2.pte\",\n    num_runs=50\n)\n\n\n\nXNNPACK Backend (Flot32): mobilenet_v2_xnnpack.pte\n# Run benchmark\nbenchmark_times = benchmark_inference(\n    model_path=\"../models/mobilenet_v2_xnnpack.pte\",\n    num_runs=50\n)\n\n\n\nQuantization (INT8): mobilenet_v2_quantized_xnnpack.pte\n# Run benchmark\nbenchmark_times = benchmark_inference(\n    model_path=\"../models/mobilenet_v2_quantized_xnnpack.pte\",\n    num_runs=50\n)\n\n\n\nPerformance Comparison Table\nBased on actual benchmarking results on Raspberry Pi 5:\n\n\n\n\n\n\n\n\n\n\n\nModel Configuration\nMean (ms)\nMedian (ms)\nStd Dev (ms)\nFile Size (MB)\nLatency\n\n\n\n\nFloat32 (basic)\n2440\n2440\n2.17\n13.58\n+600×\n\n\nFloat32 + XNNPACK\n11.24\n10.84\n1.67\n13.35\n~3×\n\n\nINT8 + XNNPACK\n3.91\n3.69\n0.55\n3.59\n1×\n\n\n\nKey Observations:\n\nXNNPACK Impact: Backend delegation provides an important speedup even without quantization\nQuantization Benefit: INT8 quantization, besides size reduction, adds additional speedup beyond XNNPACK\nVariability: Quantized model shows lower standard deviation, indicating more stable performance\nSize-Speed Tradeoff: 75% size reduction (14MB → 3.5MB) with 3× speed improvement"
  },
  {
    "objectID": "raspi/executorch/executorch.html#exploring-custom-models",
    "href": "raspi/executorch/executorch.html#exploring-custom-models",
    "title": "Image Classification with EXECUTORCH",
    "section": "Exploring Custom Models",
    "text": "Exploring Custom Models\nCIFAR-10 Dataset:\n\n10 classes: airplane, automobile, bird, cat, deer, dog, frog, horse, ship, truck\n\n\n\n\ncifar10\n\n\n\nThe images in CIFAR-10 are of size 3x32x32 (3-channel color images of 32x32 pixels in size).\n\n\nExporting a Custom Trained Model\nLet’s create a Project folder structure as below (some files are shown as they will appear later)\nEXECUTORCH/CIFAR-10/\n├── export_cifar10_xnnpack.py\n├── inference_cifar10_xnnpack.py       \n├── models/\n│   ├── cifar10_model_jit.pt                    # Float32 pytorch model\n│   └── cifar10_xnnpack.pte                     # Float32 conv model\n├── images/                                     # Test images\n│   └── cat.jpg\n└── notebooks/                                   \n    └── CIFAR-10_Inference_RPI.ipynb\nLet’s train a model from scratch on CIFAR-10. For that, we can run the Notebook below on Google Colab:\ncifar10_colab_training.ipynb\nFrom the training, we will have the trained model:\ncifar10_model_jit.pt, which should be saved on /models folder\nNext, as we did before, we should export the PyTorch model to ExecuTorch, and let’s use XNNPACK. Run the script: export_cifar10_xnnpack.py, as a result, we have:\n\nRuning it, a converted model cifar10_xnnpack.pte will be saved in ./models/ folder.\n\n\nRunning Custom Models on Raspberry Pi\nRuning the script inference_cifar10_xnnpack.py, over the “cat” image, we can see that the converted model is working fine:\npython inference_cifar10_xnnpack.py ./images/cat.jpg\n\nAnd runing 20 times….\n\nDespite the exported model being OK, when we make an inference with the original PyTorch model, in this case (a small model), we will find even lower latencies.\n\nIn short, our export script is conceptually the right pattern for ExecuTorch+XNNPACK on Arm, but for this specific small CIFAR‑10 CNN, the overhead of ExecuTorch and partial XNNPACK delegation on a Pi‑class device can easily make it slower than a well‑optimized plain PyTorch JIT model.\nOptionally, it is possible to explore those models with the notebook:\nCIFAR-10_Inference_RPI_Updated.ipynb"
  },
  {
    "objectID": "raspi/executorch/executorch.html#conclusion",
    "href": "raspi/executorch/executorch.html#conclusion",
    "title": "Image Classification with EXECUTORCH",
    "section": "Conclusion",
    "text": "Conclusion\nThis chapter adapted our image classification workflow from TensorFlow Lite to PyTorch EXECUTORCH, demonstrating that the PyTorch ecosystem provides a powerful and modern alternative for edge AI deployment on Raspberry Pi devices.\nEXECUTORCH represents a significant evolution in edge AI deployment, bringing PyTorch’s research-friendly ecosystem to production edge devices. While TensorFlow Lite remains excellent and mature, having EXECUTORCH in your toolkit makes you a more versatile edge AI practitioner.\nThe future of edge AI is multi-framework, multi-platform, and rapidly evolving. By mastering both EXECUTORCH and TensorFlow Lite, you’re positioned to make informed technical decisions and adapt as the landscape changes.\n\nRemember: The best framework is the one that serves your specific needs. This tutorial empowers you to make that choice confidently.\n\n\nKey Takeaways\nTechnical Achievements:\n\nSuccessfully set up PyTorch and EXECUTORCH on Raspberry Pi (4/5)\nLearned the complete model export pipeline from PyTorch to .pte format\nImplemented quantization for reduced model size (~3.5MB vs ~14MB)\nCreated reusable inference functions for both standard and custom models\nIntegrated camera capture with EXECUTORCH inference\n\nEXECUTORCH Advantages:\n\nUnified ecosystem: Training and deployment in the same framework\nModern architecture: Built for contemporary edge computing needs\nFlexibility: Easy export of any PyTorch model\nQuantization: Native PyTorch quantization support\nActive development: Continuous improvements from Meta and the community\n\nComparison with TFLite: Both frameworks achieve similar goals with different philosophies:\n\nEXECUTORCH: Better for PyTorch users, newer technology, growing ecosystem\nTFLite: More mature, broader hardware support, larger community\n\nThe choice between them often comes down to your training framework and specific requirements.\n\n\nPerformance Considerations\nOn Raspberry Pi 4/5, you can expect: - Float32 models: 10-20ms per inference (MobileNet V2)\n\nQuantized models: 3-5ms per inference\nMemory usage: 4-15MB, depending on model size"
  },
  {
    "objectID": "raspi/executorch/executorch.html#resources",
    "href": "raspi/executorch/executorch.html#resources",
    "title": "Image Classification with EXECUTORCH",
    "section": "Resources",
    "text": "Resources\n\nCode Repository\n\nTutorial Code Repository\nModel Export and Inference Scripts\nNotebooks\n\n\n\nOfficial Documentation\nPyTorch & EXECUTORCH:\n\nPyTorch Official Website\nEXECUTORCH Documentation\nEXECUTORCH GitHub Repository\nPyTorch Mobile\nDeep Learning with PyTorch: A 60 Minute Blitz\n\nQuantization:\n\nPyTorch Quantization\nQuantization API Tutorial\n\nModels:\n\nTorchvision Models\nPretrained Model Deployment Guide\n\nHardware Resources\n\nRaspberry Pi Official Documentation\nPicamera2 Library\nARM Architecture Optimization\n\n\n\nBooks\n\nEdge AI Engineering e-book- by Prof. Marcelo Rovai, UNIFEI\nMachine Learning Systems - by Prof. Vijay Janapa Reddi, Harvard University\nAI and ML for Coders in PyTorch by Laurence Moroney"
  },
  {
    "objectID": "raspi/hw_acceleration/hw_acceleration.html#introduction",
    "href": "raspi/hw_acceleration/hw_acceleration.html#introduction",
    "title": "Beyond CPU - Hardware Acceleration for Edge AI",
    "section": "Introduction",
    "text": "Introduction\nThroughout this course, we’ve explored various approaches to deploying AI models at the edge. We started with TensorFlow Lite running on the Raspberry Pi’s CPU, then moved to YOLO and ExecuTorch with optimized backends like XNNPACK. While these software optimizations significantly improve performance, they still rely on the general-purpose CPU to execute neural network operations.\nIn this chapter, we’ll take the next step: dedicated hardware acceleration. We’ll use the MemryX MX3 M.2 AI Accelerator Module—a specialized processor designed specifically for neural network inference. The MX3 module contains four AI accelerator chips that can run deep learning models with dramatically lower latency and power consumption compared to CPU execution.\n\n\n\n\n\n\nWhy Hardware Acceleration?\nConsider the requirements for real-time edge AI applications: - Latency: Autonomous systems need predictions in milliseconds - Power efficiency: Battery-powered devices must conserve energy - Throughput: Multi-camera systems may need to process several streams simultaneously - Cost: System designs often cannot afford high-end GPUs\nThe MX3 addresses these challenges with a unique architecture: - At-memory computing: All memory is integrated on the accelerator, eliminating bandwidth bottlenecks - Pipelined dataflow: Optimized for streaming inputs with a batch size of 1 - Floating-point accuracy: No quantization required (though supported) - Low power: Maximum 10W for four accelerator chips\n\nFor learning more about AI Acceleration, please refer to MLSys book and how the MemryX module works, read the Architecture Overview.\n\n\n\nOur goal\nBy the end of this lab, we will have installed and configured the MX3 hardware on a Raspberry Pi 5, set up the MemryX SDK and development environment, and gained a clear understanding of the MX3 compilation and deployment workflow. We will also compile neural network models for execution on the MX3 accelerator, compare their performance against CPU-based inference while analyzing the trade-offs, and finally build a complete end-to-end inference pipeline using the MemryX Python API."
  },
  {
    "objectID": "raspi/hw_acceleration/hw_acceleration.html#hardware-installation-and-verification",
    "href": "raspi/hw_acceleration/hw_acceleration.html#hardware-installation-and-verification",
    "title": "Beyond CPU - Hardware Acceleration for Edge AI",
    "section": "Hardware Installation and Verification",
    "text": "Hardware Installation and Verification\n\nPrerequisites\nBefore starting this lab, we should have:\nRaspberry Pi 5 with M.2 HAT+ adapter (or similar)\n\n\n\n\n\n\nIMPORTANT NOTE: MemryX recommends the GeeekPi N04 M.2 2280 HAT as an excellent choice for the Raspberry Pi 5. It delivers solid power and fits the 2280 MX3 M.2 form factor. Some hats can lead to instabilities, mainly due to PCIe speed (Gen3). The Raspberry Pi 5 can have stability issues on Gen3.\n\nThe Raspberry Pi M.2 HAT+ is a good option. It works very well, despite the fact that we should adapt the MX3 board to it (The MX3 is longer than the hat).\nMemryX MX3 M.2 module with the heatsink installed\nFor heatsink installation, follow the video instructions: https://youtu.be/wNmka0nrRRE\n\n\n\n\n\n\n\nInstallation and Cooling Considerations\nIt is essential to ensure we have sufficient cooling for the MemryX MX3 M.2 module, or we may experience thermal throttling and reduced performance. The chips will throttle their performance if they hit 100 °C.\nDuring normal operation, the current MemryX MX3 temperature and throttle status can be viewed at any time with:\ncat /sys/memx0/temperature\nOr measurd continuay every 1 second, for example with the command:\nwatch -n 1 cat /sys/memx0/temperature\n\n\nVerification\nAfter installing the hardware, turn on the Raspberry Pi and verify the system setup.\nls /dev/memx*\nIt should return: /dev/memx0\nIf the device is not detected, see the Troubleshooting section below.\nLet’s also check the initial temperature:\n\n\nThe lab temperature at the time of the above measurement was 25 °C."
  },
  {
    "objectID": "raspi/hw_acceleration/hw_acceleration.html#software-installation",
    "href": "raspi/hw_acceleration/hw_acceleration.html#software-installation",
    "title": "Beyond CPU - Hardware Acceleration for Edge AI",
    "section": "Software Installation",
    "text": "Software Installation\n\nCreate Project Directory\nFirst, we should create a project directory:\ncd Documents\nmkdir MEMRYX\ncd MEMRYX\n\n\nPython Version Management with Pyenv\nVerify your Python version:\npython --version \nIf using the latest Raspberry Pi OS (based on Debian Trixie), it should be:\nPython 3.13.5\nOr, if the OS is the Legacy version:\nPython 3.11.2\nImportant: As of January 2026, MemryX officially supports only Python 3.09 to 3.12. Python 3.13.5 is too new and will likely cause compatibility issues. Since Debian Trixie ships with Python 3.13 by default, we’ll need to install a compatible Python version alongside it.\nOne solution is to install Pyenv, which allows us to easily manage multiple Python versions for different projects without affecting the system Python.\n\nIf the Raspberry Pi OS is the legacy version, the Python version should be 3.11, and it is not necessary to install Pyenv.\n\n\nInstalling Pyenv on Debian Trixie\nIf you need to install Pyenv on Debian Trixie, follow these steps:\n# Install dependencies\nsudo apt install -y make build-essential libssl-dev zlib1g-dev \\\nlibbz2-dev libreadline-dev libsqlite3-dev wget curl llvm \\\nlibncursesw5-dev xz-utils tk-dev libxml2-dev libxmlsec1-dev \\\nlibffi-dev liblzma-dev\n\n# Install pyenv\ncurl https://pyenv.run | bash\n\n# Add to ~/.bashrc\necho 'export PYENV_ROOT=\"$HOME/.pyenv\"' &gt;&gt; ~/.bashrc\necho 'command -v pyenv &gt;/dev/null || export PATH=\"$PYENV_ROOT/bin:$PATH\"' \\\n&gt;&gt; ~/.bashrc\necho 'eval \"$(pyenv init -)\"' &gt;&gt; ~/.bashrc\n\n# Reload shell\nsource ~/.bashrc\n\n# Install Python 3.11.14\npyenv install 3.11.14\nThis process takes several minutes as it compiles Python from source.\n\n\nSet Python Version for Project\nOnce Pyenv and the selected Python version are installed, define it for the project directory:\npyenv local 3.11.14\nChecking the Python version again, we should see: Python 3.11.14.\n\n\n\nMemryX Drivers and SDK Installation\nThe MemryX software stack consists of two main components:\n\nDrivers (memx-drivers): Kernel-level drivers for PCIe communication with the accelerator hardware\nSDK (memx-accl): Python libraries, neural compiler, runtime, and benchmarking tools\n\n\nPrepare the System\nInstall the Linux kernel headers required for driver compilation:\nsudo apt install linux-headers-$(uname -r)\n\n\nAdd MemryX Repository and Key\nThis command downloads the repository’s GPG key for package verification and adds the MemryX package repository:\nwget -qO- https://developer.memryx.com/deb/memryx.asc | \\\nsudo tee /etc/apt/trusted.gpg.d/memryx.asc &gt;/dev/null\n\necho 'deb https://developer.memryx.com/deb stable main' | \\\nsudo tee /etc/apt/sources.list.d/memryx.list &gt;/dev/null\n\n\nUpdate and Install Drivers and SDK\nsudo apt update\nsudo apt install memx-drivers memx-accl\n\n\nConfigure Platform Settings\nRun the ARM setup utility to configure platform-specific settings. This opens a menu to select the platform and apply the necessary configurations (e.g., enabling PCIe Gen 3.0 on the Raspberry Pi 5):\nsudo mx_arm_setup\n\nSelect the appropriate option for your hardware, and press &lt;OK&gt; in the next page:\n\nAfter configuration, reboot the system:\nsudo reboot\n\n\nVerify Driver Installation\nAfter rebooting, verify that the MemryX driver is installed by checking its version:\napt policy memx-drivers\n\n\n\nInstall Utilities\nInstall additional utilities including GUI tools and plugins:\nsudo apt install memx-accl-plugins memx-utils-gui\n\n\nPrepare System Dependencies\nInstall system libraries required for the Python SDK:\nsudo apt update\nsudo apt install libhdf5-dev python3-dev cmake python3-venv build-essential\n\n\n\nInstall Tools (Inside Virtual Environment)\nIt’s best practice to use a virtual environment to avoid conflicts with system packages.\nCreate and activate a virtual environment:\npython -m venv mx-env\nsource mx-env/bin/activate\nInside the environment, install the MemryX Python package:\npip3 install --upgrade pip wheel\npip3 install --extra-index-url https://developer.memryx.com/pip memryx\nVerify the neural compiler is installed:\nmx_nc --version\n\n\n\nVerification\nVerify the complete installation by running the built-in “hello world” benchmark:\nmx_bench --hello\n\nWith the benchmark results, our MemryX MX3 is properly installed and ready to use."
  },
  {
    "objectID": "raspi/hw_acceleration/hw_acceleration.html#our-first-accelerated-model",
    "href": "raspi/hw_acceleration/hw_acceleration.html#our-first-accelerated-model",
    "title": "Beyond CPU - Hardware Acceleration for Edge AI",
    "section": "Our First Accelerated Model",
    "text": "Our First Accelerated Model\n\nUnderstanding the MX3 Workflow\nWorking with the MemryX MX3 follows a straightforward four-step workflow that differs from traditional CPU-based inference:\n\n\n\n\n\n\nStep 1: Select or Train a Model\nStart with a pre-trained model or train your own. MemryX supports models from major frameworks:\n\nTensorFlow/Keras (.h5, SavedModel)\nONNX (.onnx)\nPyTorch (.pt, .pth) (Should be converted to ONNX first)\nTensorFlow Lite (.tflite)\n\nThe model remains in its original format—no framework-specific conversions needed yet. For this lab, we’re using MobileNetV2 from Keras Applications, but we could equally use a custom model we have trained for a specific task, as we have seen before.\nSupported Operations: The MX3 supports most common deep learning operators (convolutions, pooling, activations, etc.). Check the supported operators if using custom architectures. Unsupported operations will fall back to CPU, though this is rare for standard vision models.\n\n\nStep 2: Compile with Neural Compiler\nThe MemryX Neural Compiler (mx_nc) transforms the model into a DFP (Dataflow Package):\nmx_nc [options] -m &lt;model_file&gt;\nThe MemryX Neural Compiler, mx_nc, is a command‑line tool that takes one or more neural‑network models (Keras, TensorFlow, TFLite, ONNX, etc.) and compiles them into a MemryX Dataflow Program (DFP) that can run on MemryX accelerators (MXA). Internally, it does framework import, graph optimization (fusion/splitting, operator expansion, activation approximation), resource mapping on the MXA cores, and finally emits the DFP used by the runtime or simulator.\nWhat mx_nc does:\n\nCompiles models into a single DFP file per compilation, then loads it onto one or more MXA chips to run inference.\nSupports multi‑model, multi‑stream, and multi‑chip mapping, automatically distributing models and layers across available MX3 devices for higher throughput.\nHandles mixed‑precision weights (per‑channel 4/8/16‑bit) while keeping activations in floating point on the accelerator. By default, MemryX quantizes weights to INT8 precision and activations to BFloat16.\nCan crop pre/post‑processing parts of the graph so the MXA focuses on the core CNN/ML operators while the host CPU runs the cropped sections.\n\nWhat happens during compilation?\n\nModel parsing: Loads the model and extracts the computational graph\nGraph optimization: Fuses operations, eliminates redundancies\nOperator mapping: Maps each layer to MX3 hardware instructions\nDataflow scheduling: Determines optimal execution order for pipelined processing\nMemory allocation: Assigns on-chip memory for all intermediate activations\nMulti-chip distribution: If using multiple chips, partitions the workload\n\nThe compiler is surprisingly tolerant—most models compile without any modifications. If a layer isn’t supported, you’ll get a clear error message indicating which operation failed.\nKey command‑line options (high level)\n\nModel specification:\n\n-m / --models – input model file(s) (e.g. .h5, .pb, .onnx, TFLite).\n\nMulti‑model example: mx_nc -v -m model_0.h5 model_1.onnx.\n\nInput shapes:\n\n-is, --input_shapes – specify input shape(s) when they cannot be inferred, always in NHWC order (e.g. \"224,224,3\").\nSupports \"auto\" for models where shapes can be inferred: -is \"auto\" \"300,300,3\".\n\nCropping / pre‑ and post‑processing control:\n\n--autocrop – experimental automatic cropping of pre/post‑processing layers. See the YOLOv8 example further in this chapter.\n--inputs, --outputs – manually set which graph nodes are treated as the MXA inputs/outputs; everything outside is cropped to run on the host.\nFor multi‑model graphs, inputs/outputs of different models are separated with | (vertical bar).\n--model_in_out – JSON file describing model inputs/outputs for more complex single or multi‑model cases.\n\nMulti‑chip / system sizing:\n\n-c – number of MXA chips; -c 2 means compile for two chips and distribute workload across them.\n\nDiagnostics / verbosity:\n\n-v, -vv, etc. – increase verbosity, useful to inspect graph transformations and cropping decisions.\n\nExtensions / unsupported patterns:\n\n--extensions – load Neural Compiler Extensions (.nce files or builtin names) to add or patch graph handling (e.g., complex transformer subgraphs or unsupported ops) without a new SDK release.\n\n\n\nFor the complete option list (including less common flags), run mx_nc -h or consult the Neural Compiler page in the MemryX Developer Hub, which documents all arguments and includes usage examples for single‑model, multi‑model, cropping, and mixed‑precision flows.\n\nCompilation time varies by model complexity:\n\nSmall models (MobileNet): ~30 seconds\nMedium models (ResNet50): ~2 minutes\n\nLarge models (EfficientNet): ~5 minutes\n\nOnce compiled, the DFP file is portable across all MX3 hardware.\n\n\nStep 3: Deploy and Benchmark\nBefore integrating into our application, we can verify performance with the benchmarking tool:\nmx_bench -d &lt;dfp_file&gt; -f &lt;num_frames&gt;\nThe benchmarker: - Generates synthetic input data matching the model’s input shape - Runs warm-up inferences to stabilize performance - Measures throughput (FPS), latency, and chip utilization - Reports first-inference latency (includes loading overhead)\nWhy benchmark separately? Real-world applications involve preprocessing (image loading and resizing) and postprocessing (parsing outputs). Benchmarking isolates pure inference performance, letting to identify bottlenecks in our full pipeline.\n\n\nStep 4: Integrate into the Application\nFinally, integrate the accelerator into our Python application using the MemryX API:\nfrom memryx import SyncAccl  # or AsyncAccl for concurrent processing\n\n# Initialize accelerator with our DFP\naccl = SyncAccl(dfp=\"model.dfp\")\n\n# Run inference\noutput = accl.run(input_data)\n\n# Process results\n# ...\n\n# Clean up\naccl.shutdown()\nSynchronous vs. Asynchronous APIs:\n\nSyncAccl: Blocking calls, simple to use, good for single-stream processing\nAsyncAccl: Non-blocking, better for multi-stream or real-time applications\n\nThe API handles all hardware communication, memory transfers, and scheduling. Our code just provides input tensors and receives output tensors—the complexity is abstracted away.\n\n\nComplete Workflow Example\nLet’s see the four steps in action with MobileNetV2:\n# Step 1: Get a model (already trained)\npython3 -c \"import tensorflow as tf; \\\ntf.keras.applications.MobileNetV2().save('mobilenet_v2.h5');\"\n\n# Step 2: Compile to DFP\nmx_nc -v -m mobilenet_v2.h5\n\n# Step 3: Benchmark\nmx_bench -d mobilenet_v2.dfp -f 1000\n\n# Step 4: Integrate (see full Python script in next section)\npython run_inference_mobilenetv2.py\nThis workflow is remarkably consistent across models and use cases. Once we’ve done it for one model, adapting to others is straightforward.\n\nKey Takeaway: The MX3 workflow separates compilation (done once) from inference (done repeatedly). This “compile-once, run-many” approach means the optimization overhead is amortized over thousands or millions of inferences in production.\n\n\n\n\nDownload and Compile MobileNetV2\nIn Keras Applications, we can find deep learning models that are provided with pre-trained weights. These models can be used for prediction, feature extraction, and fine-tuning.\nLet’s download MobileNetV2, which was used in previous labs:\npython3 -c \"import tensorflow as tf; \\ \ntf.keras.applications.MobileNetV2().save('mobilenet_v2.h5');\"\nThe model is saved in the current directory as mobilenet_v2.h5.\nNext, we will compile the MobileNetV2 model using the MemryX Neural Compiler. This step verifies that both the compiler and the SDK tools are installed and functioning as expected:\nmx_nc -v -m mobilenet_v2.h5\n\n\nThe compiled model, mobilenet_v2.dfp, is saved in the current folder.\n\n\nWhat is a DFP file?\nThe .dfp (Dataflow Package) file is MemryX’s proprietary compiled format. Unlike standard model formats (H5, ONNX, etc.) that describe the network architecture, a DFP file contains:\n\nOptimized operator graph: The network restructured for dataflow execution\nMemory layout: Pre-calculated memory allocations for at-memory computing\nChip mappings: Instructions for distributing work across the four MX3 accelerators\nQuantization parameters: If applicable, the bit-width and scaling factors\n\nThe neural compiler (mx_nc) performs this transformation automatically, with no manual tuning required. The compilation process: 1. Parses the input model (H5, ONNX, TFLite, etc.) 2. Maps operators to MX3-supported operations 3. Optimizes the dataflow graph 4. Allocates memory on-chip 5. Generates the DFP binary\nThis is why compilation takes a few minutes, but inference is blazingly fast—all the optimization work happens once, upfront.\n\n\n\nBenchmarking Performance\nNow that the model is compiled, it’s time to deploy it and run a benchmark to test its performance on the MXA hardware. We will run 1000 frames of random data through the accelerator to measure performance metrics:\nmx_bench -v -d mobilenet_v2.dfp -f 1000\n\nLet’s understand what these metrics mean:\n\nFPS (Frames Per Second): How many images the accelerator can process per second (~1,200 FPS for MobileNetV2)\nLatency: Time for a single inference (shown as “Avg” in the output)\n\nSubsequent inferences: True steady-state performance (~2ms)\n\nThroughput: Total data processed per second\n\nThe benchmark runs with random input data, which is why we see consistent performance. Real-world performance with actual images should be similar once the preprocessing pipeline is optimized, but we have found bigger latency.\n\nIn true dataflow architecture, latency and FPS are not coupled in the traditional sense; latency does not equal 1/FPS. Note that even though latency is ~2ms in the above benchmarking results, FPS is not measured to be 1000 ms / 2 ms = 500 FPS; rather, the FPS from the benchmarking results is ~1160.\n\nIn MemryX’s dataflow architecture, the “usual” rule (latency = 1/FPS) only applies to frame‑to‑frame latency, not to end‑to‑end in‑to‑out latency for a single frame. That is why we see ~2 ms latency per frame, yet still measure around 1160 FPS in MX3 benchmarks.\nTwo different latencies MemryX explicitly distinguishes two metrics.\n\nLatency 1 (frame‑to‑frame latency): Time between consecutive outputs once the pipeline is full. Its reciprocal is FPS.\nLatency 2 (full in‑to‑out latency): Time from when the first input frame enters the system (host + MX3 pipeline) until its output appears. This can be larger, but it does not set the FPS.\n\nIn a streaming, pipelined accelerator like MX3, multiple frames are in‑flight simultaneously, so the pipeline “fills” once and then produces results at a steady cadence."
  },
  {
    "objectID": "raspi/hw_acceleration/hw_acceleration.html#building-an-inference-application",
    "href": "raspi/hw_acceleration/hw_acceleration.html#building-an-inference-application",
    "title": "Beyond CPU - Hardware Acceleration for Edge AI",
    "section": "Building an Inference Application",
    "text": "Building an Inference Application\nNow let’s build a complete inference application that processes real images and compares CPU vs. MX3 performance.\n\nPrepare Directory Structure\nLet’s create subdirectories for organization:\nmkdir models\nmkdir images\n\n\nDownload Test Image\nLoad an image from the internet, for example, a cat (for comparison, it is the same as used on previous chapters):\nwget \"https://upload.wikimedia.org/wikipedia/commons/3/3a/Cat03.jpg\" \\\n     -O ./images/cat.jpg\nHere is the image:\n\n\n\n\n\n\n\nUnderstanding Input Requirements\nAll neural networks expect input data in a specific format, determined during training. For MobileNetV2 trained on ImageNet:\n\nInput shape: (224, 224, 3) - RGB images at 224x224 pixels\nBatch dimension: Models expect batch inputs, so (1, 224, 224, 3) for single images\nPreprocessing: MobileNetV2 uses specific normalization (scaling pixel values to [-1, 1])\nColor channels: RGB order (not BGR)\n\nThe preprocessing must match exactly what was used during training, or accuracy will suffer.\n\n\nGetting the Labels\nFor inference, we will need the ImageNet labels. The following function checks if the file exists, and if not, downloads it:\nimport os, json\nfrom pathlib import Path\nimport requests\n\nMODELS_DIR = Path(\"./models\")\nIMAGENET_JSON = MODELS_DIR / \"imagenet_class_index.json\"\nIMAGENET_JSON_URL = (\n    \"https://storage.googleapis.com/download.tensorflow.org/data/\\\n    imagenet_class_index.json\"\n)\n\n# ---- one-time label download ----\ndef ensure_imagenet_labels():\n    MODELS_DIR.mkdir(parents=True, exist_ok=True)\n    if IMAGENET_JSON.exists():\n        return\n    print(\"Downloading ImageNet class index...\")\n    resp = requests.get(IMAGENET_JSON_URL, timeout=30)\n    resp.raise_for_status()\n    IMAGENET_JSON.write_bytes(resp.content)\n    print(\"Saved:\", IMAGENET_JSON)\nThe function load_idx2label() loads the labels into a list:\ndef load_idx2label():\n    with open(IMAGENET_JSON, \"r\") as f:\n        class_idx = json.load(f)\n    idx2label = [class_idx[str(k)][1] for k in range(len(class_idx))]\n    return idx2label\n\n\nImage Preprocessing\nThe image used for inference should be preprocessed in the same way as during model training. keras.applications.mobilenet_v2.preprocess_input() takes an image of shape (224, 224) and converts it to (1, 224, 224, 3):\nimport numpy as np\nfrom PIL import Image\nimport tensorflow as tf\nfrom tensorflow import keras\n\ndef load_and_preprocess_image(image_path):\n    img = Image.open(image_path).convert(\"RGB\").resize((224, 224))\n    arr = np.array(img).astype(np.float32)\n    arr = keras.applications.mobilenet_v2.preprocess_input(arr)\n    arr = np.expand_dims(arr, 0)  # Add batch dimension\n    return arr\n\n\nPrepare Input Tensor\nThe processed image will serve as the model’s input tensor (x):\nensure_imagenet_labels()\nidx2label_full = load_idx2label()  # length 1000 for ImageNet\n\nIMAGE_PATH = Path(\"./images/cat.jpg\")\nx = load_and_preprocess_image(IMAGE_PATH)\n\n\nRun Inference on MemryX Accelerator (MXA)\nMove the models (the original and compiled) to the models folder and set up the paths:\nMODELS_DIR = Path(\"./models\")\nDFP_PATH = MODELS_DIR / \"mobilenet_v2.dfp\"\nKERAS_PATH = MODELS_DIR / \"mobilenet_v2.h5\"\nRun inference on the compiled model using the MemryX accelerator:\nfrom memryx import SyncAccl\n\naccl = SyncAccl(dfp=str(DFP_PATH))\nmxa_outputs = accl.run(x)\nWe get a list/array of outputs. In this case, with a shape of (1, 1000) and a dtype of float32. This output should be normalized to a NumPy array:\nmxa_outputs = np.array(mxa_outputs)\nif mxa_outputs.ndim == 3:\n    mxa_outputs = mxa_outputs[0]\n\n\nDecode the MXA Results\nNow, using helper functions to extract top-k predictions:\ndef topk_from_probs(probs, k=5):\n    \"\"\"\n    probs: (1, num_classes) or (num_classes,)\n    Returns [(index, prob)] sorted by prob desc.\n    \"\"\"\n    probs = np.array(probs)\n    if probs.ndim == 2:\n        probs = probs[0]\n    # If outputs are logits, uncomment this:\n    # probs = tf.nn.softmax(probs).numpy()\n    s = probs.sum()\n    if s &gt; 0:\n        probs = probs / s\n    idxs = np.argsort(probs)[::-1][:k]\n    return [(int(i), float(probs[i])) for i in idxs]\n\ndef label_for(idx, idx2label):\n    if idx2label is not None and idx &lt; len(idx2label):\n        return idx2label[idx]\n    return f\"class_{idx}\"\nWe can decode and print the results:\nmxa_top5 = topk_from_probs(mxa_outputs, k=5)\nprint(\"\\nMXA top-5:\")\nfor idx, prob in mxa_top5:\n    name = label_for(idx, idx2label_full)\n    print(f\"  #{idx:4d}: {name:20s}  ({prob*100:.1f}%)\")\nExpected output:\nMXA top-5:\n  # 282: tiger_cat             (38.6%)\n  # 281: tabby                 (18.3%)\n  # 285: Egyptian_cat          (15.2%)\n  # 287: lynx                  (3.9%)\n  # 478: carton                (1.7%)\n\n\nComparing CPU vs. MXA Performance\nWe can also run the unconverted model (mobilenet_v2.h5) on the CPU, applying the code to the same input tensor:\ncpu_model = keras.models.load_model(KERAS_PATH)\ncpu_outputs = cpu_model.predict(x)\n\nnum_classes = cpu_outputs.shape[-1]\nidx2label = idx2label_full if num_classes == len(idx2label_full) else None\n\ncpu_top5 = topk_from_probs(cpu_outputs, k=5)\nprint(\"\\nCPU top-5:\")\nfor idx, prob in cpu_top5:\n    name = label_for(idx, idx2label)\n    print(f\"  #{idx:4d}: {name:20s}  ({prob*100:.1f}%)\")\nExpected output:\nCPU top-5:\n  # 282: tiger_cat             (58.4%)\n  # 285: Egyptian_cat          (12.9%)\n  # 281: tabby                 (11.6%)\n  # 287: lynx                  (3.4%)\n  # 588: hamper                (1.3%)\nDespite the probabilities not being identical, both models reach the same top prediction. The slight differences are due to numerical precision variations between CPU and accelerator implementations.\n\n\nMeasuring Latency\nLet’s create a complete Python script (run_inference_mobilenetv2.py) that also measures and compares latency for both CPU and MXA.\n\nNote: The following sections break down the complete inference script into logical components. The full working script is available separately and integrates all these pieces together.\n\nTo measure latency accurately, we’ll add timing code:\nimport time\n\n# Warm-up run\n_ = accl.run(x)\n\n# Timed inference\nstart = time.time()\nmxa_outputs = accl.run(x)\nmxa_latency = time.time() - start\n\nprint(f\"\\nMXA latency: {mxa_latency*1000:.2f} ms\")\nRun the complete script run_inference_comp_mobilenetv2.py in the terminal:\npython run_inference_comp_mobilenetv2.py\nExpected results:\n\nThe Accelerator runs 11 times faster than the CPU!\n\nThe MobileNet V2 running with ExecuTorch/XNNPACK backend on a CPU has around 20 ms of latency.\n\n\n\nTesting with Larger Models\nWe can also test a larger model like ResNet50:\n# Download ResNet50\npython3 -c \"import tensorflow as tf; \\\ntf.keras.applications.ResNet50().save('resnet50.h5');\"\n\n# Compile\nmx_nc -v -m resnet50.h5\n\n# Run the script in the terminal:\npython run_inference_comp_resnet50.py\n\nThe script can be found in the lab repo: run_inference_comp_resnet50.py in the terminal:\n\n\n\nThe performance improvements are even more dramatic with larger models!\n\n\n\nClean Shutdown\nAlways properly shut down the accelerator when done:\naccl.shutdown()\nBTW, to shut down the Raspberry Pi via SSH, we can use\nsudo shutdown -h now"
  },
  {
    "objectID": "raspi/hw_acceleration/hw_acceleration.html#folders-structure",
    "href": "raspi/hw_acceleration/hw_acceleration.html#folders-structure",
    "title": "Beyond CPU - Hardware Acceleration for Edge AI",
    "section": "Folders Structure",
    "text": "Folders Structure\nDocuments/MEMRYX/\n├── run_inference_comp_mobilenetv2.py          # MobileNetV2 script\n├── run_inference_comp_resnet50.py             # ResNet50 script\n├── images/\n│   ├── cat.jpg                           # Test image\n├── models/\n│   ├── mobilenet_v2.h5                   # Original model\n│   ├── mobilenet_v2.dfp                  # Compiled model\n│   ├── resnet50.h5                       # Original model\n│   ├── resnet50.dfp                      # Compiled model\n│   └── imagenet_class_index.json         # Labels (auto-downloaded)\n└── mx-env/"
  },
  {
    "objectID": "raspi/hw_acceleration/hw_acceleration.html#performance-comparison-summary",
    "href": "raspi/hw_acceleration/hw_acceleration.html#performance-comparison-summary",
    "title": "Beyond CPU - Hardware Acceleration for Edge AI",
    "section": "Performance Comparison Summary",
    "text": "Performance Comparison Summary\nHere’s how the MX3 compares across different deployment approaches we’ve covered in this course:\n\n\n\n\n\n\n\n\n\n\nApproach\nHardware\nMobileNetV2 Latency\nResNet50 Latency\nPower (Active)\n\n\n\n\nTFLite (CPU)\nRaspberry Pi 5\n~110 ms\n~266 ms\n~\n\n\nExecuTorch/XNNPACK\nRaspberry Pi 5\n~20 ms\nNA\n~\n\n\nMemryX MX3\nDedicated accelerator\n~10 ms\n~10 ms\n~\n\n\n\n\nKey Observations\n\n25x faster than unoptimized TFLite on CPU (Resnet50)\n2x faster than highly optimized ExecuTorch with XNNPACK (MobileNetV2)\nMinimal CPU load: The host CPU is free for preprocessing, postprocessing, and application logic\nConsistent latency: Hardware acceleration provides deterministic performance\nPower efficiency: Not measured\n\n\n\nWhen to Use the MX3?\nThe MemryX MX3 is ideal for:\n\n✅ Real-time applications requiring &lt;20ms latency\n✅ Multi-stream processing (multiple cameras, sensors)\n✅ Power-constrained environments where CPU load matters\n✅ Production deployments requiring consistent, predictable performance\n✅ Complex models where CPU inference is too slow\n\nThe MX3 may be overkill for:\n\n❌ Simple models that run fast enough on CPU\n❌ Non-latency-critical batch processing\n❌ Prototyping where development speed matters more than performance\n❌ Very cost-sensitive applications"
  },
  {
    "objectID": "raspi/hw_acceleration/hw_acceleration.html#yolov8-object-detection-with-mx3-hardware-acceleration",
    "href": "raspi/hw_acceleration/hw_acceleration.html#yolov8-object-detection-with-mx3-hardware-acceleration",
    "title": "Beyond CPU - Hardware Acceleration for Edge AI",
    "section": "YOLOv8 Object Detection with MX3 Hardware Acceleration",
    "text": "YOLOv8 Object Detection with MX3 Hardware Acceleration\nIn this part of the lab, we’ll deploy YOLOv8n (nano) for real-time object detection on the Raspberry Pi 5 using the MemryX MX3 AI accelerator. We’ll cover the complete workflow from model export to inference optimization.\n\nBut first, we should install ULTRALYTICS\nThe MemoryX team suggested:\npip install ultralytics==8.3.161\nIf you face issues, try it with:\npip install ultralytics[export]\nIf you still have issues, reinstall memryx\npip3 install --extra-index-url https://developer.memryx.com/pip memryx\nNow, download the YOLOv8.pt model and test it:\nyolo predict model='yolov8n' source='https://ultralytics.com/images/bus.jpg'\nThe model and the image bus.jpg will be download and tested with the YOLOV8n:\n\n\n4 persons, 1 bus, and one stop signal were detected in 522 ms.\n\nUnder ./runs/detect, the output processed image can be analysed:\n\n\n\n\n\n\nModel Export and Compilation\n\nStep 1: Export YOLOv8 to ONNX\nYOLOv8 must be converted to ONNX format before compilation for the MX3:\nfrom ultralytics import YOLO\n\n#### Load pretrained YOLOv8n model\nmodel = YOLO('yolov8n.pt')\n\n#### Export to ONNX format\nmodel.export(format='onnx', simplify=True)\nThis creates yolov8n.onnx with the complete model graph.\nWe can also use CLI for the conversion:\nyolo export model=yolov8n.pt format=onnx\n\n\n\nStep 2: Compile for MX3\nWe can use the MemryX Neural Compiler to generate the DFP file:\nmx_nc -v --autocrop -m yolov8n.onnx\nKey flags:\n\n-v: Verbose output for debugging\n-m: Input model path\n--autocrop: Automatically split model for optimal MX3 execution\n\nOutput files:\n\nyolov8n.dfp - Accelerator executable (runs on MX3 chips)\nyolov8n_post.onnx - Post-processing model (runs on CPU)\n\nWhy Two Files?\nThe MX3 compiler splits the model:\n\nFeature extraction (yolov8n.dfp): Neural network backbone on MX3 hardware\nDetection head (yolov8n_post.onnx): Bounding box decoding on CPU\n\nThis hybrid approach optimizes performance by running computationally intensive operations on the accelerator while keeping final post-processing flexible.\n\nNow, let’s check the model with a benchmark\nmx_bench -d yolov8n.dfp -f 1000\n\n\n\n\nUnderstanding YOLOv8 Output Format\n\nYOLOv8 Output Structure\nThe post-processing model outputs predictions in shape (1, 84, 8400):\n\nDimension 0 (1): Batch size\nDimension 1 (84):\n\nFirst 4 values: Bounding box [x_center, y_center, width, height]\nNext 80 values: Class probabilities (COCO dataset)\n\nDimension 2 (8400): Anchor points across three detection scales:\n\n80×80 = 6400 points (small objects)\n40×40 = 1600 points (medium objects)\n20×20 = 400 points (large objects)\n\n\n\n\nDecoding Process\n\nTranspose: Convert from (1, 84, 8400) → (8400, 84)\nExtract: Separate boxes and class scores\nFilter: Keep predictions above confidence threshold\nConvert coordinates: Transform from xywh to xyxy\nNMS: Remove overlapping detections\nScale: Map to original image coordinates\n\n\n\n\nComplete Inference Pipeline\nWe should now create a script to run an object detector (YOLOv8 with a pre/post-processing pipeline), print each detection (label, confidence, bounding box), and save a copy of the image with the boxes drawn.\n\nConfiguration section\nAt the top of __main__ we define the configuration values:\nDFP_PATH = \"./models/yolov8n.dfp\"\nPOST_MODEL_PATH = \"./models/yolov8n_post.onnx\"\nIMAGE_PATH = \"./images/bus.jpg\"\nCONF_THRESHOLD = 0.25\n\nDFP_PATH: path to the compiled model used for inference.\n\nPOST_MODEL_PATH: path to a post-processing model or graph, usually converting raw outputs into boxes, scores, and class IDs.\n\nIMAGE_PATH: image file we want to run detection on.\n\nCONF_THRESHOLD: minimum confidence score; detections below this are filtered out.\n\n\n\nRunning detection\ndetections, annotated_image, inference_time = detect_objects(\n    DFP_PATH,\n    POST_MODEL_PATH,\n    IMAGE_PATH,\n    CONF_THRESHOLD\n)\nHere we call a helper function detect_objects that encapsulates the heavy lifting:\n\nLoads the model(s).\n\nLoads and preprocesses the image.\n\nRuns inference.\n\nApplies post-processing (NMS, thresholding, etc.).\n\nReturns:\n\ndetections: list/array where each element is [x1, y1, x2, y2, conf, class_id].\n\nannotated_image: a PIL image with bounding boxes and labels drawn.\n\ninference_time: time spent doing the detection (in milliseconds).\n\n\n\n\nPrinting results\nprint(f\"\\n{'='*60}\")\nprint(\"Detection Results:\")\nprint(f\"{'='*60}\")\nfor i, det in enumerate(detections):\n    x1, y1, x2, y2, conf, class_id = det\n    print(f\"  {i+1}. {COCO_CLASSES[int(class_id)]}: {conf:.3f}\")\n    print(f\"      Box: [{int(x1)}, {int(y1)}, {int(x2)}, {int(y2)}]\")\n\nThe loop goes over each detection, unpacks the bounding box coordinates, confidence, and class ID.\n\nCOCO_CLASSES[int(class_id)] converts the numeric class index into a human-readable label (e.g., “person”, “bus”).\n\nCoordinates are cast to int for cleaner printing.\n\n\n\nSaving the annotated image\nif len(detections) &gt; 0:\n    output_path = IMAGE_PATH.rsplit('.', 1)[0] + '_detected.jpg'\n    annotated_image.save(output_path)\n    print(f\"\\nSaved: {output_path}\")\n\nOnly saves an output file if at least one object was detected.\n\nThe output filename is built by taking the original name and appending _detected before the extension (e.g., bus_detected.jpg).\n\nannotated_image.save(...) writes the image with drawn boxes and labels to disk.\n\n\n\nFinal summary output\nprint(f\"\\n{'='*60}\")\nprint(f\"Total: {len(detections)} objects\")\nprint(f\"Time: {inference_time:.2f} ms\")\nprint(f\"{'='*60}\")\n\nPrints how many objects were found in total.\n\nPrints the inference time, which is useful to talk about performance (e.g., model size vs. speed, hardware differences).\n\nRun the script yolov8_m3_detect.py\npython yolov8_m3_detect.py\nAs a result, we can see that the models found 4 persons and 1 bus, missing only the stop signal. Regarding latency, the MX3 runs inference about 11 times faster than a CPU-only system.\n\nBasically, the same accuracy result that we got on the YOLO chapter running yolov11\n\n\n\n\n\n\n\n\n\nGoing deeper in the functions\n\n1. Image preprocessing (preprocess_image)\noriginal image → resized → padded → tensor\n\nLoad and normalize:\n\nOpen the image with PIL, convert to RGB, and get its original size.\nCompute a scale ratio so the image fits into 640×640 without distortion (preserving aspect ratio).\n\nLetterboxing:\n\nResize the image to (new_w, new_h) = (int(w * ratio), int(h * ratio)).\nPaste it onto a 640×640 canvas filled with color (114, 114, 114) (same as Ultralytics).\n\nCompute the padding offsets (pad_w, pad_h) so we can undo this later.\n\nTensor conversion:\n\nConvert to numpy, normalize to [0,1], permute from HWC to CHW, and add a batch dimension to get shape [1, 3, 640, 640], which matches YOLOv8’s expected input.[1][2]\n\n\n\n\n2. Decoding YOLOv8 output (decode_predictions)\nHow to turn raw model numbers into human-readable detections.\nThe raw output format:\n\nFor COCO YOLOv8n ONNX, the detection head outputs a tensor of shape (1, 84, 8400).\n84 = 4 (bbox) + 80 (class scores). Each of the 8400 positions corresponds to one candidate box.\n\nFunction Walkthrough:\n\nTranspose:\n\nFrom (1, 84, 8400) to (8400, 84) so each row is: [x_center, y_center, width, height, class_0_score, ..., class_79_score].\n\nBest class per box:\n\nTake max_scores = np.max(class_scores, axis=1) and class_ids = np.argmax(class_scores, axis=1) to select the most likely class and its score for each of the 8400 candidates.\n\nConfidence filtering:\n\nDrop boxes whose max class score is below conf_threshold.\n\nCoordinate conversion:\n\nConvert from YOLO’s center-format (x, y, w, h) to corner-format (x1, y1, x2, y2) to make drawing and IoU calculation simpler.\n\nNMS:\n\nCall apply_nms to remove overlapping boxes and keep only the best ones.\n\n\n\n\n3. IoU and NMS (compute_iou_batch and apply_nms)\nIoU:\n\nIoU (Intersection over Union) measures overlap between two boxes:\n\\[\\text{IoU} = \\frac{\\text{area of intersection}}{\\text{area of union}}\\]\ncompute_iou_batch does this between one box and many boxes at once using vectorized Numpy operations.\n\nNMS:\n\napply_nms:\n\nSort boxes by score descending.\nRepeatedly pick the highest-score box, compute its IoU with the remaining boxes, and discard those whose IoU is above iou_threshold.\n\nThe result is a list of indices for boxes that don’t overlap too much and represent unique objects.\n\n\n\n4. Mapping back to the original image (scale_boxes_to_original)\nEverything after the model must undo what preprocessing did.\n\nDuring preprocessing we:\n\nRescaled the image by ratio.\nPadded by (pad_w, pad_h).\n\nThe model’s boxes live in that padded, resized 640×640 space.\nscale_boxes_to_original:\n\nSubtracts the padding.\nDivides by ratio to go back to the original resolution.\nClips coordinates so they stay inside the original image bounds.\n\n\n\n\n5. Drawing results (draw_detections)\nmodel output → decoded boxes → drawn on the original image\n\nMake a copy of the original image and get an ImageDraw context.\nFor each detection:\n\nChoose a color deterministically using np.random.seed(class_id) so the same class always has the same color.\nDraw the rectangle [x1, y1, x2, y2].\nBuild a label string \"class_name: confidence\", measure text size using textbbox, draw a filled rectangle for the label background, and render the text.\n\n\n\n\n6. The Memryx pipeline (detect_objects and AsyncAccl usage)\nHow to integrate Memryx’s async accelerator into a typical vision pipeline\n\nPreprocess once: call preprocess_image to get the model-ready tensor and the info needed for rescaling.\nCreate the accelerator:\n\naccl = AsyncAccl(dfp_path) loads the compiled Memryx DFP model.\naccl.set_postprocessing_model(post_model_path, model_idx=0) attaches the ONNX post-processing graph.\n\nStreaming-style design:\n\nframe_queue is a queue of inputs; you put your tensor in it.\ngenerate_frame is a generator feeding frames into the accelerator.\nprocess_output is a callback that collects outputs into results.\nThe code wires them with connect_input and connect_output, then waits for completion with accl.wait().\n\nPost-processing:\n\nGrab the first output, call decode_predictions, rescale boxes, and draw.\n\n\n\n\n\nMaking Inferences\nLet’s change the script to easily handle different images and confidence threshold (yolov8_m3_detect_v2.py). We should replace the hardcoded IMAGE_PATH with a command-line argument:\nimport argparse\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\n        \"-i\", \"--image\",\n        type=str,\n        required=True,\n        help=\"Path to input image\"\n    )\n    parser.add_argument(\n        \"-c\", \"--conf\",\n        type=float,\n        default=0.25,\n        help=\"Confidence threshold\"\n    )\n    args = parser.parse_args()\n\n    # Configuration\n    DFP_PATH = \"./models/yolov8n.dfp\"\n    POST_MODEL_PATH = \"./models/yolov8n_post.onnx\"\n    IMAGE_PATH = args.image\n    CONF_THRESHOLD = args.conf\n\n    # Run detection\n    detections, annotated_image, inference_time = detect_objects(\n        DFP_PATH,\n        POST_MODEL_PATH,\n        IMAGE_PATH,\n        CONF_THRESHOLD\n    )\n\n    # Print results\n    print(f\"\\n{'='*60}\")\n    print(\"Detection Results:\")\n    print(f\"{'='*60}\")\n    for i, det in enumerate(detections):\n        x1, y1, x2, y2, conf, class_id = det\n        print(f\"  {i+1}. {COCO_CLASSES[int(class_id)]}: {conf:.3f}\")\n        print(f\"      Box: [{int(x1)}, {int(y1)}, {int(x2)}, {int(y2)}]\")\n\n    # Save annotated image\n    if len(detections) &gt; 0:\n        output_path = IMAGE_PATH.rsplit('.', 1)[0] + '_detected.jpg'\n        annotated_image.save(output_path)\n        print(f\"\\nSaved: {output_path}\")\n\n    print(f\"\\n{'='*60}\")\n    print(f\"Total: {len(detections)} objects\")\n    print(f\"Time: {inference_time:.2f} ms\")\n    print(f\"{'='*60}\")\nWe can run it as:\npython yolov8_mx3_detect_v2.py --image ./images/home-office.jpg -c 0.2\nHere are some results with other images:"
  },
  {
    "objectID": "raspi/hw_acceleration/hw_acceleration.html#inference-with-a-custom-model",
    "href": "raspi/hw_acceleration/hw_acceleration.html#inference-with-a-custom-model",
    "title": "Beyond CPU - Hardware Acceleration for Edge AI",
    "section": "Inference with a custom model",
    "text": "Inference with a custom model\nAs we saw in the YOLO chapter, we are assuming we are in an industrial facility that must sort and count wheels and special boxes.\n\n\n\n\n\nEach image can have three classes:\n\nBackground (no objects)\nBox\nWheel\n\nWe have captured a raw dataset using the Raspberry Pi Camera and labeled it with the ROBOFLOW. The Yolo model was trained on a Google Colab using Ultralytics.\n\nAfter training, we download the trained model from /runs/detect/train/weights/best.pt to our computer, renaming it to box_wheel_320_yolo.pt.\n\nUsing the FileZilla FTP, transfer a few images from the test dataset to .\\images:\n\nLet’s return to the ./MEMRYX/YOLO folder and using the Python Interpreter, to quickly do some inferences:\npython\nWe will import the YOLO library and define the model to use:\n&gt;&gt;&gt; from ultralytics import YOLO\n&gt;&gt;&gt; model = YOLO('./models/box_wheel_320_yolo.pt')\nNow, let’s define an image and call the inference (we will save the image result this time to external verification):\n&gt;&gt;&gt; img = './images/box_3_wheel_4.jpg'\n&gt;&gt;&gt; result = model.predict(img, save=True, imgsz=320, conf=0.5, iou=0.3)\n\n\n\n\n\n\nWe can see that the model is working and that the latency was 168 ms.\nLet’s now export the model first to ONNX and after to FFPls\n, to run it in the MX3 device:\ncd ./models\nyolo export model=box_wheel_320_yolo.pt format=onnx\nmx_nc -v --autocrop -m box_wheel_320_yolo.onnx\ncd ..\nIn the models folder, we will have box_wheel_320_yolo.dfp and box_wheel_320_yolo_post.onnx\nLet’s adapt the previous script to be more generic in terms of models (box_wheel_mx3_detect_v2.py):\n\nNaturally we should enter with the new models ’names and instead of COCO_LABELS, the script was changed to:\n# dataset class names \nCLASSES = [\n    'Box', 'Wheel'\n]\nThant’s all!\n\nRun it with:\npython box_wheel_mx3_detect_v2.py --image ./images/box_3_wheel_4.jpg\n\n\n\n\n\nThe Result was great! And the latency (~38 ms) was 4 times lower than with the CPU-only approach (even smaller than the model exported to NCNN, runing 100% at CPU - 80 ms).\n\nAdjusting Confidence Threshold\nLower confidence for more detections (may include false positives):\npython box_wheel_mx3_detect_v2.py --image ./images/box_3_wheel_4.jpg -c 0.15\nWe should experiment with the right confidence threshold."
  },
  {
    "objectID": "raspi/hw_acceleration/hw_acceleration.html#advanced-topics",
    "href": "raspi/hw_acceleration/hw_acceleration.html#advanced-topics",
    "title": "Beyond CPU - Hardware Acceleration for Edge AI",
    "section": "Advanced Topics",
    "text": "Advanced Topics\n\nBatch Processing (Optimization)\nFor multiple images, reuse the accelerator instance:\naccl = AsyncAccl(dfp_path)\naccl.set_postprocessing_model(post_model_path)\n\nfor image_path in image_list:\n    detections = detect_single_image(accl, image_path)\n\n\nThermal Management\nAlways monitor temperature during operation:\nwatch -n 1 cat /sys/memx0/temperature\n\n\nConfidence Threshold Tuning\n\n0.15-0.20: Maximum recall (catch everything)\n0.25-0.40: Balanced (default)\n0.45-0.50: High precision (only confident detections)\n\n\n\nModel Selection\n\nyolov8n: Fastest, 3.2M parameters\nyolov8s: Balanced, 11.2M parameters\nyolov8m: Accurate, 25.9M parameters"
  },
  {
    "objectID": "raspi/hw_acceleration/hw_acceleration.html#exploring-memryx-examples",
    "href": "raspi/hw_acceleration/hw_acceleration.html#exploring-memryx-examples",
    "title": "Beyond CPU - Hardware Acceleration for Edge AI",
    "section": "Exploring MemryX eXamples",
    "text": "Exploring MemryX eXamples\nMemryX eXamples is a collection of end-to-end AI applications and tasks powered by MemryX hardware and software solutions. These examples provide practical, hands-on use cases to help leverage MemryX technology.\n\nClone the MemryX eXamples Repository\nClone this repository plus any linked submodules:\ngit clone --recursive https://github.com/memryx/memryx_examples.git\ncd memryx_examples\nAfter cloning the repository, you’ll find several subdirectories with different categories of applications:\n\nimage_inference - Single image classification and detection\nvideo_inference - Real-time video processing\nmultistream_video_inference - Multi-camera scenarios\naudio_inference - Audio processing and speech recognition\nopen_vocabulary - Open-set classification tasks\naccuracy_calculation - Model accuracy verification\nmulti_dfp_application - Running multiple models\noptimized_multistream_apps - Production-ready multi-stream examples\nfun_projects - Creative applications and demos\n\nThese examples demonstrate best practices for:\n\nPreprocessing pipelines\nMulti-threaded inference\nOutput visualization\nPerformance optimization\nMulti-model orchestration\n\nExploring these examples is an excellent way to learn production-ready patterns for deploying MemryX applications."
  },
  {
    "objectID": "raspi/hw_acceleration/hw_acceleration.html#troubleshooting-common-issues",
    "href": "raspi/hw_acceleration/hw_acceleration.html#troubleshooting-common-issues",
    "title": "Beyond CPU - Hardware Acceleration for Edge AI",
    "section": "Troubleshooting Common Issues",
    "text": "Troubleshooting Common Issues\n\nDevice Not Detected\nSymptom: ls /dev/memx* returns “No such file or directory”\nSolutions:\n\nVerify physical connection: Reseat the M.2 module in its slot\nCheck PCIe settings:\n\nsudo raspi-config\n# Navigate to: Advanced Options → PCIe Speed → Enable PCIe Gen 3\nsudo reboot\n\nVerify in kernel logs:\n\ndmesg | grep -i memryx\nlspci | grep -i memryx\n\n\nEnsure sufficient power: Use the official Raspberry Pi 27W power supplyCheck HAT installation: Ensure the M.2 HAT is properly seated.\nLower Frequency: Try running sudo mx_set_powermode with a lower frequency, such as 200 or 300 MHz. Then restart mxa-manager for good measure with sudo service mxa-manager restart\n\nsudo mx_set_powermode\n\nsudo service mxa-manager restart\nCheck the frequency with the following command:\ncat /etc/memryx/power.conf\n\nWe can check it by the first field of the file, FREQ4. If the Raspberry Pi is set to the module’s default operating frequency (500 MHz), we should see FREQ4C=500, indicating that the module is set to a 500 MHz clock speed for 4-chip DFPs.\n\nIf decreasing the frequency solves the issue, then you can either keep the default frequency for all DFPs at 300 MHz (or 400, 450, etc.), or you can raise it back to 500 MHz and use the C++ API’s set_operating_frequency function to change the clock speed on a per-DFP basis.\n\n\n\nCompilation Errors\nSymptom: mx_nc fails with “Unsupported operator” error\nSolutions: - Check the supported operators\n\nSome custom layers may need reformulation\nTry exporting to ONNX first for better compatibility:\n\nimport tensorflow as tf\nimport tf2onnx\n\nmodel = tf.keras.models.load_model('model.h5')\nonnx_model, _ = tf2onnx.convert.from_keras(model)\nwith open(\"model.onnx\", \"wb\") as f:\n    f.write(onnx_model.SerializeToString())\n\nCheck the compilation log (-v flag) to identify which specific layer is causing issues\n\n\n\nThermal Throttling\nSymptom: Performance degrades over time, temperature &gt; 90°C\nSolutions:\n\nVerify heatsink installation: Ensure thermal paste is properly applied and heatsink is firmly attached\nImprove airflow: Position the Raspberry Pi for better air circulation\nCheck ambient temperature: Ensure the room temperature is reasonable (&lt;30°C)\nMonitor continuously:\n\nwatch -n 1 cat /sys/memx0/temperature\n= Consider additional cooling: Add a small fan directed at the heatsink\n\n\nPython Version Conflicts\nSymptom: pip install memryx fails with compatibility errors\nSolutions:\n\nVerify Python version:\n\npython --version  # Must show 3.11.x or 3.12.x\npip --version     # Should match the Python version\n\nEnsure you’re in the virtual environment:\n\nwhich python  # Should point to mx-env/bin/python\n\nTry reinstalling in a fresh virtual environment:\n\ndeactivate\nrm -rf mx-env\npython -m venv mx-env\nsource mx-env/bin/activate\npip install --upgrade pip wheel\npip install --extra-index-url https://developer.memryx.com/pip memryx\n\n\nLow FPS / Poor Performance\nSymptom: Benchmark shows much lower FPS than expected\nSolutions:\n\nCheck for thermal throttling:\n\ncat /sys/memx0/temperature  # Should be &lt;80°C\n\nVerify PCIe Gen 3 is enabled (not Gen 2):\n\nsudo raspi-config\n# Advanced Options → PCIe Speed\n\nClose other PCIe-intensive applications: Ensure no other devices are saturating the PCIe bus\nCheck for background CPU load:\n\nhtop\n\nVerify driver version: Ensure you have the latest drivers\n\napt policy memx-drivers\n\nVerify Frequency\n\n​ By default, the frequency should be at 500 MHz. Smaller frequencies will reduce the FPS (increase the latency)\ncat /etc/memryx/power.conf\n\n\nImport Errors\nSymptom: ImportError: cannot import name 'SyncAccl' from 'memryx'\nSolutions:\n\nEnsure memryx is installed in the current environment:\n\npip list | grep memryx\n\nReinstall if necessary:\n\npip install --force-reinstall --extra-index-url \\\nhttps://developer.memryx.com/pip memryx\n\nCheck Python path conflicts:\n\nimport sys\nprint(sys.path)\n\n\nModel Accuracy Issues\nSymptom: Inference results are incorrect or significantly different from CPU\nSolutions: - Verify preprocessing: Ensure the same preprocessing is applied as during training\n\nCheck input normalization: Confirm the value range matches training (e.g., [0, 1] vs [-1, 1])\nTest with known inputs: Use the validation dataset to verify accuracy\nCompare outputs numerically: Print raw logits/probabilities to identify differences\nCheck for quantization effects: If using -q flag, try without quantization first"
  },
  {
    "objectID": "raspi/hw_acceleration/hw_acceleration.html#next-steps-and-extensions",
    "href": "raspi/hw_acceleration/hw_acceleration.html#next-steps-and-extensions",
    "title": "Beyond CPU - Hardware Acceleration for Edge AI",
    "section": "Next Steps and Extensions",
    "text": "Next Steps and Extensions\n\nProject Ideas\n\nReal-time Object Detection with Camera\n\nIntegrate picamera2 with YOLO\nDisplay bounding boxes in real-time\nMeasure end-to-end latency (capture → inference → display)\n\nMulti-Model Pipeline\n\nUse detection + classification cascade\nLeverage multiple MX3 chips for parallel inference\nBuild a smart surveillance system\n\nCustom Model Deployment\n\nTrain your own model for a specific task\nOptimize and compile for MX3\nCompare against the models from previous labs\n\nPower Efficiency Study\n\nMeasure power consumption with a USB meter\nCompare CPU vs. MX3 energy per inference\nCalculate battery life for mobile applications\n\nMulti-Stream Processing\n\nProcess multiple camera streams simultaneously\nDemonstrate chip utilization across streams\nBuild a multi-camera monitoring system\n\n\n\n\nAdvanced Topics to Explore\n\nQuantization: Experiment with 8-bit and 4-bit quantization for even better performance\nModel Zoo: Explore pre-optimized models in the MemryX Model Explorer\nAsync API: Use AsyncAccl for non-blocking, concurrent processing\nCustom Operators: Learn to handle models with custom layers\nMulti-chip Scaling: Understand how workload distributes across the four accelerators"
  },
  {
    "objectID": "raspi/hw_acceleration/hw_acceleration.html#conclusion",
    "href": "raspi/hw_acceleration/hw_acceleration.html#conclusion",
    "title": "Beyond CPU - Hardware Acceleration for Edge AI",
    "section": "Conclusion",
    "text": "Conclusion\nIn this lab, we’ve explored hardware acceleration for edge AI using the MemryX MX3 accelerator. We’ve learned:\n\n✅ How to install and configure the MX3 hardware\n✅ The MX3 compilation and deployment workflow\n✅ How to benchmark and measure performance\n✅ Building complete inference applications\n✅ Comparing CPU vs. dedicated accelerator performance\n\nThe MX3 demonstrates that dedicated AI accelerators can deliver significant performance improvements for edge applications, achieving FPS several times higher (up to 25x for ResNet-50) than CPU inference while maintaining accuracy and providing deterministic latency.\nAs edge AI continues to evolve, hardware acceleration will become increasingly important for real-time, power-efficient deployments. The skills we’ve developed in this lab—understanding the compilation workflow, benchmarking methodologies, and performance optimization—will transfer to other accelerator platforms as well."
  },
  {
    "objectID": "raspi/hw_acceleration/hw_acceleration.html#references-and-further-reading",
    "href": "raspi/hw_acceleration/hw_acceleration.html#references-and-further-reading",
    "title": "Beyond CPU - Hardware Acceleration for Edge AI",
    "section": "References and Further Reading",
    "text": "References and Further Reading\n\nOfficial Documentation\n\nMemryX Developer Hub\nMX3 Product Brief\nArchitecture Overview\nSupported Operators\n\n\n\nCode and Examples\n\nMemryX eXamples Repository\nMemryX GitHub Organization\nModel eXplorer\nLab Code Repo\n\n\n\nBackground Reading\n\nMLSys Book - Hardware Acceleration\nRaspberry Pi PCIe Documentation\n\n\n\nCommunity and Support\n\nMemryX YouTube Channel\nMemryX Support Portal"
  },
  {
    "objectID": "raspi/rnn-verne/rnn-verne.html#introduction",
    "href": "raspi/rnn-verne/rnn-verne.html#introduction",
    "title": "Text Generation with RNNs",
    "section": "Introduction",
    "text": "Introduction\nIn this chapter, we will explore how to build a character-level text generation model using Recurrent Neural Networks (RNNs), specifically inspired by the works of Jules Verne.\nThe “Jules Verne Bot” project will help to show us the fundamental concepts of sequence modeling and text generation using deep learning techniques, as a preview of how the modern LLMs work.\nProject Overview:\n\nGoal: Create an AI model that generates text in the style of Jules Verne\nArchitecture: RNN with GRU (Gated Recurrent Unit) layers\nApproach: Character-level text prediction\nFramework: TensorFlow/Keras\nPlatform: Google Colab with Tesla T4 GPU\n\n\nWhat Are We Actually Building?\nImagine we could teach a computer to write like Jules Verne, the famous author of “Twenty Thousand Leagues Under the Sea” and “Around the World in Eighty Days.” That’s precisely what we’re doing with the Jules Verne Bot. This project creates an artificial intelligence system that learns the patterns, style, and vocabulary from Jules Verne’s novels, then generates new text that sounds like it could have come from his pen.\nThink of it like this: if we read enough of someone’s writing, we start to recognize their style. We notice they use certain phrases, prefer specific sentence structures, or have favorite topics. Our neural network does something similar, but with mathematical precision. It analyzes millions of characters from Verne’s works and learns to predict what character should come next in any given sequence."
  },
  {
    "objectID": "raspi/rnn-verne/rnn-verne.html#neural-network-architectures-background",
    "href": "raspi/rnn-verne/rnn-verne.html#neural-network-architectures-background",
    "title": "Text Generation with RNNs",
    "section": "Neural Network Architectures Background",
    "text": "Neural Network Architectures Background\nBefore we dive into the technical details, let’s understand why we use neural networks for this task and why we chose the specific type we did.\n\nThe Human Brain Analogy\nWhen you read a sentence like “The submarine descended into the dark…” your brain automatically starts predicting what might come next. Maybe “depths” or “ocean” or “waters.” Your brain does this because it has learned patterns from all the text you’ve ever read. Neural networks work similarly, but they learn these patterns through mathematical calculations rather than biological processes.\n\n\nRecurrent Neural Networks (RNN)\nBefore diving into our RNN implementation, let’s understand where RNNs fit in the neural network ecosystem:\nKey Neural Network Architectures:\n\n\nMLP (Multi-Layer Perceptron): Basic feedforward networks for general tasks, for example, vibration analysis\nCNN (Convolutional Neural Networks): Specialized for image processing as Image Classification tasks and spatial data\nRNN (Recurrent Neural Networks): Designed for sequential data like text and time series\nGAN (Generative Adversarial Networks): Two networks competing for realistic data generation, as images\nTransformers (Attention Networks): Modern architecture using attention mechanisms, as in LLMs (Large Language Models, such as GPT)\n\nWe chose a Recurrent Neural Network (RNN) for this project because text has a crucial property: order matters tremendously. The sequence “The cat sat on the mat” means something completely different from “Mat the on sat cat the.” Regular neural networks process all inputs simultaneously, like looking at a photograph. But for text, we need a network that processes information sequentially, remembering what came before to understand what should go next.\n\nIn text generation, we aim to predict the most probable word to follow a sentence.\n\n\nThink of reading a book. You don’t just look at all the words on a page simultaneously. You read word by word, sentence by sentence, and your understanding builds as you progress. Each new word is interpreted in the context of everything you’ve read before in that chapter. RNNs work the same way.\n\n\nThe Memory Problem and GRU Solution\nEarly RNNs had a significant problem: they couldn’t remember information for very long. Imagine trying to understand a story where you could only remember the last few words you read. You’d lose track of characters, plot points, and context very quickly.\nThis is where the Gated Recurrent Unit (GRU) comes in. Think of GRU as an improved memory system with two special abilities:\nReset Gate: This decides when to “forget” old information. If the story switches to a new scene or character, the reset gate helps the network forget irrelevant details from the previous context.\nUpdate Gate: This decides how much new information to incorporate. When encountering important plot points or character names, the update gate helps the network remember these crucial details for longer.\nIt’s like having a smart note-taking system that automatically decides what’s worth remembering and what can be forgotten.\nWhy RNNs for Text Generation?\nRecurrent Neural Networks are designed explicitly for sequential data processing. Key characteristics:\n\nMemory: RNNs maintain an internal state (memory) to remember previous inputs\nSequential Processing: Process data one element at a time, making them ideal for text\nVariable Length Input: Can handle sequences of different lengths\nParameter Sharing: Same weights applied across different time steps\n\nRNN Architecture Flow:\n\nInput Sequence: x(t-1) → x(t) → x(t+1) → ...\nHidden State:   h(t-1) → h(t) → h(t+1) → ...\nOutput:         o(t-1) → o(t) → o(t+1) → ..."
  },
  {
    "objectID": "raspi/rnn-verne/rnn-verne.html#dataset-preparation",
    "href": "raspi/rnn-verne/rnn-verne.html#dataset-preparation",
    "title": "Text Generation with RNNs",
    "section": "Dataset Preparation",
    "text": "Dataset Preparation\n\nOur model is trained on a curated collection of 10 classic Jules Verne novels, downloaded from public domain texts of the Gutenberg Project:\n\n“A Journey to the Centre of the Earth”\n“In Search of the Castaways”\n“An Antarctic Mystery”\n“In the year 2889”\n“Around the World in Eighty Days”\n“Michael Strogoff”\n“Five Weeks in a Balloon”\n“The Mysterious Island”\n“From the Earth to the Moon”\n“Twenty Thousand Leagues under the Sea”\n\n“A Journey to the Centre of the Earth” teaches the model about geological descriptions and underground adventures. “Twenty Thousand Leagues Under the Sea” provides vocabulary about marine life and submarine technology. “Around the World in Eighty Days” offers geographical references and travel descriptions. Each book contributes unique vocabulary and stylistic elements while maintaining Verne’s consistent voice.\nThe complete dataset contains 5,768,791 characters, of which 123 are unique. To put this in perspective, that’s roughly equivalent to 3,000 pages of text. This provides our neural network with ample material to learn from, enabling it to capture both common patterns and unique expressions in Verne’s writing.\n\nData Preprocessing Steps\n# Example preprocessing workflow\ndef preprocess_text(text):\n    # Convert to lowercase for consistency\n    text = text.lower()\n    \n    # Remove unwanted characters (optional)\n    # Keep punctuation for realistic text generation\n    \n    return text\n\n# Load and combine all books\nall_text = \"\"\nfor book in book_list:\n    with open(book, 'r') as f:\n        all_text += preprocess_text(f.read())"
  },
  {
    "objectID": "raspi/rnn-verne/rnn-verne.html#tokenization-and-vocabulary",
    "href": "raspi/rnn-verne/rnn-verne.html#tokenization-and-vocabulary",
    "title": "Text Generation with RNNs",
    "section": "Tokenization and Vocabulary",
    "text": "Tokenization and Vocabulary\n\nCharacter-Level Tokenization\nHere’s where our approach differs from how humans typically think about text. While we naturally think in words and sentences, our model processes text character by character. This means it learns that certain letters frequently follow others, that spaces separate words, and that punctuation marks signal sentence boundaries.\nWhy choose character-level processing? Consider the word “extraordinary,” which appears frequently in Verne’s work. A word-level model would need to have seen this exact word during training to use it. But a character-level model can generate this word by learning that ‘e’ often starts words, ‘x’ can follow ‘e’, ‘t’ often follows ‘x’, and so on. This allows our model to create new words or handle misspellings gracefully.\nThe downside is that character-level processing requires more computational steps to generate the same amount of text. Generating “Hello world” requires 11 prediction steps instead of just 2. However, for our educational purposes, this trade-off provides valuable insights into how language generation works at its most fundamental level.\n\nUnlike word-level tokenization, character-level tokenization treats each character as a token.\n\nAdvantages of Character-Level Tokenization:\n\nNo Out-of-Vocabulary Issues: Every possible character sequence can be generated\nSmaller Vocabulary: Only 123 unique characters vs thousands of words\nLanguage Agnostic: Works with any language or symbol system\nHandles Rare Words: Can generate new words character by character\n\n\nPlease see the following site for a great general visual explanation, from Andrej Karpathy, The Unreasonable Effectiveness of Recurrent Neural Networks.\n\n\n\nVocabulary Building Process\nComputers work with numbers, not letters, so we need to convert our text into a numerical representation. We start by finding every unique character in our dataset. This includes not just letters A-Z and a-z, but also numbers, punctuation marks, spaces, and even special characters that might appear in the original texts.\nOur Jules Verne collection contains 123 unique characters. These include obvious ones like letters and common punctuation, but also less common characters like accented letters from French names or special typography marks from the original publications.\n\n\nCreating the Character Dictionary\nWe create two dictionaries: one that converts characters to numbers (encoding) and another that converts numbers back to characters (decoding). For example:\n‘a’ might become 47, ‘b’ becomes 48, ‘c’ becomes 49, and so on. The space character might be 1, and the period might be 72. These assignments are arbitrary but consistent throughout our project.\nWhen we want to process the phrase “The sea”, we convert it to something like [84, 72, 69, 1, 83, 69, 47]. When the model generates numbers like [84, 72, 69, 1, 87, 47, 83], we convert them back to “The was” (as an example).\n# Create character-to-index mapping\ntext = \"Your complete dataset text here...\"\nvocab = sorted(set(text))\nchar_to_idx = {char: idx for idx, char in enumerate(vocab)}\nidx_to_char = {idx: char for idx, char in enumerate(vocab)}\n\nprint(f\"Vocabulary size: {len(vocab)}\")\nprint(f\"Unique characters: {vocab}\")\n\n\nIt is possible to experiment with (sub-word level) tokenization using OpenAI’s tokenizer tool at:\nhttps://platform.openai.com/tokenizer"
  },
  {
    "objectID": "raspi/rnn-verne/rnn-verne.html#training-sequences",
    "href": "raspi/rnn-verne/rnn-verne.html#training-sequences",
    "title": "Text Generation with RNNs",
    "section": "Training Sequences",
    "text": "Training Sequences\n\nThe Sliding Window Approach\nOur model learns by playing a sophisticated prediction game. We show it sequences of 120 characters and ask it to predict what the 121st character should be. Think of it like a fill-in-the-blank exercise, but instead of missing words, we’re missing the next character.\nFor example, if our text contains “The submarine descended into the dark depths of the ocean”, we might show the model “The submarine descended into the dark depths of the ocea” and ask it to predict “n”. Then we slide our window forward by one character and show it “he submarine descended into the dark depths of the ocean” and ask it to predict the next character.\n\n\nTraining Configuration\n\nSequence Length: 120 characters (approximately one paragraph)\nInput-Output Relationship: Predict the next character given the previous characters\n\n\nWhy 120 Characters?\nWe chose 120 characters as our context window because it roughly corresponds to one paragraph of text in English. This gives the model enough context to understand local patterns (like completing words and phrases) while remaining computationally manageable. In practical terms, 120 characters might look like:\n“The Nautilus had been cruising in these waters for some time. Captain Nemo stood on the bridge, observing the vast exp”\nFrom this context, the model might predict “a” to complete “expanse” or “l” to form “explore”.\n\nThe longer the context window, the better the model can maintain coherence, but the more computer memory and processing time it requires.\n\n\n\nTraining Example\n\nInput Sequence: “Hello my nam”\nTarget Sequence: “ello my name”\n\nThe model learns:\n\nGiven “H”, predict “e”\nGiven “He”, predict “l”\nGiven “Hel”, predict “l”\nAnd so on…\n\nThis means our dataset of 5.8 million characters becomes millions of individual training examples, each teaching the model about character sequence patterns.\n\n\n\nCreating Training Data\ndef create_training_sequences(text, seq_length):\n    sequences = []\n    targets = []\n    \n    for i in range(len(text) - seq_length):\n        # Input sequence\n        seq = text[i:i + seq_length]\n        # Target (next character)\n        target = text[i + 1:i + seq_length + 1]\n        \n        sequences.append([char_to_idx[char] for char in seq])\n        targets.append([char_to_idx[char] for char in target])\n    \n    return sequences, targets"
  },
  {
    "objectID": "raspi/rnn-verne/rnn-verne.html#character-embeddings",
    "href": "raspi/rnn-verne/rnn-verne.html#character-embeddings",
    "title": "Text Generation with RNNs",
    "section": "Character Embeddings",
    "text": "Character Embeddings\n\nFrom Sparse to Dense Representation\nInitially, each character is represented as a one-hot vector, which is mostly zeros with a single one indicating which character it is. For 123 characters, this means each character is represented by a vector with 123 elements, where 122 are zero and 1 is one. This is wasteful and doesn’t capture any relationships between characters.\nCharacter embeddings solve this problem by representing each character as a dense vector of real numbers. Instead of 123 mostly-zero values, each character becomes 256 meaningful numbers. These numbers are learned during training and end up encoding relationships between characters.\n\n\nLearning Character Relationships\nSomething fascinating happens during training: characters that behave similarly end up with similar embedding vectors. Vowels tend to cluster together because they can often substitute for each other in similar contexts. Consonants that frequently appear together (like ‘th’ or ‘ch’) develop related embeddings.\nThe model learns that uppercase and lowercase versions of the same letter are related but distinct. It discovers that digits form their own cluster since they appear in similar contexts (dates, measurements, chapter numbers). Punctuation marks develop embeddings based on their grammatical functions.\n\n\nVisualization and Understanding\nWhen we project these 256-dimensional embeddings down to 3D space for visualization, we can see these learned relationships. The embedding space becomes a map where distance represents similarity. Characters that can substitute for each other in many contexts end up close together, while characters with completely different roles end up far apart.\nThis learned representation becomes the foundation for everything else the model does. The quality of these embeddings directly affects the model’s ability to generate coherent text.\n\nYou can play with Word2Vec - Embedding Projector"
  },
  {
    "objectID": "raspi/rnn-verne/rnn-verne.html#model-architecture",
    "href": "raspi/rnn-verne/rnn-verne.html#model-architecture",
    "title": "Text Generation with RNNs",
    "section": "Model Architecture",
    "text": "Model Architecture\n\nRNN Architecture Components\nOur Jules Verne Bot consists of three main components, each serving a specific purpose in the text generation pipeline.\nEmbedding Layer: This is our translation layer. It takes character indices (numbers like 47, 83, 72) and converts them into dense 256-dimensional vectors that capture character relationships. Think of this as converting raw symbols into a format that captures meaning and relationships.\nGRU Layer: This is the brain of our operation. With 1024 hidden units, this layer processes sequences and maintains memory about what it has seen. When processing the sequence “The submarine descended”, the GRU maintains a hidden state that encodes information about the submarine, the action of descending, and the overall maritime context.\nDense Output Layer: This is our decision-making layer. It takes the GRU’s 1024-dimensional hidden state and converts it into 123 probabilities, one for each character in our vocabulary. These probabilities represent the model’s confidence about what character should come next.\n\n\n\nModel Summary\nModel: \"sequential_4\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nembedding_4 (Embedding)      (1, 120, 256)            31,488    \n_________________________________________________________________\ngru_3 (GRU)                  (1, 120, 1024)           3,938,304 \n_________________________________________________________________\ndense_3 (Dense)              (1, 120, 123)            126,075   \n=================================================================\nTotal params: 4,095,867 (15.62 MB)\nTrainable params: 4,095,867 (15.62 MB)\nNon-trainable params: 0 (0.00 B)\nOur model has 4,095,867 parameters. These are the individual numbers that the model adjusts during training to improve its predictions. To put this in perspective, each parameter is like a tiny dial that affects how the model processes information.\n\nTraining involves adjusting all 4 million dials to work together harmoniously.\n\nThe GRU layer contains most of these parameters (about 3.9 million) because it needs to learn complex patterns about how characters relate to each other across different time steps. The embedding layer has about 31,000 parameters (123 characters × 256 dimensions), and the output layer has about 126,000 parameters.\n\n\nMemory and Processing Flow\nWhen processing text, information flows through the model like this:\nA character index enters the embedding layer and becomes a 256-dimensional vector. This vector enters the GRU, which combines it with its current memory state to produce a new 1024-dimensional hidden state. This hidden state captures everything the model “knows” at this point in the sequence.\nThe hidden state goes to the dense layer, which produces probability scores for each of the 123 possible next characters. The character with the highest probability becomes the model’s prediction.\nCrucially, the GRU’s hidden state becomes its memory for the next character prediction. This creates a chain of memory that allows the model to maintain context across the entire sequence.\n\n\nWhy GRU over Basic RNN?\nGRU Advantages:\n\nSolves Vanishing Gradient: Better information flow through long sequences\nSelective Memory: Can choose what to remember and forget\nComputational Efficiency: Fewer parameters than LSTM\nBetter Performance: More stable training than basic RNNs"
  },
  {
    "objectID": "raspi/rnn-verne/rnn-verne.html#training-process-teaching-the-model-to-write",
    "href": "raspi/rnn-verne/rnn-verne.html#training-process-teaching-the-model-to-write",
    "title": "Text Generation with RNNs",
    "section": "Training Process: Teaching the Model to Write",
    "text": "Training Process: Teaching the Model to Write\n\nThe Learning Objective\nTraining a neural network means adjusting its millions of parameters so it makes better predictions. We use a loss function called sparse categorical crossentropy, which measures how far off the model’s predictions are from the correct answers.\nThink of it like teaching someone to play darts. Each throw (prediction) has a target (the correct next character). The loss function measures how far each dart lands from the bullseye. Training adjusts the player’s technique (the model’s parameters) to improve accuracy over time.\n\n\nHardware and Time Requirements\nWe trained our model on a Tesla T4 GPU, which can perform thousands of calculations simultaneously. This parallelization is crucial because each training step involves matrix multiplications with millions of numbers. The training took 33 minutes for 30 complete passes through the entire dataset.\nTo understand why we need a GPU, consider that training involves calculating gradients for all 4 million parameters, potentially thousands of times per second. A regular CPU would take many hours to complete the same training that a GPU accomplishes in minutes.\n\n\nMonitoring Progress\nDuring training, we watch the loss decrease from about 1.9 to 0.9. This represents the model’s improving ability to predict the next character. Early in training, the model makes essentially random predictions. By the end, it has learned sophisticated patterns about English spelling, grammar, and Jules Verne’s writing style.\nThe learning curve typically shows rapid improvement in the first few epochs as the model learns basic patterns like common letter combinations. Later epochs show slower but steady improvement as the model refines its understanding of more complex patterns like narrative structure and thematic elements.\n\n\n\nPreventing Overfitting\nOne challenge in training is overfitting, in which the model memorizes the training data rather than learning generalizable patterns. We use techniques like monitoring validation loss and potentially stopping training early if the model stops improving on unseen text.\n\n\nTraining Configuration\nHardware Setup:\n\nGPU: Tesla T4 (Google Colab)\nGPU RAM: 15.0 GB\nTraining Time: 33 minutes for 30 epochs\n\nTraining Parameters:\n\nLoss Function: Categorical Sparse Crossentropy\nOptimizer: Adam (adaptive learning rate)\nEpochs: 30\nBatch Size: 128\nBuffer Size: 10,000 (for dataset shuffling)\n\n\n\nTraining Implementation\n# Model compilation\nmodel.compile(\n    optimizer='adam',\n    loss='sparse_categorical_crossentropy',\n    metrics=['accuracy']\n)\n\n# Training with callbacks\nhistory = model.fit(\n    dataset,\n    epochs=30,\n    batch_size=128,\n    validation_split=0.1,\n    callbacks=[\n        tf.keras.callbacks.ReduceLROnPlateau(patience=3),\n        tf.keras.callbacks.EarlyStopping(patience=5)\n    ]\n)"
  },
  {
    "objectID": "raspi/rnn-verne/rnn-verne.html#text-generation",
    "href": "raspi/rnn-verne/rnn-verne.html#text-generation",
    "title": "Text Generation with RNNs",
    "section": "Text Generation",
    "text": "Text Generation\n\nThe Generation Process\nOnce trained, our model becomes a text generation engine. We start with a seed phrase like:\n“THE FLYING SUBMARINE”\nand ask the model to continue the story. The process works character by character:\nThe model receives “THE FLYING SUBMARINE” and predicts the most likely next character based on everything it learned from Jules Verne’s works. Maybe it predicts a space, starting a new word. Then we feed “THE FLYING SUBMARINE” (with the space) back to the model and ask for the next character.\nThis process continues indefinitely, with each new character becoming part of the context for predicting the next one. The model might generate “THE FLYING SUBMARINE descended into the mysterious depths…” as it draws upon patterns learned from Verne’s nautical adventures.\n\n\nTemperature Control\nHere’s where we can control the model’s creativity through a parameter called temperature. Temperature affects how the model chooses between different possible next characters.\nWith temperature set to 0.1, the model almost always picks the most probable next character. This produces very predictable, conservative text that closely mimics the training data but might be repetitive or boring.\nWith temperature set to 1.0, the model considers all possible next characters according to their learned probabilities. This produces more varied and creative text, but sometimes makes unusual choices that lead to interesting narrative directions.\nWith temperature above 1.5, the model becomes quite random, often producing text that starts coherently but gradually becomes nonsensical as unlikely character combinations accumulate.\nIn short:\n\nTemperature = 0.5: More predictable, conservative text\nTemperature = 1.0: More creative, diverse text\nTemperature = 1.5: Very random, potentially nonsensical text\n\n\n\nImplementation\ndef generate_text(model, start_string, num_generate=1000, temperature=1.0):\n    # Convert start string to numbers\n    input_eval = [char_to_idx[s] for s in start_string]\n    input_eval = tf.expand_dims(input_eval, 0)\n\n    text_generated = []\n    model.reset_states()\n\n    for i in range(num_generate):\n        predictions = model(input_eval)\n        predictions = tf.squeeze(predictions, 0)\n        \n        # Apply temperature\n        predictions = predictions / temperature\n        predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n        \n        # Add predicted character to input\n        input_eval = tf.expand_dims([predicted_id], 0)\n        text_generated.append(idx_to_char[predicted_id])\n\n    return start_string + ''.join(text_generated)\n\n\nGeneration Example (Temperature = 0.5)\nSeed: “THE FLYING SUBMARINE”\nGenerated Text:\nTHE FLYING SUBMARINE\nCHAPTER 100 VENTANTILE\n\nThis eBook is for the use of anyone anywhere in the United States and most \nother parts of the earth and miserable eruptions. The solar rays should be \nentirely under the shock of the intensity of the sea. We were all sorts. Are \nwe to prepare for our feelings?\"\n\n\"I can never see them a good geographer,\" said Mary.\n\n\"Well, then, John, for I get to the Pampas, that we ought to obey the same\ntime. In the country of this latitude changed my brother, and the\n_Nautilus_ floated in a sea which contained the rudder and\nlower colour visibly. The loiter was a fatalint region the two\nscientific discoverers. Several times turning toward the river, the cry\nof doors and over an inclined plains of the Angara, with a threatening\nwater and disappeared in the midst of the solar rays.\n\nThe weather was spread and strewn with closed bottoms which soon appeared\nthat the unexpected sheets of wind was soon and linen, and the whole\nseas were again landed on the subject of the natives, and the prisoners\nwere successively assuming the sides of this agreement for fifteen days\nwith a threatening voice.\n...\n\n\nExample Output Analysis\nLet’s examine some generated text: “The weather was spread and strewn with closed bottoms which soon appeared that the unexpected sheets of wind was soon and linen, and the whole seas were again landed on the subject of the natives…”\nThis excerpt shows both the model’s strengths and limitations. It successfully captures Verne’s descriptive style and maritime vocabulary (“seas,” “wind,” “natives”). The sentence structure feels appropriately Victorian and elaborate. However, the meaning becomes confused with phrases like “closed bottoms” and “sheets of wind was soon and linen.”\nThis illustrates the fundamental challenge of character-level generation: the model learns local patterns (how words are spelled, common phrases) much better than global coherence (logical narrative flow, consistent meaning)."
  },
  {
    "objectID": "raspi/rnn-verne/rnn-verne.html#challenges-and-limitations",
    "href": "raspi/rnn-verne/rnn-verne.html#challenges-and-limitations",
    "title": "Text Generation with RNNs",
    "section": "Challenges and Limitations",
    "text": "Challenges and Limitations\n\nContext Window Constraints\nOur 120-character context window creates a fundamental limitation. The model can only “see” about one paragraph of previous text when making predictions. This means it might introduce a character named Captain Smith, then 200 characters later introduce another character with the same name, having “forgotten” the first introduction.\nHumans writing stories maintain mental models of characters, plot lines, and world-building details across entire novels. Our model’s memory effectively resets every 120 characters, making long-term narrative consistency nearly impossible.\n\n\nCharacter vs. Word Level Trade-offs\nCharacter-level generation requires many more prediction steps than word-level generation. Generating the phrase “extraordinary adventure” requires 22 character predictions instead of just 2 word predictions. This makes character-level generation much slower and more computationally expensive.\nHowever, character-level generation offers unique advantages. The model can generate new words it has never seen before by combining character patterns. It can handle misspellings, made-up words, or technical terms more gracefully than word-level models that have fixed vocabularies.\n\n\nCoherence Challenges\nPerhaps the biggest limitation is maintaining semantic coherence. The model might generate grammatically correct text that makes no logical sense. It can describe “The submarine floating in the air above the mountain peaks” because it has learned that submarines float and that Verne often described mountains, but it hasn’t learned the physical constraint that submarines float in water, not air.\nThis happens because the model learns statistical patterns without understanding meaning. It knows that certain word combinations are common without understanding why they make sense.\n\n\nSummary\n\nLimited Context Window\n\nIssue: Only 120 characters of context\nImpact: Cannot maintain coherence over long passages\nExample: May forget characters or plot points mentioned earlier\n\nCharacter vs Word Level\n\nIssue: Character-level generation is slower and less efficient\nImpact: Requires more computation for equivalent output\nTrade-off: Better handling of rare words vs efficiency\n\nCoherence Problems\n\nIssue: May generate grammatically correct but semantically inconsistent text\nCause: Limited understanding of story structure and plot consistency\n\nRepetitive Patterns\n\nIssue: May fall into repetitive loops\nCause: Model overfitting to common patterns in training data\n\n\n\n\nPotential Improvements\n\nLonger Context Windows: Increase sequence length for better coherence\nHierarchical Models: Separate models for different text levels (word, sentence, paragraph)\nFine-tuning: Additional training on specific styles or topics\nBeam Search: Better text generation algorithms instead of greedy sampling\n\nWhat will lead us to modern Language Models based on Transformers arquiteture ."
  },
  {
    "objectID": "raspi/rnn-verne/rnn-verne.html#connecting-to-modern-language-models",
    "href": "raspi/rnn-verne/rnn-verne.html#connecting-to-modern-language-models",
    "title": "Text Generation with RNNs",
    "section": "Connecting to Modern Language Models",
    "text": "Connecting to Modern Language Models\n\nScale Comparison\nTo appreciate how far language modeling has advanced, consider the scale differences between our Jules Verne Bot and modern language models:\nOur model has 4 million parameters and was trained on about 5.8 million characters (10 books). GPT-3 has 175 billion parameters and was trained on 45 terabytes of text (roughly equivalent to millions of books). That’s a difference of over 40,000 times more parameters and millions of times more training data.\nModern small language models (SLMs) like Phi-3-mini still dwarf our model with 3.8 billion parameters, but they represent more efficient designs that achieve impressive performance with “only” 1,000 times more parameters than our model.\n\n\n\n\n\n\n\n\n\nAspect\nJules Verne Bot\nGPT-3 (2020)\nPhi-3-mini (2024)\n\n\n\n\nArchitecture\nRNN (GRU)\nTransformer\nTransformer\n\n\nParameters\n4 million\n175 billion\n3.8 billion\n\n\nTraining Data\n5.8M characters\n45TB text\n3.3T tokens\n\n\nContext Length\n120 characters\n2,048 tokens\n128,000 tokens\n\n\nTokenization\nCharacter-level\nSubword (BPE)\nSubword\n\n\nTraining Time\n33 minutes\nWeeks\n7 days\n\n\nGPU Requirements\n1 Tesla T4\n10,000 V100 GPUs\n512 H100 GPUs\n\n\n\n\n\nArchitectural Evolution\nThe biggest advancement since RNNs is the Transformer architecture, which uses attention mechanisms instead of recurrent processing. While RNNs process text sequentially (like reading word by word), Transformers can examine all parts of a text simultaneously and learn relationships between any two words, regardless of how far apart they are.\nThis solves the long-term memory problem that limits our RNN model. A Transformer can maintain awareness of a character introduced in a hypothetical “chapter 1” while writing “chapter 10”, something our 120-character context window makes impossible.\n\n\nTraining Efficiency\nModern models also benefit from more sophisticated training techniques. They’re pre-trained on massive, diverse datasets to learn general language patterns, then fine-tuned on specific tasks. They use techniques like instruction tuning, where they learn to follow human commands, and reinforcement learning from human feedback, where they learn to generate text that humans find helpful and appropriate.\n\n\nSummary: Why Modern Models Perform Better?\n\nTransformer Architecture\n\nAttention Mechanism: Can look at any part of the input sequence\nParallel Processing: Much faster training and inference\nBetter Long-range Dependencies: Maintains context over thousands of tokens\n\nScale\n\nMore Data: Trained on vastly more diverse text\nMore Parameters: Can memorize and generalize better\nMore Compute: Allows for more sophisticated training techniques\n\nAdvanced Techniques\n\nPre-training + Fine-tuning: Learn general language then specialize\nInstruction Tuning: Trained to follow human instructions\nRLHF: Reinforcement Learning from Human Feedback"
  },
  {
    "objectID": "raspi/rnn-verne/rnn-verne.html#conclusion",
    "href": "raspi/rnn-verne/rnn-verne.html#conclusion",
    "title": "Text Generation with RNNs",
    "section": "Conclusion:",
    "text": "Conclusion:\nBuilding the Jules Verne Bot teaches us that creating artificial intelligence systems capable of generating human-like text requires careful consideration of multiple components working together. The embedding layer learns to represent characters meaningfully, the RNN layer processes sequences and maintains memory, and the output layer makes predictions based on learned patterns.\nThe project also illustrates the fundamental trade-offs in machine learning: between model complexity and training speed, between creativity and coherence, between local accuracy and global consistency. These trade-offs appear in every AI system, from simple character-level generators to the most sophisticated language models.\nMost importantly, this project demonstrates that impressive AI capabilities emerge from relatively simple components combined thoughtfully. Our 4-million parameter model, while limited compared to modern systems, genuinely learns to write in Jules Verne’s style through nothing more than statistical pattern recognition and mathematical optimization.\nThe techniques we’ve explored, sequence processing, embedding learning, and generation strategies, form the foundation for understanding any language model. Whether you encounter RNNs, Transformers, or future architectures yet to be invented, the core concepts remain consistent: learn patterns from data, encode meaning in mathematical representations, and generate new content by predicting what should come next.\nUnderstanding these fundamentals provides the foundation for working with, improving, or creating the next generation of language models that will shape how humans and computers communicate in the future."
  },
  {
    "objectID": "raspi/rnn-verne/rnn-verne.html#resourses",
    "href": "raspi/rnn-verne/rnn-verne.html#resourses",
    "title": "Text Generation with RNNs",
    "section": "Resourses",
    "text": "Resourses\n\nGutenberg Project\nThe Unreasonable Effectiveness of Recurrent Neural Networks\nWord2Vec - Embedding Projector\nOpenAI’s tokenizer tool\nGenerating Text with RNNs: The Jules Verne Bot - CoLab"
  },
  {
    "objectID": "raspi/kd_intro/kd_intro.html#introduction",
    "href": "raspi/kd_intro/kd_intro.html#introduction",
    "title": "Knowledge Distillation in Practice",
    "section": "Introduction to Knowledge Distillation",
    "text": "Introduction to Knowledge Distillation\nKnowledge distillation is a powerful technique in machine learning that enables the transfer of knowledge from a large, complex model (the “teacher”) to a smaller, more efficient model (the “student”). This process allows us to create compact models that maintain much of the performance of their larger counterparts while being significantly faster and requiring fewer computational resources.\n\nWhy Knowledge Distillation Matters\nIn today’s AI landscape, models are becoming increasingly large and complex. While these models achieve remarkable performance, they often require substantial computational resources, making deployment challenging in resource-constrained environments such as mobile devices, edge computing systems, or real-time applications. Knowledge distillation addresses this challenge by:\n\nReducing Model Size: Creating smaller models with fewer parameters\nImproving Inference Speed: Enabling faster predictions in production\nLowering Computational Costs: Reducing memory and processing requirements\nMaintaining Performance: Preserving much of the original model’s accuracy\nEnabling Edge Deployment: Making AI accessible on resource-limited devices\n\n\n\nThe Teacher-Student Paradigm\nThe core concept of knowledge distillation revolves around the teacher-student relationship:\n\nTeacher Model: A large, well-trained model with high capacity and performance\nStudent Model: A smaller, more efficient model trained to mimic the teacher’s behavior\nKnowledge Transfer: The process of transferring the teacher’s “dark knowledge” to the student\n\n\n\n\nFigure from “Knowledge Distillation: A Survey, Jianping Gou Baosheng Yu Stephen J. Maybank Dacheng Tao, 2021\n\n\n\nSee paper: “Knowledge Distillation: A Survey, Jianping Gou, Baosheng Yu, Stephen J. Maybank, Dacheng Tao, 2021"
  },
  {
    "objectID": "raspi/kd_intro/kd_intro.html#theoretical-foundations",
    "href": "raspi/kd_intro/kd_intro.html#theoretical-foundations",
    "title": "Knowledge Distillation in Practice",
    "section": "Theoretical Foundations",
    "text": "Theoretical Foundations\n\nSoft Targets vs. Hard Targets\nTraditional supervised learning uses “hard targets” - one-hot encoded labels that provide limited information. For example, in MNIST digit classification, the label for the digit “5” would be represented as [0, 0, 0, 0, 0, 1, 0, 0, 0, 0].\nKnowledge distillation leverages “soft targets” - the probability distributions produced by the teacher model. These soft targets contain richer information about the relationships between classes. For instance, the teacher might output [0.01, 0.02, 0.01, 0.05, 0.1, 0.78, 0.02, 0.01, 0.0, 0.0] for a “5”, indicating that it’s most confident about “5” but also considers “4” somewhat similar.\n\n\nThe Temperature Parameter\nThe temperature parameter (τ) is crucial in knowledge distillation. It controls the “softness” of the probability distribution by modifying the softmax function:\nP(class_i) = exp(z_i / τ) / Σ_j exp(z_j / τ)\nWhere:\n\nz_i are the logits (pre-softmax outputs)\nτ is the temperature parameter\n\nEffects of Temperature:\n\nτ = 1: Standard softmax (normal sharpness)\nτ &gt; 1: Softer distribution (more uniform, reveals class relationships)\nτ &lt; 1: Sharper distribution (more peaked, less informative)\n\n\nIn our implementation, we use a temperature of 5.0, which creates softer distributions that effectively transfer the teacher’s “dark knowledge” to the student.\n\n\n\nDistillation Loss Function\nThe distillation loss combines two components:\n\nDistillation Loss (L_KD): Measures how well the student mimics the teacher’s soft targets\nStudent Loss (L_CE): Traditional cross-entropy loss with hard targets\n\nL_total = α * L_CE + (1 - α) * L_KD\nWhere α is a weighting parameter that balances the two objectives, in our implementation, we use α = 0.3, giving more weight to the soft targets from the teacher.\nThe distillation loss is typically computed using the KL divergence:\nL_KD = τ² * KL_divergence(Teacher_soft_targets, Student_soft_targets)\nThe τ² factor compensates for the gradient scaling effect of temperature. This is crucial for stable training."
  },
  {
    "objectID": "raspi/kd_intro/kd_intro.html#implementation-with-tensorflow-and-mnist",
    "href": "raspi/kd_intro/kd_intro.html#implementation-with-tensorflow-and-mnist",
    "title": "Knowledge Distillation in Practice",
    "section": "Implementation with TensorFlow and MNIST",
    "text": "Implementation with TensorFlow and MNIST\n\nDataset Overview\nMNIST is an ideal dataset for learning knowledge distillation:\n\n60,000 training images of handwritten digits (0-9)\n10,000 test images\n28x28 grayscale images\n10 classes (digits 0-9)\nWell-established baseline performances\n\n\n\nTeacher Model Architecture\nOur teacher model has substantial capacity with multiple convolutional layers, batch normalization, and dropout for regularization:\ndef build_teacher_model():\n    model = models.Sequential([\n        layers.Conv2D(64, 3, \n                      activation='relu', \n                      padding='same', \n                      input_shape=(28, 28, 1)),\n        layers.BatchNormalization(),\n        layers.Conv2D(64, 3, activation='relu', padding='same'),\n        layers.MaxPooling2D(2, 2),\n        layers.BatchNormalization(),\n        \n        layers.Conv2D(128, 3, activation='relu', padding='same'),\n        layers.BatchNormalization(),\n        layers.Conv2D(128, 3, activation='relu', padding='same'),\n        layers.MaxPooling2D(2, 2),\n        layers.BatchNormalization(),\n        \n        layers.Conv2D(256, 3, activation='relu', padding='same'),\n        layers.BatchNormalization(),\n        layers.GlobalAveragePooling2D(),\n        \n        layers.Dense(256, activation='relu'),\n        layers.BatchNormalization(),\n        layers.Dropout(0.5),\n        layers.Dense(128, activation='relu'),\n        layers.BatchNormalization(),\n        layers.Dropout(0.5),\n        layers.Dense(10, activation='softmax')\n    ])\n    return model\n\n\nThis architecture achieved 99.44 % accuracy on MNIST.\n\n\n\nStudent Model Architecture\nOur student model is intentionally much simpler, with fewer layers and significantly fewer parameters:\ndef build_student_model():\n    model = models.Sequential([\n        layers.Conv2D(16, 3, \n                      activation='relu', \n                      padding='same', \n                      input_shape=(28, 28, 1)),\n        layers.MaxPooling2D(2, 2),\n        layers.Conv2D(32, 3, activation='relu', padding='same'),\n        layers.MaxPooling2D(2, 2),\n        layers.Flatten(),\n        layers.Dense(64, activation='relu'),\n        layers.Dense(10, activation='softmax')\n    ])\n    return model\n\n\nThe student model has fewer parameters than the teacher (105K versus 658K), or 6.2 times smaller.\n\n\n\nKnowledge Distillation Implementation\nIn our implementation, we use a custom training loop that explicitly calculates both hard and soft losses:\n# Forward pass\nwith tf.GradientTape() as tape:\n    predictions = kd_student(x_batch, training=True)\n    \n    # Hard target loss (cross-entropy with true labels)\n    hard_loss = tf.keras.losses.categorical_crossentropy(y_hard_batch, predictions)\n    \n    # Soft target loss (KL divergence with teacher predictions)\n    soft_loss = tf.keras.losses.kullback_leibler_divergence(y_soft_batch, predictions)\n    \n    # Combined loss with temperature scaling factor for soft loss\n    loss = alpha * hard_loss + (1 - alpha) * soft_loss * (temperature ** 2)\nKey components of our implementation:\n\nTemperature Scaling: We apply a temperature of 5.0 to soften the teacher’s outputs\nAlpha Weighting: We use α = 0.3 to prioritize learning from the teacher’s soft targets\nKL Divergence: This loss function helps the student match the teacher’s probability distributions\nTemperature Correction: We multiply the soft loss by τ² to correct for gradient scaling\n\n\n\nTraining Process\nOur implementation follows these steps:\n\nTrain the Teacher: First, we train the complex teacher model using early stopping and a reduced learning rate.\nTrain a Vanilla Student: We train a student model normally on the hard labels for comparison.\nGenerate Soft Targets: We use the teacher to create softened probability distributions.\nTrain the Distilled Student: We train another student using our custom distillation training loop.\nEvaluate and Compare: We compare the performance, size, and speed of all three models.\n\n\n\nResults and Analysis\nOur implementation typically shows these results:\n\nTeacher Model: 99.44% accuracy, largest size, slowest inference (0.8632 seconds)\nVanilla Student: 98.77% accuracy, smaller size, faster inference (0.3579 seconds)\nDistilled Student: 99.32% accuracy, same size as vanilla student, but better performance (+0.55%) and faster inference than the Teacher (0.5467 seconds)\n\n\nThese results demonstrate the key benefit of knowledge distillation: the distilled student achieves performance closer to the teacher while maintaining the efficiency benefits of the smaller architecture.\n\n\nVisualizing Distillation Benefits\n\nWe analyze the models on challenging examples where the teacher succeeds but the vanilla student fails. This reveals how knowledge distillation enables students to handle complex cases by learning the teacher’s “dark knowledge.”"
  },
  {
    "objectID": "raspi/kd_intro/kd_intro.html#advanced-techniques",
    "href": "raspi/kd_intro/kd_intro.html#advanced-techniques",
    "title": "Knowledge Distillation in Practice",
    "section": "Advanced Techniques",
    "text": "Advanced Techniques\n\nFeature-Based Distillation\nBeyond output-level distillation, we can transfer knowledge from intermediate layers:\ndef feature_distillation_loss(teacher_features, student_features):\n    \"\"\"Distill knowledge from intermediate feature maps\"\"\"\n    loss = 0\n    for t_feat, s_feat in zip(teacher_features, student_features):\n        # Align dimensions if necessary\n        s_feat_aligned = align_feature_dimensions(s_feat, t_feat.shape)\n        loss += tf.keras.losses.MSE(t_feat, s_feat_aligned)\n    return loss\n\n\nAttention-Based Distillation\nTransfer attention patterns from teacher to student:\ndef attention_distillation_loss(teacher_attention, student_attention):\n    \"\"\"Distill attention mechanisms\"\"\"\n    return tf.keras.losses.MSE(teacher_attention, student_attention)\n\n\nPractical Tips for Advanced Distillation\n\nLayer Alignment: When using feature distillation, carefully align the feature dimensions\nFeature Selection: Not all features are equally important; focus on the most informative ones\nMulti-Teacher Distillation: Combine knowledge from multiple teachers for better results\nOnline Distillation: Train teacher and student simultaneously for mutual improvement"
  },
  {
    "objectID": "raspi/kd_intro/kd_intro.html#scaling-to-llms",
    "href": "raspi/kd_intro/kd_intro.html#scaling-to-llms",
    "title": "Knowledge Distillation in Practice",
    "section": "Scaling to Large Language Models",
    "text": "Scaling to Large Language Models\n\nChallenges in LLM Distillation\nScaling knowledge distillation to Large Language Models presents unique challenges:\n\nModel Size: LLMs have billions of parameters, making distillation computationally expensive\nSequence Generation: Unlike classification, LLMs generate sequences, requiring sequence-level distillation\nVocabulary Differences: Teacher and student may have different vocabularies\nContext Length: Handling variable-length sequences and attention patterns\nMulti-task Learning: LLMs perform multiple tasks simultaneously\n\n\n\nLLM Distillation Techniques\n\n1. Sequence-Level Distillation\nInstead of token-level predictions, match entire sequence probabilities:\ndef sequence_level_distillation(teacher_logits, student_logits, sequence_lengths):\n    \"\"\"Distill at the sequence level for better coherence\"\"\"\n    teacher_probs = tf.nn.softmax(teacher_logits / temperature)\n    student_probs = tf.nn.softmax(student_logits / temperature)\n    \n    # Mask out padding tokens\n    mask = create_sequence_mask(sequence_lengths)\n    \n    return masked_kl_divergence(teacher_probs, student_probs, mask)\n\n\n2. Recent LLM Distillation Examples\n\nDistilBERT: 40% smaller than BERT, retains 97% of performance.\nTinyBERT: 7.5x smaller and 9.4x faster than BERT-base\nMiniLM: Uses deep self-attention distillation for efficient transfer.\nDistil-GPT2: Compressed version of GPT-2 with minimal performance loss\nLlama 3.2 3B and 1B: Distilled from Llama 3.2 models (70B/8B parameters)\n\n\n\n3. Distilling Reasoning Abilities\nModern approaches focus on transferring reasoning abilities, not just predictions:\n\nChain-of-Thought Distillation: Transfer step-by-step reasoning process\nExplanation-Based Distillation: Use teacher’s explanations to guide student learning\nRationale Extraction: Identify key reasoning patterns for targeted transfer\n\n\n\n\nFrom MNIST to Real-World LLMs: Llama 3.2 Case Study\n\nTeacher Model: The larger Llama 3.2 models (70B/8B parameters) serve as the teachers\nStudent Model: Llama 3.2 3B and 1B are the distilled student models. They were created using pruning techniques, which systematically remove less meaningful connections (weights) in the neural network.\n\n\n\nKey Points\n\nScale Difference: The parameter reduction from 70B to 3B (~23x) or 1B (~70x) demonstrates industrial-scale distillation\nPerformance Preservation: Despite massive size reduction, the smaller models maintain impressive capabilities:\n\nThe 3B model preserves most of the reasoning abilities of larger models.\nThe 1B model remains highly functional for many tasks.\n\nPractical Benefits:\n\nThe 1B model can run on consumer laptops and even some mobile devices.\nThe 3B model offers a balance of performance and accessibility.\n\nDistillation Techniques Used:\n\nMeta likely used a combination of response-based and feature-based distillation.\nThey may have employed temperature scaling and specialized loss functions similar to what we demonstrated in our MNIST example.\nThe principles we covered (soft targets, temperature, loss weighting) all apply at this larger scale.\n\n\nWhile our MNIST example demonstrates the application of knowledge distillation principles using a simple dataset, these same principles can be applied directly to state-of-the-art language models.\nMeta’s Llama 3.2 family provides a perfect real-world example:\n\n\n\n\n\n\n\n\n\nModel\nParameters\nSize Reduction\nUse Case\n\n\n\n\nLlama 3.2 70B\n70 billion\n(Teacher)\nData centers, high-performance applications\n\n\nLlama 3.2 8B\n8 billion\n~9x\nServer deployment, high-end workstations\n\n\nLlama 3.2 3B\n3 billion\n~23x\nConsumer laptops, desktop applications\n\n\nLlama 3.2 1B\n1 billion\n~70x\nEdge devices, mobile applications, embedded systems\n\n\n\nThe 3B and 1B models represent successful distillations of the larger models, preserving core capabilities while dramatically reducing computational requirements. This demonstrates the industrial importance of knowledge distillation techniques we’ve explored.\nIt is possible to note that while the underlying principles remain the same as our MNIST example, industrial LLM distillation includes additional techniques:\n\nDistillation-Specific Data: Carefully curated datasets designed to transfer specific capabilities\nMulti-Stage Distillation: Gradual compression through intermediate models (70B → 8B → 3B → 1B)\nTask-Specific Fine-Tuning: Optimizing smaller models for specific use cases after distillation\nCustom Loss Functions: Specialized loss terms to preserve reasoning patterns and generation quality\n\nDespite these additional complexities, the core concept remains the same: using a larger, more capable model to guide the training of a smaller, more efficient one.\n\n\n\nApplications in Modern LLM Development\n\n1. From ChatGPT to Mobile Assistants\nLarge models like ChatGPT (175B+ parameters) can be distilled to create mobile-friendly assistants (1-2B parameters) that maintain core capabilities while running locally on smartphones.\n\n\n2. Domain-Specific Distillation\nInstead of distilling general capabilities, focus on specific domains:\n\nMedical LLMs: Distill medical knowledge from large models to smaller, specialized ones\nLegal Assistants: Create compact models focused on legal reasoning and terminology\nEducational Tools: Develop small models optimized for teaching specific subjects\n\n\n\n3. From Research to Production\nThe journey from research models to production deployment often involves distillation:\n\nResearch Phase: Develop large, state-of-the-art models (100B+ parameters)\nDistillation Phase: Compress knowledge into deployment-ready models (1-10B parameters)\nDeployment Phase: Further optimize for specific hardware and latency requirements"
  },
  {
    "objectID": "raspi/kd_intro/kd_intro.html#best-practices",
    "href": "raspi/kd_intro/kd_intro.html#best-practices",
    "title": "Knowledge Distillation in Practice",
    "section": "Practical Guidelines and Best Practices",
    "text": "Practical Guidelines and Best Practices\n\nChoosing the Right Architecture\nBased on our experiments, we recommend:\n\nTeacher-Student Size Ratio: Aim for a 10-20x parameter reduction for significant efficiency gains\nArchitectural Similarity: Maintain similar architectural patterns between teacher and student\nBottleneck Identification: Ensure the student has adequate capacity at critical layers\n\n\n\nHyperparameter Selection\nOur experiments suggest these optimal settings:\n\nTemperature (τ):\n\nFor MNIST: 3-5 works well\nFor complex tasks: 5-10 may be better\nIf outputs are already soft: Lower temperatures (2-3) may suffice\n\nAlpha Weighting (α):\n\nFor simpler tasks: 0.3-0.5 (balanced approach)\nFor complex reasoning: 0.1-0.3 (more emphasis on teacher’s knowledge)\nWhen teacher is extremely accurate: Lower α values work better\n\nTraining Duration:\n\nDistilled students often benefit from longer training (1.5-2x the epochs)\nUse early stopping with patience to avoid overfitting\n\n\n\n\nCommon Pitfalls and Solutions\n\nTeacher Performance: Ensure the teacher actually outperforms the student (we fixed this in our implementation)\nTemperature Selection: If knowledge transfer is poor, experiment with different temperatures\nLoss Weighting: If the student ignores soft targets, reduce α to emphasize distillation loss\nGradient Scaling: Always apply the τ² correction factor to the soft loss\n\n\n\nPerformance Evaluation\nAlways measure multiple dimensions of performance:\n\nAccuracy: Primary performance metric (should be closer to teacher than vanilla student)\nModel Size: Parameter count and memory footprint (should match vanilla student)\nInference Speed: Time per prediction (should be significantly faster than teacher)\nChallenging Cases: Performance on difficult examples (should be better than vanilla student)"
  },
  {
    "objectID": "raspi/kd_intro/kd_intro.html#conclusion",
    "href": "raspi/kd_intro/kd_intro.html#conclusion",
    "title": "Knowledge Distillation in Practice",
    "section": "Conclusion",
    "text": "Conclusion\nKnowledge distillation provides a powerful approach to create efficient, deployable models without sacrificing performance. Our implementation demonstrates that even with a 15-20x reduction in model size, we can maintain performance close to the larger teacher model.\n\nKey Takeaways\n\nEfficiency Without Sacrifice: Knowledge distillation enables smaller models to achieve performance similar to larger ones\nDark Knowledge Matters: The soft probability distributions contain valuable information beyond just the predicted class\nTemperature and Alpha: These hyperparameters are crucial for effective knowledge transfer\nPractical Benefits: Smaller size, faster inference, and lower resource requirements make AI more accessible\n\n\n\nFrom MNIST to LLMs\nThe principles we’ve demonstrated with MNIST directly scale to Large Language Models:\n\nSame Core Concept: Transfer knowledge from larger to smaller models\nSame Hyperparameters: Temperature and alpha weighting remain critical\nSame Benefits: Size reduction, speed improvement, and accessibility\nSame Challenges: Ensuring adequate student capacity and effective knowledge transfer\n\n\n\nFinal Thoughts\nKnowledge distillation isn’t just an academic technique—it’s essential for practical AI deployment. The same principles that helped us compress our MNIST classifier can be scaled to compress models with hundreds of billions of parameters. This universality makes knowledge distillation an indispensable skill for engineering students entering the field of AI."
  },
  {
    "objectID": "raspi/kd_intro/kd_intro.html#resources",
    "href": "raspi/kd_intro/kd_intro.html#resources",
    "title": "Knowledge Distillation in Practice",
    "section": "Resources",
    "text": "Resources\nNotebook"
  },
  {
    "objectID": "raspi/llm/slm_intro.html#introduction",
    "href": "raspi/llm/slm_intro.html#introduction",
    "title": "Small Language Models (SLM)",
    "section": "Introduction",
    "text": "Introduction\nIn the fast-growing area of artificial intelligence, edge computing presents an opportunity to decentralize capabilities traditionally reserved for powerful, centralized servers. This chapter explores the practical integration of small versions of traditional large language models (LLMs) into a Raspberry Pi 5, transforming this edge device into an AI hub capable of real-time, on-site data processing.\nAs large language models grow in size and complexity, Small Language Models (SLMs) offer a compelling alternative for edge devices, striking a balance between performance and resource efficiency. By running these models directly on Raspberry Pi, we can create responsive, privacy-preserving applications that operate even in environments with limited or no internet connectivity.\nThis chapter guides us through setting up, optimizing, and leveraging SLMs on Raspberry Pi. We will explore the installation and utilization of Ollama. This open-source framework allows us to run LLMs locally on our machines (desktops or edge devices such as NVIDIA Jetson or Raspberry Pis).\nOllama is designed to be efficient, scalable, and easy to use, making it a good option for deploying AI models such as Microsoft Phi, Google Gemma, Meta Llama, and more. We will integrate some of those models into projects using Python’s ecosystem, exploring their potential in real-world scenarios (or at least point in this direction)."
  },
  {
    "objectID": "raspi/llm/slm_intro.html#setup",
    "href": "raspi/llm/slm_intro.html#setup",
    "title": "Small Language Models (SLM)",
    "section": "Setup",
    "text": "Setup\nWe could use any Raspberry Pi model in the previous labs, but here, the choice must be the Raspberry Pi 5 (Raspi-5). It is a robust platform that substantially upgrades the last version 4, equipped with the Broadcom BCM2712, a 2.4GHz quad-core 64-bit Arm Cortex-A76 CPU featuring Cryptographic Extension and enhanced caching capabilities. It boasts a VideoCore VII GPU, dual 4Kp60 HDMI® outputs with HDR, and a 4Kp60 HEVC decoder. Memory options include 4GB, 8GB, and 16GB of high-speed LPDDR4X SDRAM, with 8GB as our choice for running SLMs. It also features expandable storage via a microSD card slot and a PCIe 2.0 interface for fast peripherals such as M.2 SSDs (Solid State Drives).\n\nFor real SSL applications, SSDs are a better option than SD cards.\n\nBy the way, as Alasdair Allan discussed, inferencing directly on the Raspberry Pi 5 CPU—with no GPU acceleration—is now on par with the performance of the Coral TPU.\n\nFor more info, please see the complete article: Benchmarking TensorFlow and TensorFlow Lite on Raspberry Pi 5.\n\nRaspberry Pi Active Cooler\nWe suggest installing an Active Cooler, a dedicated clip-on cooling solution for Raspberry Pi 5 (Raspi-5), for this lab. It combines an aluminum heatsink with a temperature-controlled blower fan to keep the Raspi-5 operating comfortably under heavy loads, such as running SLMs.\n\n\n\n\n\nThe Active Cooler has pre-applied thermal pads for heat transfer and is mounted directly to the Raspberry Pi 5 board using spring-loaded push pins. The Raspberry Pi firmware actively manages it: at 60°C, the blower’s fan is turned on; at 67.5°C, the fan speed is increased; and finally, at 75°C, the fan increases to full speed. The blower’s fan will spin down automatically when the temperature drops below these limits.\n\n\nTo prevent overheating, all Raspberry Pi boards begin to throttle the processor when the temperature reaches 80°Cand throttle even further when it reaches the maximum temperature of 85°C (more detail here)."
  },
  {
    "objectID": "raspi/llm/slm_intro.html#generative-ai-genai",
    "href": "raspi/llm/slm_intro.html#generative-ai-genai",
    "title": "Small Language Models (SLM)",
    "section": "Generative AI (GenAI)",
    "text": "Generative AI (GenAI)\nGenerative AI is an artificial intelligence system capable of creating new, original content across various media such as text, images, audio, and video. These systems learn patterns from existing data and use that knowledge to generate novel outputs that didn’t previously exist. Large Language Models (LLMs), Small Language Models (SLMs), and multimodal models can all be considered types of GenAI when used for generative tasks.\nGenAI provides the conceptual framework for AI-driven content creation, with LLMs serving as powerful general-purpose text generators. SLMs adapt this technology for edge computing, while multimodal models extend GenAI capabilities across different data types. Together, they represent a spectrum of generative AI technologies, each with its strengths and applications, collectively driving AI-powered content creation and understanding.\n\nLarge Language Models (LLMs)\nLarge Language Models (LLMs) are advanced artificial intelligence systems that understand, process, and generate human-like text. These models are characterized by their massive scale in terms of the amount of data they are trained on and the number of parameters they contain. Critical aspects of LLMs include:\n\nSize: LLMs typically contain billions of parameters. For example, GPT-3 has 175 billion parameters, while some newer models exceed a trillion parameters.\nTraining Data: They are trained on vast amounts of text data, often including books, websites, and other diverse sources, amounting to hundreds of gigabytes or even terabytes of text.\nArchitecture: Most LLMs use transformer-based architectures, which allow them to process and generate text by paying attention to different parts of the input simultaneously.\nCapabilities: LLMs can perform a wide range of language tasks without specific fine-tuning, including:\n\nText generation\nTranslation\nSummarization\nQuestion answering\nCode generation\nLogical reasoning\n\nFew-shot Learning: They can often understand and perform new tasks with minimal examples or instructions.\nResource-Intensive: Due to their size, LLMs typically require significant computational resources to run, often needing powerful GPUs or TPUs.\nContinual Development: The field of LLMs is rapidly evolving, with new models and techniques constantly emerging.\nEthical Considerations: The use of LLMs raises important questions about bias, misinformation, and the environmental impact of training such large models.\nApplications: LLMs are used in various fields, including content creation, customer service, research assistance, and software development.\nLimitations: Despite their power, LLMs can produce incorrect or biased information and lack true understanding or reasoning capabilities.\n\nWe must note that we use large models beyond text, which we call multi-modal models. These models integrate and process information from multiple types of input simultaneously. They are designed to understand and generate content across various data types, such as text, images, audio, and video.\nCertainly. Let’s define open and closed models in the context of AI and language models:\n\n\nClosed vs Open Models:\nClosed models, also called proprietary models, are AI models whose internal workings, code, and training data are not publicly disclosed. Examples: GPT-3 and beyond (by OpenAI), Claude (by Anthropic), Gemini (by Google).\nOpen models, also known as open-source models, are AI models whose code, architecture, and, often, training data are publicly available. Examples: Gemma (by Google), LLaMA (by Meta), and Phi (by Microsoft).\nOpen models are particularly relevant for running models on edge devices like Raspberry Pi as they can be more easily adapted, optimized, and deployed in resource-constrained environments. Still, it is crucial to verify their Licenses. Open models come with various open-source licenses that may affect their use in commercial applications, while closed models have clear, albeit restrictive, terms of service.\n\n\n\nAdapted from https://arxiv.org/pdf/2304.13712\n\n\n\n\nSmall Language Models (SLMs)\nIn the context of edge computing on devices like Raspberry Pi, full-scale LLMs are typically too large and resource-intensive to run directly. This limitation has driven the development of smaller, more efficient models, such as the Small Language Models (SLMs).\nSLMs are compact versions of LLMs designed to run efficiently on resource-constrained devices such as smartphones, IoT devices, and single-board computers like the Raspberry Pi. These models are significantly smaller in size and computational requirements than their larger counterparts while still retaining impressive language understanding and generation capabilities.\nKey characteristics of SLMs include:\n\nReduced parameter count: Typically ranging from a few hundred million to a few billion parameters, compared to two-digit billions in larger models.\nLower memory footprint: Requiring, at most, a few gigabytes of memory rather than tens or hundreds of gigabytes.\nFaster inference time: Can generate responses in milliseconds to seconds on edge devices.\nEnergy efficiency: Consuming less power, making them suitable for battery-powered devices.\nPrivacy-preserving: Enabling on-device processing without sending data to cloud servers.\nOffline functionality: Operating without an internet connection.\n\nSLMs achieve their compact size through various techniques such as knowledge distillation, model pruning, and quantization. While they may not match the broad capabilities of larger models, SLMs excel in specific tasks and domains, making them ideal for targeted applications on edge devices.\n\nWe will generally consider SLMs —language models with fewer than 5 to 8 billion parameters — quantized to 4 bits.\n\n\nExamples of SLMs include compressed versions of models like Meta Llama, Microsoft PHI, and Google Gemma. These models enable a wide range of natural language processing tasks directly on edge devices, from text classification and sentiment analysis to question answering and limited text generation.\nFor more information on SLMs, the paper, LLM Pruning and Distillation in Practice: The Minitron Approach, provides an approach applying pruning and distillation to obtain SLMs from LLMs. And, SMALL LANGUAGE MODELS: SURVEY, MEASUREMENTS, AND INSIGHTS, presents a comprehensive survey and analysis of Small Language Models (SLMs), which are language models with 100 million to 5 billion parameters designed for resource-constrained devices."
  },
  {
    "objectID": "raspi/llm/slm_intro.html#ollama",
    "href": "raspi/llm/slm_intro.html#ollama",
    "title": "Small Language Models (SLM)",
    "section": "Ollama",
    "text": "Ollama\n\n\n\nollama logo\n\n\nThe primary and most user-friendly tool for running Small Language Models (SLMs) directly on a Raspberry Pi, especially the Pi 5, is Ollama. It is an open-source framework that allows us to install, manage, and run various SLMs (such as TinyLlama, smollm, Microsoft Phi, Google Gemma, Meta Llama, MoonDream, LLaVa, among others) locally on our Raspberry Pi for tasks such as text generation, image captioning, and translation.\n\nAlternatives options:\nllama.cppand the Hugging Face Transformers library are well-supported for running SLMs on Raspberry Pi. llama.cpp is particularly efficient for running quantized models natively. At the same time, Hugging Face Transformers offers a broader range of models and tasks, which are best suited to smaller architectures due to hardware limitations. Note that Ollama run llama.cpp under the hood.\n\nHere are some critical points about Ollama:\n\nLocal Model Execution: Ollama enables running LMs on personal computers or edge devices such as the Raspi-5, eliminating the need for cloud-based API calls.\nEase of Use: It provides a simple command-line interface for downloading, running, and managing different language models.\nModel Variety: Ollama supports various LLMs, including Phi, Gemma, Llama, Mistral, and other open-source models.\nCustomization: Users can create and share custom models tailored to specific needs or domains.\nLightweight: Designed to be efficient and run on consumer-grade hardware.\nAPI Integration: Offers an API that allows integration with other applications and services.\nPrivacy-Focused: By running models locally, it addresses privacy concerns associated with sending data to external servers.\nCross-Platform: Available for macOS, Windows, and Linux systems (our case, here).\nActive Development: Regularly updated with new features and model support.\nCommunity-Driven: Benefits from community contributions and model sharing.\n\nTo learn more about what Ollama is and how it works under the hood, you should see this short video from Matt Williams, one of the founders of Ollama:\n\n\nMatt has an entirely free course about Ollama that we recommend: \n\n\nInstalling Ollama\nLet’s set up and activate a Virtual Environment for working with Ollama:\npython3 -m venv ~/ollama\nsource ~/ollama/bin/activate\nAnd run the command to install Ollama:\ncurl -fsSL https://ollama.com/install.sh | sh\nAs a result, an API will run in the background on 127.0.0.1:11434. From now on, we can run Ollama via the terminal. For starting, let’s verify the Ollama version, which will also tell us that it is correctly installed:\nollama -v\n\nOn the Ollama Library page, we can find the models Ollama supports. For example, by filtering by Most popular, we can see Meta Llama, Google Gemma, Microsoft Phi, LLaVa, etc.\n\n\nMeta Llama 3.2 1B/3B\n\n\n\n\n\nLet’s install and run our first small language model, Llama 3.2 1B (and 3B). The Meta Llama, 3.2 collections of multilingual large language models (LLMs), is a collection of pre-trained and instruction-tuned generative models in 1B and 3B sizes (text in/text out). The Llama 3.2 instruction-tuned text-only models are optimized for multilingual dialogue use cases, including agentic retrieval and summarization tasks.\nThe 1B and 3B models were pruned from the Llama 8B, and then logits from the 8B and 70B models were used as token-level targets (token-level distillation). Knowledge distillation was used to recover performance (they were trained with 9 trillion tokens). The 1B model has 1,24B, quantized to integer (Q8_0), and the 3B, 3.12B parameters, with a Q4_0 quantization, which ends with a size of 1.3 GB and 2GB, respectively. Its context window is 131,072 tokens.\n\nInstall and run the Model\nollama run llama3.2:1b\nRunning the model with the command before, we should have the Ollama prompt available for us to input a question and start chatting with the LLM model; for example,\n&gt;&gt;&gt; What is the capital of France?\nAlmost immediately, we get the correct answer:\nThe capital of France is Paris.\nUsing the option --verbose when calling the model will generate several statistics about its performance (The model will be polling only the first time we run the command).\n\nEach metric gives insights into how the model processes inputs and generates outputs. Here’s a breakdown of what each metric means:\n\nTotal Duration (2.620170326s): This is the complete time taken from the start of the command to the completion of the response. It encompasses loading the model, processing the input prompt, and generating the response.\nLoad Duration (39.947908ms): This duration indicates the time to load the model or necessary components into memory. If this value is minimal, it can suggest that the model was preloaded or that only a minimal setup was required.\nPrompt Eval Count (32 tokens): The number of tokens in the input prompt. In NLP, tokens are typically words or subwords, so this count includes all the tokens that the model evaluated to understand and respond to the query.\nPrompt Eval Duration (1.644773s): This measures the model’s time to evaluate or process the input prompt. It accounts for the bulk of the total duration, implying that understanding the query and preparing a response is the most time-consuming part of the process.\nPrompt Eval Rate (19.46 tokens/s): This rate indicates how quickly the model processes tokens from the input prompt. It reflects the model’s speed in terms of natural language comprehension.\nEval Count (8 token(s)): This is the number of tokens in the model’s response, which in this case was, “The capital of France is Paris.”\nEval Duration (889.941ms): This is the time taken to generate the output based on the evaluated input. It’s much shorter than the prompt evaluation, suggesting that generating the response is less complex or computationally intensive than understanding the prompt.\nEval Rate (8.99 tokens/s): Similar to the prompt eval rate, this indicates the speed at which the model generates output tokens. It’s a crucial metric for understanding the model’s efficiency in output generation.\n\nThis detailed breakdown can help understand the computational demands and performance characteristics of running SLMs like Llama on edge devices like the Raspberry Pi 5. It shows that while prompt evaluation is more time-consuming, the actual generation of responses is relatively quicker. This analysis is crucial for optimizing performance and diagnosing potential bottlenecks in real-time applications.\nLoading and running the 3B model, we can see the difference in performance for the same prompt;\n\nThe eval rate is lower, 5.3 tokens/s versus 9 tokens/s with the smaller model.\nWhen question about\n&gt;&gt;&gt; What is the distance between Paris and Santiago, Chile?\nThe 1B model answered 9,841 kilometers (6,093 miles), which is inaccurate, and the 3B model answered 7,300 miles (11,700 km), which is close to the correct (11,642 km).\nLet’s ask for the Paris’s coordinates:\n&gt;&gt;&gt; what is the latitude and longitude of Paris?\nThe latitude and longitude of Paris are 48.8567° N (48°55' \n42\" N) and 2.3510° E (2°22' 8\" E), respectively.\n\n\n\n\n\nBoth 1B and 3B models gave correct answers.\n\n\nGoogle Gemma\n\n\n\n\n\nGoogle Gemma, is a collection of lightweight, state-of-the-art open models built from the same technology that powers our Gemini models. Today, the Gemma family has the Gemma3 and Gemma3n models.\n\nGemma3\nWe can, for example, install gemma3:latest, using ollama run gemma3:latest. This model has 4.3B parameters, with a context length of 8,192 and an embedding length of 2,560. A typical quantization schema is the Q4_K_M. This model has vision capabilities. Besides the 4B, we can also install the 1B parameter model.\nInstall and run the Model\nollama run gemma3:4b --verbose\nRunning the model with the command before, we should have the Ollama prompt available for us to input a question and start chatting with the LLM model; for example,\n&gt;&gt;&gt; What is the capital of France?\nAlmost immediately, we get the correct answer:\nThe capital of France is **Paris**. It's a global center for art, fashion, gastronomy, and culture. 😊 Do you want to know anything more about Paris?\nAnd its statistics.\n\n\nWe can see that Gemma 3:4B has roughly the same performance as Lama 3.2:3B, despite having more parameters.\n\nOther examples:\n\nAlso, a good response but less accurate than Llama3.2:3B.\n\nA good and accurate answer (a little more verbose than the Llama answers).\nAn advantage of the Gemma3 models are their vision capability, for example, we can ask it to caption an image:\n\nThis is a very accurate model, but it still has high latency, as we can see in the example above (more than 3 minutes to caption the image).\n\nThe Gemma 3 1B size models are text only and don’t support image input.\n\n\n\nGemma 3n\nGemma3 is a powerful, efficient open-source model that runs locally on phones, tablets, and laptops. The models are listed with parameter counts, such as E2B and E4B, that are lower than the total number of parameters contained in the models. The E prefix indicates these models can operate with a reduced set of Effective parameters. This reduced-parameter operation can be achieved using the flexible parameter technology built into Gemma 3n models, which helps them run efficiently on lower-resource devices.\nThe parameters in Gemma 3n models are divided into four main groups: text, visual, audio, and per-layer embedding (PLE) parameters. In the standard execution of the E2B model, over 5 billion parameters are loaded. However, by using parameter skipping and PLE caching, this model can be operated with an effective memory load of just under 2 billion (1.91B) parameters, as illustrated below:\n\n\n\nImmage from Google: Gemma 3n diagram of parameter usage\n\n\nOnce installed, (using ollama run gemma3n:e2b), inspecting the model we get:\n\nArchitecture: gemma3n\n\nParameters: 4.5B\n\nContext length 32768\n\nEmbedding length 2048\n\nQuantization Q4_K_M\nCapabilities: completion only\n\nWhen we run it, we can see that, despite having 4.5B parameters, it is faster than Llama3.3:3B.\n\n\n\n\nMicrosoft Phi3.5 3.8B\n\n\n\n\n\nLet’s pull now the PHI3.5, a 3.8B lightweight state-of-the-art open model by Microsoft. The model belongs to the Phi-3 model family and supports 128K token context length and the languages: Arabic, Chinese, Czech, Danish, Dutch, English, Finnish, French, German, Hebrew, Hungarian, Italian, Japanese, Korean, Norwegian, Polish, Portuguese, Russian, Spanish, Swedish, Thai, Turkish, and Ukrainian.\nThe model size, in terms of bytes, will depend on the specific quantization format used. The size can go from 2-bit quantization (q2_k) of 1.4 GB (higher performance/lower quality) to 16-bit quantization (fp-16) of 7.6 GB (lower performance/higher quality).\nLet’s run the 4-bit quantization (Q4_0), which will need 2.2 GB of RAM, with an intermediary trade-off regarding output quality and performance.\nollama run phi3.5:3.8b --verbose\n\nYou can use run or pull to download the model. What happens is that Ollama keeps note of the pulled models, and once the PHI3 does not exist, before running it, Ollama pulls it.\n\nLet’s enter with the same prompt used before:\n&gt;&gt;&gt; What is the capital of France?\n\nThe capital of France is Paris. It' extradites significant\nhistorical, cultural, and political importance to the country as\nwell as being a major European city known for its art, fashion,\ngastronomy, and culture. Its influence extends beyond national\nborders, with millions of tourists visiting each year from around\nthe globe. The Seine River flows through Paris before it reaches\nthe broader English Channel at Le Havre. Moreover, France is one\nof Europe's leading economies with its capital playing a key role\n\n...\nThe answer was very “verbose”, let’s specify a better prompt:\n\nIn this case, the answer was still longer than we expected, with an eval rate of 2.25 tokens/s, more than double that of Gemma and Llama.\n\nChoosing the most appropriate prompt is one of the most important skills to be used with LLMs, no matter its size.\n\nWhen we asked the same questions about distance and Latitude/Longitude, we did not get a good answer for a distance of 13,507 kilometers (8,429 miles), but it was OK for coordinates. Again, it could have been less verbose (more than 200 tokens for each answer).\nWe can use any model as an assistant since their speed is relatively decent, but in October 2024, the Llama2:3B or Gemma 3 are better choices. Try other models, depending on your needs. 🤗 Open LLM Leaderboard can give you an idea about the best models in size, benchmark, license, etc.\n\nThe best model to use is the one fit for your specific necessity. Also, take into consideration that this field evolves with new models everyday.\n\n\n\nMoonDream\n\n\n\n\n\nMoondream is an open-source visual language model that understands images using simple text prompts. It’s fast and wildly capable. It is 3 to 4 times faster than the Gemma3, for example.\n\nMoondream is a compact and efficient open-source vision-language model (VLM) designed to analyze images and generate descriptive text, answer questions, detect objects, count, point, and perform OCR—all through natural-language instructions, even on resource-limited devices like the Raspberry Pi or edge hardware.\nBelow is a diagram showing the flux from an image and its description:\n\nMoondream can interpret images much like a human would, enabling tasks like: • Captioning and describing images in detail • Visual question answering (VQA) such as “What is the color of the shirt?” • Object detection and pointing out locations • Counting and contextual reasoning about visual scenes\n\n\n\nLava-Phi-3\n\n\n\n\n\nAnother multimodal model is the LLaVA-Phi-3, a fine-tuned LLaVA model from Phi 3 Mini 4k. It has strong performance benchmarks that are on par with the original LLaVA (Large Language and Vision Assistant) model.\n\nIn terms of latency, it is a pair with Gemma3 and much slower than MoonDream\n\nThe LLaVA-Phi-3 is an end-to-end trained large multimodal model designed to understand and generate content based on visual inputs (images) and textual instructions. It combines the capabilities of a visual encoder and a language model to process and respond to multimodal inputs.\nLet’s install the model:\nollama run llava-phi3:3.8b --verbose\nLet’s start with a text input:\n&gt;&gt;&gt; You are a helpful AI assistant. What is the capital of France?\n\nAs an AI language model, I can tell you that the capital of France \nis Paris. It's not only the largest city in the country but also \nserves as its political and administrative center. Paris is known \nfor its iconic landmarks such as the Eiffel Tower, Notre-Dame \nCathedral, and the Louvre Museum. The city has a rich history, \nbeautiful architecture, and is widely considered to be one of the \nmost romantic cities in the world.\nThe response took around 30s, with an eval rate of 3.93 tokens/s! Not bad!\nBut let us know to enter with an image as input. For that, let’s create a directory for working:\ncd Documents/\nmkdir OLLAMA\ncd OLLAMA\nLet’s download a 640x320 image from the internet, for example (Wikipedia: Paris, France):\n\nUsing FileZilla, for example, let’s upload the image to the OLLAMA folder on the Raspi-5 and name it image_test_1.jpg. We should have the whole image path (we can use pwd to get it).\n/home/mjrovai/Documents/OLLAMA/image_test_1.jpg\nIf you use a desktop, you can copy the image path by right-clicking the image.\n\nLet’s enter with this prompt:\n&gt;&gt;&gt; Describe the image /home/mjrovai/Documents/OLLAMA/image_test_1.jpg\nThe result was great, but the overall latency was significant; almost 4 minutes to perform the inference.\n\n\n\nInspecting Model parameters\nIt is possible to know more about each model downloaded by Ollama, using the command ollama show &lt;model&gt;. For example:\nollama show llama3.2:3b\n\nWe’re looking at a 3.2B‑parameter Llama‑family instruction model, quantized for efficient local use. Let’s walk through each field and what it implies in practice.\n\nHigh‑level identity\n\nArchitecture (Family): llama\nThis is an LLaMA‑style transformer (same general architecture as Meta’s Llama 3.x family): decoder‑only transformer, causal (left‑to‑right) language model, multi‑head attention, MLP blocks, rotary position embeddings, etc.\n\nParameter Size: 3.2B\nThe base model has about 3.2 billion trainable parameters. This puts it in the “small to mid‑size” range: much larger than tiny 1B models, but far below 7B/8B/70B. On a modern CPU/GPU it’s suitable for real‑time-ish local inference, especially after quantization.\n\n\n\nCapabilities\n\nSupported Capabilities: ['completion', 'tools']\n\ncompletion: classic text generation—answering questions, writing code, explanations, etc.\n\ntools: the model has been trained/fine‑tuned to call tools/functions (e.g., JSON function calls) when the host runtime exposes them, so it can decide “I should call a function to get data” vs just answering from its own weights.\n\n\n\nThis doesn’t mean the tools are built into the model; it means it knows how to format tool calls when the surrounding system supports them.\n\n\n\nQuantization: Q4_K_M\n\nQuantization Level: Q4_K_M\nThis is a 4‑bit K‑quantization variant, typically from GGUF/GGML families:\n\nWeights are stored at about 4 bits per parameter instead of 16/32, drastically reducing RAM and disk size.\nK_M is a specific quantization scheme/variant that balances compression with accuracy; it’s usually better than older naive 4‑bit schemes (higher effective precision per parameter block).\n\nPractical impact:\n\nMuch smaller VRAM/RAM requirement, so a 3.2B model can run comfortably on a Pi 5 + NPU/GPU or a modest desktop GPU.\nSlight drop in raw quality vs the original FP16/FP32 model, but for many tasks (chat, simple coding, small reasoning tasks) it’s still very usable.\nIdeal for edge or low‑power devices.\n\n\n\n\nContext and embedding sizes\n\nllama.context_length: 131072\nContext length is 131,072 tokens (~131k):\n\nThis is a very large context window (comparable to “128k context” models).\nIt can ingest long documents, multiple files, or extended conversations without immediately forgetting earlier content.\nUnder the hood, this usually implies some form of advanced attention scaling (e.g., RoPE scaling, attention optimizations, maybe sparse/flash attention), but the key user effect is: we can feed a lot of text at once.\n\nllama.embedding_length: 3072\nThe hidden/embedding dimension is 3072:\n\nEach token is represented as a 3072‑dimensional vector internally.\nThis roughly sets the “width” of the model: wider models can represent richer patterns but cost more per token.\nOften, this is also the size of embeddings we’d extract if using the model as an encoder (assuming our runtime supports that mode).\n\n\n\n\nLicense and usage\n\nLicense Short: LLAMA 3.2 COMMUNITY LICENSE AGREEMENT\n\nIt’s under a Llama community license, not pure open source.\nTypically allows broad research and many commercial uses, but with conditions (e.g., restrictions around competition, misuse, or deployment size).\n\nWe should read the exact license text bundled with the model before using it in commercial products, especially at scale.\n\n\n\n\nParameters\nThe only parameters explicitly set in this Modelfile are three stop strings:\n\n&lt;|start_header_id|&gt;\n&lt;|end_header_id|&gt;\n&lt;|eot_id|&gt;\n\nNo temperature, top_k, top_p, etc., are defined in the Model file. Those, therefore, use Ollama’s defaults for this model at runtime.\n\ntemperature 0.3,\ntop_p 0.9and\ntop_k 40\n\nWe will learn more about those parameters in the next section.\n\n\n\nInspecting local resources\nUsing htop, we can monitor the resources running on our device.\nhtop\nDuring the time that the model is running, we can inspect the resources:\n\nAll four CPUs run at almost 100% of their capacity, and the memory used with the model loaded is 3.24GB. Exiting Ollama, the memory goes down to around 377MB (with no desktop).\nIt is also essential to monitor the temperature. When running the Raspberry with a desktop, you can have the temperature shown on the taskbar:\n\nIf you are “headless”, the temperature can be monitored continuously (for example, every 2 seconds) with the command:\nwatch -n 2 vcgencmd measure_temp\nIf you are doing nothing, the temperature is around 50°C for CPUs running at 1%. During inference, with the CPUs at 100%, the temperature can rise to almost 70°C. This is OK and means the active cooler is working, keeping the temperature below 80°C / 85°C (its limit)."
  },
  {
    "objectID": "raspi/llm/slm_intro.html#ollama-python-library",
    "href": "raspi/llm/slm_intro.html#ollama-python-library",
    "title": "Small Language Models (SLM)",
    "section": "Ollama Python Library",
    "text": "Ollama Python Library\nSo far, we have explored SLMs’ chat capability using the command line on a terminal. However, we want to integrate those models into our projects, so Python seems to be the right path. The good news is that Ollama has such a library.\nThe Ollama Python library simplifies interaction with advanced LLM models, enabling more sophisticated responses and capabilities, besides providing the easiest way to integrate Python 3.8+ projects with Ollama.\nFor a better understanding of how to create apps using Ollama with Python, we can follow Matt Williams’s videos, as the one below:\n\nInstallation:\nIn the terminal (and inside the virtual environment), run the command:\npip install ollama\nLet’s test the installation on terminal using the Python interpetrer:\n\nFor better using, we will need a text editor or an IDE to create a Python script. If we run Raspberry Pi OS on a desktop, several options, such as Thonny and Geany, are already installed by default (accessed via [Menu][Programming]). You can download other IDEs, such as Visual Studio Code, from [Menu][Recommended Software]. When the window pops up, go to [Programming], select your option, and press [Apply].\n\nA good option to run Python scripts during development is to use the Jupyter Notebook:\npip install jupyter\njupyter notebook --generate-config\nWe can use the Jupyter Notebook using SSH on our host computer. Run the command below, changing the IP address for the Raspi:\njupyter notebook --ip=192.168.4.209 --no-browser\nOn the terminal, you can see the local URL address to open the notebook:\n\nWe can access it from another computer by entering the Raspberry Pi’s IP address and the provided token in a web browser (we should copy it from the terminal).\nIn our working directory (Documents/OLLAMA/) on the Raspberry Pi, we will create a new Python 3 notebook.\n\nLet’s enter with a very simple script to verify the installed models:\nimport ollama\nmodels = ollama.list()\nfor model in models['models']:\n    print(model['model'])\ngemma3:4b\nriven/smolvlm:latest\ngemma3n:e2b\nmoondream:latest\nllama3.2:3b\nSame as on the terminal using ollama show llama3.2: 3b, we can get model information with the Python command ollama.show('llama3.2:3b'):\ninfo = ollama.show('llama3.2:3b')\n\nprint(\"Model/Format          :\", getattr(info, 'model', None))\nprint(\"Parameter Size        :\", getattr(info.details, 'parameter_size', None))\nprint(\"Quantization Level    :\", getattr(info.details, 'quantization_level', None))\nprint(\"Family                :\", getattr(info.details, 'family', None))\nprint(\"Supported Capabilities:\", getattr(info, 'capabilities', None))\nprint(\"Date Modified (Local) :\", getattr(info, 'modified_at', None))\nprint(\"License Short         :\", (str(getattr(info, 'license', None)).split('\\n')[0]))\n\nprint(\"Key Architecture Details:\")\n\nfor key in [\n    'general.architecture', 'general.finetune',\n    'llama.context_length', 'llama.embedding_length', 'llama.block_count'\n]:\n    print(f\" - {key}: {info.modelinfo.get(key)}\")\nAs a result, we have:\nModel/Format               : None\nParameter Size             : 3.2B\nQuantization Level         : Q4_K_M\nFamily                     : llama\nSupported Capabilities      : ['completion', 'tools']\nDate Modified (Local)      : 2025-09-15 15:39:48.153510-03:00\nLicense Short              : LLAMA 3.2 COMMUNITY LICENSE AGREEMENT\nKey Architecture Details   :\n - general.architecture: llama\n - general.finetune: Instruct\n - llama.context_length: 131072\n - llama.embedding_length: 3072\n - llama.block_count: 28\nBesides the information got on terminal, here we can also see:\ngeneral.finetune: Instruct : The base Llama model has been fine‑tuned on instruction‑following data (Q&A, chat, tool‑using examples). This means:\n\nIt expects prompts that look like “user asks → model answers”.\nIt tends to produce helpful, direct responses instead of random continuation.\nIt may follow some chat‑style formatting conventions (system/user/assistant turns) depending on the template used in our runtime.\n\nllama.block_count: 28 The transformer has 28 decoder blocks (layers):\n\nMore layers typically improve the model’s ability to perform multi‑step reasoning and to build hierarchical representations.\nFor 3.2B parameters, 28 layers with width 3072 is a sensible trade‑off (moderately deep, not extremely wide).\nCompared with a 7B model (~32–40 layers, 4k+ width), this will be less capable on very complex reasoning or coding, but noticeably more capable than 1B‑class models.\n\nModel/Format: None : This usually means the frontend (e.g., our UI or manager) didn’t detect a specific “format template” (like “chat”, “instruct”, “plain”) beyond knowing it’s Llama:\n\nThe underlying file is probably something like a GGUF or similar binary weight file.\nPrompt formatting will depend on our runtime’s default “llama‑instruct” template; if nothing is configured, we may need to wrap instructions manually (system/user style or clear “You are an assistant…” preamble).\n\n\nOllama Generate\nLet’s repeat one of the questions that we did before, but now using ollama.generate() from the Ollama Python library. This API generates a response for the given prompt using the provided model. This is a streaming endpoint, so there will be a series of responses. The final response object will include statistics and additional data from the request.\nresponse = ollama.generate(\n    model=\"llama3.2:3b\",\n    prompt=\"What is the capital of Brazil\"\n)\nprint(response['response'])\nWe get the response: The capital of Brazil is Brasília.\n\nIf you are running the code as a Python script, you should save it as, for example, test_ollama.py. You can run it in the IDE or run it directly in the terminal. Also, remember always to call the model and define it when running a stand-alone script.\npython test_ollama.py\n\nLet’s create a function to work better the model:\ndef simple_query(prompt, model=MODEL):\n    response = ollama.generate(\n    model=MODEL,\n    prompt=prompt\n    )\n    return response\nAnd run a new query:\nMODEL=\"llama3.2:3b\"\nresponse = simple_query (\"What is the capital of Peru?\")\nresponse\nLet’s print the full response now. As a result, we will have the model response in a JSON format:\nGenerateResponse(model='llama3.2:3b', created_at='2026-02-26T18:58:30.730414614Z', done=True, done_reason='stop', total_duration=2307368739, load_duration=343767495, prompt_eval_count=32, prompt_eval_duration=722764406, eval_count=8, eval_duration=1229567370, response='The capital of Peru is Lima.', thinking=None, context=[128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 271, 128009, 128006, 882, 128007, 271, 3923, 374, 279, 6864, 315, 48847, 30, 128009, 128006, 78191, 128007, 271, 791, 6864, 315, 48847, 374, 63053, 13], logprobs=None)\nWe can print the full answer better:\nimport json\nprint(json.dumps(response.__dict__, indent=2))\n{\n  \"model\": \"llama3.2:3b\",\n  \"created_at\": \"2026-02-26T18:58:30.730414614Z\",\n  \"done\": true,\n  \"done_reason\": \"stop\",\n  \"total_duration\": 2307368739,\n  \"load_duration\": 343767495,\n  \"prompt_eval_count\": 32,\n  \"prompt_eval_duration\": 722764406,\n  \"eval_count\": 8,\n  \"eval_duration\": 1229567370,\n  \"response\": \"The capital of Peru is Lima.\",\n  \"thinking\": null,\n  \"context\": [\n    128006,\n    9125,\n    ...\n    13\n  ],\n  \"logprobs\": null\n}\nAs we can see, several pieces of information are generated, such as:\n\nresponse: the main output text generated by the model in response to our prompt.\n\nThe capital of Peru is Lima.\n\ncontext: the token IDs representing the input and context used by the model. Tokens are numerical representations of text that the language model uses to process it.\n\n\nThe Performance Metrics:\n\ntotal_duration: The total time taken for the operation in nanoseconds.\nload_duration: The time taken to load the model or components in nanoseconds.\nprompt_eval_duration: The time taken to evaluate the prompt in nanoseconds.\neval_count: The number of tokens evaluated during the generation. Here, 9 tokens.\neval_duration: The time taken for the model to generate the response in nanoseconds.\n\nBut what we want is the plain ‘response’ and, perhaps for analysis, the total duration of the inference, so let’s change the code to extract it from the dictionary:\nprint(f\"\\n{response['response']}\")\nprint(f\"\\nTotal Duration: {(response['total_duration']/1e9):.2f} seconds\")\nNow, we got:\nThe capital of Peru is Lima.\n\nTotal Duration: 2.31 seconds\nWe can also get the other available metrics:\nprint(f\"eval_count: {response['eval_count']}\")\nprint(f\"eval_duration: {(response['eval_duration']/1e9):.2f} s\")\nprint(f\"eval_rate: {response['eval_count']/(response['eval_duration']/1e9):.2f} tokens/s\")\neval_count: 8\neval_duration: 1.23 s\neval_rate: 6.51 tokens/s\n\nStreaming with ollama.generate()\nTo stream the output from ollama.generate() in Python, set stream=True and iterate over the generator to print each chunk as it’s produced. This enables real-time response streaming, similar to chat models.\nstream = ollama.generate(\n    model='llama3.2:3b',\n    prompt='Tell me an interesting fact about Brazil',\n    stream=True\n)\n\nfor chunk in stream:\n    print(chunk['response'], end='', flush=True)\n\n\nEach chunk['response'] is a part of the generated text, streamed as it’s created\nThis allows responsive, real-time interaction in the terminal or UI.\n\nThis approach is ideal for long or complex generations, making the user experience feel faster and more interactive.\n\n\nSystem Prompt\nAdd a system parameter to set overall instructions or behavior for the model (useful for role assignment and tone control):\nresponse = ollama.generate(model='llama3.2:3b', \n                prompt='Tell about industry', \n                system='You are an expert on Brazil.', \n                stream=False)\nprint(response['response']) \n\n\n\nTemperature and Sampling\nControl creativity/randomness via temperature, and customize output style with extra settings like top_p and num_ctx (context window size)\nresponse = ollama.generate(\n    model='llama3.2:3b', \n    prompt='Why the sky is blue?', \n    options={'temperature':0.1}, \n    stream=True)\nfor chunk in response:\n    print(chunk['response'], end='', flush=True)\n\nresponse = ollama.chat(\n    messages=[\n        {\"role\": \"user\", \n         \"content\": \"Poetically describe Paris in one short sentence\"},\n    ],\n    model='llama3.2:3b',\n    options={\"temperature\": 1.0} # Set. temp. to 1.0 for more creativity\n)\nprint(response['message']['content'])\nLike a velvet-draped secret, Paris whispers ancient mysteries to the moonlit \nSeine, her sighing shadows weaving an eternal waltz of love and dreams.\n\n\nMore Options\nBesides temperature, we can control a lot via the options={…} parameter in ollama.generate. Key ones:\nSampling/style\n\ntop_k: limit candidates to top K tokens each step, e.g., top_k=40\ntop_p: nucleus sampling cutoff, e.g., top_p=0.9\nrepeat_penalty: discourage repetition, e.g., repeat_penalty=1.1\nseed: fixed seed for reproducible outputs\n\nLength/context\n\nnum_predict: max new tokens, e.g., num_predict=256\nnum_ctx: context window (tokens to keep in RAM), e.g., num_ctx=4096\n\nPutting it together in your code:\nresponse = ollama.generate(\n    model='llama3.2:3b',\n    prompt='Why is the sky blue?',\n    options={\n        'temperature': 0.1,\n        'top_p': 0.9,\n        'top_k': 40,\n        'repeat_penalty': 1.1,\n        'num_predict': 50, # Limit the answer\n        'num_ctx': 4096,\n        'seed': 42,\n    },\n    stream=True,\n)\nfor chunk in response:\n    print(chunk['response'], end='', flush=True)\nHere we will limit the response on 50 tokens:\nThe sky appears blue because of a phenomenon called Rayleigh scattering, named after the\nBritish physicist Lord Rayleigh, who first described it in the late 19th century.\n\nHere's what happens:\n\n1. When sunlight enters Earth's atmosphere, it encounters\n\n\nHow top_k and top_p affect generation\ntop_k and top_p both control how random and diverse the model’s next tokens can be, but they do it in different ways.\ntop_k: fixed shortlist size\n\nAt each step, the model ranks all possible next tokens by probability.\n\ntop_k = K means: keep only the K most likely tokens, set all others to probability 0, then sample from those K.\n\nEffects:\n\nSmall k (1–20) → very focused, deterministic, “on‑rails”; less creative, fewer weird tokens.\nLarge k (50–200+) → more variety and creativity; higher chance of unusual or off‑topic words.\n\nExtreme:\n\ntop_k = 1 → greedy decoding: always pick the single most likely token, almost no randomness.\n\n\ntop_p: dynamic shortlist by probability mass\n\nTokens are sorted by probability.\n\nStarting from the top, you add tokens until their cumulative probability ≥ p. Then you sample only from that set.\n\ntop_p = 0.9 means: “consider just enough top tokens to cover 90% of the probability mass”.\nEffects:\n\nIn confident situations (one token is clearly best), the shortlist may be very small → behavior similar to low k.\nIn uncertain situations (probabilities spread out), more tokens enter the shortlist → more exploration.\n\nTypical:\n\ntop_p ≈ 0.9–0.95 is a common sweet spot for natural but not too wild text.\n\n\nHow they feel in practice\n\nLower top_k / top_p → safer, more repetitive, less surprising.\n\nHigher top_k / top_p → more diverse, more creative, but also more risk of nonsense.\n\nWe can use them together—for example, top_k=40, top_p=0.9—where top_k gives a hard cap and top_p then trims that set by probability mass.\n\n\n\nOllama.chat()\nAnother way to get our response is to use ollama.chat(), which generates the next message in a chat with a provided model. This is a streaming endpoint, so a series of responses will occur. Streaming can be disabled using \"stream\": false. The final response object will also include statistics and additional data from the request.\nPROMPT_1 = 'What is the capital of France?'\n\nresponse = ollama.chat(model=MODEL, messages=[\n{'role': 'user','content': PROMPT_1,},])\nresp_1 = response['message']['content']\nprint(f\"\\n{resp_1}\")\nprint(f\"\\n [INFO] Total Duration: {(res['total_duration']/1e9):.2f} seconds\")\nThe answer is the same as before.\nAn important consideration is that by using ollama.generate(), the response is “clear” from the model’s “memory” after the end of inference (only used once), but If we want to keep a conversation, we must use ollama.chat(). Let’s see it in action:\nPROMPT_1 = 'What is the capital of France?'\nresponse = ollama.chat(model=MODEL, messages=[\n{'role': 'user','content': PROMPT_1,},])\nresp_1 = response['message']['content']\nprint(f\"\\n{resp_1}\")\nprint(f\"\\n [INFO] Total Duration: {(response['total_duration']/1e9):.2f} seconds\")\n\nPROMPT_2 = 'and of Italy?'\nresponse = ollama.chat(model=MODEL, messages=[\n{'role': 'user','content': PROMPT_1,},\n{'role': 'assistant','content': resp_1,},\n{'role': 'user','content': PROMPT_2,},])\nresp_2 = response['message']['content']\nprint(f\"\\n{resp_2}\")\nprint(f\"\\n [INFO] Total Duration: {(response['total_duration']/1e9):.2f} seconds\")\nIn the above code, we are running two queries, and the second prompt considers the result of the first one.\nHere is how the model responded:\nThe capital of France is **Paris**. 🇫🇷 \n\n [INFO] Total Duration: 2.82 seconds\n\nThe capital of Italy is **Rome**. 🇮🇹 \n\n [INFO] Total Duration: 4.46 seconds\nThe above code works with two prompts. Let’s include a conversation variable to really provide the chat with a memory:\n# Initialize conversation history\nconversation = []\n\n# Function to chat with memory\ndef chat_with_memory(prompt, model=MODEL):\n    global conversation\n    \n    # Add user message to conversation\n    conversation.append({\"role\": \"user\", \"content\": prompt})\n    \n    # Generate response with conversation history\n    response = ollama.chat(\n        model=MODEL,\n        messages=conversation\n    )\n    \n    # Add assistant's response to conversation history\n    conversation.append(response[\"message\"])\n    \n    # Return just the response text\n    return response[\"message\"][\"content\"]\n# Question\nprompt = \"What is the capital of Brazil\"\nprint(chat_with_memory(prompt))\n\n# Ask a follow-up question that relies on memory\nfollow_up = \"And Peru?\"\nprint(chat_with_memory(follow_up))\nThe capital of Brazil is Brasília.\nThe capital of Peru is Lima.\n\n\nImage Description:\nAs we did with the visual models and the command line to analyze an image, the same can be done here with Python. Let’s use the same image of Paris, but now with the ollama.generate():\nMODEL = 'llava-phi3:3.8b'\nPROMPT = \"Describe this picture\"\n\nwith open('image_test_1.jpg', 'rb') as image_file:\n    img = image_file.read()\n\nresponse = ollama.generate(\n    model=MODEL,\n    prompt=PROMPT,\n    images= [img]\n)\nprint(f\"\\n{response['response']}\")\nprint(f\"\\n [INFO] Total Duration: {(res['total_duration']/1e9):.2f} seconds\")\nHere is the result:\nThis image captures the iconic cityscape of Paris, France. The vantage point \nis high, providing a panoramic view of the Seine River that meanders through \nthe heart of the city. Several bridges arch gracefully over the river, \nconnecting different parts of the city. The Eiffel Tower, an iron lattice \nstructure with a pointed top and two antennas on its summit, stands tall in the \nbackground, piercing the sky. It is painted in a light gray color, contrasting \nagainst the blue sky speckled with white clouds.\n\nThe buildings that line the river are predominantly white or beige, their uniform\ncolor palette broken occasionally by red roofs peeking through. The Seine River \nitself appears calm and wide, reflecting the city's architectural beauty in its \nsurface. On either side of the river, trees add a touch of green to the urban \nlandscape.\n\nThe image is taken from an elevated perspective, looking down on the city. This \nviewpoint allows for a comprehensive view of Paris's beautiful architecture and \nlayout. The relative positions of the buildings, bridges, and other structures \ncreate a harmonious composition that showcases the city's charm.\n\nIn summary, this image presents a serene day in Paris, with its architectural \nmarvels - from the Eiffel Tower to the river-side buildings - all bathed in soft \ncolors under a clear sky.\n\n [INFO] Total Duration: 256.45 seconds\nThe model took about 4 minutes (256.45 s) to return with a detailed image description.\nLet’s capture an image from the Raspberry Pi camera and get the description, now using the MoonDream model:\nimport time\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom picamera2 import Picamera2\nfrom PIL import Image\n\ndef capture_image(image_path):\n    # Initialize camera\n    picam2 = Picamera2() # default is index 0\n    \n    # Configure the camera\n    config = picam2.create_still_configuration(main={\"size\": (520, 520)})\n    picam2.configure(config)\n    picam2.start()\n    \n    # Wait for the camera to warm up\n    time.sleep(2)\n    \n    # Capture image\n    picam2.capture_file(image_path)\n    print(\"Image captured: \"+image_path)\n    \n    # Stop camera\n    picam2.stop()\n    picam2.close()\nUsing the above code, we can capture an image, which can be displayed with:\ndef show_image(image_path):\n    img = Image.open(image_path)\n    \n    # Display the image\n    plt.figure(figsize=(6, 6))\n    plt.imshow(img)\n    plt.title(\"Captured Image\")\n    plt.show()\nLet’s also create a function to describe the image:\ndef image_description(img_path, model):\n    with open(img_path, 'rb') as file:\n        response = ollama.chat(\n            model=model,\n            messages=[\n              {\n                'role': 'user',\n                'content': '''return the description of the image''',\n                'images': [file.read()],\n              },\n            ],\n            options = {\n              'temperature': 0,\n              }\n      )\n    return response\nNow, let’s put all togheter and capture an image from the camera:\nIMG_PATH = \"/home/mjrovai/Documents/OLLAMA/SST/capt_image.jpg\"\nMODEL = \"moondream:latest\"\napture_image(IMG_PATH)\nshow_image(IMG_PATH)\nresponse = image_description(IMG_PATH, MODEL)\ncaption = response['message']['content']\nprint (\"\\n==&gt; AI Response:\", caption)\nprint(f\"\\n[INFO] ==&gt; Total Duration: {\n(response['total_duration']/1e9):.2f} seconds\")\nPointing the camera at my table:\n\nWe got the description:\nThe image features a green table with various items on it. A white mug adorned with \nblack faces is prominently displayed, and there are several other mugs scattered around \nthe table as well. In addition to the mugs, there's also a microphone placed near them,\nsuggesting that this might be an office or workspace setting where someone could enjoy \ntheir coffee while recording podcasts or audio content.\n\nA computer keyboard can be seen in the background, indicating that it is likely \nconnected to a computer for work purposes. A mouse and a cell phone are also present on\nthe table, further emphasizing the technology-oriented nature of this scene.\n\n[INFO] ==&gt; Total Duration: 82.57 seconds\nWe can now change the image_description function to ask “Who are the faces in the mug?”. The answer:\n==&gt; AI Response: \n The mug has a picture of the Beatles on it.\n\nIn the Ollama_Python_Library Intro notebook, we can find the experiments using the Ollama Python library."
  },
  {
    "objectID": "raspi/llm/slm_intro.html#calling-direct-api",
    "href": "raspi/llm/slm_intro.html#calling-direct-api",
    "title": "Small Language Models (SLM)",
    "section": "Calling Direct API",
    "text": "Calling Direct API\nOne alternative to running an SLM in Python using Ollama is to call the API directly. Let’s explore some advantages and disadvantages of both methods.\nPython Library:\nresponse = ollama.generate(\n  model=MODEL, \n  prompt=QUERY)\nresult = response['response']\nDirect API Calls:\nimport requests\nimport json\n\n# Configuration\nOLLAMA_URL = \"http://localhost:11434/api\"\nMODEL = MODEL \n\nresponse = requests.post(\n    f\"{OLLAMA_URL}/generate\",\n    json={\n        \"model\": MODEL,\n        \"prompt\": QUERY,\n        \"stream\": False\n    }\n)\nresponse = requests.post(\n    f\"{OLLAMA_URL}/generate\",\n    json={\"model\": MODEL, \n          \"prompt\": query, \n          \"stream\": False}\n)\nresult = response.json().get(\"response\", \"\")\n\nOne clear advantage of the Python library is that it handles URL construction, request formatting, and response parsing.\n\n\nError Handling\nPython Library:\n\nRaises specific exceptions (e.g., ollama.ResponseError, ollama.RequestError)\nBetter typed error messages\nAutomatically handles connection issues\n\nDirect API Calls:\n\nWe should manually check the response.status_code\nGeneric HTTP errors\nNeed to handle connection exceptions yourself\n\n\n\nConnection Management\nPython Library:\n\nAutomatically detects Ollama instance (checks OLLAMA_HOST env variable or defaults to localhost:11434)\nBuilt-in connection pooling and retry logic\nHandles timeouts gracefully\n\nDirect API Calls:\n\nWe should specify the URL manually\nMust implement our own retry logic\nNeed to manage connection pooling yourself\n\n\n\nFeatures & Functionality\nPython Library:\n\nClean access to all Ollama features (generate, chat, embeddings, list models, pull, etc.)\nStreaming is simple: for chunk in ollama.generate(..., stream=True)\nType hints and better IDE support\n\nDirect API Calls:\n\nAccess to any API endpoint, including undocumented ones\nFull control over request headers, timeouts, etc.\nCan use any HTTP library (requests, httpx, urllib, etc.)\n\n\n\nDependencies\nPython Library:\npip install ollama\n\nAdds a dependency to our project\nDepends on httpx under the hood\n\nDirect API Calls:\npip install requests\n\nWe choose our HTTP library\nMore lightweight if we only need basic functionality\n\n\n\nAdvanced Features\nPython Library:\n# Streaming\nfor chunk in ollama.generate(model=MODEL, prompt=query, stream=True):\n    print(chunk['response'], end='')\n\n# Chat history\nresponse = ollama.chat(\n    model=MODEL,\n    messages=[\n        {'role': 'user', 'content': 'Hello!'},\n        {'role': 'assistant', 'content': 'Hi there!'},\n        {'role': 'user', 'content': 'How are you?'}\n    ]\n)\n\n# List models\nmodels = ollama.list()\n\n# Pull models\nollama.pull('llama3.2:3b')\nDirect API Calls:\n\nRequire us to implement all these patterns manually\nMore boilerplate code\n\n\n\nWhen to Use Each?\n\nUse the Python Library when:\n\n✅ Building standard applications\n✅ Need cleaner, more maintainable code\n✅ Need good error handling out of the box\n✅ Need streaming support\n✅ Okay with adding a dependency\n\n\n\nUse Direct API Calls when:\n\n✅Need fine-grained control over HTTP requests\n✅ Are working in a constrained environment\n✅ Need to customize timeouts, headers, or proxies\n✅ Need to minimize dependencies\n✅ Are debugging API issues\n✅ Need to access experimental/undocumented endpoints\n\n\n\n\nPerformance Difference\nMinimal difference in practice! The Python library uses httpx which is comparable to requests. Both make the same underlying HTTP calls to Ollama.\n\nBottom line: For most use cases, the Python library is the better choice due to its simplicity and built-in features. Use direct API calls only when you need specific control or have constraints that prevent adding the dependency."
  },
  {
    "objectID": "raspi/llm/slm_intro.html#going-further",
    "href": "raspi/llm/slm_intro.html#going-further",
    "title": "Small Language Models (SLM)",
    "section": "Going Further",
    "text": "Going Further\nThe small LLM models tested worked well at the edge, both with text and with images, but, of course, the last one had high latency. A combination of specific, dedicated models can lead to better results; for example, in real cases, an Object Detection model (such as YOLO) can provide a general description and count of objects in an image, which, once passed to an LLM, can help extract essential insights and actions.\nAccording to Avi Baum, CTO at Hailo,\n\nIn the vast landscape of artificial intelligence (AI), one of the most intriguing journeys has been the evolution of AI on the edge. This journey has taken us from classic machine vision to the realms of discriminative AI, enhancive AI, and now, the groundbreaking frontier of generative AI. Each step has brought us closer to a future where intelligent systems seamlessly integrate with our daily lives, offering an immersive experience of not just perception but also creation at the palm of our hand."
  },
  {
    "objectID": "raspi/llm/slm_intro.html#conclusion",
    "href": "raspi/llm/slm_intro.html#conclusion",
    "title": "Small Language Models (SLM)",
    "section": "Conclusion",
    "text": "Conclusion\nThis chapter has demonstrated how a Raspberry Pi 5 can be transformed into a potent AI hub capable of running large language models (LLMs) for real-time, on-site data analysis and insights using Ollama and Python. The Raspberry Pi’s versatility and power, coupled with the capabilities of lightweight LLMs like Llama 3.2 and MoonDream, make it an excellent platform for edge computing applications.\nThe potential of running LLMs on the edge extends far beyond simple data processing, as in this lab’s examples. Here are some innovative suggestions for using this project:\n1. Smart Home Automation:\n\nIntegrate SLMs to interpret voice commands or analyze sensor data for intelligent home automation. This could include real-time monitoring and control of home devices, security systems, and energy management, all processed locally without relying on cloud services.\n\n2. Field Data Collection and Analysis:\n\nDeploy SLMs on Raspberry Pi in remote or mobile setups for real-time data collection and analysis. This can be used in agriculture to monitor crop health, in environmental studies for wildlife tracking, or in disaster response for situational awareness and resource management.\n\n3. Educational Tools:\n\nCreate interactive educational tools that leverage SLMs to provide instant feedback, language translation, and tutoring. This can be particularly useful in developing regions with limited access to advanced technology and internet connectivity.\n\n4. Healthcare Applications:\n\nUse SLMs for medical diagnostics and patient monitoring. They can provide real-time analysis of symptoms and suggest potential treatments. This can be integrated into telemedicine platforms or portable health devices.\n\n5. Local Business Intelligence:\n\nImplement SLMs in retail or small business environments to analyze customer behavior, manage inventory, and optimize operations. The ability to process data locally ensures privacy and reduces dependency on external services.\n\n6. Industrial IoT:\n\nIntegrate SLMs into industrial IoT systems for predictive maintenance, quality control, and process optimization. The Raspberry Pi can serve as a localized data processing unit, reducing latency and improving the reliability of automated systems.\n\n7. Autonomous Vehicles:\n\nUse SLMs to process sensory data from autonomous vehicles, enabling real-time decision-making and navigation. This can be applied to drones, robots, and self-driving cars for enhanced autonomy and safety.\n\n8. Cultural Heritage and Tourism:\n\nImplement SLMs to provide interactive and informative cultural heritage sites and museum guides. Visitors can use these systems to get real-time information and insights, enhancing their experience without internet connectivity.\n\n9. Artistic and Creative Projects:\n\nUse SLMs to analyze and generate creative content, such as music, art, and literature. This can foster innovative projects in the creative industries and allow for unique interactive experiences in exhibitions and performances.\n\n10. Customized Assistive Technologies:\n\nDevelop assistive technologies for individuals with disabilities, providing personalized and adaptive support through real-time text-to-speech, language translation, and other accessible tools."
  },
  {
    "objectID": "raspi/llm/slm_intro.html#resources",
    "href": "raspi/llm/slm_intro.html#resources",
    "title": "Small Language Models (SLM)",
    "section": "Resources",
    "text": "Resources\n\nOllama_Python_Library Intro notebook"
  },
  {
    "objectID": "raspi/llm/slm_opt_tech.html#introduction",
    "href": "raspi/llm/slm_opt_tech.html#introduction",
    "title": "SLM: Basic Optimization Techniques",
    "section": "Introduction",
    "text": "Introduction\nLarge Language Models (LLMs) have revolutionized natural language processing, but their deployment and optimization come with unique challenges. One significant issue is the tendency for LLMs (and more, the SLMs) to generate plausible-sounding but factually incorrect information, a phenomenon known as hallucination. This occurs when models produce content that appears coherent but lacks grounding in truth or real-world facts.\nOther challenges include the immense computational resources required for training and running these models, the difficulty in maintaining up-to-date knowledge within the model, and the need for domain-specific adaptations. Privacy concerns also arise when handling sensitive data during training or inference. Additionally, ensuring consistent performance across diverse tasks and maintaining ethical use of these powerful tools present ongoing challenges. Addressing these issues is crucial for the effective and responsible deployment of LLMs in real-world applications.\nThe fundamental and more common techniques for enhancing LLM (and SLM) performance and efficiency are Function (or Tool) Calling, Prompt engineering, Fine-tuning, and Retrieval-Augmented Generation (RAG).\n\nFunction (Tool) calling allows models to perform actions beyond generating text. By integrating with external functions or APIs, SLMs can access real-time data, automate tasks, and perform precise calculations—addressing the reliability issues that arise from the model’s limitations in mathematical operations.\nPrompt engineering is at the forefront of LLM optimization. By carefully crafting input prompts, we can guide models to produce more accurate and relevant outputs. This technique involves structuring queries that leverage the model’s pre-trained knowledge and capabilities, often incorporating examples or specific instructions to shape the desired response.\nRetrieval-Augmented Generation (RAG) represents a powerful approach that’s ideal for resource-constrained edge devices. This method combines the knowledge embedded in pre-trained models with the ability to access external, up-to-date information without requiring fine-tuning. By retrieving relevant data from a local knowledge base, RAG significantly enhances accuracy and reduces hallucinations—all without the computational overhead of model retraining.\nFine-tuning, while more resource-intensive, offers a way to specialize LLMs for specific domains or tasks. This process involves further training the model on carefully curated datasets, allowing it to adapt its vast general knowledge to particular applications. Fine-tuning can lead to substantial performance improvements, especially in specialized fields or for unique use cases.\n\nIn this chapter, we’ll start focusing on two techniques that are particularly well-suited for edge devices like the Raspberry Pi: Function Calling and RAG.\n\nWe will learn more in detail about optimization techniques for SLMs, in the chapter: Advancing EdgeAI: Beyond Basic SLMs"
  },
  {
    "objectID": "raspi/llm/slm_opt_tech.html#function-calling-introduction",
    "href": "raspi/llm/slm_opt_tech.html#function-calling-introduction",
    "title": "SLM: Basic Optimization Techniques",
    "section": "Function Calling Introduction",
    "text": "Function Calling Introduction\nSo far, we can see that, with the model’s (“response”) answer to a variable, we can efficiently work with it and integrate it into real-world projects. However, a big problem is that the model can respond differently to the same prompt. Let’s say, as in the last examples, that we want the model’s response to be only the name of a given country’s capital and its coordinates, nothing more, even with very verbose models such as the Microsoft Phi. We can use the Ollama function's calling to guarantee the same answers, which is perfectly compatible with the OpenAI API.\n\nBut what exactly is “function calling”?\nIn modern artificial intelligence, function calling with Large Language Models (LLMs) allows these models to perform actions beyond generating text. By integrating with external functions or APIs, LLMs can access real-time data, automate tasks, and interact with various systems.\nFor instance, instead of merely responding to a weather query, an LLM can call a weather API to fetch the current conditions and provide accurate, up-to-date information. This capability enhances the relevance and accuracy of the model’s responses, making it a powerful tool for driving workflows and automating processes, thereby transforming it into an active participant in real-world applications.\nFor more details about Function Calling, please see this video made by Marvin Prison:\n\nAnd on this link: HuggingFace Function Calling\n\n\nUsing the SLM for calculations\nLet’s do a simple calculation in Python:\n123456*123456\nThe result would be: 15,241,383,936. No issues on it, but let’s ask a SLM to do the same simple task:\nimport ollama\n\nresponse = ollama.chat(\n    model='llama3.2:3B',\n    messages=[{\n        \"role\": \"user\",\n        \"content\": \"What is 123456 multiplied by 123456? Only give me the answer\"\n    }],\n    options={\"temperature\": 0}\n)\nExamining the response: response.message.content, we would get: 51,131,441,376, what is completely wrong!\nThis is a fundamental limitation of LLMs - they’re not calculators. Here’s why the answer is wrong:\n\nWhy LLMs Fail at Math\n\n1. LLMs Predict Text, They Don’t Calculate\n\nLLMs work by predicting the next most likely token based on patterns in training data\nThey don’t perform actual arithmetic operations\nThey’re essentially “guessing” what a plausible answer looks like\n\n\n\n2. Tokenization Issues\nNumbers are broken into tokens in ways that don’t align with mathematical operations:\n\"123456\" might tokenize as: [\"123\", \"456\"] or [\"12\", \"34\", \"56\"]\nThis makes it nearly impossible for the model to “see” the actual numbers properly for computation.\n\n\n3. Pattern Matching vs. Computation\n\nThe model has seen similar multiplication problems in training\nIt tries to recall patterns rather than compute\nFor simple problems (2×3), it might seem to work because it memorizes common facts\nFor larger numbers (123456×123456), it has no memorized pattern to fall back on\n\n\n\n4. The Correct Answer\n123456 × 123456 = 15,241,383,936\nThe LLM will likely give something that “looks” like a big number but is mathematically incorrect.\n\n\n\n\nThe Solution: Function Calling / Tool Use\nThe pattern is:\n\nUse the LLM for understanding intent (classification)\nUse Python for actual computation (the multiply() function)\n\ndef multiply(a, b):\n    \"\"\"Actual computation - always correct\"\"\"\n    result = a * b\n    return f\"The product of {a} and {b} is {result}.\"\n\nWhy Even Temperature=0 Doesn’t Help\nSetting temperature=0 makes the output deterministic (same input → same output), but it doesn’t make it correct. The model will confidently give the same wrong answer every time.\n\n\nLLM/SLM Math Performance by Problem Type\n\n\n\n\n\n\n\n\nProblem Type\nLLM Accuracy\nWhy\n\n\n\n\n2 + 2\n~99%\nMemorized in training\n\n\n47 + 89\n~70%\nSome pattern recognition\n\n\n123 × 456\n~30%\nStruggles with multi-digit\n\n\n123456 × 123456\n~0%\nNo chance without computation\n\n\nCalculate 15% tip on $47.83\n~40%\nMulti-step reasoning fails\n\n\n\n\n\n\nBest Practices\n# ❌ DON'T: Ask LLM/SLM to calculate directly\nresponse = ollama.chat(\n    model='llama3.2:3b',\n    messages=[{\"role\": \"user\", \"content\": \"What is 123456 * 123456?\"}]\n)\n\n# ✅ DO: Use LLM to understand, Python to compute\nclassification = ask_ollama_for_classification(\"What is 123456 * 123456?\")\nif classification[\"type\"] == \"multiplication\":\n    result = multiply(123456, 123456)  # Python does the math\n\nBottom line: Use LLMs for natural language understanding and intent classification, but delegate actual computations to proper tools/functions. This is the core principle behind tool use and function calling in modern LLM applications!"
  },
  {
    "objectID": "raspi/llm/slm_opt_tech.html#function-calling-solution-for-calculations",
    "href": "raspi/llm/slm_opt_tech.html#function-calling-solution-for-calculations",
    "title": "SLM: Basic Optimization Techniques",
    "section": "Function Calling Solution for Calculations",
    "text": "Function Calling Solution for Calculations\n\nDefine the Tool (Function Schema)\nmultiply_tool = {\n    \"type\": \"function\",\n    \"function\": {\n        \"name\": \"multiply_numbers\",\n        \"description\": \"Multiply two numbers together\",\n        \"parameters\": {\n            \"type\": \"object\",\n            \"required\": [\"a\", \"b\"],\n            \"properties\": {\n                \"a\": {\"type\": \"number\", \"description\": \"First number\"},\n                \"b\": {\"type\": \"number\", \"description\": \"Second number\"}\n            }\n        }\n    }\n}\n\n\nImplement the Function\ndef multiply_numbers(a, b):\n    # Convert to int or float as needed\n    a = float(a)\n    b = float(b)\n    return {\"result\": a * b}\nNow, let’s create a function to handle the user query and calling for the tool when needed.\ndef answer_query(QUERY):\n    response = ollama.chat(\n        'llama3.2:3B',\n        messages=[{\"role\": \"user\", \"content\": QUERY}],\n        tools=[multiply_tool]\n    )\n\n    # Check if the model wants to call the tool\n    if response.message.tool_calls:\n        for tool in response.message.tool_calls:\n            if tool.function.name == \"multiply_numbers\":\n                # Ensure arguments are passed as numbers\n                result = multiply_numbers(**tool.function.arguments)\n                print(f\"Result: {result['result']:,.2f}\")\n            else:\n                print(f\"It is not a Multiplication\")\n \n\nNote the line tools=[multiply_tool], now as a part of the ollama’s calling.\n\nAnd run the function, we will get the correct answer.\nanswer_query(\"What is 123456 multiplied by 123456?\")\nResult: 15,241,383,936.00\nGreat! And now, can I use the same code to answer general questions? Let’s test it:\nanswer_query(\"What is the capital of Brazil?\")\nResult: 1,000,000.00\nThe result is wrong. So, the above approach works fine for using the tool, but to answer it correctly (even without a tool), we should implement an “agentic approach”, which is a subject for later (See the Chapter: Advancing EdgeAI: Beyond Basic SLMs)"
  },
  {
    "objectID": "raspi/llm/slm_opt_tech.html#project-calculating-distances",
    "href": "raspi/llm/slm_opt_tech.html#project-calculating-distances",
    "title": "SLM: Basic Optimization Techniques",
    "section": "Project: Calculating Distances",
    "text": "Project: Calculating Distances\nSuppose we want an SLM to return the distance in km from the capital city of the country specified by the user to the user’s current location. We can see that the first is not so simple: it is not always enough to enter only the country’s name; the SLM can also give us a different (and incorrect) answer every time.\n\nOK, for trying to mitigate it, let’s create an app where the user enters a country’s name and gets, as an output, the distance in km from the capital city of such a country and the app’s location (for simplicity, we will use Santiago, Chile, as the app location).\n\nOnce the user enters a country name, the model should return the capital city’s name (as a string) and its latitude and longitude (as floats). Using those coordinates, we can use a simple Python library (haversine) to compute the great‑circle distance between the two latitude/longitude points.\nThe idea of this project is to demonstrate a combination of language model interaction (IA) and geospatial calculations using the Haversine formula (traditional computing).\nFirst, let us install the Haversine library:\npip install haversine\nNow, we should create a Python script designed to interact with our model (LLM) to determine the coordinates of a country’s capital city and calculate the distance from Santiago de Chile to that capital.\nLet’s go over the code:\nImporting Libraries\nimport time\nfrom haversine import haversine\nfrom ollama import chat\nBasic Variables and Model\nMODEL = 'llama3.2:3B'     # The name of the model to be used\nmylat = -33.33              # Latitude of Santiago de Chile\nmylon = -70.51              # Longitude of Santiago de Chile\n\nMODEL: Specifies the model being used, which is, in this example, the Lhama3.2.\nmylat and mylon: Coordinates of Santiago de Chile, used as the starting point for the distance calculation.\n\nDefining a Python Function That Acts as a Tool\ndef calc_distance(lat, lon, city):\n    \"\"\"Compute distance and print a descriptive message.\"\"\"\n    distance = haversine((mylat, mylon), (lat, lon), unit=\"km\")\n    msg = f\"\\nSantiago de Chile is about {int(round(distance, -1)):,} I am running a \\\nfew minutes late; my previous meeting is running over.kilometers away from {city}.\"\n    return {\"city\": city, \"distance_km\": int(round(distance, -1)), \"message\": msg}\nThis is the real Python function that Ollama will be allowed to call. It performs the following steps: • Takes latitude, longitude, and city name as input arguments. • Uses the haversine library to calculate the distance from Santiago to the target city. • Returns a JSON‑like dictionary containing the computed distance and a human‑readable text summary.\n\nIn Ollama’s terminology, this is a tool — a callable external function that the LLM may invoke automatically\n\nDeclaring the tool descriptor (schema)\ntools = [\n    {\n        \"type\": \"function\",\n        \"function\": {\n            \"name\": \"calc_distance\",\n            \"description\": \"Calculates the distance from Santiago, Chile to a \\\ngiven city's coordinates.\",\n            \"parameters\": {\n                \"type\": \"object\",\n                \"properties\": {\n                    \"lat\": {\"type\": \"number\", \"description\": \"Latitude of the city\"},\n                    \"lon\": {\"type\": \"number\", \"description\": \"Longitude of the city\"},\n                    \"city\": {\"type\": \"string\", \"description\": \"Name of the city\"}\n                },\n                \"required\": [\"lat\", \"lon\", \"city\"]\n            }\n        }\n    }\n]\nThis JSON object describes the metadata and input schema of the tool so that the LLM knows: • Name: which function to call. • Description: what purpose it serves. • Parameters: input argument types and their descriptions. This schema mirrors the OpenAI function‑calling format and is fully supported in Ollama ≥ 0.4 .\n\nDefining tools this way allows Ollama to validate arguments before sending a call request back.\n\nAsking the Model to Use the Tool\nresponse = chat(\n    model=MODEL,\n    messages=[{\n        \"role\": \"user\",\n        \"content\": f\"Find the decimal latitude and longitude of the capital of \\\nI am running a few minutes late; my previous meeting is running over. running a few\\ \nminutes late; my previous meeting is running over.\n      {country},\"\n                   \" then use the calc_distance tool to determine how far it is from \\\nSantiago de Chile.\"\n    }],\n    tools=tools\n)\n\nThe chat() function is called with the chosen model, a message, and the tools list.\nThe prompt instructs the model first to identify the capital and its coordinates, and then invoke the tool (calc_distance) with those values.\nOllama returns a structured response that may include a tool_calls section, indicating which tool to execute.\n\nExecuting the Model’s Tool Call\nFor example, if country  = \"Colombia\", the model will will return as the result:\n\nWhere the arguments are the expected response in JSON format.\nTo get and display the result to the user, we should iterate over all tool_calls, extract the JSON arguments (lat, lon, city), and execute the local Python function using them. To finish it, we should print a human‑readable message (e.g., “Santiago de Chile is about X  kilometers away from ”CITY”).\nSo, let’s first get the name of the city and the coordinates with:\nfor call in response.message.tool_calls:\n    raw_args = call[\"function\"][\"arguments\"]\n    \ncity = raw_args['city'] \nlat = float(raw_args['lat'])\nlon = float(raw_args['lon'])\nNow, we can calculate and print the distance, using haversine():\ndistance = haversine((mylat, mylon), (lat, lon), unit='km')\nprint(f\"Santiago de Chile is about {int(round(distance, -1)):,}\n      kilometers away from {city}.\")\nIn this case:\nSantiago de Chile is about 4,240 kilometers away from Bogota.\nNOTE: Sometimes the model returns parameter names that differ from what your function expects. Specifically, Ollama occasionally returns argument objects like:\n{\"lat1\": -33.33, \"lon1\": -70.51, \"lat2\": 48.8566, \"lon2\": 2.3522, \"city\": \"Paris\"}\nInstead of the schema-defined keys (lat, lon, city).\nThis happens because some LLMs (such as Llama 3.2 and Qwen 3) attempt to be “helpful” by naming coordinates explicitly—lat1/lon1 for origin and lat2/lon2 for destination—even when the schema only defines lat/lon.\nTo handle this, optionally a mapping-correction step can be added after decoding the tocall arguments.\nif hasattr(response.message, \"tool_calls\") and response.message.tool_calls:\n    for call in response.message.tool_calls:\n        if call[\"function\"][\"name\"] == \"calc_distance\":\n            raw_args = call[\"function\"][\"arguments\"]\n\n            # Decode JSON if necessary\n            args = json.loads(raw_args) if isinstance(raw_args, str) else raw_args\n\n            # Normalize key names\n            if \"lat1\" in args or \"lat2\" in args:\n                args[\"lat\"] = args.get(\"lat2\") or args.get(\"lat1\")\n                args[\"lon\"] = args.get(\"lon2\") or args.get(\"lon1\")\n            if \"latitude\" in args:\n                args[\"lat\"] = args[\"latitude\"]\n            if \"longitude\" in args:\n                args[\"lon\"] = args[\"longitude\"]\n            args = {k: v for k, v in args.items() if k in (\"lat\", \"lon\", \"city\")}\n\n            # Convert numbers\n            args[\"lat\"] = float(args[\"lat\"])\n            args[\"lon\"] = float(args[\"lon\"])\n\n            result = calc_distance(**args)\n            print(result[\"message\"])\nTiming and Diagnostic Output\nelapsed = time.perf_counter() - start\n    print(f\"[INFO] ==&gt; Model {MODEL} : {elapsed:.1f} s\")\nThis records how long the operation took from prompt submission to tool execution, useful for benchmarking response performance.\nExample Usage\nIf we enter different countries, for example, France, Colombia, and the United States, We can note that we always receive the same structured information:\nask_and_measure(\"France\")\nask_and_measure(\"Colombia\")\nask_and_measure(\"United States\")\nIf you run the code as a script, the result will be printed on the terminal:\n\nAnd the calculations are pretty good!\n\nThe complete script can be found at: func_call_dist_calc.py and on the 10-Ollama_Function_Calling notebook.\n\nRunning with other models\nThe models that will run with the described approach are the ones that can handle tools. For example, Gemma 3 and 3n will not work.\nAn alternative is to use the Pydantic library to serialize the schema using model_json_schema(). Using the Pydantic library, models as Gemma can also be used, as explored in the:\n20-Ollama_Function_Calling_Pydantic notebook\n\n\nAdding images\nNow it is time to wrap up everything so far! Let’s modify the script using Pydantic so that instead of entering the country name (as a text), the user enters an image, and the application (based on SLM) returns the city in the image and its geographic location. With that data, we can calculate the distance as before.\n\nFor simplicity, we will implement this new code in two steps. First, the LLM will analyze the image and create a description (text). This text will be passed on to another instance, where the model will extract the information needed to pass along.\n\nWe will start importing the libraries\nimport time\nfrom haversine import haversine\nfrom ollama import chat\nfrom pydantic import BaseModel, Field\nWe can see the image if you run the code on the Jupyter Notebook. For that, we also need to import:\nimport matplotlib.pyplot as plt\nfrom PIL import Image\n\nThose libraries are unnecessary if we run the code as a script.\n\nNow, we define the model and the local coordinates:\nMODEL = 'gemma3:4b'\nmylat = -33.33\nmylon = -70.51\nWe can download a new image, for example, Machu Picchu from Wikipedia. On the Notebook we can see it:\n# Load the image\nimg_path = \"image_test_3.jpg\"\nimg = Image.open(img_path)\n\n# Display the image\nplt.figure(figsize=(8, 8))\nplt.imshow(img)\nplt.axis('off')\n#plt.title(\"Image\")\nplt.show()\n\n\n\n\n\nNow, let’s define a function that will receive the image and will return the decimal latitude and decimal longitude of the city in the image, its name, and what country it is located\ndef image_description(img_path):\n    with open(img_path, 'rb') as file:\n        response = chat(\n            model=MODEL,\n            messages=[\n              {\n                'role': 'user',\n                'content': '''return the decimal latitude and decimal longitude \n                              of the city in the image, its name, and \n                              what country it is located''',\n                'images': [file.read()],\n              },\n            ],\n            options = {\n              'temperature': 0,\n              }\n      )\n    #print(response['message']['content'])\n    return response['message']['content']\n\nWe can print the entire response for debug purposes. In this case, we can get something as:\n'{\\n  \"city\": \"Machu Picchu\",\\n  \"country\": \"Peru\",\\n  \"lat\": -13.1631,\\n  \"lon\": -72.5450\\n}\\n'\n\nLet’s define a Pydantic model (CityCoord) that describes the expected structure of the SLM’s response. It expects four fields: country, city (city name), lat (latitude), and lon (longitude).\nclass CityCoord(BaseModel):\n    city: str = Field(..., description=\"Name of the city in the image\")\n    country: str = Field(..., description=\"Name of the country where the city in the\\ image is located\")\n    lat: float = Field(..., description=\"Decimal Latitude of the city in the image\")\n    lon: float = Field(..., description=\"Decimal Longitude of the city in the image\")\nThe image description generated for the function will be passed as a prompt for the model again.\nresponse = chat(\n    model=MODEL,\n    messages=[{\n        \"role\": \"user\",\n        \"content\": image_description # image_description from previous model's run\n    }],\n    format=CityCoord.model_json_schema(),  # Structured JSON format\n    options={\"temperature\": 0}\n)\nNow we can get the required data using:\nresp = CityCoord.model_validate_json(response.message.content)\nAnd so, we can calculate and print the distance, using haversine():\ndistance = haversine((mylat, mylon), (resp.lat, resp.lon), unit='km')\nprint(f\"\\nThe image shows {resp.city}, with lat:{round(resp.lat, 2)} and \\\nlong: {round(resp.lon, 2)}, located in {resp.country} and \\\nabout {int(round(distance, -1)):,} kilometers away from Santiago, Chile.\\n\")\nAnd we will get:\nThe image shows Machu Picchu, with lat:-13.16 and long: -72.55, located in Peru and about 2,250 kilometers away from Santiago, Chile.\nIn the 30-Function_Calling_with_images notebook, you can find experiments with multiple images.\nLet’s now download the script calc_distance_image.py from the GitHub and run it on the terminal with the command:\npython calc_distance_image.py /home/mjrovai/Documents/OLLAMA/image_test_3.jpg\nEnter with the Machu Picchu image full patch as an argument. We will get the same previous result.\n\nLet’s change the model for the gemma3:4b:\n\nThe app is working fine with both models, with the Gemma being faster.\nHow about Paris?\n\n\nOf course, there are many ways to optimize the code used here. Still, the idea is to explore the considerable potential of function calling with SLMs at the edge, allowing those models to integrate with external functions or APIs. Going beyond text generation, SLMs can access real-time data, automate tasks, and interact with various systems."
  },
  {
    "objectID": "raspi/llm/slm_opt_tech.html#retrievel-augmentation-generation-rag",
    "href": "raspi/llm/slm_opt_tech.html#retrievel-augmentation-generation-rag",
    "title": "SLM: Basic Optimization Techniques",
    "section": "Retrievel Augmentation Generation (RAG)",
    "text": "Retrievel Augmentation Generation (RAG)\nIn a basic interaction between a user and a language model, the user asks a question, which is sent to the model as a prompt. The model generates a response based solely on its pre-trained knowledge.\n\nIn a RAG process, there’s an additional step between the user’s question and the model’s response. The user’s question triggers a retrieval process from a knowledge base.\n\n\nA simple RAG project\nHere are the steps to implement a basic Retrieval Augmented Generation (RAG):\n\nDetermine the type of documents you’ll be using: The best types are documents from which we can get clean and unobscured text. PDFs can be problematic because they are designed for printing, not for extracting sensible text. To work with PDFs, we should get the source document or use tools to handle it.\nChunk the text: We can’t store the text as one long stream because of context size limitations and the potential for confusion. Chunking involves splitting the text into smaller pieces. Chunk text has many ways, such as character count, tokens, words, paragraphs, or sections. It is also possible to overlap chunks.\nCreate embeddings: Embeddings are numerical representations of text that capture semantic meaning. We create embeddings by passing each chunk of text through a particular embedding model. The model outputs a vector, the length of which depends on the embedding model used. We should pull one (or more) embedding models from Ollama, to perform this task. Here are some examples of embedding models available at Ollama.\n\n\n\nModel\nParameter Size\nEmbedding Size\n\n\n\n\nmxbai-embed-large\n334M\n1024\n\n\nnomic-embed-text\n137M\n768\n\n\nall-minilm\n23M\n384\n\n\n\n\nGenerally, larger embedding sizes capture more nuanced information about the input. Still, they also require more computational resources to process, and a higher number of parameters should increase the latency (but also the quality of the response).\n\nStore the chunks and embeddings in a vector database: We will need a way to efficiently find the most relevant chunks of text for a given prompt, which is where a vector database comes in. We will use Chromadb, an AI-native open-source vector database, which simplifies building RAGs by creating knowledge, facts, and skills pluggable for LLMs. Both the embedding and the source text for each chunk are stored.\nBuild the prompt: When we have a question, we create an embedding and query the vector database for the most similar chunks. Then, we select the top few results and include their text in the prompt.\n\nThe goal of RAG is to provide the model with the most relevant information from our documents, allowing it to generate more accurate and informative responses. So, let’s implement a simple example of an SLM incorporating a particular set of facts about bees (“Bee Facts”).\nInside the ollama env, enter the command in the terminal for Chromadb instalation:\npip install ollama chromadb\nLet’s pull an intermediary embedding model, nomic-embed-text\nollama pull nomic-embed-text\nAnd create a working directory:\ncd Documents/OLLAMA/\nmkdir RAG-simple-bee\ncd RAG-simple-bee/\nLet’s create a new Jupyter notebook, 40-RAG-simple-bee for some exploration:\nImport the needed libraries:\nimport ollama\nimport chromadb\nimport time\nAnd define aor models:\nEMB_MODEL = \"nomic-embed-text\"\nMODEL = 'llama3.2:3B'\nInitially, a knowledge base about bee facts should be created. This involves collecting relevant documents and converting them into vector embeddings. These embeddings are then stored in a vector database, allowing for efficient similarity searches later. Enter with the “document,” a base of “bee facts” as a list:\ndocuments = [\n    \"Bee-keeping, also known as apiculture, involves the maintenance of bee \\\n    colonies, typically in hives, by humans.\",\n    \"The most commonly kept species of bees is the European honey bee (Apis \\\n    mellifera).\",\n    \n    ...\n    \n    \"There are another 20,000 different bee species in the world.\",  \n    \"Brazil alone has more than 300 different bee species, and the \\\n    vast majority, unlike western honey bees, don’t sting.\", \n    \"Reports written in 1577 by Hans Staden, mention three native bees \\\n    used by indigenous people in Brazil.\",\n    \"The indigenous people in Brazil used bees for medicine and food purposes\",\n    \"From Hans Staden report: probable species: mandaçaia (Melipona \\\n    quadrifasciata), mandaguari (Scaptotrigona postica) and jataí-amarela \\\n    (Tetragonisca angustula).\"\n]\n\nWe do not need to “chunk” the document here because we will use each element of the list as a chunk.\n\nNow, we will create our vector embedding database bee_facts and store the document in it:\nclient = chromadb.Client()\ncollection = client.create_collection(name=\"bee_facts\")\n\n# store each document in a vector embedding database\nfor i, d in enumerate(documents):\n  response = ollama.embeddings(model=EMB_MODEL, prompt=d)\n  embedding = response[\"embedding\"]\n  collection.add(\n    ids=[str(i)],\n    embeddings=[embedding],\n    documents=[d]\n  )\nNow that we have our “Knowledge Base” created, we can start making queries, retrieving data from it:\n\nUser Query: The process begins when a user asks a question, such as “How many bees are in a colony? Who lays eggs, and how much? How about common pests and diseases?”\nprompt = \"How many bees are in a colony? Who lays eggs and how much? How about\\\n          common pests and diseases?\"\nQuery Embedding: The user’s question is converted into a vector embedding using the same embedding model used for the knowledge base.\nresponse = ollama.embeddings(\n  prompt=prompt,\n  model=EMB_MODEL\n)\nRelevant Document Retrieval: The system searches the knowledge base using the query embedding to find the most relevant documents (in this case, the 5 more probable). This is done using a similarity search, which compares the query embedding to the document embeddings in the database.\nresults = collection.query(\n  query_embeddings=[response[\"embedding\"]],\n  n_results=5\n)\ndata = results['documents']\nPrompt Augmentation: The retrieved relevant information is combined with the original user query to create an augmented prompt. This prompt now contains the user’s question and pertinent facts from the knowledge base.\nprompt=f\"Using this data: {data}. Respond to this prompt: {prompt}\",\nAnswer Generation: The augmented prompt is then fed into a language model, in this case, the llama3.2:3b model. The model uses this enriched context to generate a comprehensive answer. Parameters like temperature, top_k, and top_p are set to control the randomness and quality of the generated response.\noutput = ollama.generate(\n  model=MODEL,\n  prompt=f\"Using this data: {data}. Respond to this prompt: {prompt}\",\n  options={\n    \"temperature\": 0.0,\n    \"top_k\":10,\n    \"top_p\":0.5                          }\n)\nResponse Delivery: Finally, the system returns the generated answer to the user.\nprint(output['response'])\nBased on the provided data, here are the answers to your questions:\n\n1. How many bees are in a colony?\nA typical bee colony can contain between 20,000 and 80,000 bees.\n\n2. Who lays eggs and how much?\nThe queen bee lays up to 2,000 eggs per day during peak seasons.\n\n3. What about common pests and diseases?\nCommon pests and diseases that affect bees include varroa mites, hive beetles,\nand foulbrood.\nLet’s create a function to help answer new questions:\ndef rag_bees(prompt, n_results=5, temp=0.0, top_k=10, top_p=0.5):\n    start_time = time.perf_counter()  # Start timing\n    \n    # generate an embedding for the prompt and retrieve the data \n    response = ollama.embeddings(\n      prompt=prompt,\n      model=EMB_MODEL\n    )\n    \n    results = collection.query(\n      query_embeddings=[response[\"embedding\"]],\n      n_results=n_results\n    )\n    data = results['documents']\n    \n    # generate a response combining the prompt and data retrieved\n    output = ollama.generate(\n      model=MODEL,\n      prompt=f\"Using this data: {data}. Respond to this prompt: {prompt}\",\n      options={\n        \"temperature\": temp,\n        \"top_k\": top_k,\n        \"top_p\": top_p                          }\n    )\n    \n    print(output['response'])\n    \n    end_time = time.perf_counter()  # End timing\n    elapsed_time = round((end_time - start_time), 1)  # Calculate elapsed time\n    \n    print(f\"\\n[INFO] ==&gt; The code for model: {MODEL}, took {elapsed_time}s \\\nto generate the answer.\\n\")\nWe can now create queries and call the function:\nprompt = \"Are bees in Brazil?\"\nrag_bees(prompt)\nYes, bees are found in Brazil. According to the data, Brazil has more than 300\ndifferent bee species, and indigenous people in Brazil used bees for medicine and\nfood purposes. Additionally, reports from 1577 mention three native bees used by\nindigenous people in Brazil.\n\n [INFO] ==&gt; The code for model: llama3.2:3b, took 22.7s to generate the answer.\nBy the way, if the model used supports multiple languages, we can use it (for example, Portuguese), even if the dataset was created in English:\nprompt = \"Existem abelhas no Brazil?\"\nrag_bees(prompt)\nSim, existem abelhas no Brasil! De acordo com o relato de Hans Staden, há três \nespécies de abelhas nativas do Brasil que foram mencionadas: mandaçaia (Melipona\nquadrifasciata), mandaguari (Scaptotrigona postica) e jataí-amarela (Tetragonisca\nangustula). Além disso, o Brasil é conhecido por ter mais de 300 espécies \ndiferentes de abelhas, a maioria das quais não é agressiva e não põe veneno.\n\n [INFO] ==&gt; The code for model: llama3.2:3b, took 54.6s to generate the answer.\n\nIn the Chapter Advancing EdgeAI: Beyond Basic SLMs, we will learn how to implement a Naive RAG System"
  },
  {
    "objectID": "raspi/llm/slm_opt_tech.html#conclusion",
    "href": "raspi/llm/slm_opt_tech.html#conclusion",
    "title": "SLM: Basic Optimization Techniques",
    "section": "Conclusion",
    "text": "Conclusion\nThroughout this chapter, we’ve explored two fundamental optimization techniques that significantly enhance the capabilities of Small Language Models (SLMs) running on edge devices like the Raspberry Pi: Function Calling and Retrieval-Augmented Generation (RAG).\nWe began by addressing a critical limitation of language models—their inability to perform accurate calculations. By implementing function calling, we demonstrated how to transform SLMs from text generators into actionable agents that can interact with external tools and APIs. Whether extracting structured data like geographic coordinates, fetching real-time weather information, or performing precise mathematical operations, function calling bridges the gap between natural language understanding and deterministic computation. The key principle remains:\n\nUse SLMs for intent classification and understanding, while delegating specific tasks to specialized functions that guarantee accuracy.\n\nThe RAG implementation showcased an elegant solution to another fundamental challenge—the static nature of model knowledge and the tendency toward hallucinations. By integrating ChromaDB with vector embeddings, we created a system that augments the model’s responses with relevant, factual information retrieved from a knowledge base. This approach not only grounds the model’s answers in verifiable data but also enables the system to stay current without the resource-intensive process of retraining or fine-tuning.\nThese techniques are particularly valuable for edge AI applications where computational resources are limited, and offline operation is often required. Function calling ensures reliability and precision, while RAG provides flexibility and accuracy without demanding continuous internet connectivity once the knowledge base is established.\nAs we move forward with our edge AI projects, remember that the power of SLMs lies not just in their standalone capabilities but in how effectively we orchestrate them with complementary tools and techniques. The patterns demonstrated here—structured tool use and knowledge augmentation—form the foundation for building robust, production-ready AI applications on resource-constrained devices.\nIn the chapter Advancing EdgeAI: Beyond Basic SLMs, we’ll go deeper into these concepts and explore more advanced implementations."
  },
  {
    "objectID": "raspi/llm/slm_opt_tech.html#resources",
    "href": "raspi/llm/slm_opt_tech.html#resources",
    "title": "SLM: Basic Optimization Techniques",
    "section": "Resources",
    "text": "Resources\n\n10-Ollama_Function_Calling notebook\n20-Ollama_Function_Calling_Pydantic\n30-Function_Calling_with_images notebook\n40-RAG-simple-bee notebook\ncalc_distance_image python script"
  },
  {
    "objectID": "raspi/vlm/vlm.html#why-florence-2-at-the-edge",
    "href": "raspi/vlm/vlm.html#why-florence-2-at-the-edge",
    "title": "Vision-Language Models at the Edge",
    "section": "Why Florence-2 at the Edge?",
    "text": "Why Florence-2 at the Edge?\nFlorence-2 is a vision-language model open-sourced by Microsoft under the MIT license, which significantly advances vision-language models by combining a lightweight architecture with robust capabilities. Thanks to its training on the massive FLD-5B dataset, which contains 126 million images and 5.4 billion visual annotations, it achieves performance comparable to larger models. This makes Florence-2 ideal for deployment at the edge, where power and computational resources are limited.\nIn this tutorial, we will explore how to use Florence-2 for real-time computer vision applications, such as:\n\nImage captioning\nObject detection\nSegmentation\nVisual grounding\n\n\nVisual grounding involves linking textual descriptions to specific regions within an image. This enables the model to understand where particular objects or entities described in a prompt are in the image. For example, if the prompt is “a red car,” the model will identify and highlight the region where the red car is found in the image. Visual grounding is helpful for applications where precise alignment between text and visual content is needed, such as human-computer interaction, image annotation, and interactive AI systems.\n\nIn the tutorial, we will walk through:\n\nSetting up Florence-2 on the Raspberry Pi\nRunning inference tasks such as object detection and captioning\nOptimizing the model to get the best performance from the edge device\nExploring practical, real-world applications with fine-tuning."
  },
  {
    "objectID": "raspi/vlm/vlm.html#florence-2-model-architecture",
    "href": "raspi/vlm/vlm.html#florence-2-model-architecture",
    "title": "Vision-Language Models at the Edge",
    "section": "Florence-2 Model Architecture",
    "text": "Florence-2 Model Architecture\nFlorence-2 utilizes a unified, prompt-based representation to handle various vision-language tasks. The model architecture consists of two main components: an image encoder and a multi-modal transformer encoder-decoder.\n\n\nImage Encoder: The image encoder is based on the DaViT (Dual Attention Vision Transformers) architecture. It converts input images into a series of visual token embeddings. These embeddings serve as the foundational representations of the visual content, capturing both spatial and contextual information about the image.\nMulti-Modal Transformer Encoder-Decoder: Florence-2’s core is the multi-modal transformer encoder-decoder, which combines visual token embeddings from the image encoder with textual embeddings generated by a BERT-like model. This combination allows the model to simultaneously process visual and textual inputs, enabling a unified approach to tasks such as image captioning, object detection, and segmentation.\n\nThe model’s training on the extensive FLD-5B dataset ensures it can effectively handle diverse vision tasks without requiring task-specific modifications. Florence-2 uses textual prompts to activate specific tasks, making it highly flexible and capable of zero-shot generalization. For tasks like object detection or visual grounding, the model incorporates additional location tokens to represent regions within the image, ensuring a precise understanding of spatial relationships.\n\nFlorence-2’s compact architecture and innovative training approach allow it to perform computer vision tasks accurately, even on resource-constrained devices like the Raspberry Pi."
  },
  {
    "objectID": "raspi/vlm/vlm.html#technical-overview",
    "href": "raspi/vlm/vlm.html#technical-overview",
    "title": "Vision-Language Models at the Edge",
    "section": "Technical Overview",
    "text": "Technical Overview\nFlorence-2 introduces several innovative features that set it apart:\n\nArchitecture\n\n\nLightweight Design: Two variants available\n\nFlorence-2-Base: 232 million parameters\nFlorence-2-Large: 771 million parameters\n\nUnified Representation: Handles multiple vision tasks through a single architecture\nDaViT Vision Encoder: Converts images into visual token embeddings\nTransformer-based Multi-modal Encoder-Decoder: Processes combined visual and text embeddings\n\n\n\nTraining Dataset (FLD-5B)\n\n\n126 million unique images\n5.4 billion comprehensive annotations, including:\n\n500M text annotations\n1.3B region-text annotations\n3.6B text-phrase-region annotations\n\nAutomated annotation pipeline using specialist models\nIterative refinement process for high-quality labels\n\n\n\nKey Capabilities\nFlorence-2 excels in multiple vision tasks:\n\nZero-shot Performance\n\nImage Captioning: Achieves 135.6 CIDEr score on COCO\nVisual Grounding: 84.4% recall@1 on Flickr30k\nObject Detection: 37.5 mAP on COCO val2017\nReferring Expression: 67.0% accuracy on RefCOCO\n\n\n\nFine-tuned Performance\n\nCompetitive with specialist models despite the smaller size\nOutperforms larger models in specific benchmarks\nEfficient adaptation to new tasks\n\n\n\n\nPractical Applications\nFlorence-2 can be applied across various domains:\n\nContent Understanding\n\nAutomated image captioning for accessibility\nVisual content moderation\nMedia asset management\n\nE-commerce\n\nProduct image analysis\nVisual search\nAutomated product tagging\n\nHealthcare\n\nMedical image analysis\nDiagnostic assistance\nResearch data processing\n\nSecurity & Surveillance\n\nObject detection and tracking\nAnomaly detection\nScene understanding\n\n\n\n\nComparing Florence-2 with other VLMs\nFlorence-2 stands out from other visual language models due to its impressive zero-shot capabilities. Unlike models like Google PaliGemma, which rely on extensive fine-tuning to adapt to various tasks, Florence-2 works right out of the box, as we will see in this lab. It can also compete with larger models like GPT-4V and Flamingo, which often have many more parameters but only sometimes match Florence-2’s performance. For example, Florence-2 achieves better zero-shot results than Kosmos-2 despite having over twice the parameters.\nIn benchmark tests, Florence-2 has shown remarkable performance in tasks like COCO captioning and referring expression comprehension. It outperformed models like PolyFormer and UNINEXT in object detection and segmentation tasks on the COCO dataset. It is a highly competitive choice for real-world applications where both performance and resource efficiency are crucial."
  },
  {
    "objectID": "raspi/vlm/vlm.html#setup-and-installation",
    "href": "raspi/vlm/vlm.html#setup-and-installation",
    "title": "Vision-Language Models at the Edge",
    "section": "Setup and Installation",
    "text": "Setup and Installation\nOur choice of edge device is the Raspberry Pi 5 (Raspi-5). Its robust platform is equipped with the Broadcom BCM2712, a 2.4GHz quad-core 64-bit Arm Cortex-A76 CPU featuring Cryptographic Extension and enhanced caching capabilities. It boasts a VideoCore VII GPU, dual 4Kp60 HDMI® outputs with HDR, and a 4Kp60 HEVC decoder. Memory options include 4GB and 8GB of high-speed LPDDR4X SDRAM, with 8GB being our choice to run Florence-2. It also features expandable storage via a microSD card slot and a PCIe 2.0 interface for fast peripherals such as M.2 SSDs (Solid State Drives).\n\nFor real applications, SSDs are a better option than SD cards.\n\nWe suggest installing an Active Cooler, a dedicated clip-on cooling solution for Raspberry Pi 5 (Raspi-5), for this lab. It combines an aluminum heatsink with a temperature-controlled blower fan to keep the Raspi-5 operating comfortably under heavy loads, such as running Florense-2.\n\n\nEnvironment configuration\nTo run Microsoft Florense-2 on the Raspberry Pi 5, we’ll need a few libraries:\n\nTransformers:\n\nFlorence-2 uses the transformers library from Hugging Face for model loading and inference. This library provides the architecture for working with pre-trained vision-language models, making it easy to perform tasks like image captioning, object detection, and more. Essentially, transformers helps in interacting with the model, processing input prompts, and obtaining outputs.\n\nPyTorch:\n\nPyTorch is a deep learning framework that provides the infrastructure needed to run the Florence-2 model, which includes tensor operations, GPU acceleration (if a GPU is available), and model training/inference functionalities. The Florence-2 model is trained in PyTorch, and we need it to leverage its functions, layers, and computation capabilities to perform inferences on the Raspberry Pi.\n\nTimm (PyTorch Image Models):\n\nFlorence-2 uses timm to access efficient implementations of vision models and pre-trained weights. Specifically, the timm library is utilized for the image encoder part of Florence-2, particularly for managing the DaViT architecture. It provides model definitions and optimized code for common vision tasks and allows the easy integration of different backbones that are lightweight and suitable for edge devices.\n\nEinops:\n\nEinops is a library for flexible and powerful tensor operations. It makes it easy to reshape and manipulate tensor dimensions, which is especially important for the multi-modal processing done in Florence-2. Vision-language models like Florence-2 often need to rearrange image data, text embeddings, and visual embeddings to align correctly for the transformer blocks, and einops simplifies these complex operations, making the code more readable and concise.\n\n\nIn short, these libraries enable different essential components of Florence-2:\n\nTransformers and PyTorch are needed to load the model and run the inference.\nTimm is used to access and efficiently implement the vision encoder.\nEinops helps reshape data, facilitating the integration of visual and text features.\n\nAll these components work together to help Florence-2 run seamlessly on our Raspberry Pi, allowing it to perform complex vision-language tasks relatively quickly.\nConsidering that the Raspberry Pi already has its OS installed, let’s use SSH to reach it from another computer:\nssh mjrovai@raspi-5.local\nAnd check the IP allocated to it:\nhostname -I\n192.168.4.209\n\nUpdating the Raspberry Pi\nFirst, ensure your Raspberry Pi is up to date:\nsudo apt update\nsudo apt upgrade -y\nInitial setup for using PIP:\nsudo apt install python3-pip\nsudo rm /usr/lib/python3.11/EXTERNALLY-MANAGED\npip3 install --upgrade pip\nInstall Dependencies\nsudo apt-get install libjpeg-dev libopenblas-dev libopenmpi-dev libomp-dev\nLet’s set up and activate a Virtual Environment for working with Florence-2:\npython3 -m venv ~/florence\nsource ~/florence/bin/activate\nInstall PyTorch\npip3 install setuptools numpy Cython\npip3 install requests\npip3 install torch torchvision --index-url https://download.pytorch.org/whl/cpu\npip3 install torchaudio --index-url https://download.pytorch.org/whl/cpu\nLet’s verify that PyTorch is correctly installed:\n\nInstall Transformers, Timm and Einops:\npip3 install transformers\npip3 install timm einops\nInstall the model:\npip3 install autodistill-florence-2\nJupyter Notebook and Python libraries\nInstalling a Jupyter Notebook to run and test our Python scripts is possible.\npip3 install jupyter\npip3 install numpy Pillow matplotlib\njupyter notebook --generate-config\n\n\nTesting the installation\nRunning the Jupyter Notebook on the remote computer\njupyter notebook --ip=192.168.4.209 --no-browser\nRunning the above command on the SSH terminal, we can see the local URL address to open the notebook:\n\nThe notebook with the code used on this initial test can be found on the Lab GitHub:\n\n10-florence2_test.ipynb\n\nWe can access it on the remote computer by entering the Raspberry Pi’s IP address and the provided token in a web browser ( copy the entire URL from the terminal).\nFrom the Home page, create a new notebook [Python 3 (ipykernel) ] and copy and paste the example code from Hugging Face Hub.\nThe code is designed to run Florence-2 on a given image to perform object detection. It loads the model, processes an image and a prompt, and then generates a response to identify and describe the objects in the image.\n\nThe processor helps prepare text and image inputs.\nThe model takes the processed inputs to generate a meaningful response.\nThe post-processing step refines the generated output into a more interpretable form, like bounding boxes for detected objects.\n\n\nThis workflow leverages the versatility of Florence-2 to handle vision-language tasks and is implemented efficiently using PyTorch, Transformers, and related image-processing tools.\n\nimport requests\nfrom PIL import Image\nimport torch\nfrom transformers import AutoProcessor, AutoModelForCausalLM \n\ndevice = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\ntorch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32\n\nmodel = AutoModelForCausalLM.from_pretrained(\"microsoft/Florence-2-base\",\n                                             torch_dtype=torch_dtype, \n                                             trust_remote_code=True).to(device)\nprocessor = AutoProcessor.from_pretrained(\"microsoft/Florence-2-base\", \n                                          trust_remote_code=True)\n\nprompt = \"&lt;OD&gt;\"\n\nurl = \"https://huggingface.co/datasets/huggingface/documentation-\nimages/resolve/main/transformers/tasks/car.jpg?download=true\"\nimage = Image.open(requests.get(url, stream=True).raw)\n\ninputs = processor(text=prompt, images=image, return_tensors=\"pt\").to(\n  device, torch_dtype)\n\ngenerated_ids = model.generate(\n    input_ids=inputs[\"input_ids\"],\n    pixel_values=inputs[\"pixel_values\"],\n    max_new_tokens=1024,\n    do_sample=False,\n    num_beams=3,\n)\ngenerated_text = processor.batch_decode(generated_ids, skip_special_tokens=False)[0]\n\nparsed_answer = processor.post_process_generation(generated_text, task=\"&lt;OD&gt;\", \n                                                  image_size=(image.width, \n                                                              image.height))\n\nprint(parsed_answer)\nLet’s break down the provided code step by step:\n\n1. Importing Required Libraries\nimport requests\nfrom PIL import Image\nimport torch\nfrom transformers import AutoProcessor, AutoModelForCausalLM \n\nrequests: Used to make HTTP requests. In this case, it downloads an image from a URL.\nPIL (Pillow): Provides tools for manipulating images. Here, it’s used to open the downloaded image.\ntorch: PyTorch is imported to handle tensor operations and determine the hardware availability (CPU or GPU).\ntransformers: This module provides easy access to Florence-2 by using AutoProcessor and AutoModelForCausalLM to load pre-trained models and process inputs.\n\n\n\n2. Determining the Device and Data Type\ndevice = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\ntorch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32\n\nDevice Setup: The code checks if a CUDA-enabled GPU is available (torch.cuda.is_available()). The device is set to “cuda:0” if a GPU is available. Otherwise, it defaults to \"cpu\" (our case here).\nData Type Setup: If a GPU is available, torch.float16 is chosen, which uses half-precision floats to speed up processing and reduce memory usage. On the CPU, it defaults to torch.float32 to maintain compatibility.\n\n\n\n3. Loading the Model and Processor\nmodel = AutoModelForCausalLM.from_pretrained(\"microsoft/Florence-2-base\", \n                                             torch_dtype=torch_dtype,\n                                             trust_remote_code=True).to(device)\nprocessor = AutoProcessor.from_pretrained(\"microsoft/Florence-2-base\",\n                                          trust_remote_code=True)\n\nModel Initialization:\n\nAutoModelForCausalLM.from_pretrained() loads the pre-trained Florence-2 model from Microsoft’s repository on Hugging Face. The torch_dtype is set according to the available hardware (GPU/CPU), and trust_remote_code=True allows the use of any custom code that might be provided with the model.\n.to(device) moves the model to the appropriate device (either CPU or GPU). In our case, it will be set to CPU.\n\nProcessor Initialization:\n\nAutoProcessor.from_pretrained() loads the processor for Florence-2. The processor is responsible for transforming text and image inputs into a format the model can work with (e.g., encoding text, normalizing images, etc.).\n\n\n\n\n\n4. Defining the Prompt\nprompt = \"&lt;OD&gt;\"\n\nPrompt Definition: The string \"&lt;OD&gt;\" is used as a prompt. This refers to “Object Detection”, instructing the model to detect objects on the image.\n\n\n5. Downloading and Loading the Image\nurl = \"https://huggingface.co/datasets/huggingface/documentation-\\\nimages/resolve/main/transformers/tasks/car.jpg?download=true\"\nimage = Image.open(requests.get(url, stream=True).raw)\n\nDownloading the Image: The requests.get() function fetches the image from the specified URL. The stream=True parameter ensures the image is streamed rather than downloaded completely at once.\nOpening the Image: Image.open() opens the image so the model can process it.\n\n\n\n6. Processing Inputs\ninputs = processor(text=prompt, images=image, return_tensors=\"pt\").to(device, \n                                                                      torch_dtype)\n\nProcessing Input Data: The processor() function processes the text (prompt) and the image (image). The return_tensors=\"pt\" argument converts the processed data into PyTorch tensors, which are necessary for inputting data into the model.\nMoving Inputs to Device: .to(device, torch_dtype) moves the inputs to the correct device (CPU or GPU) and assigns the appropriate data type.\n\n\n\n\n7. Generating the Output\ngenerated_ids = model.generate(\n    input_ids=inputs[\"input_ids\"],\n    pixel_values=inputs[\"pixel_values\"],\n    max_new_tokens=1024,\n    do_sample=False,\n    num_beams=3,\n)\n\nModel Generation: model.generate() is used to generate the output based on the input data.\n\ninput_ids: Represents the tokenized form of the prompt.\npixel_values: Contains the processed image data.\nmax_new_tokens=1024: Specifies the maximum number of new tokens to be generated in the response. This limits the response length.\ndo_sample=False: Disables sampling; instead, the generation uses deterministic methods (beam search).\nnum_beams=3: Enables beam search with three beams, which improves output quality by considering multiple possibilities during generation.\n\n\n\n8. Decoding the Generated Text\ngenerated_text = processor.batch_decode(generated_ids, skip_special_tokens=False)[0]\n\nBatch Decode: processor.batch_decode() decodes the generated IDs (tokens) into readable text. The skip_special_tokens=False parameter means that the output will include any special tokens that may be part of the response.\n\n\n\n9. Post-processing the Generation\nparsed_answer = processor.post_process_generation(generated_text, task=\"&lt;OD&gt;\", \n                                                  image_size=(image.width, \n                                                              image.height))\n\nPost-Processing: processor.post_process_generation() is called to process the generated text further, interpreting it based on the task (\"&lt;OD&gt;\" for object detection) and the size of the image.\nThis function extracts specific information from the generated text, such as bounding boxes for detected objects, making the output more useful for visual tasks.\n\n\n\n10. Printing the Output\nprint(parsed_answer)\n\nFinally, print(parsed_answer) displays the output, which could include object detection results, such as bounding box coordinates and labels for the detected objects in the image.\n\n\n\nResult\nRunning the code, we get as the Parsed Answer:\n{'&lt;OD&gt;': {'bboxes': [[34.23999786376953, 160.0800018310547, 597.4400024414062, \n371.7599792480469], [272.32000732421875, 241.67999267578125, 303.67999267578125, \n247.4399871826172], [454.0799865722656, 276.7200012207031, 553.9199829101562, \n370.79998779296875], [96.31999969482422, 280.55999755859375, 198.0800018310547, \n371.2799987792969]], 'labels': ['car', 'door handle', 'wheel', 'wheel']}}\nFirst, Let’s inspect the image:\nimport matplotlib.pyplot as plt\nplt.figure(figsize=(8, 8))\nplt.imshow(image)\nplt.axis('off')\nplt.show()\n\nBy the Object Detection result, we can see that:\n'labels': ['car', 'door handle', 'wheel', 'wheel']\nIt seems that at least a few objects were detected. we can also implement a code to draw the bounding boxes in the find objects:\ndef plot_bbox(image, data):\n   # Create a figure and axes\n    fig, ax = plt.subplots()\n\n    # Display the image\n    ax.imshow(image)\n\n    # Plot each bounding box\n    for bbox, label in zip(data['bboxes'], data['labels']):\n        # Unpack the bounding box coordinates\n        x1, y1, x2, y2 = bbox\n        # Create a Rectangle patch\n        rect = patches.Rectangle((x1, y1), x2-x1, y2-y1, linewidth=1, \n                                 edgecolor='r', facecolor='none')\n        # Add the rectangle to the Axes\n        ax.add_patch(rect)\n        # Annotate the label\n        plt.text(x1, y1, label, color='white', fontsize=8, \n                 bbox=dict(facecolor='red', alpha=0.5))\n\n    # Remove the axis ticks and labels\n    ax.axis('off')\n\n    # Show the plot\n    plt.show()\n\nBox (x0, y0, x1, y1): Location tokens correspond to the top-left and bottom-right corners of a box.\n\nAnd running\nplot_bbox(image, parsed_answer['&lt;OD&gt;'])\nWe get:"
  },
  {
    "objectID": "raspi/vlm/vlm.html#florence-2-tasks",
    "href": "raspi/vlm/vlm.html#florence-2-tasks",
    "title": "Vision-Language Models at the Edge",
    "section": "Florence-2 Tasks",
    "text": "Florence-2 Tasks\nFlorence-2 is designed to perform a variety of computer vision and vision-language tasks through prompts. These tasks can be activated by providing a specific textual prompt to the model, as we saw with &lt;OD&gt; (Object Detection).\nFlorence-2’s versatility comes from combining these prompts, allowing us to guide the model’s behavior to perform specific vision tasks. Changing the prompt allows us to adapt Florence-2 to different tasks without needing task-specific modifications in the architecture. This capability directly results from Florence-2’s unified model architecture and large-scale multi-task training on the FLD-5B dataset.\nHere are some of the key tasks that Florence-2 can perform, along with example prompts:\n\n1. Object Detection (OD)\n\nPrompt: \"&lt;OD&gt;\"\nDescription: Identifies objects in an image and provides bounding boxes for each detected object. This task is helpful for applications like visual inspection, surveillance, and general object recognition.\n\n\n\n2. Image Captioning\n\nPrompt: \"&lt;CAPTION&gt;\"\nDescription: Generates a textual description for an input image. This task helps the model describe what is happening in the image, providing a human-readable caption for content understanding.\n\n\n\n3. Detailed Captioning\n\nPrompt: \"&lt;DETAILED_CAPTION&gt;\"\nDescription: Generates a more detailed caption with more nuanced information about the scene, such as the objects present and their relationships.\n\n\n\n4. Visual Grounding\n\nPrompt: \"&lt;CAPTION_TO_PHRASE_GROUNDING&gt;\"\nDescription: Links a textual description to specific regions in an image. For example, given a prompt like “a green car,” the model highlights where the red car is in the image. This is useful for human-computer interaction, where you must find specific objects based on text.\n\n\n\n5. Segmentation\n\nPrompt: \"&lt;REFERRING_EXPRESSION_SEGMENTATION&gt;\"\nDescription: Performs segmentation based on a referring expression, such as “the blue cup.” The model identifies and segments the specific region containing the object mentioned in the prompt (all related pixels).\n\n\n\n6. Dense Region Captioning\n\nPrompt: \"&lt;DENSE_REGION_CAPTION&gt;\"\nDescription: Provides captions for multiple regions within an image, offering a detailed breakdown of all visible areas, including different objects and their relationships.\n\n\n\n7. OCR with Region\n\nPrompt: \"&lt;OCR_WITH_REGION&gt;\"\nDescription: Performs Optical Character Recognition (OCR) on an image and provides bounding boxes for the detected text. This is useful for extracting and locating textual information in images, such as reading signs, labels, or other forms of text in images.\n\n\n\n8. Phrase Grounding for Specific Expressions\n\nPrompt: \"&lt;CAPTION_TO_PHRASE_GROUNDING&gt;\" along with a specific expression, such as \"a wine glass\".\nDescription: Locates the area in the image that corresponds to a specific textual phrase. This task allows for identifying particular objects or elements when prompted with a word or keyword.\n\n\n\n9. Open Vocabulary Object Detection\n\nPrompt: \"&lt;OPEN_VOCABULARY_OD&gt;\"\nDescription: The model can detect objects without being restricted to a predefined list of classes, making it helpful in recognizing a broader range of items based on general visual understanding."
  },
  {
    "objectID": "raspi/vlm/vlm.html#exploring-computer-vision-and-vision-language-tasks",
    "href": "raspi/vlm/vlm.html#exploring-computer-vision-and-vision-language-tasks",
    "title": "Vision-Language Models at the Edge",
    "section": "Exploring computer vision and vision-language tasks",
    "text": "Exploring computer vision and vision-language tasks\nFor exploration, all codes can be found on the GitHub:\n\n20-florence_2.ipynb\n\nLet’s use a couple of images created by Dall-E and upload them to the Rasp-5 (FileZilla can be used for that). The images will be saved on a sub-folder named images :\ndogs_cats = Image.open('./images/dogs-cats.jpg')\ntable = Image.open('./images/table.jpg')\n\nLet’s create a function to facilitate our exploration and to keep track of the latency of the model for different tasks:\ndef run_example(task_prompt, text_input=None, image=None):\n    start_time = time.perf_counter()  # Start timing\n    if text_input is None:\n        prompt = task_prompt\n    else:\n        prompt = task_prompt + text_input\n    inputs = processor(text=prompt, images=image, \n                       return_tensors=\"pt\").to(device)\n    generated_ids = model.generate(\n      input_ids=inputs[\"input_ids\"],\n      pixel_values=inputs[\"pixel_values\"],\n      max_new_tokens=1024,\n      early_stopping=False,\n      do_sample=False,\n      num_beams=3,\n    )\n    generated_text = processor.batch_decode(generated_ids, \n                                            skip_special_tokens=False)[0]\n    parsed_answer = processor.post_process_generation(\n        generated_text,\n        task=task_prompt,\n        image_size=(image.width, image.height)\n    )\n    \n    end_time = time.perf_counter()  # End timing\n    elapsed_time = end_time - start_time  # Calculate elapsed time\n    print(f\" \\n[INFO] ==&gt; Florence-2-base ({task_prompt}), \n          took {elapsed_time:.1f} seconds to execute.\\n\")\n    \n    return parsed_answer\n\nCaption\n1. Dogs and Cats\nrun_example(task_prompt='&lt;CAPTION&gt;',image=dogs_cats)\n[INFO] ==&gt; Florence-2-base (&lt;CAPTION&gt;), took 16.1 seconds to execute.\n\n{'&lt;CAPTION&gt;': 'A group of dogs and cats sitting in a garden.'}\n2. Table\nrun_example(task_prompt='&lt;CAPTION&gt;',image=table)\n[INFO] ==&gt; Florence-2-base (&lt;CAPTION&gt;), took 16.5 seconds to execute.\n\n{'&lt;CAPTION&gt;': 'A wooden table topped with a plate of fruit and a glass of wine.'}\n\n\nDETAILED_CAPTION\n1. Dogs and Cats\nrun_example(task_prompt='&lt;DETAILED_CAPTION&gt;',image=dogs_cats)\n[INFO] ==&gt; Florence-2-base (&lt;DETAILED_CAPTION&gt;), took 25.5 seconds to execute.\n\n{'&lt;DETAILED_CAPTION&gt;': 'The image shows a group of cats and dogs sitting on top of a\nlush green field, surrounded by plants with flowers, trees, and a house in the \nbackground. The sky is visible above them, creating a peaceful atmosphere.'}\n2. Table\nrun_example(task_prompt='&lt;DETAILED_CAPTION&gt;',image=table)\n[INFO] ==&gt; Florence-2-base (&lt;DETAILED_CAPTION&gt;), took 26.8 seconds to execute.\n\n{'&lt;DETAILED_CAPTION&gt;': 'The image shows a wooden table with a bottle of wine and a \nglass of wine on it, surrounded by a variety of fruits such as apples, oranges, and \ngrapes. In the background, there are chairs, plants, trees, and a house, all slightly \nblurred.'}\n\n\nMORE_DETAILED_CAPTION\n1. Dogs and Cats\nrun_example(task_prompt='&lt;MORE_DETAILED_CAPTION&gt;',image=dogs_cats)\n[INFO] ==&gt; Florence-2-base (&lt;MORE_DETAILED_CAPTION&gt;), took 49.8 seconds to execute.\n\n{'&lt;MORE_DETAILED_CAPTION&gt;': 'The image shows a group of four cats and a dog in a garden. \nThe garden is filled with colorful flowers and plants, and there is a pathway leading up \nto a house in the background. The main focus of the image is a large German Shepherd dog \nstanding on the left side of the garden, with its tongue hanging out and its mouth open, \nas if it is panting or panting. On the right side, there are two smaller cats, one orange \nand one gray, sitting on the grass. In the background, there is another golden retriever \ndog sitting and looking at the camera. The sky is blue and the sun is shining, creating a \nwarm and inviting atmosphere.'}\n2. Table\nrun_example(task_prompt='&lt; MORE_DETAILED_CAPTION&gt;',image=table)\nINFO] ==&gt; Florence-2-base (&lt;MORE_DETAILED_CAPTION&gt;), took 32.4 seconds to execute.\n\n{'&lt;MORE_DETAILED_CAPTION&gt;': 'The image shows a wooden table with a wooden tray on it. On \nthe tray, there are various fruits such as grapes, oranges, apples, and grapes. There is \nalso a bottle of red wine on the table. The background shows a garden with trees and a \nhouse. The overall mood of the image is peaceful and serene.'}\n\nWe can note that the more detailed the caption task, the longer the latency and the possibility of mistakes (like “The image shows a group of four cats and a dog in a garden”, instead of two dogs and three cats).\n\n\n\nOD - Object Detection\nWe can run the same previous function for object detection using the prompt &lt;OD&gt;.\ntask_prompt = '&lt;OD&gt;'\nresults = run_example(task_prompt,image=dogs_cats)\nprint(results)\nLet’s see the result:\n[INFO] ==&gt; Florence-2-base (&lt;OD&gt;), took 20.9 seconds to execute.\n\n{'&lt;OD&gt;': {'bboxes': [[737.7920532226562, 571.904052734375, 1022.4640502929688, \n980.4800415039062], [0.5120000243186951, 593.4080200195312, 211.4560089111328, \n991.7440185546875], [445.9520263671875, 721.4080200195312, 680.4480590820312, \n850.4320678710938], [39.42400360107422, 91.64800262451172, 491.0080261230469, \n933.3760375976562], [570.8800048828125, 184.83201599121094, 974.3360595703125, \n782.8480224609375]], 'labels': ['cat', 'cat', 'cat', 'dog', 'dog']}}\nOnly by the labels ['cat,' 'cat,' 'cat,' 'dog,' 'dog'] is it possible to see that the main objects in the image were captured. Let’s apply the function used before to draw the bounding boxes:\nplot_bbox(dogs_cats, results['&lt;OD&gt;'])\n\nLet’s also do it with the Table image:\ntask_prompt = '&lt;OD&gt;'\nresults = run_example(task_prompt,image=table)\nplot_bbox(table, results['&lt;OD&gt;'])\n[INFO] ==&gt; Florence-2-base (&lt;OD&gt;), took 40.8 seconds to execute.\n\n\n\nDENSE_REGION_CAPTION\nIt is possible to mix the classic Object Detection with the Caption task in specific sub-regions of the image:\ntask_prompt = '&lt;DENSE_REGION_CAPTION&gt;'\n\nresults = run_example(task_prompt,image=dogs_cats)\nplot_bbox(dogs_cats, results['&lt;DENSE_REGION_CAPTION&gt;'])\n\nresults = run_example(task_prompt,image=table)\nplot_bbox(table, results['&lt;DENSE_REGION_CAPTION&gt;'])\n\n\n\nCAPTION_TO_PHRASE_GROUNDING\nWith this task, we can enter with a caption, such as “a wine glass”, “a wine bottle,” or “a half orange,” and Florence-2 will localize the object in the image:\ntask_prompt = '&lt;CAPTION_TO_PHRASE_GROUNDING&gt;'\n\nresults = run_example(task_prompt, text_input=\"a wine bottle\",image=table)\nplot_bbox(table, results['&lt;CAPTION_TO_PHRASE_GROUNDING&gt;'])\n\nresults = run_example(task_prompt, text_input=\"a wine glass\",image=table)\nplot_bbox(table, results['&lt;CAPTION_TO_PHRASE_GROUNDING&gt;'])\n\nresults = run_example(task_prompt, text_input=\"a half orange\",image=table)\nplot_bbox(table, results['&lt;CAPTION_TO_PHRASE_GROUNDING&gt;'])\n\n[INFO] ==&gt; Florence-2-base (&lt;CAPTION_TO_PHRASE_GROUNDING&gt;), took 15.7 seconds to execute\neach task.\n\n\nCascade Tasks\nWe can also enter the image caption as the input text to push Florence-2 to find more objects:\ntask_prompt = '&lt;CAPTION&gt;'\nresults = run_example(task_prompt,image=dogs_cats)\ntext_input = results[task_prompt]\ntask_prompt = '&lt;CAPTION_TO_PHRASE_GROUNDING&gt;'\nresults = run_example(task_prompt, text_input,image=dogs_cats)\nplot_bbox(dogs_cats, results['&lt;CAPTION_TO_PHRASE_GROUNDING&gt;'])\nChanging the task_prompt among &lt;CAPTION,&gt; &lt;DETAILED_CAPTION&gt; and &lt;MORE_DETAILED_CAPTION&gt;, we will get more objects in the image.\n\n\n\nOPEN_VOCABULARY_DETECTION\n&lt;OPEN_VOCABULARY_DETECTION&gt; allows Florence-2 to detect recognizable objects in an image without relying on a predefined list of categories, making it a versatile tool for identifying various items that may not have been explicitly labeled during training. Unlike &lt;CAPTION_TO_PHRASE_GROUNDING&gt;, which requires a specific text phrase to locate and highlight a particular object in an image, &lt;OPEN_VOCABULARY_DETECTION&gt; performs a broad scan to find and classify all objects present.\nThis makes &lt;OPEN_VOCABULARY_DETECTION&gt; particularly useful for applications where you need a comprehensive overview of everything in an image without prior knowledge of what to expect. Enter with a text describing specific objects not previously detected, resulting in their detection. For example:\ntask_prompt = '&lt;OPEN_VOCABULARY_DETECTION&gt;'\ntext = [\"a house\", \"a tree\", \"a standing cat at the left\", \n        \"a sleeping cat on the ground\", \"a standing cat at the right\", \n        \"a yellow cat\"]\nfor txt in text:\n    results = run_example(task_prompt, text_input=txt,image=dogs_cats)\n    bbox_results  = convert_to_od_format(results['&lt;OPEN_VOCABULARY_DETECTION&gt;'])\n    plot_bbox(dogs_cats, bbox_results)\n\n[INFO] ==&gt; Florence-2-base (&lt;OPEN_VOCABULARY_DETECTION&gt;), took 15.1 seconds \nto execute each task.\n\nNote: Trying to use Florence-2 to find objects that were not found can leads to mistakes (see exaamples on the Notebook).\n\n\n\nReferring expression segmentation\nWe can also segment a specific object in the image and give its description (caption), such as “a wine bottle” on the table image or “a German Sheppard” on the dogs_cats.\nReferring expression segmentation results format: {'&lt;REFERRING_EXPRESSION_SEGMENTATION&gt;': {'Polygons': [[[polygon]], ...], 'labels': ['', '', ...]}}, one object is represented by a list of polygons. each polygon is [x1, y1, x2, y2, ..., xn, yn].\n\nPolygon (x1, y1, …, xn, yn): Location tokens represent the vertices of a polygon in clockwise order.\n\nSo, let’s first create a function to plot the segmentation:\nfrom PIL import Image, ImageDraw, ImageFont\nimport copy\nimport random\nimport numpy as np\ncolormap = ['blue','orange','green','purple','brown','pink','gray','olive',\n    'cyan','red','lime','indigo','violet','aqua','magenta','coral','gold',\n    'tan','skyblue']\n\ndef draw_polygons(image, prediction, fill_mask=False):\n    \"\"\"\n    Draws segmentation masks with polygons on an image.\n\n    Parameters:\n    - image_path: Path to the image file.\n    - prediction: Dictionary containing 'polygons' and 'labels' keys.\n                  'polygons' is a list of lists, each containing vertices \n                  of a polygon.\n                  'labels' is a list of labels corresponding to each polygon.\n    - fill_mask: Boolean indicating whether to fill the polygons with color.\n    \"\"\"\n    # Load the image\n\n    draw = ImageDraw.Draw(image)\n\n\n    # Set up scale factor if needed (use 1 if not scaling)\n    scale = 1\n\n    # Iterate over polygons and labels\n    for polygons, label in zip(prediction['polygons'], prediction['labels']):\n        color = random.choice(colormap)\n        fill_color = random.choice(colormap) if fill_mask else None\n\n        for _polygon in polygons:\n            _polygon = np.array(_polygon).reshape(-1, 2)\n            if len(_polygon) &lt; 3:\n                print('Invalid polygon:', _polygon)\n                continue\n\n            _polygon = (_polygon * scale).reshape(-1).tolist()\n\n            # Draw the polygon\n            if fill_mask:\n                draw.polygon(_polygon, outline=color, fill=fill_color)\n            else:\n                draw.polygon(_polygon, outline=color)\n\n            # Draw the label text\n            draw.text((_polygon[0] + 8, _polygon[1] + 2), label, fill=color)\n    \n    # Save or display the image\n    #image.show()  # Display the image\n    display(image)\nNow we can run the functions:\ntask_prompt = '&lt;REFERRING_EXPRESSION_SEGMENTATION&gt;'\n\nresults = run_example(task_prompt, text_input=\"a wine bottle\",image=table)\noutput_image = copy.deepcopy(table)\ndraw_polygons(output_image, \n              results['&lt;REFERRING_EXPRESSION_SEGMENTATION&gt;'], \n              fill_mask=True)\n\nresults = run_example(task_prompt, text_input=\"a german sheppard\",image=dogs_cats)\noutput_image = copy.deepcopy(dogs_cats)\ndraw_polygons(output_image, \n              results['&lt;REFERRING_EXPRESSION_SEGMENTATION&gt;'], \n              fill_mask=True)\n\n[INFO] ==&gt; Florence-2-base (&lt;REFERRING_EXPRESSION_SEGMENTATION&gt;), \ntook 207.0 seconds to execute each task.\n\n\nRegion to Segmentation\nWith this task, it is also possible to give the object coordinates in the image to segment it. The input format is '&lt;loc_x1&gt;&lt;loc_y1&gt;&lt;loc_x2&gt;&lt;loc_y2&gt;', [x1, y1, x2, y2] , which is the quantized coordinates in [0, 999].\nFor example, when running the code:\ntask_prompt = '&lt;CAPTION_TO_PHRASE_GROUNDING&gt;'\nresults = run_example(task_prompt, text_input=\"a half orange\",image=table)\nresults\nThe results were:\n{'&lt;CAPTION_TO_PHRASE_GROUNDING&gt;': {'bboxes': [[343.552001953125,\n    689.6640625,\n    530.9440307617188,\n    873.9840698242188]],\n  'labels': ['a half']}}\nUsing the bboxes rounded coordinates:\ntask_prompt = '&lt;REGION_TO_SEGMENTATION&gt;'\nresults = run_example(task_prompt, \n                      text_input=\"&lt;loc_343&gt;&lt;loc_690&gt;&lt;loc_531&gt;&lt;loc_874&gt;\",\n                      image=table)\noutput_image = copy.deepcopy(table)\ndraw_polygons(output_image, results['&lt;REGION_TO_SEGMENTATION&gt;'], fill_mask=True)  \nWe got the segmentation of the object on those coordinates (Latency: 83 seconds):\n\n\n\nRegion to Texts\nWe can also give the region (coordinates and ask for a caption):\ntask_prompt = '&lt;REGION_TO_CATEGORY&gt;'\nresults = run_example(task_prompt, text_input=\"&lt;loc_343&gt;&lt;loc_690&gt;&lt;loc_531&gt;\n                      &lt;loc_874&gt;\",image=table)\nresults\n[INFO] ==&gt; Florence-2-base (&lt;REGION_TO_CATEGORY&gt;), took 14.3 seconds to execute.\n{'&lt;REGION_TO_CATEGORY&gt;': 'orange&lt;loc_343&gt;&lt;loc_690&gt;&lt;loc_531&gt;&lt;loc_874&gt;'}\nThe model identified an orange in that region. Let’s ask for a description:\ntask_prompt = '&lt;REGION_TO_DESCRIPTION&gt;'\nresults = run_example(task_prompt, text_input=\"&lt;loc_343&gt;&lt;loc_690&gt;&lt;loc_531&gt;\n                      &lt;loc_874&gt;\",image=table)\nresults\n[INFO] ==&gt; Florence-2-base (&lt;REGION_TO_CATEGORY&gt;), took 14.6 seconds to execute.\n\n{'&lt;REGION_TO_CATEGORY&gt;': 'orange&lt;loc_343&gt;&lt;loc_690&gt;&lt;loc_531&gt;&lt;loc_874&gt;'}\nIn this case, the description did not provide more details, but it could. Try another example.\n\n\nOCR\nWith Florence-2, we can perform Optical Character Recognition (OCR) on an image, getting what is written on it (task_prompt = '&lt;OCR&gt;' and also get the bounding boxes (location) for the detected text (ask_prompt = '&lt;OCR_WITH_REGION&gt;'). Those tasks can help extract and locate textual information in images, such as reading signs, labels, or other forms of text in images.\nLet’s upload a flyer from a talk in Brazil to Raspi. Let’s test works in another language, here Portuguese):\nflayer = Image.open('./images/embarcados.jpg')\n# Display the image\nplt.figure(figsize=(8, 8))\nplt.imshow(flayer)\nplt.axis('off')\n#plt.title(\"Image\")\nplt.show()\n\nLet’s examine the image with '&lt;MORE_DETAILED_CAPTION&gt;' :\n[INFO] ==&gt; Florence-2-base (&lt;MORE_DETAILED_CAPTION&gt;), took 85.2 seconds to execute.\n\n{'&lt;MORE_DETAILED_CAPTION&gt;': 'The image is a promotional poster for an event called \n\"Machine Learning Embarcados\" hosted by Marcelo Roval. The poster has a black \nbackground with white text. On the left side of the poster, there is a logo of a \ncoffee cup with the text \"Café Com Embarcados\" above it. Below the logo, it says \n\"25 de Setembro as 17th\" which translates to \"25th of September as 17\" in English. \n\\n\\nOn the right side, there aretwo smaller text boxes with the names of the \nparticipants and their names. The first text box reads \"Democratizando a Inteligência \nArtificial para Paises em Desenvolvimento\" and the second text box says \"Toda quarta-feira\" which is Portuguese for \"Transmissão via in Portuguese\".\\n\\nIn the center of\nthe image, there has a photo of Marcelo, a man with a beard and glasses, smiling at\nthe camera. He is wearing a white hard hat and a white shirt. The text boxes are \nin orange and yellow colors.'}\nThe description is very accurate. Let’s get to the more important words with the task OCR:\ntask_prompt = '&lt;OCR&gt;'\nrun_example(task_prompt,image=flayer)\n[INFO] ==&gt; Florence-2-base (&lt;OCR&gt;), took 37.7 seconds to execute.\n{'&lt;OCR&gt;': 'Machine LearningCafécomEmbarcadoEmbarcadosDemocratizando a \nInteligênciaArtificial para Paises em25 de Setembro ás 17hDesenvolvimentoToda quarta-\nfeiraMarcelo RovalProfessor na UNIFIEI eTransmissão viainCo-Director do TinyML4D'}\nLet’s locate the words in the flyer:\ntask_prompt = '&lt;OCR_WITH_REGION&gt;'\nresults = run_example(task_prompt,image=flayer)\nLet’s also create a function to draw bounding boxes around the detected words:\ndef draw_ocr_bboxes(image, prediction):\n    scale = 1\n    draw = ImageDraw.Draw(image)\n    bboxes, labels = prediction['quad_boxes'], prediction['labels']\n    for box, label in zip(bboxes, labels):\n        color = random.choice(colormap)\n        new_box = (np.array(box) * scale).tolist()\n        draw.polygon(new_box, width=3, outline=color)\n        draw.text((new_box[0]+8, new_box[1]+2),\n                    \"{}\".format(label),\n                    align=\"right\",\n\n                    fill=color)\n    display(image)\noutput_image = copy.deepcopy(flayer)\ndraw_ocr_bboxes(output_image, results['&lt;OCR_WITH_REGION&gt;'])\n\nWe can inspect the detected words:\nresults['&lt;OCR_WITH_REGION&gt;']['labels']\n'&lt;/s&gt;Machine Learning',\n 'Café',\n 'com',\n 'Embarcado',\n 'Embarcados',\n 'Democratizando a Inteligência',\n 'Artificial para Paises em',\n '25 de Setembro ás 17h',\n 'Desenvolvimento',\n 'Toda quarta-feira',\n 'Marcelo Roval',\n 'Professor na UNIFIEI e',\n 'Transmissão via',\n 'in',\n 'Co-Director do TinyML4D']"
  },
  {
    "objectID": "raspi/vlm/vlm.html#latency-summary",
    "href": "raspi/vlm/vlm.html#latency-summary",
    "title": "Vision-Language Models at the Edge",
    "section": "Latency Summary",
    "text": "Latency Summary\nThe latency observed for different tasks using Florence-2 on the Raspberry Pi (Raspi-5) varied depending on the complexity of the task:\n\nImage Captioning: It took approximately 16-17 seconds to generate a caption for an image.\nDetailed Captioning: Increased latency to around 25-27 seconds, requiring generating more nuanced scene descriptions.\nMore Detailed Captioning: It took about 32-50 seconds, and the latency increased as the description grew more complex.\nObject Detection: It took approximately 20-41 seconds, depending on the image’s complexity and the number of detected objects.\nVisual Grounding: Approximately 15-16 seconds to localize specific objects based on textual prompts.\nOCR (Optical Character Recognition): Extracting text from an image took around 37-38 seconds.\nSegmentation and Region to Segmentation: Segmentation tasks took considerably longer, with a latency of around 83-207 seconds, depending on the complexity and the number of regions to be segmented.\n\nThese latency times highlight the resource constraints of edge devices like the Raspberry Pi and emphasize the need to optimize the model and the environment to achieve real-time performance.\n\n\nRunning complex tasks can use all 8GB of the Raspi-5’s memory. For example, the above screenshot during the Florence OD task shows 4 CPUs at full speed and over 5GB of memory in use. Consider increasing the SWAP memory to 2 GB.\n\nChecking the CPU temperature with vcgencmd measure_temp , showed that temperature can go up to +80oC."
  },
  {
    "objectID": "raspi/vlm/vlm.html#fine-tunning",
    "href": "raspi/vlm/vlm.html#fine-tunning",
    "title": "Vision-Language Models at the Edge",
    "section": "Fine-Tunning",
    "text": "Fine-Tunning\nAs explored in this lab, Florence supports many tasks out of the box, including captioning, object detection, OCR, and more. However, like other pre-trained foundational models, Florence-2 may need domain-specific knowledge. For example, it may need to improve with medical or satellite imagery. In such cases, fine-tuning with a custom dataset is necessary. The Roboflow tutorial, How to Fine-tune Florence-2 for Object Detection Tasks, shows how to fine-tune Florence-2 on object detection datasets to improve model performance for our specific use case.\nBased on the above tutorial, it is possible to fine-tune the Florence-2 model to detect boxes and wheels used in previous labs:\n\nIt is important to note that after fine-tuning, the model can still detect classes that don’t belong to our custom dataset, like cats, dogs, grapes, etc, as seen before).\nThe complete fine-tunning project using a previously annotated dataset in Roboflow and executed on CoLab can be found in the notebook:\n\n30-Finetune_florence_2_on_detection_dataset_box_vs_wheel.ipynb\n\nIn another example, in the post, Fine-tuning Florence-2 - Microsoft’s Cutting-edge Vision Language Models, the authors show an example of fine-tuning Florence on DocVQA. The authors report that Florence 2 can perform visual question answering (VQA), but the released models don’t include VQA capability."
  },
  {
    "objectID": "raspi/vlm/vlm.html#conclusion",
    "href": "raspi/vlm/vlm.html#conclusion",
    "title": "Vision-Language Models at the Edge",
    "section": "Conclusion",
    "text": "Conclusion\nFlorence-2 offers a versatile and powerful approach to vision-language tasks at the edge, providing performance that rivals larger, task-specific models, such as YOLO for object detection, BERT/RoBERTa for text analysis, and specialized OCR models.\nThanks to its multi-modal transformer architecture, Florence-2 is more flexible than YOLO in terms of the tasks it can handle. These include object detection, image captioning, and visual grounding.\nUnlike BERT, which focuses purely on language, Florence-2 integrates vision and language, allowing it to excel in applications that require both modalities, such as image captioning and visual grounding.\nMoreover, while traditional OCR models such as Tesseract and EasyOCR are designed solely for recognizing and extracting text from images, Florence-2’s OCR capabilities are part of a broader framework that includes contextual understanding and visual-text alignment. This makes it particularly useful for scenarios that require both reading text and interpreting its context within images.\nOverall, Florence-2 stands out for its ability to seamlessly integrate various vision-language tasks into a unified model that is efficient enough to run on edge devices like the Raspberry Pi. This makes it a compelling choice for developers and researchers exploring AI applications at the edge.\n\nKey Advantages of Florence-2\n\nUnified Architecture\n\nSingle model handles multiple vision tasks vs. specialized models (YOLO, BERT, Tesseract)\nEliminates the need for multiple model deployments and integrations\nConsistent API and interface across tasks\n\nPerformance Comparison\n\nObject Detection: Comparable to YOLOv8 (~37.5 mAP on COCO vs. YOLOv8’s ~39.7 mAP) despite being general-purpose\nText Recognition: Handles multiple languages effectively like specialized OCR models (Tesseract, EasyOCR)\nLanguage Understanding: Integrates BERT-like capabilities for text processing while adding visual context\n\nResource Efficiency\n\nThe Base model (232M parameters) achieves strong results despite smaller size\nRuns effectively on edge devices (Raspberry Pi)\nSingle model deployment vs. multiple specialized models\n\n\n\n\nTrade-offs\n\nPerformance vs. Specialized Models\n\nYOLO series may offer faster inference for pure object detection\nSpecialized OCR models might handle complex document layouts better\nBERT/RoBERTa provide deeper language understanding for text-only tasks\n\nResource Requirements\n\nHigher latency on edge devices (15-200s depending on task)\nRequires careful memory management on Raspberry Pi\nIt may need optimization for real-time applications\n\nDeployment Considerations\n\nInitial setup is more complex than single-purpose models\nRequires understanding of multiple task types and prompts\nThe learning curve for optimal prompt engineering\n\n\n\n\nBest Use Cases\n\nResource-Constrained Environments\n\nEdge devices requiring multiple vision capabilities\nSystems with limited storage/deployment capacity\nApplications needing flexible vision processing\n\nMulti-modal Applications\n\nContent moderation systems\nAccessibility tools\nDocument analysis workflows\n\nRapid Prototyping\n\nQuick deployment of vision capabilities\nTesting multiple vision tasks without separate models\nProof-of-concept development"
  },
  {
    "objectID": "raspi/vlm/vlm.html#future-implications",
    "href": "raspi/vlm/vlm.html#future-implications",
    "title": "Vision-Language Models at the Edge",
    "section": "Future Implications",
    "text": "Future Implications\nFlorence-2 represents a shift toward unified vision models that could eventually replace task-specific architectures in many applications. While specialized models maintain advantages in specific scenarios, the convenience and efficiency of unified models like Florence-2 make them increasingly attractive for real-world deployments.\nThe lab demonstrates Florence-2’s viability on edge devices, suggesting future IoT, mobile computing, and embedded systems applications where deploying multiple specialized models would be impractical."
  },
  {
    "objectID": "raspi/vlm/vlm.html#resources",
    "href": "raspi/vlm/vlm.html#resources",
    "title": "Vision-Language Models at the Edge",
    "section": "Resources",
    "text": "Resources\n\n10-florence2_test.ipynb\n20-florence_2.ipynb\n30-Finetune_florence_2_on_detection_dataset_box_vs_wheel.ipynb"
  },
  {
    "objectID": "raspi/audio_pipeline/audio_pipeline.html#introduction",
    "href": "raspi/audio_pipeline/audio_pipeline.html#introduction",
    "title": "Audio and Vision AI Pipeline",
    "section": "Introduction",
    "text": "Introduction\nIn this chapter, we extend our SLM and SVL capabilities by creating a complete audio processing pipeline that transforms voice or image input into intelligent vocal responses. We will learn to integrate Speech-to-Text (STT), Small Language (or Visual) Models, and Text-to-Speech (TTS) technologies to build conversational AI systems that run entirely on Raspberry Pi hardware.\nThis chapter bridges the gap between our existing computer vision knowledge and multimodal AI applications, demonstrating how different AI components work together in real-world edge deployments."
  },
  {
    "objectID": "raspi/audio_pipeline/audio_pipeline.html#the-audio-to-audio-ai-pipeline-architecture",
    "href": "raspi/audio_pipeline/audio_pipeline.html#the-audio-to-audio-ai-pipeline-architecture",
    "title": "Audio and Vision AI Pipeline",
    "section": "The Audio to Audio AI Pipeline Architecture",
    "text": "The Audio to Audio AI Pipeline Architecture\n\nWe will understand how to architect and implement multimodal AI systems by building a complete voice interaction pipeline. The goal is to gain practical experience with audio processing on edge devices while learning to efficiently integrate multiple AI models within resource constraints. Additionally, you will develop troubleshooting skills for complex AI pipelines and understand the engineering trade-offs involved in edge audio processing.\n\nUnderstanding Multimodal AI Systems\nWhen we built computer vision systems earlier in the course, we processed visual data to extract meaningful information. Audio AI systems follow a similar principle but work with temporal audio signals instead of static images. The key insight is that speech processing requires multiple specialized models working together, rather than a single end-to-end system.\n\nModern small models, such as Gemma 3n, can process audio directly and its prompt. Today (September 2025), Gemma 3n can transcribe text from audio files using Hugging Face Transformers, but it is not available with Ollama\n\nConsider how humans process spoken language. We simultaneously parse the acoustic signal, understand the linguistic content, reason about the meaning, and formulate responses. Our AI pipeline mimics this process by breaking it into distinct, manageable components.\n\n\nSystem Architecture Overview\nOur comprehensive audio AI pipeline comprises four main components, connected in sequence.\n[Microphone] → [STT Model] → [SLM] → [TTS Model] → [Speaker]\n   Audio        Text        Text      Audio\nAudio captured by the microphone is processed through a Speech-to-Text model, which converts sound waves into text transcriptions. This text becomes input for our Small Language Model, which generates intelligent responses. Finally, a Text-to-Speech system converts the written response back into spoken audio.\nEach component has specific requirements and limitations. The STT model must handle various accents and noise conditions. The SLM needs sufficient context to generate coherent responses. The TTS system must produce speech that sounds natural. Understanding these individual requirements helps us optimize the overall system performance.\n\n\nEdge AI Considerations\nRunning this pipeline on a Raspberry Pi presents unique challenges compared to cloud-based solutions. We must carefully manage memory usage, processing time, and model sizes. The benefit is complete local processing with no internet dependency and enhanced privacy protection.\nThe choice of models becomes critical. We select Moonshine for STT because it’s specifically optimized for edge devices. We utilize small language models, such as llama3.2:3b, for reasonable performance on limited hardware. For TTS, we choose PIPER for its balance between quality and computational efficiency."
  },
  {
    "objectID": "raspi/audio_pipeline/audio_pipeline.html#hardware-setup-and-audio-capture",
    "href": "raspi/audio_pipeline/audio_pipeline.html#hardware-setup-and-audio-capture",
    "title": "Audio and Vision AI Pipeline",
    "section": "Hardware Setup and Audio Capture",
    "text": "Hardware Setup and Audio Capture\n\nAudio Hardware Detection\nBegin by identifying the audio capabilities of our system. The Raspberry Pi can work with various audio input and output devices, but proper configuration is essential for reliable operation.\nUse the command arecord -l to list available recording devices. You should see output showing your microphone’s card and device numbers. For USB microphones, this typically appears as something like card 2: Microphone [USB Condenser Microphone], device 0: USB Audio [USB Audio]. The critical information is the card number and device number, which you’ll reference as hw:2,0 in the code.\n\n\n\n\nTesting Basic Audio Functionality\nBefore writing Python code, we should verify that our audio setup works correctly at the system level. Let’s record a short test file using:\narecord --device=\"plughw:2,0\" --format=S16_LE --rate=16000 -c2 myaudio.wav\nPlay back the recording with aplay myaudio.wav to confirm that both capture and playback work correctly (use [CTRL]+[C] to stop the recording or add a duration in seconds to the command line).\n\n\nThe .WAV file can be played on another device (such as a computer) or on the Raspberry Pi, as a speaker can be connected via USB or Bluetooth.\n\n\n\nPython Audio Integration\nNow, we should install the necessary audio processing dependencies.\nsource ~/ollama/bin/activate\nThe PyAudio library requires system-level audio libraries; therefore, install them using sudo apt-get.\nsudo apt-get update\nsudo apt-get install libasound-dev libportaudio2 libportaudiocpp0 portaudio19-dev\nsudo apt-get install python3-pyaudio\n\nlibasound-dev covers ALSA development headers needed for audio libraries on Pi\npython3-pyaudio provides a prebuilt PyAudio package for most use cases\n\nLet’s create a working directory: Documents/OLLAMA/SST and verify the USB device index, with the below script (verify_usb_index.py:\nimport pyaudio\np = pyaudio.PyAudio()\nfor ii in range(p.get_device_count()):\n    print(ii, p.get_device_info_by_index(ii).get('name'))\nAs a result, we should get:\n0 USB Condenser Microphone: Audio (hw:2,0)\n1 pulse\n2 default\nWhat confirms that the index is 2 (hw:2,0)\n\nA lot of messages should appear. They are mostly ALSA and JACK warnings about missing or undefined virtual/surround sound devices—they are common on Raspberry Pi systems with minimal or headless sound configs and typically do not impact basic USB microphone capture. If our USB Microphone appears as a recording device (as it does: “hw:2,0”), we can safely ignore most of these unless audio capture fails.\n\nTo clean the output, we can use:\npython verify_usb_index.py 2&gt;/dev/null\n\n\n\nTest Audio Python Script\nThis script (test_audio_capture.py) records 10 seconds of mono audio at 16 kHz to output.wav using the USB microphone.\nimport pyaudio\nimport wave\n\nFORMAT = pyaudio.paInt16\nCHANNELS = 1    \nRATE = 16000    # 16 kHz\nCHUNK = 1024\nRECORD_SECONDS = 10\nDEVICE_INDEX = 2   # replace this with your detected USB mic's index\nWAVE_OUTPUT_FILENAME = \"output.wav\"\n\naudio = pyaudio.PyAudio()\n\nstream = audio.open(format=FORMAT, channels=CHANNELS,\n                    rate=RATE, input=True, input_device_index=DEVICE_INDEX,\n                    frames_per_buffer=CHUNK)\n\nprint(\"Recording...\")\nframes = []\n\nfor i in range(0, int(RATE / CHUNK * RECORD_SECONDS)):\n    data = stream.read(CHUNK, exception_on_overflow=False)\n    frames.append(data)\n\nprint(\"Finished recording.\")\n\nstream.stop_stream()\nstream.close()\naudio.terminate()\n\nwf = wave.open(WAVE_OUTPUT_FILENAME, 'wb')\nwf.setnchannels(CHANNELS)\nwf.setsampwidth(audio.get_sample_size(FORMAT))\nwf.setframerate(RATE)\nwf.writeframes(b''.join(frames))\nwf.close()\nUnderstanding the audio configuration parameters helps prevent common problems. We use 16-bit PCM format (pyaudio.paInt16) because it provides good quality while remaining computationally efficient. The 16kHz sampling rate balances audio quality with processing requirements - most speech recognition models expect this rate.\nThe buffer size (CHUNK = 1024) affects latency and reliability. Smaller buffers reduce latency but may cause audio dropouts on busy systems. Larger buffers increase latency but provide more stable recording.\nLet’s Playback to verify if we get it correctly:\n\n\n\nYour browser does not support the audio element."
  },
  {
    "objectID": "raspi/audio_pipeline/audio_pipeline.html#speech-recognition-sst-with-moonshine",
    "href": "raspi/audio_pipeline/audio_pipeline.html#speech-recognition-sst-with-moonshine",
    "title": "Audio and Vision AI Pipeline",
    "section": "Speech Recognition (SST) with Moonshine",
    "text": "Speech Recognition (SST) with Moonshine\n\nWhy Moonshine for Edge Deployment\nTraditional speech recognition systems, such as OpenAI’s Whisper, are highly accurate but require substantial computational resources. Moonshine is specifically designed for edge devices, using optimized model architectures and quantization techniques to achieve good performance on resource-constrained hardware.\nThe ONNX (Open Neural Network Exchange) version of Moonshine provides additional optimization benefits. ONNX Runtime includes hardware-specific optimizations that can significantly improve inference speed on ARM processors, such as those found in Raspberry Pi devices.\n\n\nModel Selection Strategy\nMoonshine offers different model sizes with clear trade-offs between accuracy and computational requirements. The “tiny” model processes audio quickly but may struggle with difficult audio conditions. The “base” model provides better accuracy but requires more processing time and memory.\nFor initial development, we should start with the tiny model to ensure that the pipeline works correctly. Once the complete system is functional, we can experiment with larger models to find the optimal balance for our specific use case and hardware capabilities.\n\n\nImplementation and Preprocessing\nInstall Moonshine with\npip install useful-moonshine-onnx@git+https://github.com/moonshine\nai/moonshine.git#subdirectory=moonshine-onnx`\nThis specific installation method ensures compatibility with the ONNX runtime optimizations.\nLet’s run the test script below (transcription_test.py):\nimport moonshine_onnx\ntext = moonshine_onnx.transcribe('output.wav', 'moonshine/tiny')\nprint(text[0])\nAs a result, we will get the corresponding text, which was recorded before:\n\n\nEmpty or incorrect transcriptions are common issues in speech recognition systems. These failures can result from background noise, insufficient volume, unclear speech, or mismatches in audio format. Implementing robust error handling prevents these issues from crashing your entire pipeline."
  },
  {
    "objectID": "raspi/audio_pipeline/audio_pipeline.html#slm-integration-and-response-generation",
    "href": "raspi/audio_pipeline/audio_pipeline.html#slm-integration-and-response-generation",
    "title": "Audio and Vision AI Pipeline",
    "section": "SLM Integration and Response Generation",
    "text": "SLM Integration and Response Generation\n\nConnecting STT Output to SLMs\nThe text output from your speech recognition system becomes input for your Small Language Model. However, the characteristics of spoken language differ significantly from written text, especially when filtered through speech recognition systems.\nSpoken language tends to be more informal, may contain false starts and repetitions, and might include transcription errors. Our SLM integration should account for these characteristics. On a final implementation, we should consider preprocessing the STT output to clean up obvious transcription errors or providing context to the SLM about the voice interaction nature of the input.\n\n\nOptimizing Prompts for Voice Interaction\nVoice-based interactions have different expectations than text-based chats. Responses should be concise since users must listen to the entire output. Avoid complex formatting or long lists that work well in text but become cumbersome when spoken aloud.\nWe should design our system prompts to encourage responses appropriate for voice interaction. For example, “Provide a brief, conversational response suitable for speaking aloud” can help guide the SLM toward more appropriate output formatting.\nUnlike single-query text interactions, voice conversations often involve multiple exchanges. Implementing conversation context memory significantly enhances the user experience. However, context management on edge devices requires careful consideration of memory usage.\nConsider implementing a sliding window approach, where you maintain the last few exchanges in memory but discard older context to prevent memory exhaustion, balancing context length with available system resources.\nLet’s create a function to handle this. For test, run slm_test.py:\nimport ollama\n\ndef generate_voice_response(user_input, model=\"llama3.2:3b\"):\n    \"\"\"\n    Generate a response optimized for voice interaction\n    \"\"\"\n    \n     # Context-setting prompt that guides the model's behavior\n    system_context = \"\"\"\n        You are a helpful AI assistant designed for voice interactions. \n        Your responses will be converted to speech and spoken aloud to the user.\n    \n      Guidelines for your responses:\n      - Keep responses conversational and concise (ideally under 50 words)\n      - Avoid complex formatting, lists, or visual elements\n      - Speak naturally, as if having a friendly conversation\n      - If the user's input seems unclear, ask for clarification politely\n      - Provide direct answers rather than lengthy explanations unless specifically \n        requested\n    \"\"\"\n    \n    # Combine system context with user input\n    full_prompt = f\"{system_context}\\n\\nUser said: {user_input}\\n\\nResponse:\"\n    \n    response = ollama.generate(\n        model=model,\n        prompt=full_prompt\n    )\n    \n    return response['response']\n\n\n# Answering the user question: \nuser_input = \"What is the capital of Malawi?\"\nresponse = generate_voice_response(user_input)\nprint (response)"
  },
  {
    "objectID": "raspi/audio_pipeline/audio_pipeline.html#text-to-speech-tts-with-piper",
    "href": "raspi/audio_pipeline/audio_pipeline.html#text-to-speech-tts-with-piper",
    "title": "Audio and Vision AI Pipeline",
    "section": "Text-to-Speech (TTS) with PIPER",
    "text": "Text-to-Speech (TTS) with PIPER\nText-to-speech systems face different challenges than speech recognition systems. While STT must handle various input conditions, TTS must generate consistent, natural-sounding output across diverse text inputs. The quality of TTS has a significant impact on the user experience in voice interaction systems.\nPIPER provides an excellent balance between voice quality and computational efficiency for edge deployments. Unlike cloud-based TTS services, PIPER runs entirely locally, ensuring privacy and eliminating network dependencies.\n\nVoice Model Selection and Installation\nPIPER offers various voice models with different characteristics. The “low”, “medium”, and “high” quality designations primarily refer to model size and computational requirements rather than dramatic quality differences. For most applications, the low-quality models provide acceptable voice output while running efficiently on Raspberry Pi hardware.\nInstall PIPER with pip install piper-tts, create a voices directory:\npip install piper-tts\nmkdir -p voices\nDownload voice models from the Hugging Face repository. Each voice model requires both the model file (.onnx) and a configuration file (.json). The configuration file contains model-specific parameters essential for generating proper audio.\nWe should download both files for our chosen voice; for example, the English female “lessac” voice provides clear, natural speech suitable for most applications.\nwget -O voices/en_US-lessac-low.onnx   https://huggingface.co/rhasspy/piper-\nvoices/resolve/v1.0.0/en/en_US/lessac/low/en_US-lessac-low.onnx\n\nwget -O voices/en_US-lessac-low.onnx.json   https://huggingface.co/rhasspy/piper-\nvoices/resolve/v1.0.0/en/en_US/lessac/low/en_US-lessac-low.onnx.json\nLet’s run the code below (tts_test.py) for testing:\nimport subprocess\nimport os\n\ndef text_to_speech_piper(text, output_file=\"piper_output.wav\"):\n    \"\"\"\n    Convert text to speech using PIPER and save to WAV file\n    \n    Args:\n        text (str): Text to convert to speech\n        output_file (str): Output WAV file path\n    \"\"\"\n    # Path to your voice model\n    model_path = \"voices/en_US-lessac-low.onnx\"\n    \n    # Check if model exists\n    if not os.path.exists(model_path):\n        print(f\"Error: Model file not found at {model_path}\")\n        return False\n    \n    try:\n        # Run PIPER command\n        process = subprocess.Popen(\n            ['piper', '--model', model_path, '--output_file', output_file],\n            stdin=subprocess.PIPE,\n            stdout=subprocess.PIPE,\n            stderr=subprocess.PIPE,\n            text=True\n        )\n        \n        # Send text to PIPER\n        stdout, stderr = process.communicate(input=text)\n        \n        if process.returncode == 0:\n            print(f\"\\nSpeech generated successfully: {output_file}\")\n            return True\n        else:\n            print(f\"Error: {stderr}\")\n            return False\n            \n    except Exception as e:\n        print(f\"Error running PIPER: {e}\")\n        return False\n\n# converting text to sound:\ntxt = \"Lilongwe is the capital of Malawi. Would you like to know more about \\\n       Lilongwe or Malawi in general?\"\n\nif text_to_speech_piper(txt):\n    print(\"You can now play the file with: aplay piper_output.wav\")\nelse:\n    print(\"Failed to generate speech\")       \nRuning the script, a piper_output.wav file will be generated, which is the text converted into speech.\n\n\nYour browser does not support the audio element. \nTo listen to the sound, we can run: aplay piper_output.wav.\n\n\n\nHandling Long Text and Special Cases\nTTS systems may struggle with very long input text or special characters. Implement text preprocessing to handle these cases gracefully. Break long responses into shorter segments, handle abbreviations and numbers appropriately, and filter out problematic characters that might cause TTS failures.\nConsider implementing text chunking for responses longer than a reasonable speaking length. This prevents both TTS processing issues and user fatigue from overly long audio responses."
  },
  {
    "objectID": "raspi/audio_pipeline/audio_pipeline.html#pipeline-integration-and-optimization",
    "href": "raspi/audio_pipeline/audio_pipeline.html#pipeline-integration-and-optimization",
    "title": "Audio and Vision AI Pipeline",
    "section": "Pipeline Integration and Optimization",
    "text": "Pipeline Integration and Optimization\n\nBuilding the Complete System\nIntegrating all components requires careful attention to error handling and resource management. Each stage of the pipeline can fail independently, and robust systems must handle these failures gracefully rather than crashing.\nDesign our integration with modularity in mind. Test each component independently before combining them. This approach simplifies debugging and allows you to optimize individual components separately.\nWe should also implement proper logging throughout our pipeline. When complex systems fail, detailed logs help identify whether the issue occurs in audio capture, speech recognition, language model processing, text-to-speech conversion, or audio playback.\n\n\nPerformance Optimization Strategies\nMeasure the timing of each pipeline component to identify bottlenecks. Typically, the SLM inference takes the longest time, followed by TTS generation. Understanding these timing characteristics helps prioritize optimization efforts.\nConsider implementing concurrent processing where possible. For example, you might begin TTS processing for the first part of an extended response while the SLM is still generating the remainder. However, be cautious about memory usage when implementing parallel processing on resource-constrained devices.\n\n\nMemory Management Considerations\nEdge devices have limited RAM, and loading multiple large models simultaneously can cause memory pressure. Implement strategies to manage memory efficiently, such as loading models only when needed or using model swapping for infrequently used components.\nMonitor system memory usage during operation and implement safeguards to prevent memory exhaustion. Consider implementing graceful degradation where your system switches to smaller, more efficient models if memory becomes constrained.\n\n\nDesigning Resilient Systems\nProduction-quality voice interaction systems must handle various failure modes gracefully. Network interruptions, hardware disconnections, model loading failures, and unexpected input conditions should not cause your system to crash.\nImplement comprehensive error handling at each pipeline stage. When speech recognition produces empty output, provide the user with meaningful feedback rather than processing empty strings. When TTS fails, consider falling back to text display or simplified audio feedback.\nDesign user feedback mechanisms that work within your voice interaction paradigm. Audio beeps, LED indicators, or simple voice messages can communicate system status without requiring visual displays.\n\n\nDebugging Complex Pipelines\nMulti-stage systems present unique debugging challenges. When the overall system fails, identifying the specific failure point requires systematic testing approaches.\nImplement test modes that allow you to inject known inputs at each pipeline stage. This capability enables you to isolate problems to specific components rather than repeatedly testing the entire system.\nCreate diagnostic outputs that help understand system behavior. For example, displaying transcription confidence scores, SLM response times, or TTS processing status helps identify performance issues or quality problems.\n\n\nThe full Python Script\nConsidering the previous points, let’s assemble all the essential components that work together: audio capture, transcription, language model processing, and text-to-speech. We should combine these into a complete voice pipeline that flows naturally from one step to the next.\nThe key insight here is that each of our developed scripts represents a stage in what’s called an “audio processing pipeline”. Let’s walk through how we can connect these pieces.\nUnderstanding the Pipeline Architecture\nThe pipeline follows a logical sequence: the voice becomes audio data, that audio is converted into text, the text is processed into an AI response, the response is converted into speech audio, and finally, that speech audio is transformed into sound that we can hear.\nWe should have a run_voice_pipeline() function in addition to the previous ones that acts as a coordinator, ensuring each step completes successfully before proceeding to the next. If any step fails, the entire pipeline stops gracefully rather than trying to continue with missing data.\nKey Integration Points\nWe should connect the scripts by ensuring the output of each function becomes the input for the following function. For example, record_audio() creates “user_input.wav”, which transcribe_audio() reads to produce text, which generate_response() processes to develop an AI response, and so on.\nThe error handling at each step ensures that if our microphone isn’t working, or if the AI model is busy, or if the voice model files are missing, we get clear feedback about what went wrong rather than mysterious crashes.\nOptimizations for Raspberry Pi\nThe Raspberry Pi has limited resources compared to a desktop computer, so we should include several optimizations. The cleanup_temp_files() function prevents our storage from filling up with temporary audio files. The audio configuration uses 16kHz sampling (which matches Moonshine’s expectations) rather than CD-quality 44kHz, reducing processing overhead.\nThe continuous assistant mode includes a manual trigger (pressing Enter) rather than voice activation detection, which saves CPU cycles that would otherwise be spent constantly monitoring audio input.\n\nOn a final product, we could include, for example, a KWS (Keyword Spotting) function, based on a TinyML device, where only when a trigger word is spoken, the record_audio() function starts to work.\n\nUnderstanding Voice Activity Detection\nOne of the main improvements from the single scripts stacked and tested separately is what audio engineers call “voice activity detection” (VAD). When we speak to someone face-to-face, we don’t announce, “I’m going to talk for exactly 10 seconds now.” Instead, we say our thoughts, pause naturally, and the listener intuitively knows when you’ve finished.\nOur new record_audio_with_silence_detection() function mimics this natural process by continuously analyzing the audio signal’s amplitude—essentially measuring how “loud” each tiny slice of audio is. When the amplitude stays below a threshold for 2 seconds, for example, the system intelligently concludes that we have finished speaking.\nThe Technical Magic Behind Silence Detection\nThe recording process now works like a sophisticated audio surveillance system. Every fraction of a second, it captures a small chunk of audio data (determined by your CHUNK size of 1024 samples) and immediately analyzes it. Using Python’s struct module, it converts the raw byte data into numerical values representing sound pressure levels.\nThe crucial calculation happens in this line: volume = max(chunk_data) / 32768.0. This finds the loudest moment in that tiny audio slice and normalizes it to a scale from 0 to 1, where 0 represents complete silence and 1 represents the maximum possible volume your microphone can capture.\nCalibrating the Sensitivity\nThe silence_threshold=0.01 parameter is our sensitivity control knob: too sensitive (closer to 0.00) and it might stop recording when you pause to think; not sensitive enough (closer to 0.1) and it might keep recording through long periods of quiet background noise.\nFor a typical indoor Raspberry Pi setup, 0.01 strikes a good balance. It’s sensitive enough to detect when we have stopped talking, but robust enough to ignore minor background sounds, such as air conditioning or distant traffic.\n\nWe should experiment with this value based on the specific environment.\n\nTesting and Deployment Strategy\nDownload the complete script from GitHub: voice_ai_pipeline.py\nWe can start by testing individual components using detect_audio_devices() first to confirm our USB microphone is still at index 2. Then, we can run run_voice_pipeline() with a simple question to verify the complete flow works.\nOnce we are confident in single interactions, we can use continuous_voice_assistant() for extended conversations. This mode lets us have back-and-forth exchanges with your AI assistant, making it feel more like a natural conversation partner.\n\nIf you want to experiment with different SLM models, we need to change the model parameter in generate_response().\n\n\nThe test with sound can be followed in the video:"
  },
  {
    "objectID": "raspi/audio_pipeline/audio_pipeline.html#the-image-to-audio-ai-pipeline-architecture",
    "href": "raspi/audio_pipeline/audio_pipeline.html#the-image-to-audio-ai-pipeline-architecture",
    "title": "Audio and Vision AI Pipeline",
    "section": "The Image to Audio AI Pipeline Architecture",
    "text": "The Image to Audio AI Pipeline Architecture\nVoice interaction systems have significant potential for educational applications and accessibility improvements by designing interfaces that adapt to different user needs and capabilities. By integrating small visual models, such as Moondream, with existing TTS pipelines, we can create multimodal assistants that describe images for people with visual impairments, converting visual content into detailed spoken descriptions of scenes, objects, and spatial relationships.\n\nTo use the camera, we must ensure that the NumPy version is compatible with the Raspberry Pi system (1.24.2). If this version was changed (what it should be due to the Moonshine installation), revert it to the 1.24.2 version.\npip uninstall numpy\npip install 'numpy==1.24.2'\nNow, let’s apply what we have explored in previous chapters, along with what we learn in this one, to create a simple code that converts an image captured by a camera into its corresponding spoken caption by the speaker.\nThe code is straightforward and is intended solely to test the solution’s potential.\n\nThe capture_image(IMG_PATH) function captures a photo and saves it to the specified path (IMG_PATH).\nThe caption = image_description(IMG_PATH, MODEL) function will describe the image using MoonDream, a powerful yet compact visual language model (VLM). The description in a text format will be saved in the variable caption.\nThe text_to_speech_piper(caption) function will create a .WAV file from the caption.\nAnd finally, the play_audio() function will play the .WAV file generated by PIPER.\n\nHere is the complete Python script (img_caption_speech.py):\nimport os\nimport time\nimport subprocess\nimport ollama\nfrom picamera2 import Picamera2\n\n\ndef capture_image(img_path):\n    # Initialize camera\n    picam2 = Picamera2()\n    picam2.start()\n\n    # Wait for camera to warm up\n    time.sleep(2)\n\n    # Capture image\n    picam2.capture_file(img_path)\n    print(\"\\n==&gt; Image captured: \"+img_path)\n\n    # Stop camera\n    picam2.stop()\n    picam2.close()\n\n\ndef image_description(img_path, model):\n    print (\"\\n==&gt; WAIT, SVL Model working ...\")\n    with open(img_path, 'rb') as file:\n        response = ollama.chat(\n            model=model,\n            messages=[\n              {\n                'role': 'user',\n                'content': '''return the description of the image''',\n                'images': [file.read()],\n              },\n            ],\n            options = {\n              'temperature': 0,\n              }\n      )\n    return response['message']['content']\n\n\ndef text_to_speech_piper(text, output_file=\"assistant_response.wav\"):\n\n    # Path to your voice model\n    model_path = \"voices/en_US-lessac-low.onnx\"\n    \n    # Check if model exists\n    if not os.path.exists(model_path):\n        print(f\"Error: Model file not found at {model_path}\")\n        return False\n    \n    try:\n        # Run PIPER command\n        process = subprocess.Popen(\n            ['piper', '--model', model_path, '--output_file', output_file],\n            stdin=subprocess.PIPE,\n            stdout=subprocess.PIPE,\n            stderr=subprocess.PIPE,\n            text=True\n        )\n        \n        # Send text to PIPER\n        stdout, stderr = process.communicate(input=text)\n        \n        if process.returncode == 0:\n            print(f\"\\nSpeech generated successfully: {output_file}\")\n            return True\n        else:\n            print(f\"Error: {stderr}\")\n            return False\n            \n    except Exception as e:\n        print(f\"Error running PIPER: {e}\")\n        return False\n\ndef play_audio(filename=\"assistant_response.wav\"):\n    try:\n        # Use aplay to play the audio file\n        result = subprocess.run(['aplay', filename], \n                              capture_output=True, \n                              text=True)\n        \n        if result.returncode == 0:\n            print(\"\\nAudio playback completed\")\n            return True\n        else:\n            print(f\"\\nPlayback error: {result.stderr}\")\n            return False\n            \n    except Exception as e:\n        print(f\"\\nError playing audio: {e}\")\n        return False\n\n\n# Example usage and testing functions\nif __name__ == \"__main__\":\n\n    print(\"\\n============= Image to Speech AI Pipeline Ready ===========\")\n    print(\"Press [Enter] to capture an image and voice caption it ...\")\n\n    # Step 1: Wait for user to initiate recording\n    input(\"Press Enter to start ...\")\n\n    IMG_PATH = \"/home/mjrovai/Documents/OLLAMA/SST/capt_image.jpg\"\n    MODEL = \"moondream:latest\"\n\n    capture_image(IMG_PATH)\n    caption = image_description(IMG_PATH, MODEL)\n    print (\"\\n==&gt; AI Response:\", caption)\n\n    text_to_speech_piper(caption)\n    play_audio()\nAnd here we can see (and listen) to the result:\n\n\n\nYour browser does not support the audio element."
  },
  {
    "objectID": "raspi/audio_pipeline/audio_pipeline.html#troubleshooting",
    "href": "raspi/audio_pipeline/audio_pipeline.html#troubleshooting",
    "title": "Audio and Vision AI Pipeline",
    "section": "Troubleshooting",
    "text": "Troubleshooting\nTo work simultaneously with STT and the camera in the same environment, we should ensure that all core scientific packages are version-aligned for the Raspberry Pi environment. To use NumPy 1.24.2 on our Raspberry Pi, we must ensure that both our SciPy and Librosa versions are compatible with that NumPy version.\nSo, to avoid an eventual numpy.exceptions import error, and other possible incompatibilities, we should downgrade scipy (and librosa) to versions that support numpy 1.24.2. According to official compatibility tables, scipy 1.11.x works with numpy 1.24.x. Librosa versions released after 0.9.0 also provide better support for recent NumPy releases. However, some older versions of Librosa are not compatible with NumPy 1.24.2 due to deprecated NumPy attributes. So, to fix it, we should run the lines below:\npip uninstall scipy librosa\npip install 'scipy&gt;=1.11,&lt;1.12' 'librosa&gt;=0.10,&lt;0.11'"
  },
  {
    "objectID": "raspi/audio_pipeline/audio_pipeline.html#conclusion",
    "href": "raspi/audio_pipeline/audio_pipeline.html#conclusion",
    "title": "Audio and Vision AI Pipeline",
    "section": "Conclusion",
    "text": "Conclusion\nThis chapter introduced us to multimodal AI system development through an audio and vision processing pipeline. We learned to integrate speech recognition, language models, and speech synthesis into a cohesive system that runs efficiently on edge hardware.\nWe explored how to architect systems with multiple AI components, handle complex error conditions, and optimize performance within resource constraints.\nThese skills are essential for the advanced topics in upcoming chapters, including RAG systems, agent architectures, and the integration of physical computing. The system thinking approach used here will be essential for future AI engineering work.\nWe should consider how the voice interaction capabilities we built might enhance other AI systems. Many applications benefit from voice interfaces, and the foundation established here can be adapted and extended for various use cases. We can, for example, transform our audio pipeline into a smart home assistant by integrating physical computing elements. Voice commands can trigger LED indicators, read sensor values, or control actuators connected to our Raspberry Pi GPIO pins. Voice queries about environmental conditions can trigger sensor readings, while voice commands can control connected devices.\nThis chapter extends our SLM and VLM work by adding input and/or output modalities beyond text and images. The same language models we used previously now process voice-derived input and generate responses for speech synthesis.\nConsider how RAG systems from later chapters might integrate with voice interactions. Voice queries could trigger document retrieval, with synthesized responses incorporating retrieved information."
  },
  {
    "objectID": "raspi/audio_pipeline/audio_pipeline.html#resources",
    "href": "raspi/audio_pipeline/audio_pipeline.html#resources",
    "title": "Audio and Vision AI Pipeline",
    "section": "Resources",
    "text": "Resources\nPython Scripts"
  },
  {
    "objectID": "raspi/physical_comp/RPi_Physical_Computing.html#introduction",
    "href": "raspi/physical_comp/RPi_Physical_Computing.html#introduction",
    "title": "Physical Computing with Raspberry Pi",
    "section": "Introduction",
    "text": "Introduction\nPhysical computing creates interactive systems that sense and respond to the analog world. While this field has traditionally focused on direct sensor readings and programmed responses, we’re entering an exciting new era where Large Language Models (LLMs) can add sophisticated decision-making and natural language interaction to physical computing projects.\nIn the Small Language Models (SLM) chapter, we learned how to run an LLM (or, more precisely, an SLM) on a Single Board Computer (SBC) such as the Raspberry Pi. In this chapter, we will go through the process of setting up a Raspberry Pi for physical computing, with an eye toward future AI integration. We’ll cover:\n\nSetting up the Raspberry Pi for physical computing\nWorking with essential sensors and actuators\nUnderstanding GPIO (General Purpose Input/Output) programming\nEstablishing a foundation for integrating LLMs with physical devices\nCreating interactive systems that can respond to both sensor data and natural language commands\n\nWe will also use a Jupyter notebook (programmed in Python) to interact with sensors and actuators—an important and necessary first step toward the goal of integrating the Raspi with an SLM.\nThe combination of Raspberry Pi’s versatility and the power of SLMs opens up exciting possibilities for creating more intelligent and responsive physical computing systems.\nThe diagram below gives us an overview of the project:"
  },
  {
    "objectID": "raspi/physical_comp/RPi_Physical_Computing.html#prerequisites",
    "href": "raspi/physical_comp/RPi_Physical_Computing.html#prerequisites",
    "title": "Physical Computing with Raspberry Pi",
    "section": "Prerequisites",
    "text": "Prerequisites\n\nRaspberry Pi (model 4 or 5)\nDHT22 Temperature and Relative Humidity Sensor\nBMP280 Barometric Pressure, Temperature and Altitude Sensor\nColored LEDs (3x)\nPush Button (1x)\nResistor 4K7 ohm (2x)\nResistor 220 or 330 ohm (3x)"
  },
  {
    "objectID": "raspi/physical_comp/RPi_Physical_Computing.html#accessing-the-gpios",
    "href": "raspi/physical_comp/RPi_Physical_Computing.html#accessing-the-gpios",
    "title": "Physical Computing with Raspberry Pi",
    "section": "Accessing the GPIOs",
    "text": "Accessing the GPIOs\nThe Raspberry Pi’s GPIO (General Purpose Input/Output) pins allow us to connect electronic components and control them with Python code. This opens up endless possibilities for creating interactive projects, home automation systems, robotics, and more.\nThis chapter covers the modern GPIO Zero library for interactions with buttons and LEDs.\n⚠️ IMPORTANT: RPi.GPIO does NOT support Raspberry Pi 5!\n\nWith the Raspberry Pi 5, we must use GPIO Zero or the newer lgpio library. RPi.GPIO only works on Pi models 1-4, the Pi Zero, Pi Zero 2, and the Pi Zero 2W.\n\nThe Raspberry Pi has 40 pins on its header, but not all of them are GPIO pins. Some provide power (3.3V and 5V), others are ground pins, and the rest are programmable GPIO pins.\n\nPin Numbering Systems\nThere are two ways to reference GPIO pins:\n\nBCM (Broadcom): Uses the GPIO number (e.g., GPIO17, GPIO27)\nBOARD: Uses the physical pin number on the header (e.g., Pin 11, Pin 13)\n\n\nIn this chapter, we’ll use BCM numbering as it’s more commonly used in Python programming.\n\n\n\n\nSafety First\nBefore connecting any components, please read these important safety guidelines:\n\nNever connect 5V directly to GPIO pins - GPIO pins are 3.3V tolerant only\nAlways use current-limiting resistors with LEDs - Without them, you risk damaging the LED or GPIO pin\nDouble-check your connections before powering on\nDisconnect power when making circuit changes\nRespect polarity - LEDs for example, have positive (long leg) and negative (short leg) sides\n\n\n\nGPIO Zero Library\nA modern, high-level library that makes GPIO programming much simpler and more intuitive. It uses object-oriented programming and includes built-in features like automatic pin cleanup, device abstraction, and event detection.\n\nIt is essential to note that the GPIO Zero Library uses Broadcom (BCM) pin numbering for GPIO pins, rather than physical (board) numbering. Any pin marked “GPIO” in the previous diagram can be used as a PIN. For example, if an LED were attached to GPIO13, we would specify the PIN as 13 rather than 33 (the physical one).\n\nIt was created by Ben Nuttall of the Raspberry Pi Foundation, Dave Jones, and other contributors (GitHub).\nAdvantages of GPIO Zero:\n\nSimpler, more readable code\nObject-oriented approach (LED, Button, etc.)\nBuilt-in device patterns and behaviors\nAutomatic cleanup on program exit\nBetter for beginners and rapid prototyping\nIncludes composite devices (robots, traffic lights, etc.)\n\nInstallation:\n\nGPIO Zero comes pre-installed on Raspberry Pi OS!"
  },
  {
    "objectID": "raspi/physical_comp/RPi_Physical_Computing.html#hello-world-blinking-an-led",
    "href": "raspi/physical_comp/RPi_Physical_Computing.html#hello-world-blinking-an-led",
    "title": "Physical Computing with Raspberry Pi",
    "section": "“Hello World”: Blinking an LED",
    "text": "“Hello World”: Blinking an LED\nLet’s start with the classic ‘Hello World’ of physical computing - making an LED blink!\nTo connect our RPi to the world, let’s first connect:\n\nPhysical Pin 6 (GND) to GND Breadboard Power Grid (Blue -), using a black jumper\nPhysical Pin 1 (3.3V) to +VCC Breadboard Power Grid (Red +), using a red jumper\n\nNow, let’s connect an LED (red) using the physical pin 33 (GPIO13) connected to the LED cathode (longer LED leg). Connect the LED anode to the breadboard GND using a 220 ohms resistor to reduce the current drawn from the Raspberry Pi, as shown below:\n\n\nUnderstanding the LED Circuit\nAn LED (Light-Emitting Diode) requires current to flow through it to produce light. However, without a resistor, too much current can flow, damaging the LED or your Raspberry Pi. We use a resistor to limit the current to a safe level.\nWhy 220Ω or 330Ω Resistors?\nThese values limit the current to approximately 10-15mA, which is safe for most standard LEDs and GPIO pins. The exact value isn’t critical—anything from 220 Ω to 1 kΩ will work fine.\n\n\nTesting with GPIO Zero\nWe can use the built-in Python interpreter to test the LED. In the terminal, enter python, and once in the interpreter, enter with the commands below:\npython\n&gt;&gt;&gt; from gpiozero import LED\n&gt;&gt;&gt; led = LED(13)\n&gt;&gt;&gt; led.on()\n&gt;&gt;&gt; led.off\n \nOn the Raspberry, start at home and go to Documents.\ncd Documents\nCreate a directory to save the scripts and install the libraries. Move to there:\nmkdir GPIO\ncd GPIO\nWe can use any text editor (such as Nano) to create and run the script. Save the file, for example, as led_test.py, and then execute it using the terminal:\npython led_test.py\nNow, let’s blink the LED (the actual “Hello world”) when talking about physical computing. To do that, we must also import another library: time. We need it to define how long the LED will be ON and OFF. In the case below, the LED will blink every 1 second.\nfrom gpiozero import LED\nfrom time import sleep\nled = LED(13)\nwhile True:\n    led.on()\n    sleep(1)\n    led.off()\n    sleep(1)\nWe can use any text editor (such as Nano) to create and run the script. Save the file, for example, as blink.py, and then execute it using the terminal:\npython blink.py\nAlternatively, we can reduce the blink code as below:\nfrom gpiozero import LED\nfrom signal import pause\nled = LED(13)\nled.blink() # 1 second by default\n# led.blink(on_time=0.5, off_time=0.5) # Fast blink\npause()\n\n\nInstalling all LEDs (the “actuators”)\nThe LEDs can be used as “actuators”; depending on the condition of a code running on our Pi, we can command one of the LEDs to fire! We will install two more LEDs, in addition to the red one already installed. Follow the diagram and install the yellow (on GPIO 19 ) and the green (on GPIO 26).\n\nFor testing we can run a similar code as the used with the single red led, changing the pin accordantly, for example.\nfrom gpiozero import LED\nfrom time import sleep\n\nledRed = LED(13)\nledYlw = LED(19)\nledGrn = LED(26)\n\nledRed.off()\nledYlw.off()\nledGrn.off()\n\nledRed.on()\nledYlw.on()\nledGrn.on()\n\nsleep(5)\n\nledRed.off()\nledYlw.off()\nledGrn.off()\n\n\n\n\n\n\nRemember that instead of LEDs, we could have relays, motors, etc."
  },
  {
    "objectID": "raspi/physical_comp/RPi_Physical_Computing.html#sensors-installation-and-setup",
    "href": "raspi/physical_comp/RPi_Physical_Computing.html#sensors-installation-and-setup",
    "title": "Physical Computing with Raspberry Pi",
    "section": "Sensors Installation and setup",
    "text": "Sensors Installation and setup\nIn this section, we will setup the Raspberry Pi to capture data from several different sensors:\nSensors and Communication type:\n\nButton (Command via a Push-Button) ==&gt; Digital direct connection\nDHT22 (Temperature and Humidity) ==&gt; Digital communication\nBMP280 (Temperature and Pressure) ==&gt; I2C Protocol\n\n\nButton\nNow, let’s learn how to read input from a button. This allows your Raspberry Pi to respond to physical interactions!\n\nUnderstanding Pull-Up and Pull-Down Resistors\nWhen a button is not pressed, the GPIO pin is ‘floating’ - it’s not connected to anything and can read random values. We use pull-up or pull-down resistors to set the pin’s default state.\n\nPull-Down: Default state is LOW (0V), becomes HIGH when button pressed\nPull-Up: Default state is HIGH (3.3V), becomes LOW when button pressed\n\n\nThe Raspberry Pi has internal pull-up and pull-down resistors!\n\n\n\n\nimg\n\n\nThe simplest way to run an external command is with a push button, and the GPIO Zero Library makes it easy to include in the project. We do not need to think about Pull-up or Pull-down resistors, etc. In terms of HW, the only thing to do is to connect one leg of our push-button to any one of the Raspi GPIOs and the other one to GND, as shown in the diagram:\n\n\nPush-Button leg1 to GPIO 20\nPush-Button leg2 to GND\n\nA simple code for reading the button can be:\nfrom gpiozero import Button\nbutton = Button(20)\nwhile True: \n    if button.is_pressed: \n        print(\"Button is pressed\") \n    else:\n        print(\"Button is not pressed\")\n\n\n\n\n\nOn a Raspberry Pi Zero 2W, for example, we could use the RPi.GPIO, whose code is:\nimport RPi.GPIO as GPIO\nimport time\n\nGPIO.setmode(GPIO.BCM)\nGPIO.setwarnings(False)\n\nBUTTON_PIN = 20\n\n# Set up with internal pull-up\nGPIO.setup(BUTTON_PIN, GPIO.IN, pull_up_down=GPIO.PUD_UP)\n\ntry:\n    while True:\n        if GPIO.input(BUTTON_PIN) == GPIO.LOW:\n            print('Button pressed!')\n        else:\n            print('Button not pressed')\n        time.sleep(0.1)\n\nexcept KeyboardInterrupt:\n    GPIO.cleanup()\n\n\n\nInstalling Adafruit CircuitPython\nThe GPIO Zero library is an excellent hardware interfacing library for Raspberry Pi. It’s great for digital in/out, analog inputs, servos, basic sensors, etc. However, it doesn’t cover SPI/I2C sensors or drivers. By using CircuitPython via adafruit_blinka, we can take advantage of all of the drivers and example code developed by Adafruit!\n\nNote that we will keep using GPIO Zero for pins, buttons, and LEDs.\n\nEnable Interfaces\nRun these commands to enable the various interfaces such as I2C and SPI:\nsudo raspi-config nonint do_i2c 0\nsudo raspi-config nonint do_spi 0\nsudo raspi-config nonint do_serial_hw 0\nsudo raspi-config nonint disable_raspi_config_at_boot 0\nInstall required dependencies\nsudo apt-get install -y python3-libgpiod i2c-tools libgpiod-dev\nInstall Blinka\nLet’s enter the Ollama environment, alheady created to install Blinka:\nsource ~/ollama/bin/activate\nInstall the library\npip install adafruit-blinka\nCheck I2C and SPI\nThe script will automatically enable I2C and SPI. You can run the following command to verify:\nls /dev/i2c* /dev/spi*\n\nVerify Blinka Version\npython3 -m pip show adafruit-blinka\n\nBlinka Installation Test\nCreate a new file called blinka_test.py with nano or your favorite text editor and put the following in:\nimport board\nimport digitalio\nimport busio\n\nprint(\"Hello, blinka!\")\n\n# Try to create a Digital input\npin = digitalio.DigitalInOut(board.D4)\nprint(\"Digital IO ok!\")\n\n# Try to create an I2C device\ni2c = busio.I2C(board.SCL, board.SDA)\nprint(\"I2C ok!\")\n\n# Try to create an SPI device\nspi = busio.SPI(board.SCLK, board.MOSI, board.MISO)\nprint(\"SPI ok!\")\n\nprint(\"done!\")\nSave it and run it at the command line:\npython blinka_test.py\n\n\n\nDHT22 - Temperature & Humidity Sensor\nThe first sensor to be installed will be the DHT22 for capturing air temperature and relative humidity data.\nOverview\nThe low-cost DHT temperature and humidity sensors are elementary and slow, but great for logging basic data. They consist of a capacitive humidity sensor and a thermistor. A bare chip inside performs the analog-to-digital conversion and outputs a digital signal containing the temperature and humidity. The digital signal is relatively easy to read using any microcontroller.\nDHT22 Main characteristics:\n\nSuitable for 0-100% humidity readings with 2-5% accuracy\nSuitable for -40 to 125°C temperature readings ±0.5°C accuracy\nNo more than 0.5 Hz sampling rate (once every 2 seconds)\nLow cost\n3 to 5V power and I/O\n2.5mA max current use during conversion (while requesting data)\nBody size 15.1mm x 25mm x 7.7mm\n4 pins with 0.1” spacing\n\nOnce we use the sensor at distances less than 20m, a 4K7 ohm resistor should be connected between the Data and VCC pins. The DHT22 output data pin will be connected to Raspberry GPIO 16. Check the electrical diagram, connecting the sensor to RPi pins as below:\n\nPin 1 - Vcc ==&gt; 3.3V\nPin 2 - Data ==&gt; GPIO 16\nPin 3 - Not Connect\nPin 4 - Gnd ==&gt; Gnd\n\n\nDo not forget to Install the 4K7 ohm resistor between the VCC and Data pins.\n\n\nOnce the sensor is connected, we must install its library on our Raspberry Pi. First, we should install the Adafruit CircuitPython library, which we have already done, and the Adafruit_CircuitPython_DHT.\npip install adafruit-circuitpython-dht\nCreate a new Python script as below and name it, for example, dht_test.py:\nimport time\nimport board\nimport adafruit_dht\ndhtDevice = adafruit_dht.DHT22(board.D16)\n\nwhile True:\n    try:\n        # Print the values to the serial port\n        temperature_c = dhtDevice.temperature\n        temperature_f = temperature_c * (9 / 5) + 32\n        humidity = dhtDevice.humidity\n        print(\n            \"Temp: {:.1f} F / {:.1f} C    Humidity: {}% \".format(\n                temperature_f, temperature_c, humidity\n            )\n        )\n        time.sleep(2.0)\n        \n    except RuntimeError as error:\n        # Errors happen fairly often, DHT's are hard to read, \n        # just keep going\n        print(error.args[0])\n        time.sleep(2.0)\n        continue\n    except Exception as error:\n        dhtDevice.exit()\n        raise error\nPlacing a finger on the sensor, we can see that both temperature and humidity begin to rise.\n\n\n\n\n\n\n\n\nInstalling the BMP280: Barometric Pressure & Altitude Sensor\nSensor Overview:\nEnvironmental sensing has become increasingly important in various industries, from weather forecasting to indoor navigation and consumer electronics. At the forefront of this technological advancement are sensors like the BMP280 and BMP180 (deprected), which excel in measuring temperature and barometric pressure with exceptional precision and reliability.\nAs its predecessor, the BMP180, the BMP280 is an absolute barometric pressure sensor, which is especially feasible for mobile applications. Its diminutive dimensions and low power consumption allow for its implementation in battery-powered devices such as mobile phones, GPS modules, or watches. The BMP280 is based on Bosch’s proven piezo-resistive pressure sensor technology featuring high accuracy and linearity as well as long-term stability and high EMC robustness. Numerous device operation options guarantee the highest flexibility. The device is optimized for power consumption, resolution, and filter performance.\nTechnical data\n\n\n\n\n\n\n\nParameter\nTechnical data\n\n\n\n\nOperation range\nPressure: 300…1100 hPa Temp.: -40…85°C\n\n\nAbsolute accuracy (950…1050 hPa, 0…+40°C)\n~ ±1 hPa\n\n\nRelative accuracy p = 700…900hPa (Temp. @ 25°C)\n± 0.12 hPa (typical) equivalent to ±1 m\n\n\nAverage typical current consumption (1 Hz dt/rate)\n3.4 μA @ 1 Hz\n\n\nAverage current consumption (1 Hz dt refresh rate)\n2.74 μA, typical (ultra-low power mode)\n\n\nAverage current consumption in sleep mode\n0.1 μA\n\n\nAverage measurement time\n5.5 msec (ultra-low power preset)\n\n\nSupply voltage VDDIO\n1.2 … 3.6 V\n\n\nSupply voltage VDD\n1.71 … 3.6 V\n\n\nResolution of data\nPressure: 0.01 hPa ( &lt; 10 cm) Temp.: 0.01° C\n\n\nTemperature coefficient offset (+25°…+40°C @ 900hPa)\n1.5 Pa/K, equiv. to 12.6 cm/K\n\n\nInterface\nI²C and SPI\n\n\n\nBMP280 Sensor Installation\nFollow the diagram and make the connections:\n\nVin ==&gt; 3.3V\nGND ==&gt; GND\nSCL ==&gt; GPIO 3\nSDA ==&gt; GPIO 2\n\n\nEnabling I2C Interface\nGo to RPi Configuration and confirm that the I2C interface is enabled. If not, enable it.\nsudo raspi-config nonint do_i2c 0\nUsing the BMP280\nIf everything has been installed and connected correctly, you can turn on your Rapspi and start interpreting the BMP180’s information about the environment.\nThe first thing to do is to check if the Raspi sees your BMP280. Try the following in a terminal:\nsudo i2cdetect -y 1\nWe should confirm that the BMP280 is on channel 77 (default) or 76.\n\n\nIn my case, the bus address is 0x76, so we should define it in the code.\n\nInstalling the BMP 280 Library:\nOnce the sensor is connected, we must install its library on our Raspi. For that, we should install the Adafruit_CircuitPython_BMP280.\npip install adafruit-circuitpython-bmp280\nCreate a new Python script as below and name it, for example, bmp280_test.py:\nimport time\nimport board\n\nimport adafruit_bmp280\n\ni2c = board.I2C()\nbmp280 = adafruit_bmp280.Adafruit_BMP280_I2C(i2c, address = 0x76)\nbmp280.sea_level_pressure = 1013.25\n\nwhile True:\n    print(\"\\nTemperature: %0.1f C\" % bmp280.temperature)\n    print(\"Pressure: %0.1f hPa\" % bmp280.pressure)\n    print(\"Altitude = %0.2f meters\" % bmp280.altitude)\n    time.sleep(2)\nExecute the script:\npython bmp280Test.py\nThe Terminal shows the result.\n\n\nNote that pressure is presented in hPa. See the next section to better understand this unit.\n\nBelow is the complete setup for our SLM tests.\n\n\n\n\n\n\n\nMeasuring Weather and Altitude With BMP280\n\nLet’s take some time to understand more about what we will get with the BMP readings.\n\nYou can skip this part of the tutorial, or return later.\n\nThe BMP280 (and its predecessor, the BMP180) was designed to measure atmospheric pressure accurately. Atmospheric pressure varies with both weather and altitude.\nWhat is Atmospheric Pressure?\nAtmospheric pressure is a force that the air around you exerts on everything. The weight of the gasses in the atmosphere creates atmospheric pressure. A standard unit of pressure is “pounds per square inch” or psi. We will use the international notation, newtons per square meter, called pascals (Pa).\n\nIf you took 1 cm wide column of air would weigh about 1 kg\n\nThis weight, pressing down on the footprint of that column, creates the atmospheric pressure that we can measure with sensors like the BMP280. Because that cm-wide column of air weighs about 1 kg, the average sea level pressure is about 101,325 pascals, or better, 1013.25 hPa (1 hPa is also known as milibar - mbar). This will drop about 4% for every 300 meters you ascend. The higher you get, the less pressure you’ll see because the column to the top of the atmosphere is much shorter and weighs less. This is useful because you can determine your altitude by measuring the pressure and doing math.\n\nThe air pressure at 3,810 meters is only half that at sea level.\n\nThe BMP280 outputs absolute pressure in hPa (mbar). One pascal is a minimal amount of pressure, approximately the amount that a sheet of paper will exert resting on a table. You will often see measurements in hectopascals (1 hPa = 100 Pa). The library here provides outputs of floating-point values in hPa, equaling one millibar (mbar).\nHere are some conversions to other pressure units:\n\n1 hPa = 100 Pa = 1 mbar = 0.001 bar\n1 hPa = 0.75006168 Torr\n1 hPa = 0.01450377 psi (pounds per square inch)\n1 hPa = 0.02953337 inHg (inches of mercury)\n1 hPa = 0.00098692 atm (standard atmospheres)\n\nTemperature Effects\nBecause temperature affects the density of a gas, density affects the mass of a gas, and mass affects the pressure (whew), atmospheric pressure will change dramatically with temperature. Pilots know this as “density altitude”, which makes it easier to take off on a cold day than a hot one because the air is denser and has a more significant aerodynamic effect. To compensate for temperature, the BMP280 includes a rather good temperature sensor and a pressure sensor.\nTo perform a pressure reading, you first take a temperature reading, then combine that with a raw pressure reading to come up with a final temperature-compensated pressure measurement. (The library makes all of this very easy.)\nMeasuring Absolute Pressure\nIf your application requires measuring absolute pressure, all you have to do is get a temperature reading, then perform a pressure reading (see the test script for details). The final pressure reading will be in hPa = mbar. You can convert this to a different unit using the above conversion factors.\n\nNote that the absolute pressure of the atmosphere will vary with both your altitude and the current weather patterns, both of which are useful things to measure.\n\nWeather Observations\nThe atmospheric pressure at any given location on Earth (or anywhere with an atmosphere) isn’t constant. The complex interaction between the earth’s spin, axis tilt, and many other factors result in moving areas of higher and lower pressure, which in turn cause the variations in weather we see every day. By watching for changes in pressure, you can predict short-term changes in the weather. For example, dropping pressure usually means wet weather or a storm is approaching (a low-pressure system is moving in). Rising pressure usually means clear weather is coming (a high-pressure system is moving through). But remember that atmospheric pressure also varies with altitude. The absolute pressure in my home, Lo Barnechea, in Chile (altitude 960m), will always be lower than that in San Francisco (less than 2 meters, almost sea level). If weather stations just reported their absolute pressure, it would be challenging to compare pressure measurements from one location to another (and large-scale weather predictions depend on measurements from as many stations as possible).\nTo solve this problem, weather stations continuously remove the effects of altitude from their reported pressure readings by mathematically adding the equivalent fixed pressure to make it appear that the reading was taken at sea level. When you do this, a higher reading in San Francisco than in Lo Barnechea will always be because of weather patterns and not because of altitude.\nSea Level Pressure Calculation\nThe See Level Pressure can be calculated with the formula:\n\nWhere,\npo = SeaLevel Pressure \np = Atmospheric Pressure \nL = Temperature Lapse Rate \nh = Altitude \nTo = Sea Level Standard Temperature \ng = Earth Surface Gravitational Acceleration \nM = Molar Mass Of Dry Air \nR = Universal Gas Constant\n\nHaving the absolute pressure in Pa, you check the sea level pressure using the Calculator.\n\nOr calculating in Python, where the altitude is the real altitude in meters where the sensor is located.\npresSeaLevel = pres / pow(1.0 - altitude/44330.0, 5.255) \nDetermining Altitude\nSince pressure varies with altitude, you can use a pressure sensor to measure altitude (with a few caveats). The average pressure of the atmosphere at sea level is 1013.25 hPa (or mbar). This drops off to zero as you climb towards the vacuum of space. Because the curve of this drop-off is well understood, you can compute the altitude difference between two pressure measurements (p and p0) by using a specific equation. The BMP280 gives the measured altitude using bmp280Sensor.altitude.\n\nThe above explanation was based on the BMP 180 Sparkfun tutorial."
  },
  {
    "objectID": "raspi/physical_comp/RPi_Physical_Computing.html#playing-with-sensors-and-actuators",
    "href": "raspi/physical_comp/RPi_Physical_Computing.html#playing-with-sensors-and-actuators",
    "title": "Physical Computing with Raspberry Pi",
    "section": "Playing with Sensors and Actuators",
    "text": "Playing with Sensors and Actuators\nIn this section, using the Jupyter Notebook, we will read sensors and act on actuators directly on the Pi.\nOn the terminal, start the Jupyter notebook server with the command (change the IP address with the one for your Raspi):\njupyter notebook --ip=192.168.4.209 --no-browser\n\n\nYou will need the Token; you can copy it from the terminal as shown above.\n\nThe Jupyter Notebook will be running as a server on:\nhttp:localhost:8888\n\nThe first time you connect, you’ll need the token that appears in the Pi terminal when you start the notebook server.\n\n\n\nWhen you start your Pi and want to use Jupyter Notebook, type the “Jupyter Notebook” command on your terminal and keep it running. This is very important! If you need to use the terminal for another task, such as running a program, open a new Terminal window.\n\nTo stop the server and close the “kernels” (the Jupyter notebooks), press [Ctrl] + [C].\n\nTesting the Notebook setup\nLet’s create a new notebook (Kernel: Python 3). Open dht_test.py, copy the code, and paste it into the notebook. That’s it. We can see the temperature and humidity values appearing on the cell. To interrupt the execution, go to the [stop] button at the top menu.\n\nOK, this means we can access the physical world from our notebook! Let’s create a more structured code for dealing with sensors and actuators.\n\n\nInitialization\nImport libraries, instantiate and initialize sensors/actuators\n# time library \nimport time\nimport datetime\n\n# Adafruit DHT library (Temperature/Humidity)\nimport board\nimport adafruit_dht\nDHT22Sensor = adafruit_dht.DHT22(board.D16)\n\n# BMP library (Pressure/Temperature)\nimport adafruit_bmp280\ni2c = board.I2C()\nbmp280Sensor = adafruit_bmp280.Adafruit_BMP280_I2C(i2c, address = 0x76)\nbmp280Sensor.sea_level_pressure = 1013.25\n\n# LEDs\nfrom gpiozero import LED\n\nledRed = LED(13)\nledYlw = LED(19)\nledGrn = LED(26)\n\nledRed.off()\nledYlw.off()\nledGrn.off()\n\n# Push-Button\nfrom gpiozero import Button\nbutton = Button(20)\n\n\nGPIO Input and Output\nCreate a function to get GPIO status:\n# Get GPIO status data \ndef getGpioStatus():\n    global timeString\n    global buttonSts\n    global ledRedSts\n    global ledYlwSts\n    global ledGrnSts\n\n    # Get time of reading\n    now = datetime.datetime.now()\n    timeString = now.strftime(\"%Y-%m-%d %H:%M\")\n    \n    # Read GPIO Status\n    buttonSts = button.is_pressed\n    ledRedSts = ledRed.is_lit\n    ledYlwSts = ledYlw.is_lit\n    ledGrnSts = ledGrn.is_lit \nAnd another to print the status:\n# Print GPIO status data \ndef PrintGpioStatus():\n    print (\"Local Station Time: \", timeString)\n    print (\"Led Red Status:     \", ledRedSts)\n    print (\"Led Yellow Status:  \", ledYlwSts)\n    print (\"Led Green Status:   \", ledGrnSts)\n    print (\"Push-Button Status: \", buttonSts)\nNow, we can, for example, turn on the LEDs:\nledRed.on()\nledYlw.on()\nledGrn.on()\n\n\n\n\n\nAnd see their status:\n\nIf you press the push-button, its status will also be shown:\n\nAnd turning off the LEDS:\nledRed.off()\nledYlw.off()\nledGrn.off()\n\nWe can create a function to simplify turning LEDs on and off:\n# Acting on GPIOs and printing Status\ndef controlLeds(r, y, g):\n    if (r):\n        ledRed.on()\n    else:\n        ledRed.off()        \n    if (y):\n        ledYlw.on()\n    else:\n        ledYlw.off() \n    if (g):\n        ledGrn.on()\n    else:\n        ledGrn.off() \n    \n    getGpioStatus()\n    PrintGpioStatus()\nFor example, turning on the Yellow LED:\n\n\n\nGetting and displaying Sensor Data\nFirst, we should create a function to read the BMP280 and calculate the pressure value at sea level, once the sensor only gives us the absolute pressure based on the actual altitude:\n# Read data from BMP280\ndef bmp280GetData(real_altitude):\n    \n    temp = bmp280Sensor.temperature\n    pres = bmp280Sensor.pressure\n    alt =  bmp280Sensor.altitude\n    presSeaLevel = pres / pow(1.0 - real_altitude/44330.0, 5.255) \n    \n    temp = round (temp, 1)\n    pres = round (pres, 2) # absolute pressure in mbar\n    alt = round (alt)\n    presSeaLevel = round (presSeaLevel, 2) # absolute pressure in mbar\n    \n    return temp, pres, alt, presSeaLevel\nEntering the BMP280 real altitude where it is located, run the code:\nbmp280GetData(960)\nAs a result, we will get (26.9, 906.73, 927, 1017.29)which means:\n\nTemperature of 26.9 oC\nAbsolute Pressure of 906.73 hPa\nMeasured Altitude (from Pressure) of 927 m\nSea Level converted Pressure: 1,017.29 hPa\n\nNow, we will generate a unique function to get the BMP280 and the DHT data, including a timestamp:\n# Get data (from local sensors)\ndef getSensorData(altReal=0):\n    global timeString\n    global humExt\n    global tempLab\n    global tempExt\n    global presSL\n    global altLab\n    global presAbs\n    global buttonSts\n    \n    # Get time of reading\n    now = datetime.datetime.now()\n    timeString = now.strftime(\"%Y-%m-%d %H:%M\")\n    \n    tempLab, presAbs, altLab, presSL = bmp280GetData(altReal) \n    \n    tempDHT =  DHT22Sensor.temperature\n    humDHT =  DHT22Sensor.humidity\n    \n    if humDHT is not None and tempDHT is not None:\n        tempExt = round (tempDHT)\n        humExt = round (humDHT)\nAnd another function to print the values:\n# Display important data on-screen\ndef printData():\n    print (\"Local Station Time:             \", timeString)\n    print (\"External Air Temperature (DHT): \", tempExt, \"oC\")\n    print (\"External Air Humidity    (DHT): \", humExt, \"%\")\n    print (\"Station Air Temperature  (BMP): \", tempLab, \"oC\")\n    print (\"Sea Level Air Pressure:         \", presSL, \"mBar\")\n    print (\"Absolute Station Air Pressure:  \", presAbs, \"mBar\")\n    print (\"Station Measured Altitude:      \", altLab, \"m\")\nRuning them:\nreal_altitude = 960 # real altitude of where the BMP280 is installed\ngetSensorData(real_altitude)\nprintData()\nResults:\n\nUsing Python, we can command the actuators (LEDs) and read the sensors and GIPOs status at this stage. This is important, for example, to generate a data log to be read by an SLM in the future.\n\nThe notebook can be found on Github: Monitoring_Actuating_GPIOs.ipynb\n\n⚠️ IMPORTANT: The Notebook Kernel should end after using to liberate GPIOs\n\nThe problem is that Jupyter Notebook is still holding onto the GPIO pins even after our code finishes running. When we try to run it from the terminal, those pins are already claimed by the Jupyter process. So, before running a code that deals with GPIO in the terminal, In Jupyter:\n\nClick Kernel → Restart Kernel\nOr: Click Kernel → Shutdown Kernel"
  },
  {
    "objectID": "raspi/physical_comp/RPi_Physical_Computing.html#widgets",
    "href": "raspi/physical_comp/RPi_Physical_Computing.html#widgets",
    "title": "Physical Computing with Raspberry Pi",
    "section": "Widgets",
    "text": "Widgets\npywidgets, or jupyter-widgets orwidgets, are interactive HTML widgets for Jupyter notebooks and the IPython kernel. Notebooks come alive when interactive widgets are used. We can gain control of our data and visualize changes in them.\nWidgets are eventful Python objects that have a representation in the browser, often as a control like a slider, text box, etc. We can use widgets to build interactive GUIs for our project.\nIn this lab, for example, we will use a slide bar to control the state of actuators in real time, such as by turning on or off the LEDs. Widgets are great for adding more dynamic behavior to Jupyter Notebooks.\nInstallation\nTo use Widgets, we must install the Ipywidgets library using the commands:\npip install ipywidgets\nAfter installation, we should call the library:\n# widget library\nfrom ipywidgets import interactive\nimport ipywidgets as widgetsfrom \nIPython.display import display\nAnd running the below line, we can control the LEDs in real-time:\nf = interactive(controlLeds, r=(0,1,1), y=(0,1,1), g=(0,1,1))\ndisplay(f)\n\n\nThis interactive widget is very easy to implement and very powerful. You can learn more about Interactive on this link: Interactive Widget."
  },
  {
    "objectID": "raspi/physical_comp/RPi_Physical_Computing.html#advanced-gpio-zero-features",
    "href": "raspi/physical_comp/RPi_Physical_Computing.html#advanced-gpio-zero-features",
    "title": "Physical Computing with Raspberry Pi",
    "section": "Advanced GPIO Zero Features",
    "text": "Advanced GPIO Zero Features\nGPIO Zero includes many advanced features that make complex projects much easier to build.\n\nPWM LED Brightness Control\nUse PWMLED to control LED brightness:\nfrom gpiozero import PWMLED\nfrom time import sleep\n\nled = PWMLED(13)\n\n# Fade in and out\nwhile True:\n    led.pulse() # Smooth fade in and out\n  sleep(5)\n\n\nComposite Devices\nCreate a traffic light system with one object:\nfrom gpiozero import TrafficLights\nfrom time import sleep\n\nlights = TrafficLights(red=13, amber=19, green=26)\n\nwhile True:\n  lights.red.on()\n  sleep(2)\n  lights.amber.on()\n  sleep(1)\n  lights.red.off()\n  lights.amber.off()\n  lights.green.on()\n  sleep(3)\n  lights.green.off()\n  lights.amber.on()\n  sleep(1)\n  lights.amber.off()\n\n\nOther Useful GPIO Zero Devices\n\nBuzzer: from gpiozero import Buzzer\nMotor: from gpiozero import Motor\nServo: from gpiozero import Servo\nDistanceSensor: from gpiozero import DistanceSensor\nMotionSensor: from gpiozero import MotionSensor\nRobot: from gpiozero import Robot"
  },
  {
    "objectID": "raspi/physical_comp/RPi_Physical_Computing.html#interacting-an-slm-with-the-physical-world",
    "href": "raspi/physical_comp/RPi_Physical_Computing.html#interacting-an-slm-with-the-physical-world",
    "title": "Physical Computing with Raspberry Pi",
    "section": "Interacting an SLM with the Physical world",
    "text": "Interacting an SLM with the Physical world\nThis section demonstrates in a simple way how to integrate a Small Language Model (SLM) with the sensors and LEDs we have set up. The diagram below shows how data flows from sensors through processing and AI analysis to control the actuators and ultimately provide user feedback.\n\n\n\n\n\nWe will use the Transformers library from Hugging Face for model loading and inference. This library provides the architecture for working with pre-trained language models, helping interact with the model, processing input prompts, and obtaining outputs.\nInstallation\npip install transformers torch\nLet’s create a simple SLM test in the Jupyter Notebook that checks if the model loads and measures inference time. The model used here is the TinyLLama 1.1B. We will ask a straightforward question:\n\"The weather today is\"\nAs a result, besides the SLM answer, we will also measure the latency.\nRun this script:\nimport time\nfrom transformers import pipeline\nimport torch\n\n# Check if CUDA is available (it won't be on our case, Raspberry Pi)\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(f\"Using device: {device}\")\n\n# Load the model and measure loading time\nstart_time = time.time()\n\nmodel='TinyLlama/TinyLlama-1.1B-intermediate-step-1431k-3T'\ngenerator = pipeline('text-generation', \n                    model=model,\n                    device=device)\nload_time = time.time() - start_time\nprint(f\"Model loading time: {load_time:.2f} seconds\")\n\n# Test prompt\ntest_prompt = \"The weather today is\"\n\n# Measure inference time\nstart_time = time.time()\nresponse = generator(test_prompt, \n                    max_length=50,\n                    num_return_sequences=1,\n                    temperature=0.7)\ninference_time = time.time() - start_time\n\nprint(f\"\\nTest prompt: {test_prompt}\")\nprint(f\"Generated response: {response[0]['generated_text']}\")\nprint(f\"Inference time: {inference_time:.2f} seconds\")\nAs we can see, the SLM works, but the latency is very high (+3 minutes). It is OK because this particular test is on a Raspberry Pi 4. With a Raspberry Pi 5, the result would be better.:\n\nThe Raspi uses around 1GB of memory (model + process) and all four cores to process the answer. The model alone needs around 800MB.\n\nNow, let us create a code showing a basic interaction pattern where the SLM can respond to sensor data and interact with the LEDs.\nInstall the Libraries:\nimport time\nimport datetime\nimport board\nimport adafruit_dht\nimport adafruit_bmp280\nfrom gpiozero import LED, Button\nfrom transformers import pipeline\nInitialize sensors\nDHT22Sensor = adafruit_dht.DHT22(board.D16)\ni2c = board.I2C()\nbmp280Sensor = adafruit_bmp280.Adafruit_BMP280_I2C(i2c, address=0x76)\nbmp280Sensor.sea_level_pressure = 1013.25\nInitialize LEDs and Button\nledRed = LED(13)\nledYlw = LED(19)\nledGrn = LED(26)\nbutton = Button(20)\nInitialize the SLM pipeline\n# We're using a small model suitable for Raspberry Pi\n\nmodel='TinyLlama/TinyLlama-1.1B-intermediate-step-1431k-3T'\ngenerator = pipeline('text-generation', \n                    model=model,\n                    device='cpu')\nSupport Functions\nNow, let’s create support functions for readings from all sensors and control the LEDs:\ndef get_sensor_data():\n    \"\"\"Get current readings from all sensors\"\"\"\n    try:\n        temp_dht = DHT22Sensor.temperature\n        humidity = DHT22Sensor.humidity\n        temp_bmp = bmp280Sensor.temperature\n        pressure = bmp280Sensor.pressure\n        \n        return {\n            'temperature_dht': round(temp_dht, 1) if temp_dht else None,\n            'humidity': round(humidity, 1) if humidity else None,\n            'temperature_bmp': round(temp_bmp, 1),\n            'pressure': round(pressure, 1)\n        }\n    except RuntimeError:\n        return None\n\n\ndef control_leds(red=False, yellow=False, green=False):\n    \"\"\"Control LED states\"\"\"\n    ledRed.value = red\n    ledYlw.value = yellow\n    ledGrn.value = green\n    \n\ndef process_conditions(sensor_data):\n    \"\"\"Process sensor data and control LEDs based on conditions\"\"\"\n    if not sensor_data:\n        control_leds(red=True)  # Error condition\n        return\n    \n    temp = sensor_data['temperature_dht']\n    humidity = sensor_data['humidity']\n    \n    # Example conditions for LED control\n    if temp &gt; 30:  # Hot\n        control_leds(red=True)\n    elif humidity &gt; 70:  # Humid\n        control_leds(yellow=True)\n    else:  # Normal conditions\n        control_leds(green=True)\nGenerating an SLM’s response\nSo far, the LEDs reaction is only based on logic, but let’s also use the SLM to “analyse” the sensors condition, generating a response based on that:\ndef generate_response(sensor_data):\n    \"\"\"Generate response based on sensor data using SLM\"\"\"\n    if not sensor_data:\n        return \"Unable to read sensor data\"\n    \n    prompt = f\"\"\"Based on these sensor readings:\n    Temperature: {sensor_data['temperature_dht']}°C\n    Humidity: {sensor_data['humidity']}%\n    Pressure: {sensor_data['pressure']} hPa\n    \n    Provide a brief status and recommendation in 2 sentences.\n    \"\"\"\n    \n    # Generate response from SLM\n    response = generator(prompt, \n                       max_length=100,\n                       num_return_sequences=1,\n                       temperature=0.7)[0]['generated_text']\n    \n    return response\nMain Function\nAnd now, let’s create a main() function to wait for the user to, for example, press a button and, capture the data generated by the sensors, delivering some observation or recommendation from the SLM:\ndef main_loop():\n    \"\"\"Main program loop\"\"\"\n    print(\"Starting Physical Computing with SLM Integration...\")\n    print(\"Press the button to get a reading and SLM response.\")\n    \n    try:\n        while True:\n            if button.is_pressed:\n                # Get sensor readings\n                sensor_data = get_sensor_data()\n                \n                # Process conditions and control LEDs\n                process_conditions(sensor_data)\n                \n                if sensor_data:\n                    # Get SLM response\n                    response = generate_response(sensor_data)\n                    \n                    # Print current status\n                    print(\"\\nCurrent Readings:\")\n                    print(f\"Temperature: {sensor_data['temperature_dht']}°C\")\n                    print(f\"Humidity: {sensor_data['humidity']}%\")\n                    print(f\"Pressure: {sensor_data['pressure']} hPa\")\n                    print(\"\\nSLM Response:\")\n                    print(response)\n                    \n                time.sleep(2)  # Debounce and allow time to read\n            \n            time.sleep(0.1)  # Reduce CPU usage\n            \n    except KeyboardInterrupt:\n        print(\"\\nShutting down...\")\n        control_leds(False, False, False)  # Turn off all LEDs\nTest Result\nThe sensors are read after the user presses the button to trigger a reading, and LEDs are controlled based on conditions. Sensor data is formatted into a prompt for the SLM to generate a response analyzing the current conditions. The results are displayed in the terminal, and the LED indicators are shown.\n\nRed: High temperature (&gt;30°C) or error condition\nYellow: High humidity (&gt;70%)\nGreen: Normal conditions\n\nThis simple code integrates a Small Language Model (TinyLlama model (1.1B parameters) with our physical computing setup, providing raw sensor data and intelligent responses from the SLM about the environmental conditions.\n\nWe can extend this first test to more sophisticated and valuable uses of the SLM integration, for example: adding:\n\nStarting the process from a User Prompt.\nReceive commands from the User to switch LEDs ON or OFF\nProvide the status of LEDS, Button, or specific sensor data from the user prompt\nLog data and responses to a file. Provide historical information by user request\nImplement different types of prompts for various use cases"
  },
  {
    "objectID": "raspi/physical_comp/RPi_Physical_Computing.html#other-models",
    "href": "raspi/physical_comp/RPi_Physical_Computing.html#other-models",
    "title": "Physical Computing with Raspberry Pi",
    "section": "Other Models",
    "text": "Other Models\nWe can use other SLMs in a Raspberry Pi that have distinct ways of handling them. For example, many modern models use GGUF formats, and to use them, we need to install llama-cpp-python, which is designed to work with GGUF models.\nAlso, as we saw in a previous lab, Ollama is a great way to download and test SLMs on the Raspberry Pi."
  },
  {
    "objectID": "raspi/physical_comp/RPi_Physical_Computing.html#conclusion",
    "href": "raspi/physical_comp/RPi_Physical_Computing.html#conclusion",
    "title": "Physical Computing with Raspberry Pi",
    "section": "Conclusion",
    "text": "Conclusion\n\nKey Achievements\nThroughout this tutorial, we’ve successfully: - Set up a complete physical computing environment using Raspberry Pi - Integrated multiple environmental sensors (DHT22 and BMP280) - Implemented visual feedback through LED actuators - Created interactive controls using push buttons - Integrated a Small Language Model (TinyLLama 1.1B) for intelligent analysis - Developed a foundation for AI-enhanced environmental monitoring\n\n\nTechnical Insights\n\nHardware Integration\nThe combination of digital (DHT22) and I2C (BMP280) sensors demonstrated different communication protocols and their implementations. This multi-sensor approach provides redundancy and comprehensive environmental monitoring capabilities. The LED actuators and push-button interface created a responsive and interactive system that bridges the digital and physical worlds.\n\n\nSoftware Architecture\nThe layered software architecture we developed supports: 1. Low-level sensor communication and actuator control 2. Data preprocessing and validation 3. SLM integration for intelligent analysis 4. Interactive user interfaces through both hardware and software\n\n\nAI Integration Learnings\nThe integration of TinyLLama 1.1B revealed several important insights: - Small Language Models can effectively run on edge devices like Raspberry Pi - Natural language processing can enhance sensor data interpretation - Real-time analysis is possible, though with some latency considerations - The system can provide human-readable insights from complex sensor data\n\n\n\nPractical Applications\nThis project serves as a foundation for numerous real-world applications: - Environmental monitoring systems - Smart home automation - Industrial sensor networks - Educational platforms for IoT and AI integration - Prototyping platforms for larger-scale deployments\n\n\nChallenges and Solutions\nThroughout the development, we encountered and addressed several challenges: 1. Resource Constraints: - Optimized SLM inference for Raspberry Pi capabilities - Implemented efficient sensor reading strategies - Managed memory usage for stable operation\n\nData Integration:\n\nDeveloped robust sensor data validation\nCreated effective data preprocessing pipelines\nImplemented error handling for sensor failures\n\nAI Integration:\n\nDesigned effective prompting strategies\nManaged inference latency\nBalanced accuracy with response time\n\n\n\n\nFuture Enhancements\nThe system can be extended in several directions: 1. Hardware Expansions: - Additional sensor types (air quality, light, motion) - Camera for IA applications - More complex actuators (displays, motors, relays) - Wireless connectivity options as WiFI, BLE, or LoRa 2. Software Improvements: - Advanced data logging and analysis - Web-based monitoring interface - Real-time visualization tools 3. AI Capabilities: 1. Models for detecting and counting objects 2. RAG or Fine-tuning SLM for specific applications 3. Multi-modal AI integration via sensor integration 4. Automated decision-making systems 5. Predictive maintenance capabilities\n\n\nFinal Thoughts\nThis chapter demonstrates that integrating physical computing with AI is feasible and practical on readily accessible hardware such as the Raspberry Pi. Combining sensors, actuators, and AI creates a powerful platform for developing intelligent environmental monitoring and control systems.\nWhile the current implementation focuses on environmental monitoring, the principles and techniques can be adapted to various applications. The modular nature of hardware and software components allows for customization and expansion based on specific needs.\nIntegrating small language models into physical computing opens new possibilities for creating more intuitive and intelligent IoT devices. As edge AI capabilities evolve, projects like this will become increasingly important in developing the next generation of smart devices and systems.\nRemember that this is just the beginning. Our foundation can be extended in countless ways to create more sophisticated and capable systems. The key is to build on these basics while balancing functionality, reliability, and resource usage."
  },
  {
    "objectID": "raspi/physical_comp/RPi_Physical_Computing.html#resources",
    "href": "raspi/physical_comp/RPi_Physical_Computing.html#resources",
    "title": "Physical Computing with Raspberry Pi",
    "section": "Resources",
    "text": "Resources\n\nGPIOs - Scripts\nSensors - Scripts\nNotebooks"
  },
  {
    "objectID": "raspi/iot/slm_iot.html",
    "href": "raspi/iot/slm_iot.html",
    "title": "Experimenting with SLMs for IoT Control",
    "section": "",
    "text": "Conclusion\nThis chapter has demonstrated the progressive evolution of an IoT system from basic sensor integration to an intelligent, interactive platform powered by Small Language Models. Through our journey, we’ve explored several key aspects of combining edge AI with physical computing:"
  },
  {
    "objectID": "raspi/iot/slm_iot.html#introduction",
    "href": "raspi/iot/slm_iot.html#introduction",
    "title": "Experimenting with SLMs for IoT Control",
    "section": "Introduction",
    "text": "Introduction\nThis chapter explores the implementation of Small Language Models (SLMs) in IoT control systems, demonstrating the possibility of creating a monitoring and control system using edge AI. We’ll integrate these models with physical sensors and actuators, creating an intelligent IoT system capable of natural language interaction. While this implementation shows the potential of integrating AI with physical systems, it also highlights current limitations and areas for improvement.\n\nThis chapter builds on the concepts introduced in “Small Language Models (SLMs)” and “Physical Computing with Raspberry Pi.”\n\nThe Physical Computing chapter laid the groundwork for interfacing with hardware components using the Raspberry Pi’s GPIO pins. We’ll revisit these concepts, focusing on connecting and interacting with sensors (DHT22 for temperature and humidity, BMP280 for temperature and pressure, and a push-button for digital inputs), as well as controlling actuators (LEDs) in a more sophisticated setup.\nWe will progress from a simple IoT system to a more advanced platform that combines real-time monitoring, historical data analysis, and natural language processing (NLP).\n\n\n\n\n\nThis chapter demonstrates a progressive evolution through several key stages:\n\nBasic Sensor Integration\n\nHardware interface with DHT22 (temperature/humidity) and BMP280 (temperature/pressure) sensors\nDigital input through a push-button\nOutput control via RGB LEDs\nFoundational data collection and device control\n\nSLM Basic Analysis\n\nInitial integration with small language models\nSimple observation and reporting of system state\nDemonstration of SLM’s ability to interpret sensor data\n\nActive Control Implementation\n\nDirect LED control based on SLM decisions\nTemperature threshold monitoring\nEmergency state detection via button input\nReal-time system state analysis\n\nNatural Language Interaction\n\nFree-form command interpretation\nContext-aware responses\nMultiple SLM model support\nFlexible query handling\n\nData Logging and Analysis\n\nContinuous system state recording\nTrend analysis and pattern detection\nHistorical data querying\nPerformance monitoring\n\n\nLet’s begin by setting up our hardware and software environment, building upon the foundation established in our previous labs."
  },
  {
    "objectID": "raspi/iot/slm_iot.html#setup",
    "href": "raspi/iot/slm_iot.html#setup",
    "title": "Experimenting with SLMs for IoT Control",
    "section": "Setup",
    "text": "Setup\n\nHardware Setup\n\nConnection Diagram\n\n\n\nComponent\nGPIO Pin\n\n\n\n\nDHT22\nGPIO16\n\n\nBMP280 - SCL\nGPIO03\n\n\nBMP280 - SDA\nGPIO02\n\n\nRed LED\nGPIO13\n\n\nYellow LED\nGPIO19\n\n\nGreen LED\nGPIO26\n\n\nButton\nGPIO20\n\n\n\n\n\nRaspberry Pi 5 (with an OS installed, as detailed in previous labs)\nDHT22 temperature and humidity sensor\nBMP280 temperature and pressure sensor\n3 LEDs (red, yellow, green)\nPush button\n330Ω resistors (3)\nJumper wires and breadboard\n\n\n\n\nSoftware Prerequisites\n\nInstall required libraries:\n\npip install adafruit-circuitpython-dht \npip install adafruit-circuitpython-bmp280"
  },
  {
    "objectID": "raspi/iot/slm_iot.html#basic-sensor-integration",
    "href": "raspi/iot/slm_iot.html#basic-sensor-integration",
    "title": "Experimenting with SLMs for IoT Control",
    "section": "Basic Sensor Integration",
    "text": "Basic Sensor Integration\nLet’s create a Python script (monitor.py) to handle the sensors and actuators. This script will contain functions to be called from other scripts later:\nimport time\nimport board\nimport adafruit_dht\nimport adafruit_bmp280\nfrom gpiozero import LED, Button\n\nDHT22Sensor = adafruit_dht.DHT22(board.D16)\ni2c = board.I2C()\nbmp280Sensor = adafruit_bmp280.Adafruit_BMP280_I2C(i2c, address=0x76)\nbmp280Sensor.sea_level_pressure = 1013.25\n\nledRed = LED(13)\nledYlw = LED(19)\nledGrn = LED(26)\nbutton = Button(20)\n\ndef collect_data():\n    try:\n        temperature_dht = DHT22Sensor.temperature\n        humidity = DHT22Sensor.humidity\n        temperature_bmp = bmp280Sensor.temperature\n        pressure = bmp280Sensor.pressure\n        button_pressed = button.is_pressed\n        return temperature_dht, humidity, temperature_bmp, pressure, button_pressed\n    except RuntimeError:\n        return None, None, None, None, None\n\ndef led_status():\n    ledRedSts = ledRed.is_lit\n    ledYlwSts = ledYlw.is_lit\n    ledGrnSts = ledGrn.is_lit \n    return ledRedSts, ledYlwSts, ledGrnSts\n\n\ndef control_leds(red, yellow, green):\n    ledRed.on() if red else ledRed.off()\n    ledYlw.on() if yellow else ledYlw.off()\n    ledGrn.on() if green else ledGrn.off()\nWe can test the functions using:\nwhile True:\n    ledRedSts, ledYlwSts, ledGrnSts  = led_status()\n    temp_dht, hum, temp_bmp, press, button_state  = collect_data()\n\n    #control_leds(True, True, True)\n\n    if all(v is not None for v in [temp_dht, hum, temp_bmp, press]):\n        print(f\"DHT22 Temp: {temp_dht:.1f}°C, Humidity: {hum:.1f}%\")\n        print(f\"BMP280 Temp: {temp_bmp:.1f}°C, Pressure: {press:.2f}hPa\")\n        print(f\"Button {'pressed' if button_state else 'not pressed'}\")\n        print(f\"Red LED {'is on' if ledRedSts else 'is off'}\")\n        print(f\"Yellow LED {'is on' if ledYlwSts else 'is off'}\")\n        print(f\"Green LED {'is on' if ledGrnSts else 'is off'}\")\n\n\n    time.sleep(2)"
  },
  {
    "objectID": "raspi/iot/slm_iot.html#slm-basic-analysis",
    "href": "raspi/iot/slm_iot.html#slm-basic-analysis",
    "title": "Experimenting with SLMs for IoT Control",
    "section": "SLM Basic Analysis",
    "text": "SLM Basic Analysis\nNow, let’s create a new script, slm_basic_analysis.py, which will be responsible for analysing the hardware components’ status, according to the following diagram:\n\n\n\n\n\nThe diagram shows the basic analysis system, which consists of:\n\nHardware Layer:\n\nSensors: DHT22 (temperature/humidity), BMP280 (temperature/pressure)\nInput: Emergency button\nOutput: Three LEDs (Red, Yellow, Green)\n\nmonitor.py:\n\nHandles all hardware interactions\nProvides two main functions:\n\ncollect_data(): Reads all sensor values\nled_status(): Checks current LED states\n\n\nslm_basic_analysis.py:\n\nCreates a descriptive prompt using sensor data\nSends prompt to SLM (for example, the Llama 3.2 1B)\nDisplays analysis results\nIn this step we will not control the LEDs (observation only)\n\n\nOkay, let’s implement the code, starting for importing the Ollama library and the functions to monitor the HW (from the previous script):\nimport ollama\nfrom monitor import collect_data, led_status\nCalling the monitor functions, we will get all data:\nledRedSts, ledYlwSts, ledGrnSts  = led_status()\ntemp_dht, hum, temp_bmp, press, button_state  = collect_data()\nNow, the heart of out code, we will generate the Prompt, using the data captured on the previous variables:\nprompt = f\"\"\"\n        You are an experienced environmental scientist. \n        Analyze the information received from an IoT system:\n\n        DHT22 Temp: {temp_dht:.1f}°C and Humidity: {hum:.1f}%\n        BMP280 Temp: {temp_bmp:.1f}°C and Pressure: {press:.2f}hPa\n        Button {\"pressed\" if button_state else \"not pressed\"}\n        Red LED {\"is on\" if ledRedSts else \"is off\"}\n        Yellow LED {\"is on\" if ledYlwSts else \"is off\"}\n        Green LED {\"is on\" if ledGrnSts else \"is off\"}\n\n        Where,\n        - The button, not pressed, shows a normal operation\n        - The button, when pressed, shows an emergency\n        - Red LED when is on, indicates a problem/emergency.\n        - Yellow LED when is on indicates a warning situation.\n        - Green LED when is on, indicates system is OK.\n\n        If the temperature is over 20°C, mean a warning situation\n\n        You should answer only with: \"Activate Red LED\" or \n        \"Activate Yellow LED\" or \"Activate Green LED\"\n\n\"\"\"\nNow, the Prompt will be passed to the SLM, which will generate a response:\nMODEL = 'llama3.2:3b'\nPROMPT = prompt\nresponse = ollama.generate(\n    model=MODEL, \n    prompt=PROMPT\n    )\nThe last stage will be show the real monitored data and the SLM’s response:\nprint(f\"\\nSmart IoT Analyser using {MODEL} model\\n\")\n\nprint(f\"SYSTEM REAL DATA\")\nprint(f\" - DHT22 ==&gt; Temp: {temp_dht:.1f}°C, Humidity: {hum:.1f}%\")\nprint(f\" - BMP280 =&gt; Temp: {temp_bmp:.1f}°C, Pressure: {press:.2f}hPa\")\nprint(f\" - Button {'pressed' if button_state else 'not pressed'}\")\nprint(f\" - Red LED {'is on' if ledRedSts else 'is off'}\")\nprint(f\" - Yellow LED {'is on' if ledYlwSts else 'is off'}\")\nprint(f\" - Green LED {'is on' if ledGrnSts else 'is off'}\")\n\nprint(f\"\\n&gt;&gt; {MODEL} Response: {response['response']}\")\nRuning the Python script, we got:\n\nIn this initial experiment, the system successfully collected sensor data (temperatures of 26.3°C and 26.1°C from DHT22 and BMP280, respectively, 40.2% humidity, and 908.84hPa pressure) and processed this information through the SLM, which produced a coherent response recommending the activation of the yellow LED due to elevated temperature conditions.\nThe model’s ability to interpret sensor data and provide logical, rule-based decisions shows promise. Still, the simplistic nature of the current implementation (using basic thresholds and binary LED outputs) suggests significant room for improvement through more sophisticated prompting strategies, historical data integration, and the implementation of safety mechanisms. Also, the result is probabilistic, meaning it should change after execution.\n\nAct on Output (Actuators)\nLet’s use the output generated and use it to actuate on the LEDs, our “actuators”:\n\n\n\n\n\nWe can add a new function, parse_llm_response(), to return a command to the the LEDs based on the SLM’s response:\ndef parse_llm_response(response_text):\n    \"\"\"Parse the LLM response to extract LED control instructions.\"\"\"\n    response_lower = response_text.lower()\n    red_led = 'activate red led' in response_lower\n    yellow_led = 'activate yellow led' in response_lower\n    green_led = 'activate green led' in response_lower\n    return (red_led, yellow_led, green_led)\nThe return to this function:\nred, yellow, green = parse_llm_response(response['response'])\nWould be:(False, True, False), which can be the input to the function control_leds(red, yellow, green)'\ncontrol_leds(red, yellow, green)\nled_status()\n(False, True, False)\n\n\n\n\n\n\n\nCreating new functions for the Actuation:\nOne for the inference:\ndef slm_inference(PROMPT, MODEL):\n    response = ollama.generate(\n        model=MODEL, \n        prompt=PROMPT\n        )\n    return response\nAnd another for output analysis and actuation:\ndef output_actuator(response, MODEL):\n    print(f\"\\nSmart IoT Actuator using {MODEL} model\\n\")\n    \n    print(f\"SYSTEM REAL DATA\")\n    print(f\" - DHT22 ==&gt; Temp: {temp_dht:.1f}°C, Humidity: {hum:.1f}%\")\n    print(f\" - BMP280 =&gt; Temp: {temp_bmp:.1f}°C, Pressure: {press:.2f}hPa\")\n    print(f\" - Button {'pressed' if button_state else 'not pressed'}\")\n    \n    print(f\"\\n&gt;&gt; {MODEL} Response: {response['response']}\")\n    \n    # Control LEDs based on response\n    red, yellow, green = parse_llm_response(response['response'])\n    control_leds(red, yellow, green)\n    \n    print(f\"\\nSYSTEM ACTUATOR STATUS\")\n    ledRedSts, ledYlwSts, ledGrnSts  = led_status()\n    print(f\" - Red LED {'is on' if ledRedSts else 'is off'}\")\n    print(f\" - Yellow LED {'is on' if ledYlwSts else 'is off'}\")\n    print(f\" - Green LED {'is on' if ledGrnSts else 'is off'}\")\nBy updating the system and calling the two functions in sequence, we can have the full cycle of analysis by the SLM and the actuation on the output LEDs:\nledRedSts, ledYlwSts, ledGrnSts  = led_status()\ntemp_dht, hum, temp_bmp, press, button_state  = collect_data()\n\nresponse = slm_inference(PROMPT, MODEL)\noutput_actuator(response, MODEL)\n\nThe new script can be found on GitHub: slm_basic_act_leds.\n\nLet’s press the button, call the functions, and see what happens:\nledRedSts, ledYlwSts, ledGrnSts  = led_status()\ntemp_dht, hum, temp_bmp, press, button_state  = collect_data()\nresponse = slm_inference(PROMPT, MODEL)\noutput_actuator(response, MODEL)\n\nWe can see that, despite the button being pressed, the SLM did not consider it and misinterpreted the temperature value as ABOVE the threshold, not below. Also, despite the fact that we asked for a straight answer about which LED to turn on, the model lost time with analysis,\nThis issue relates to the prompt we wrote. Let’s cover it in the next section."
  },
  {
    "objectID": "raspi/iot/slm_iot.html#prompting-engineering",
    "href": "raspi/iot/slm_iot.html#prompting-engineering",
    "title": "Experimenting with SLMs for IoT Control",
    "section": "Prompting Engineering",
    "text": "Prompting Engineering\n\n\n\n\n\nLooking at the answers we got with the previous implementation, which are not always correct, we can see that the main issue is unreliable text parsing. Using JSON to parse the answer should be much better!\n\nKey Changes in the code:\n\nAdded JSON import\nFixed variable scope bug - The previous code tried to use temp_dht, hum, etc. in the prompt before they were defined. The prompt is now created in a function after receiving sensor data.\nChanged to JSON format - The prompt now asks for a structured JSON response:\n{\"red_led\": true, \"yellow_led\": false, \"green_led\": false}\nUpdated parser - Now parses JSON instead of searching for text strings. Includes error handling and fallback to safe state (all LEDs off) if parsing fails.\n\n\nWhy JSON is Better:\n\nMore reliable: No ambiguity about which LEDs to activate\nStructured: Clear true/false values instead of parsing text\nError-resistant: The parser handles markdown code blocks (some models wrap JSON in ```) and provides safe fallback\nFlexible: Easy to add more fields later if needed\n\n\nLet’s revise the previous code, with a better prompt and parcing funcion, now based on JSON.\n\n\nSystem Overview: Enhanced IoT Environmental Monitoring with SLM Control\nThis project demonstrates how a Small Language Model (SLM) can make intelligent decisions in an IoT system. The system monitors environmental conditions using sensors and controls LED indicators based on the SLM’s analysis.\n\nNew System Architecture\n\n\n\n\n\nThe system is organized into three main layers:\n\n\n1. Hardware Components Layer\nThe physical layer consists of:\n\nDHT22 Sensor: Measures temperature and humidity\nBMP280 Sensor: Measures temperature and atmospheric pressure\nEmergency Button: Manual override for emergency situations\nThree LEDs: Visual indicators\n\nRed LED: Emergency/Problem state\nYellow LED: Warning state\nGreen LED: Normal operation\n\n\n\n\n2. Hardware Interface Layer (monitor.py)\nThis layer provides the bridge between hardware and software with three key functions:\n\ncollect_data(): Reads all sensor values (temperature, humidity, pressure) and button state\nled_status(): Checks the current state of all LEDs\ncontrol_leds(red, yellow, green): Controls which LED is turned on based on boolean values\n\n\n\n3. Intelligence Layer (slm_act_leds.py)\nThis is where the SLM makes decisions. The workflow follows these steps:\na) Data Collection & Preparation\n\nslm_analyse_act(MODEL, TEMP_THRESHOLD): Main function that orchestrates the entire process\n\nCalls collect_data() to get current sensor readings\nCalls led_status() to check LED states\n\n\nb) Prompt Generation\n\ncreate_prompt(): Builds a detailed prompt for the SLM, including:\n\nCurrent sensor readings\nTemperature threshold value\nDecision rules with clear priorities:\n\nButton pressed → Red LED (Emergency - highest priority)\nTemperature exceeds threshold → Yellow LED (Warning)\nNormal conditions → Green LED (All OK)\n\nPre-analyzed data to help the SLM understand the current state\n\n\nc) SLM Inference\n\nslm_inference(PROMPT, MODEL): Sends the prompt to the Ollama SLM (llama3.2:3b or any other SLM, as Gemma or Phi)\nThe SLM analyzes the situation and returns a JSON response:\n\n{\n  \"red_led\": false,\n  \"yellow_led\": false,\n  \"green_led\": true\n}\nd) Response Processing\n\nparse_llm_response(): Parses the JSON response from the SLM\n\nHandles edge cases like markdown code blocks\nExtracts boolean values for each LED\nReturns a tuple: (red, yellow, green)\n\n\ne) Actuation\n\noutput_actuator(): Takes the SLM’s decision and executes it\n\nDisplays all sensor data\nShows the SLM’s JSON response\nShows the final decision\nCalls control_leds() to physically turn LEDs on/off\nDisplays final LED status for verification\n\n\n\n\nHow It Works: A Complete Cycle\n\nSensors continuously monitor the environment (temperature, humidity, pressure, button state)\nData flows from hardware through monitor.py to slm_act_leds.py\nThe SLM receives a structured prompt with current conditions and decision rules\nThe SLM analyzes the data and decides which LED should be activated\nThe decision returns as a JSON object specifying exactly which LED to turn on\nThe system executes the decision by controlling the LEDs through monitor.py\nVisual feedback is provided via the LED, and the system logs all data and decisions\n\n\n\nKey Design Principles\nJSON Communication: Using JSON format ensures reliable, structured communication between the SLM and the actuation system, reducing parsing errors.\nSLM-Driven Decisions: The SLM has complete control over LED decisions, demonstrating how AI can autonomously manage IoT systems.\nConfigurable Threshold: The temperature threshold is a parameter that makes the system adaptable to different environments without code changes. Can be updated by the user.\nClear Priority Rules: The system follows explicit priority rules that the SLM understands:\n\nSafety first (button press = emergency)\nThen environmental warnings (temperature)\nThen normal operation\n\nNow, this architecture showcases how Small Language Models can be integrated into IoT systems to provide intelligent, context-aware decision-making while maintaining simplicity and reliability.\n\n\n\nThe new code\nimport ollama\nimport json\nfrom monitor import collect_data, led_status, control_leds\ndef create_prompt(temp_dht, hum, temp_bmp, press, button_state, \n                  ledRedSts, ledYlwSts, ledGrnSts, TEMP_THRESHOLD):\n    \"\"\"Create a prompt for the LLM with current sensor data.\"\"\"\n    return f\"\"\"\n        You are controlling an IoT LED system. Analyze the sensor data and decide \n        which ONE LED to activate.\n        \n        SENSOR DATA:\n        - DHT22 Temperature: {temp_dht:.1f}°C\n        - BMP280 Temperature: {temp_bmp:.1f}°C\n        - Humidity: {hum:.1f}%\n        - Pressure: {press:.2f}hPa\n        - Button: {\"PRESSED\" if button_state else \"NOT PRESSED\"}\n        \n        TEMPERATURE THRESHOLD: {TEMP_THRESHOLD}°C\n        \n        DECISION RULES (apply in this priority order):\n        1. IF button is PRESSED → Activate Red LED (EMERGENCY - highest priority)\n        2. IF button is NOT PRESSED AND (DHT22 temp &gt; {TEMP_THRESHOLD}°C OR \n        BMP280 temp &gt; {TEMP_THRESHOLD}°C) → Activate Yellow LED (WARNING)\n        3. IF button is NOT PRESSED AND (DHT22 temp ≤ {TEMP_THRESHOLD}°C AND \n        BMP280 temp ≤ {TEMP_THRESHOLD}°C) → Activate Green LED (NORMAL)\n        \n        CURRENT ANALYSIS:\n        - Button status: {\"PRESSED\" if button_state else \"NOT PRESSED\"}\n        - DHT22 temp ({temp_dht:.1f}°C) is {\"OVER\" if temp_dht &gt; TEMP_THRESHOLD \n          else \"AT OR BELOW\"} threshold ({TEMP_THRESHOLD}°C)\n        - BMP280 temp ({temp_bmp:.1f}°C) is {\"OVER\" if temp_bmp &gt; TEMP_THRESHOLD \n          else \"AT OR BELOW\"} threshold ({TEMP_THRESHOLD}°C)\n        \n        Based on these rules, respond with ONLY a JSON object (no other text):\n        {{\"red_led\": true, \"yellow_led\": false, \"green_led\": false}}\n        \n        Only ONE LED should be true, the other two must be false.\n    \"\"\"\ndef parse_llm_response(response_text):\n    \"\"\"Parse the LLM JSON response to extract LED control instructions.\"\"\"\n    try:\n        # Clean the response - remove any markdown code blocks if present\n        response_text = response_text.strip()\n        if response_text.startswith('```'):\n            # Extract JSON from markdown code block\n            lines = response_text.split('\\n')\n            response_text = '\\n'.join(lines[1:-1]) \n            if len(lines) &gt; 2 \n            else response_text\n        \n        # Parse JSON\n        data = json.loads(response_text)\n        red_led = data.get('red_led', False)\n        yellow_led = data.get('yellow_led', False)\n        green_led = data.get('green_led', False)\n        return (red_led, yellow_led, green_led)\n    except (json.JSONDecodeError, KeyError) as e:\n        print(f\"Error parsing JSON response: {e}\")\n        print(f\"Response was: {response_text}\")\n        # Fallback to safe state (all LEDs off)\n        return (False, False, False)\ndef output_actuator(response, MODEL, temp_dht, hum, temp_bmp, \n                    press, button_state):\n    print(f\"\\nSmart IoT Actuator using {MODEL} model\\n\")\n    \n    print(f\"SYSTEM REAL DATA\")\n    print(f\" - DHT22 ==&gt; Temp: {temp_dht:.1f}°C, Humidity: {hum:.1f}%\")\n    print(f\" - BMP280 =&gt; Temp: {temp_bmp:.1f}°C, Pressure: {press:.2f}hPa\")\n    print(f\" - Button {'pressed' if button_state else 'not pressed'}\")\n    \n    print(f\"\\n&gt;&gt; {MODEL} Response: {response['response']}\")\n    \n    # Parse LLM response and use it directly (no validation)\n    red, yellow, green = parse_llm_response(response['response'])\n    print(f\"&gt;&gt; SLM decision: Red={red}, Yellow={yellow}, Green={green}\")\n    \n    # Control LEDs based on SLM decision\n    control_leds(red, yellow, green)\n    \n    print(f\"\\nSYSTEM ACTUATOR STATUS\")\n    ledRedSts, ledYlwSts, ledGrnSts  = led_status()\n    print(f\" - Red LED {'is on' if ledRedSts else 'is off'}\")\n    print(f\" - Yellow LED {'is on' if ledYlwSts else 'is off'}\")\n    print(f\" - Green LED {'is on' if ledGrnSts else 'is off'}\")\ndef slm_analyse_act(MODEL, TEMP_THRESHOLD):\n    \"\"\"Main function to get sensor data, run SLM inference, and actuate LEDs.\"\"\"\n    # Get system info\n    ledRedSts, ledYlwSts, ledGrnSts  = led_status()\n    temp_dht, hum, temp_bmp, press, button_state  = collect_data()\n    \n    # Create prompt with current sensor data\n    PROMPT = create_prompt(temp_dht, \n                           hum, \n                           temp_bmp, \n                           press, \n                           button_state, \n                           ledRedSts, \n                           ledYlwSts, \n                           ledGrnSts,\n                           TEMP_THRESHOLD)\n    \n    # Analyse and actuate on LEDs\n    response = slm_inference(PROMPT, MODEL)\n    output_actuator(response, MODEL, temp_dht, hum, temp_bmp, press, button_state)\n\n\nCode Flow Diagram\n\n\n\n\n\n\n\nTEST1: Temp above the threshold\n\nDefinitions\n# Model to be used\nMODEL = 'llama3.2:3b'\n\n# Temperature threshold for warning\nTEMP_THRESHOLD = 20.0\n\n\nCalling the program\nslm_analyse_act(MODEL, TEMP_THRESHOLD)\n\n\nResult\nSmart IoT Actuator using llama3.2:3b model\n\nSYSTEM REAL DATA\n - DHT22 ==&gt; Temp: 21.5°C, Humidity: 28.8%\n - BMP280 =&gt; Temp: 22.4°C, Pressure: 910.04hPa\n - Button not pressed\n\n&gt;&gt; llama3.2:3b Response: {\"red_led\": false, \"yellow_led\": true, \"green_led\": false}\n&gt;&gt; SLM decision: Red=False, Yellow=True, Green=False\n\nSYSTEM ACTUATOR STATUS\n - Red LED is off\n - Yellow LED is on\n - Green LED is off\n\n\n\n\n\n\n\n\nTEST2: Temp below the threshold\n# Temperature threshold for warning\nTEMP_THRESHOLD = 25.0\nslm_analyse_act(MODEL, TEMP_THRESHOLD)\nSmart IoT Actuator using llama3.2:3b model\n\nSYSTEM REAL DATA\n - DHT22 ==&gt; Temp: 21.6°C, Humidity: 29.1%\n - BMP280 =&gt; Temp: 22.4°C, Pressure: 910.07hPa\n - Button not pressed\n\n&gt;&gt; llama3.2:3b Response: {\"red_led\": false, \"yellow_led\": false, \"green_led\": true}\n&gt;&gt; SLM decision: Red=False, Yellow=False, Green=True\n\nSYSTEM ACTUATOR STATUS\n - Red LED is off\n - Yellow LED is off\n - Green LED is on\n\n\n\n\n\n\n\nTEST3: Alarm Button pressed\nslm_analyse_act(MODEL, TEMP_THRESHOLD)\nSmart IoT Actuator using llama3.2:3b model\n\nSYSTEM REAL DATA\n - DHT22 ==&gt; Temp: 21.6°C, Humidity: 29.0%\n - BMP280 =&gt; Temp: 22.5°C, Pressure: 909.99hPa\n - Button pressed\n\n&gt;&gt; llama3.2:3b Response: {\"red_led\": true, \"yellow_led\": false, \"green_led\": false}\n&gt;&gt; SLM decision: Red=True, Yellow=False, Green=False\n\nSYSTEM ACTUATOR STATUS\n - Red LED is on\n - Yellow LED is off\n - Green LED is off"
  },
  {
    "objectID": "raspi/iot/slm_iot.html#interacting-with-iot-systems-using-natural-language-commands",
    "href": "raspi/iot/slm_iot.html#interacting-with-iot-systems-using-natural-language-commands",
    "title": "Experimenting with SLMs for IoT Control",
    "section": "Interacting with IoT Systems, using Natural Language Commands",
    "text": "Interacting with IoT Systems, using Natural Language Commands\nNow, let’s transform our IoT monitoring setup into an interactive assistant that accepts natural language commands and queries. Instead of autonomously monitoring conditions, the system will now respond to our requests in real-time.\n\n\n\n\n\n\nWhat will change?\n\nOriginal System (Autonomous)\n\nContinuously monitored sensors\nAutomatically decided LED states based on pre-defined rules\nNo user interaction\n\n\n\nNew System (Interactive)\n\nWaits for user commands\nAccepts natural language queries and commands\nProvides conversational responses\nExecutes actions based on user requests\nDisplays comprehensive system status\n\n\n\n\nHow It will Work\n\n1. Interactive Loop\nThe system runs in a continuous loop:\nUser Input → Sensor Reading → SLM Analysis → LED Control → Status Display → Wait for Next Input\n\n\n2. Dual-Purpose Response\nThe SLM now returns a JSON object with TWO components:\n{\n  \"message\": \"Helpful text response to the user\",\n  \"leds\": {\n    \"red_led\": false,\n    \"yellow_led\": true,\n    \"green_led\": false\n  }\n}\n\nmessage: What the assistant tells you (conversational response)\nleds: What the assistant does (LED control)\n\n\n\n3. Command Types\nA. Information Queries\nAsk questions about sensor readings - LEDs remain unchanged.\nExamples:\n\n“What’s the current temperature?”\n“What are the actual conditions?”\n“What’s the humidity level?”\n“Is the button pressed?”\n\nResponse format:\n{\n  \"message\": \"The current temperature is 21.5°C from DHT22 and 22.3°C from BMP280.\",\n  \"leds\": {\"red_led\": false, \"yellow_led\": true, \"green_led\": false}  // keeps current state\n}\nB. Direct LED Commands\nTell the system which LEDs to turn on/off.\nExamples:\n\n“Turn on the yellow LED”\n“Turn on all LEDs”\n“Turn off all LEDs”\n“Turn on the red and green LEDs”\n\nResponse format:\n{\n  \"message\": \"Yellow LED turned on.\",\n  \"leds\": {\"red_led\": false, \"yellow_led\": true, \"green_led\": false}\n}\nC. Conditional Commands\nCommands that depend on sensor readings or button state.\nExamples:\n\n“If the temperature is above 20°C, turn on the yellow LED”\n“If the button is pressed, turn on the red LED”\n“If the button is not pressed, turn on the green LED”\n\nResponse format:\n{\n  \"message\": \"Temperature is 21.5°C, which is above 20°C. Yellow LED turned on.\",\n  \"leds\": {\"red_led\": false, \"yellow_led\": true, \"green_led\": false}\n}\nD. Toggle/Switch Commands\nCommands that change LED states based on current conditions.\nExamples:\n\n“If the button is pressed, switch the LED conditions”\n“Toggle all LEDs”\n\nResponse format:\n{\n  \"message\": \"Button is pressed. LED states switched.\",\n  \"leds\": {\"red_led\": true, \"yellow_led\": false, \"green_led\": false}  // inverted from current\n}\nE. Analysis Queries\nAsk the SLM to analyze sensor data.\nExamples:\n\n“Based on the conditions, would we have rain?”\n“Is the weather getting warmer?”\n“What do the sensor readings indicate?”\n\nResponse format:\n{\n  \"message\": \"Based on pressure of 910.18hPa and humidity of 28.8%, conditions are dry. Rain is unlikely. LEDs unchanged.\",\n  \"leds\": {\"red_led\": false, \"yellow_led\": false, \"green_led\": true}  // keeps current state\n}\n\n\n\nKey Functions\n\ncreate_interactive_prompt()\n\nCreates a comprehensive prompt with:\n\nCurrent sensor readings\nCurrent LED states\nUser’s request\nExamples of how to respond\nRules for the SLM to follow\n\n\n\n\nparse_interactive_response()\n\nExtracts the message from the SLM\nExtracts the LED control commands\nHandles JSON parsing errors gracefully\n\n\n\ndisplay_system_status()\n\nShows comprehensive system status:\n\nAll sensor readings\nButton state\nLED states (with visual indicators ● ○)\n\n\n\n\ninteractive_mode()\n\nMain loop that:\n\nAccepts user input\nReads current sensor data\nSends request to SLM\nParses response\nControls LEDs\nDisplays results\n\n\n\nHere is the complete code: slm_act_leds_interactive.py\n\n\n\n\nRunning the System\npython slm_act_leds_interactive.py\n\n\nTests\n============================================================\nIoT Environmental Monitoring System - Interactive Mode\nUsing Model: llama3.2:3b\n============================================================\n\nCommands you can try:\n  - What's the current temperature?\n  - What are the actual conditions?\n  - Turn on the yellow LED\n  - If temperature is above 20°C, turn on yellow LED\n  - If button is pressed, turn on red LED\n  - Turn on all LEDs\n  - Turn off all LEDs\n  - Will it rain based on current conditions?\n  - Type 'status' to see system status\n  - Type 'exit' or 'quit' to stop\n============================================================\n\nYou: what's the current temperature?\nAssistant: [Thinking...]\nAssistant: The current temperature is 21.5°C from DHT22 and 22.3°C from BMP280.\n\nLED Update: Red=OFF, Yellow=OFF, Green=ON\n\nYou: if temperature is above 20°C, turn on yellow LED\nAssistant: [Thinking...]\nAssistant: Temperature is 21.5°C, which is above 20°C. Yellow LED turned on.\n\nLED Update: Red=OFF, Yellow=ON, Green=OFF\n\nYou: will it rain?\nAssistant: [Thinking...]\nAssistant: Based on pressure of 910.18hPa and humidity of 28.8%, the air is \nrelatively dry. Rain is unlikely in the immediate future. LEDs unchanged.\n\nLED Update: Red=OFF, Yellow=ON, Green=OFF\n\nYou: status\n============================================================\nSYSTEM STATUS\n============================================================\nDHT22 Sensor:  Temp = 21.5°C, Humidity = 28.8%\nBMP280 Sensor: Temp = 22.3°C, Pressure = 910.18hPa\nButton:        NOT PRESSED\n\nLED Status:\n  Red LED:    ○ OFF\n  Yellow LED: ● ON\n  Green LED:  ○ OFF\n============================================================\n\nYou: exit\nExiting interactive mode. Goodbye!\n\n\nSpecial Commands\n\nstatus: Display comprehensive system status without calling the SLM\nexit or quit: Exit the interactive mode\n\n\n\nLanguages\nOne advantage of using SLMs is that, once multilingual models are used (as Llama 3,2 or Gemma), the user can choose the better language for them, independent of the language used during coding.\n\n\n\n\n\n\n\nAdvantages of This Approach\n\nNatural Language Interface: Users don’t need to know programming or specific commands\nFlexible Control: Can handle complex conditional logic based on sensor data\nInformational: Can answer questions about the system without changing states\nContext-Aware: SLM understands current conditions and makes appropriate decisions\nConversational: Provides helpful feedback about what it’s doing\n\n\n\nError Handling\n\nIf sensor data cannot be read, the system will notify you and wait for the next command\nIf the SLM response cannot be parsed, the system keeps LEDs in their current state\n\n\n\nTips for Best Results\n\nBe specific: “Turn on the yellow LED” works better than “turn on the light”\nUse conditions clearly: “If temperature is above 20°C” is clearer than “when it’s hot”\nAsk for status: Use the status command frequently to verify system state\nOne LED at a time: Unless you specifically say “all LEDs”, the system defaults to one LED on\n\n\n\nFlow Diagram\n\n\nThe development of our Interactive IoT-SLM system can also be followed using the Jupyter Notebook: SLM_IoT.ipynb."
  },
  {
    "objectID": "raspi/iot/slm_iot.html#adding-data-logging",
    "href": "raspi/iot/slm_iot.html#adding-data-logging",
    "title": "Experimenting with SLMs for IoT Control",
    "section": "Adding Data Logging",
    "text": "Adding Data Logging\nNow, we will develop an enhanced version that adds data logging, analysis, and historical query capabilities. The system automatically logs all sensor readings and commands, allowing it to analyze trends and query historical data using natural language.\n\n\n\n\n\n\nKey Features\n\n1. Automatic Background Logging\n\nLogs sensor readings every 60 seconds\nRuns in background thread\nStores data in CSV files\n\n\n\n2. CSV Data Storage\n\nsensor_readings.csv: All sensor data + LED states\ncommand_history.csv: All commands and responses\n\n\n\n3. Statistical Analysis\n\nMin/max/average calculations\nLED state change tracking\nButton press counting\nTrend analysis\n\n\n\n4. Natural Language Queries\nAsk questions like:\n\n“Show me temperature trends for the last 24 hours”\n“What was the average humidity today?”\n“How many times was the button pressed?”\n\n\n\n\nRunning the System\npython slm_act_leds_with_logging.py\n\n\nExample Queries\nReal-time:\n\n“What’s the current temperature?”\n“Turn on the yellow LED”\n\nHistorical:\n\n“Show me temperature trends”\n“What was the average humidity today?”\n“How many times was the button pressed?”\n\nBuilt-in commands:\n\nstatus - Show current system status\nstats - Show 24-hour statistics\nexit - Stop system\n\n\n\nData Files\n\n\nsensor_readings.csv\ntimestamp,temp_dht,humidity,temp_bmp,pressure,\nbutton_pressed,red_led,yellow_led,green_led\n\n\ncommand_history.csv\ntimestamp,user_command,slm_response,red_led,yellow_led,green_led\n\n\nTips\n\nLet system run 1+ hour for meaningful trends\nUse specific time frames: “last 6 hours”\nUse stats command for quick overview\nCSV files can be opened in Excel\n\n\nThe complete scripts for the datalogger version arehere: data_logger.py and slm_act_leds_with_logging.py\n\n\n\nFlow Diagram\n\n\n\n\n\n\n\nExamples:"
  },
  {
    "objectID": "raspi/iot/slm_iot.html#prompt-optimization-and-efficiency",
    "href": "raspi/iot/slm_iot.html#prompt-optimization-and-efficiency",
    "title": "Experimenting with SLMs for IoT Control",
    "section": "Prompt Optimization and Efficiency",
    "text": "Prompt Optimization and Efficiency\nWe can modify for example the interactive_mode(MODEL, USER_INPUT) function, to include at its end, some statistics about the model’s performance:\n...\n# Display Latency\n    print(f\"\\nTotal Duration: {(response['total_duration']/1e9):.2f} seconds\")\n    print(f\"prompt_eval_duration: {(response['prompt_eval_duration']/1e9):.2f} s\")\n    print(f\"load_duration: {(response['load_duration']/1e9):.2f} s\")\n    print(f\"eval_count: {response['eval_count']}\")\n    print(f\"eval_duration: {(response['eval_duration']/1e9):.2f} s\")\n    print(f\"eval_rate: {response['eval_count']/(response['eval_duration']/1e9):.2f} \\\ntokens/s\")\nFor example, running\nUSER_INPUT = \"Turn off all LEDs\"\ninteractive_mode(MODEL, USER_INPUT)\nWe will get:\nTotal Duration: 88.81 seconds\nprompt_eval_duration: 80.41 s\nload_duration: 1.98 s\neval_count: 32\neval_duration: 6.41 s\neval_rate: 4.99 tokens/s\nBased on the prompt_eval_duration value, the PROMPT is our main bottleneck, so we must make it as concise as possible. The SLM must process all of the context before it can generate the first output token.\n\nQuick Solution:\n\n1. Drastically Shorten the Prompt\n\nCondense System Status: Remove unnecessary descriptive text and present the status information in a compact, structured format.\n\nExample (Before):\nCURRENT SYSTEM STATUS:\n- DHT22: Temperature 25.5°C, Humidity 45.1%\n- BMP280: Temperature 25.6°C, Pressure 1012.34hPa\n- Button: NOT PRESSED\n- Red LED: OFF\n- Yellow LED: OFF\n- Green LED: ON\nExample (After):\nSTATUS: DHT22=22.5°C/65.0% BMP280=22.3°C/1013.25hPa Button=OFF LEDs:R=OFF/Y=ON/G=OFF\n\nReduce Examples: While examples are crucial for instruction-following, eliminate redundancy. Keep only the most diverse and representative examples. The current prompt is very long due to verbose examples and instructions. Focus on the single-shot example that shows the required JSON output format.\nSimplify Instructions: Make the instructions as direct and short as possible. Use keywords instead of full sentences where clarity is maintained.\nSystem message: It defines the assistant’s behavior and should sent once at initialization, not at PROMPT\nSYSTEM_MESSAGE = \"\"\"You are an IoT assistant controlling an environmental \nmonitoring system with LEDs.\n\nRespond with JSON only:\n{\"message\": \"your helpful response\", \"leds\": {\"red_led\": bool, \"yellow_led\": \nbool, \"green_led\": bool}}\n\nRULES:\n\n- Information queries: keep current LED states unchanged\n- LED commands: update LEDs as requested\n- Conditional commands (if/when): evaluate condition from sensor data first\n- Only ONE LED should be on at a time UNLESS user explicitly says \"all\"\n- Be concise and conversational\n\nAlways respond with valid JSON containing both \"message\" and \"leds\" fields.\"\"\"\n\n\n\n2. chat API Instead of generate\n\nUses ollama.chat() to maintain conversation context\nSystem message sent only once at initialization\nSubsequent messages are much shorter (only status + user query)\n\n\n\nSmart Context Management\n\nKeeps last four conversation exchanges (8 messages) + system message\nPrevents context from growing too large over time\n\n\n\nModel Pre-loading\n\nLoads model into memory before first query\nEliminates the 2-second load delay on first run\n\nLet’s re-create the functions or add new as bellow:\n\n\n\nNew Code\n\nOriginal Functions:\n\ncreate_interactive_prompt() - Now creates compact user messages (was ~800 tokens, now ~100 tokens)\nslm_inference() - Now uses ollama.chat() API instead of ollama.generate()\nparse_interactive_response() - Unchanged\ndisplay_system_status() - Unchanged\ninteractive_mode() - Updated with all optimizations\n\n\n\nNew functions:\n\nSYSTEM_MESSAGE - Constant for system prompt (sent only once)\npreload_model() - Helper to pre-load model into memory\n\n\n\n\nKey Changes Under the Hood:\n\ncreate_interactive_prompt() now returns a compact format:\n\nBefore: Long prompt with 8 examples (~800 tokens)\nAfter: Compact status line + user input (~100 tokens)\n\nslm_inference() now uses chat API:\n\nBefore: ollama.generate(model, prompt) - regenerates context each time\nAfter: ollama.chat(model, messages) - maintains conversation context\n\nSystem prompt is sent only once at initialization, not with every query\n\nUsing the optimized code: slm_act_leds_interactive_optimized.py, we get as a response:\n\n\n\n\n\nThe Prompt evaluation time was reduced drastically!\n\n\nUsing Pydantic\nUsing Pydantic is a robust way to improve the reliability, efficiency, and maintainability of our system, mainly since we rely on the LLM to output precise JSON.\nHere’s how Pydantic can help and what you would need to do:\n\n1. How Pydantic Reduces Latency and Improves Reliability\nPydantic doesn’t directly speed up the model’s token generation, but it can indirectly reduce latency and eliminate error-handling overhead by enabling cleaner, faster parsing and more robust communication.\n\nA. Reliable and Fast JSON Parsing\nThe most critical benefit is moving from the general, potentially brittle json.loads(response_text) in our parse_interactive_response function to Pydantic’s fast validation engine.\n\nPydantic’s JSON Parsing is C-Accelerated: Pydantic is built on top of the fast pydantic-core library written in Rust, which is significantly faster than Python’s standard json module, especially for large or complex data structures.\nStructured Output: Our current code manually handles potential JSON formatting issues (such as the jsoncode block delimiters) and then uses a try/except block to extract fields. Pydantic handles all of this automatically and throws a clean error if the structure is wrong.\n\n\n\nB. Stronger Instruction for the SLM\nThe model is more likely to generate the exact JSON required when you give it a formal schema to follow.\n\nJSON Schema: Pydantic can generate a JSON Schema from your Python classes. We can include this schema directly in our prompt, which acts as an unambiguous, machine-readable instruction for the SLM. This often leads to fewer errors in the model’s output, reducing the need for costly retries or complex string manipulation.\n\n\n\nC. Cleaner Python Code\nIt simplifies our parse_interactive_response function immensely, making your code easier to read and debug.\n\n\n\n\n2. Implementing Pydantic in the Code\n\nLet’s test it on the Optimized Interactive version: slm_act_leds_optimized.py\n\nWe cshould modify the workflow in three main steps:\n\nStep 1: Define the Pydantic Models\nWe must replace the implicit structure with explicit Pydantic models:\nfrom pydantic import BaseModel, Field\n\nclass LEDControl(BaseModel):\n    \"\"\"LED control configuration.\"\"\"\n    red_led: bool = Field(description=\"Red LED state (on/off)\")\n    yellow_led: bool = Field(description=\"Yellow LED state (on/off)\")\n    green_led: bool = Field(description=\"Green LED state (on/off)\")\n\n\nclass AssistantResponse(BaseModel):\n    \"\"\"Complete assistant response with message and LED control.\"\"\"\n    message: str = Field(description=\"Helpful response to the user\")\n    leds: LEDControl = Field(description=\"LED control configuration\")\n\n\nStep 2: Update the SLM inference function\nWe would update the create_interactive_prompt to include the schema:\ndef slm_inference(messages, MODEL):\n    \"\"\"Send chat request to Ollama using chat API with structured output \n    (Pydantic).\"\"\"\n    response = ollama.chat(\n        model=MODEL,\n        messages=messages,\n        format=AssistantResponse.model_json_schema()  # Using Pydantic schema\n    )\n    return response\n\n\nStep 3: Update the Parsing Function\nOur parse_interactive_response becomes much cleaner and more reliable:\ndef parse_interactive_response(response_text):\n    \"\"\"Parse the interactive SLM response using Pydantic (guaranteed valid).\"\"\"\n    try:\n        # Parse directly into Pydantic model - guaranteed valid JSON structure\n        data = AssistantResponse.model_validate_json(response_text)\n        \n        # Extract values from Pydantic model\n        message = data.message\n        red_led = data.leds.red_led\n        yellow_led = data.leds.yellow_led\n        green_led = data.leds.green_led\n        \n        return message, (red_led, yellow_led, green_led)\n    \n    except Exception as e:\n        print(f\"Error parsing response: {e}\")\n        print(f\"Response was: {response_text}\")\n        return \"Error: Could not parse SLM response.\", (False, False, False)\nWith such modifications, the final latency was reduced from 90 to around 60 seconds\n\n\n\n\n\nNote that when we sent two different commands to turn on the LEDs, the new one did not turn off the previous one. This is due to the change in the rules. If we want, we can modify it to match whatever we wish to.\n\nThe final code can be found on GitHub: slm_act_leds_interactive_pydantic.py and in notebook SLM_IoT.ipynb\n\nIn short, using Pydantic over tradicional approuch with JSON, we have as benefits:\n\nNo more parsing errors - Guaranteed valid JSON\nCleaner output - No markdown code blocks\nType safety - IDE autocomplete and type checking\nFaster generation - Constrained decoding is more efficient\nBetter validation - Pydantic catches invalid values"
  },
  {
    "objectID": "raspi/iot/slm_iot.html#next-steps",
    "href": "raspi/iot/slm_iot.html#next-steps",
    "title": "Experimenting with SLMs for IoT Control",
    "section": "Next Steps",
    "text": "Next Steps\nThis chapter involved experimenting with simple applications and verifying the feasibility of using an SLM to control IoT devices. The final result is far from usable in the real world, but it can serve as a starting point for more interesting applications. Below are some observations and suggestions for improvement:\n\nSLM responses can be probabilistic and inconsistent. To increase reliability, consider implementing a confidence threshold or voting system using multiple prompts/responses.\nTry to add data validation and sanity checks for sensor readings before passing them to the SLM.\nApply Structured Response Parsing as discussed early. Future improvements in this approuch could include:\n\nAdd more sophisticated validation rules\nImplement command history tracking\nAdd support for compound commands\nIntegrate with the logging system\nAdd user permission levels\nImplement command templates for common operations\n\nConsider implementing a fallback mechanism when SLM responses are ambiguous or inconsistent.\nStudy using RAG and fine-tuning to increase the system’s reliability when using very small models.\nConsider adding input validation for user commands to prevent potential issues.\nThe current implementation queries the SLM for every command. We did it to study how SLMs would behave. We should consider implementing a caching mechanism for common queries.\nSome simple commands could be handled without SLM intervention. We can do it programmatically.\nConsider implementing a proper state machine for LED control to ensure consistent behavior.\nImplement more sophisticated trend analysis using statistical methods.\nAdd support for more complex queries combining multiple data points."
  },
  {
    "objectID": "raspi/iot/slm_iot.html#resourses",
    "href": "raspi/iot/slm_iot.html#resourses",
    "title": "Experimenting with SLMs for IoT Control",
    "section": "Resourses",
    "text": "Resourses\n\nPython Scripts and notebook"
  },
  {
    "objectID": "raspi/advancing_adgeai/adv_edgeai.html#understanding-slm-limitations",
    "href": "raspi/advancing_adgeai/adv_edgeai.html#understanding-slm-limitations",
    "title": "Advancing EdgeAI: Beyond Basic SLMs",
    "section": "Understanding SLM Limitations",
    "text": "Understanding SLM Limitations\nSmall Language Models, while impressive in their ability to run on edge devices, face several key limitations:\n\n1. Knowledge Constraints\nSLMs have limited knowledge based on their training data, often outdated and incomplete. Unlike their larger counterparts, they cannot store the vast information needed for comprehensive expertise across all domains.\nLet’s run the below example to verify this limitation.\nimport ollama\n\nresponse = ollama.generate(\n    model=\"llama3.2:1b\",\n    prompt=\"Who won the 2024 Summer Olympics men's 100m sprint final?\"\n)\nprint(response['response'])\nThe output of the previous code will likely show hallucination or admission of not knowing, as in the case below:\n\nThis constraint could be solved simply by having an Agent search the Internet for the answer or using Retrieval-Augmented Generation (RAG), as we will see later.\n\n\n2. Reasoning Limitations\nComplex reasoning tasks often exceed the capabilities of SLMs, which struggle with multi-step logical deductions, mathematical computations, and a nuanced understanding of context. Agents can be used to mitigate such limitations.\nFor example, let’s reuse the previous code and ask to the SLM to multiply two numbers :\nimport ollama\n\nresponse = ollama.generate(\n    model=\"llama3.2:3b\",\n    prompt=\"Multiply 123456 by 123456\"\n)\nprint(response['response'])\n\nThe response is wrong; once the multiplication result should be 15,241,383,936. This is expected once the language models are not suitable for mathematical computations. Still, we can use an “agent” to determine whether a user asks for multiplication or a general question. We will learn how to create an agent later.\n\n\n3. Inconsistent Outputs\nSLMs may produce inconsistent responses to the same query, making them unreliable for critical applications requiring deterministic outputs. Several enhancements, such as Function Calling and Response Validation, can improve reliability.\n\n\n4. Domain Specialization\nSLMs perform worse than specialized models in domain-specific tasks like visual recognition or time-series analysis. Fine-tuning can adapt models to specific domains or tasks, improving performance for targeted applications."
  },
  {
    "objectID": "raspi/advancing_adgeai/adv_edgeai.html#techniques-for-enhancing-slm-at-the-edge",
    "href": "raspi/advancing_adgeai/adv_edgeai.html#techniques-for-enhancing-slm-at-the-edge",
    "title": "Advancing EdgeAI: Beyond Basic SLMs",
    "section": "Techniques for Enhancing SLM at the Edge",
    "text": "Techniques for Enhancing SLM at the Edge\nSmall Language Models (SLMs) offer remarkable capabilities for edge devices, but various techniques can significantly enhance their effectiveness. Here, we present a comprehensive framework for optimizing SLMs on resource-constrained devices like the Raspberry Pi, organized from fundamental to advanced approaches.\nWe will divide those technics into 3 segments:\n\nFundamentals: Optimizing Prompting Strategies\n\nChain-of-Thought Prompting\nFew-Shot Learning\nTask Decomposition\n\nIntermediate: Building Intelligence Systems\n\nBuilding Agents with SLMs\nGeneral Knowledge Router\nFunction Calling\nResponse Validation\n\nAdvanced: Extending Knowledge and Specialization\n\nRetrieval-Augmented Generation (RAG)\nFine-Tuning for Domain Specialization\n\nIntegration: Combining Techniques for Optimal Performance\n\nThe true power of these techniques emerges when they’re strategically combined:\n\nAgent Architecture with RAG: Create agents that can access both tools and knowledge bases\nValidation-Enhanced RAG: Apply response validation to ensure RAG outputs are accurate\nFine-Tuned Routers: Use specialized fine-tuned models to handle routing decisions\nChain-of-Thought with Function Calling: Combine reasoning traces with structured outputs\n\nFor example, a comprehensive weather monitoring system, as we introduced in the chapter “Experimenting with SLMs for IoT Control,” might use the following:\n\nRAG to access historical weather patterns and interpretation guides\nFunction calling to structure sensor data analysis\nResponse validation to verify recommendations\nTask decomposition to handle complex multi-part weather analysis"
  },
  {
    "objectID": "raspi/advancing_adgeai/adv_edgeai.html#optimizing-prompting-strategies",
    "href": "raspi/advancing_adgeai/adv_edgeai.html#optimizing-prompting-strategies",
    "title": "Advancing EdgeAI: Beyond Basic SLMs",
    "section": "Optimizing Prompting Strategies",
    "text": "Optimizing Prompting Strategies\n\nChain-of-Thought Prompting\nChain-of-thought prompting encourages SLMs to break down complex problems into step-by-step reasoning, leading to more accurate results:\ndef solve_math_problem(problem):\n    prompt = f\"\"\"\n    Problem: {problem}\n    Let's think about this step by step:\n    1. First, I'll identify what we're looking for\n    2. Then, I'll identify the relevant information\n    3. Next, I'll set up the appropriate equations\n    4. Finally, I'll solve the problem carefully\n    Solving:\n    \"\"\"\n    response = ollama.generate(model=\"llama3.2:3b\", prompt=prompt)\n    return response['response']\nThis technique significantly improves performance on reasoning tasks by emulating human problem-solving approaches.\n\n\nFew-Shot Learning\nFew-shot learning provides examples within the prompt, helping SLMs understand the expected response format and reasoning pattern:\ndef classify_sentiment(text):\n    prompt = f\"\"\"\n    Task: Classify the sentiment of the text as positive, negative, or neutral.\n    Examples:\n    Text: \"I love this product, it works perfectly!\"\n    Sentiment: positive\n    Text: \"This is the worst experience I've ever had.\"\n    Sentiment: negative\n    Text: \"The package arrived on time.\"\n    Sentiment: neutral\n    Text: \"{text}\"\n    Sentiment:\n    \"\"\"\n    response = ollama.generate(model=\"llama3.2:1b\", prompt=prompt)\n    return response['response'].strip()\nThis approach is particularly effective for classification tasks and standardized outputs.\n\n\nTask Decomposition\nFor complex tasks, breaking them into smaller subtasks helps SLMs manage complexity:\ndef analyze_product_review(review):\n    # Step 1: Extract main points\n    points_prompt = f\"Extract the main points from this product review: {review}\"\n    points_response = ollama.generate(model=\"llama3.2:1b\", prompt=points_prompt)\n    main_points = points_response['response']\n    \n    # Step 2: Determine sentiment\n    sentiment_prompt = f\"Determine the overall sentiment of this review: {review}\"\n    sentiment_response = ollama.generate(model=\"llama3.2:1b\", \n                                         prompt=sentiment_prompt)\n    sentiment = sentiment_response['response']\n    \n    # Step 3: Identify improvement suggestions\n    improvements_prompt = f\"What suggestions for improvement can be found in \\\n    this review? {review}\"\n    improvements_response = ollama.generate(model=\"llama3.2:1b\",\n                                            prompt=improvements_prompt)\n    improvements = improvements_response['response']\n    \n    # Final synthesis\n    final_prompt = f\"\"\"\n    Create a concise analysis of this product review based on:\n    Main points: {main_points}\n    Overall sentiment: {sentiment}\n    Improvement suggestions: {improvements}\n    \"\"\"\n    final_response = ollama.generate(model=\"llama3.2:1b\", \n                                     prompt=final_prompt)\n    return final_response['response']\nThis technique distributes cognitive load across multiple simpler prompts, enabling SLMs to handle tasks that might otherwise exceed their capabilities."
  },
  {
    "objectID": "raspi/advancing_adgeai/adv_edgeai.html#building-agents-with-slms",
    "href": "raspi/advancing_adgeai/adv_edgeai.html#building-agents-with-slms",
    "title": "Advancing EdgeAI: Beyond Basic SLMs",
    "section": "Building Agents with SLMs",
    "text": "Building Agents with SLMs\nTo address some of these limitations, we can develop agents that leverage SLMs as part of a more extensive system with additional capabilities.\n\nWhat is an Agent? It is an AI model capable of reasoning, planning, and interacting with its environment. It can be called an Agent because it has an agency that can interact with the environment.\n\nLet’s think about the multiplication problem that we faced before. An Agent can be used for that.\nAn agent is a system that uses an AI Model as its core reasoning engine to:\n\nUnderstand natural language: (1) Interpret and respond to human instructions meaningfully.\nReason and plan: (2) Analyze information, make decisions, and devise problem-solving strategies.\nInteract with its environment: (3 and 4) Gather information, take actions, and observe the results of those actions.\n\n\nFor example, if it is a multiplication, we can use a Python function as a “tool” to calculate it, as shown in the diagram:\n\nOur code works through the following steps:\n\nUser Input: The user types a query like “What is 7 times 8?” or “What is the capital of France?”\nProcess Query: The process_query() function handles the input and decides what to do with it.\nClassification: The ask_ollama_for_classification() function sends the user’s query to the SLM (using Ollama) with a prompt asking it to classify whether the query is requesting multiplication or asking a general question.\nDecision: Based on the SLM’s classification:\n\nIf it’s a multiplication request, the SLM also extracts the numbers, and we use our multiply() function.\nIf it’s a general question, we send the original query to the SLM for a direct answer.\n\nResponse: The system returns either the multiplication result or the SLM’s answer to the general question.\n\nHere’s a Python script that creates a simple agent (or router) between multiplication operations and general questions as described:\nimport requests\nimport json\n\n# Configuration\nOLLAMA_URL = \"http://localhost:11434/api\"\nMODEL = \"llama3.2:3b\"  # You can change this to any model you have installed\nVERBOSE = True\n\ndef ask_ollama_for_classification(user_input):\n    \"\"\"\n    Ask Ollama to classify whether the query is a multiplication request or a \\\n    general question.\n    \"\"\"\n    classification_prompt = f\"\"\"\n    Analyze the following query and determine if it's asking for multiplication \\\n    or if it's a general question.\n    \n    Query: \"{user_input}\"\n    \n    If it's asking for multiplication, respond with a JSON object in this format:\n    {{\n      \"type\": \"multiplication\",\n      \"numbers\": [number1, number2]\n    }}\n    \n    If it's a general question, respond with a JSON object in this format:\n    {{\n      \"type\": \"general_question\"\n    }}\n    \n    Respond ONLY with the JSON object, nothing else.\n    \"\"\"\n    \n    try:\n        if VERBOSE:\n            print(f\"Sending classification request to Ollama\")\n        \n        response = requests.post(\n            f\"{OLLAMA_URL}/generate\",\n            json={\n                \"model\": MODEL,\n                \"prompt\": classification_prompt,\n                \"stream\": False\n            }\n        )\n        \n        if response.status_code == 200:\n            response_text = response.json().get(\"response\", \"\").strip()\n            if VERBOSE:\n                print(f\"Classification response: {response_text}\")\n            \n            # Try to parse the JSON response\n            try:\n                # Find JSON content if there's any surrounding text\n                start_index = response_text.find('{')\n                end_index = response_text.rfind('}') + 1\n                if start_index &gt;= 0 and end_index &gt; start_index:\n                    json_str = response_text[start_index:end_index]\n                    return json.loads(json_str)\n                return {\"type\": \"general_question\"}\n            except json.JSONDecodeError:\n                if VERBOSE:\n                    print(f\"Failed to parse JSON: {response_text}\")\n                return {\"type\": \"general_question\"}\n        else:\n            if VERBOSE:\n                print(f\"Error: Received status code {response.status_code} \\\n                from Ollama.\")\n            return {\"type\": \"general_question\"}\n    \n    except Exception as e:\n        if VERBOSE:\n            print(f\"Error connecting to Ollama: {str(e)}\")\n        return {\"type\": \"general_question\"}\n\ndef multiply(a, b):\n    \"\"\"\n    Perform multiplication and return a formatted response.\n    \"\"\"\n    result = a * b\n    return f\"The product of {a} and {b} is {result}.\"\n\ndef ask_ollama(query):\n    \"\"\"\n    Send a query to Ollama for general question answering.\n    \"\"\"\n    try:\n        if VERBOSE:\n            print(f\"Sending query to Ollama\")\n        \n        response = requests.post(\n            f\"{OLLAMA_URL}/generate\",\n            json={\n                \"model\": MODEL,\n                \"prompt\": query,\n                \"stream\": False\n            }\n        )\n        \n        if response.status_code == 200:\n            return response.json().get(\"response\", \"\")\n        else:\n            return f\"Error: Received status code {response.status_code} \\\n            from Ollama.\"\n    \n    except Exception as e:\n        return f\"Error connecting to Ollama: {str(e)}\"\n\ndef process_query(user_input):\n    \"\"\"\n    Process the user input by first asking Ollama to classify it,\n    then either performing multiplication or sending it back as a \n    general question.\n    \"\"\"\n    # Let Ollama classify the query\n    classification = ask_ollama_for_classification(user_input)\n    \n    if VERBOSE:\n        print(\"Ollama classification:\", classification)\n    \n    if classification.get(\"type\") == \"multiplication\":\n        numbers = classification.get(\"numbers\", [0, 0])\n        if len(numbers) &gt;= 2:\n            return multiply(numbers[0], numbers[1])\n        else:\n            return \"I understood you wanted multiplication, but couldn't \\\n            extract the numbers properly.\"\n    else:\n        return ask_ollama(user_input)\n\ndef main():\n    \"\"\"\n    Main function to run the agent interactively.\n    \"\"\"\n    print(\"Ollama Agent (Type 'exit' to quit)\")\n    print(\"-----------------------------------\")\n    \n    while True:\n        user_input = input(\"\\nYou: \")\n        \n        if user_input.lower() in [\"exit\", \"quit\", \"bye\"]:\n            print(\"Goodbye!\")\n            break\n        \n        response = process_query(user_input)\n        print(f\"\\nAgent: {response}\")\n\n# Example usage\nif __name__ == \"__main__\":\n    # Set to True to see detailed logging\n    VERBOSE = True\n    main()\nWhen we run the script, we can see that, first, the SLM chooses multiplication, passing the numbers entered by the user to the “tool,” which, in this case, is the multiply() function. As a result, we got 15,241,383,936, which it is correct.\n\nLet’s now enter with another question that has no relation with arithmetic, for example: What is the capital of Brazil? In this case, the SLM will decide that the query is a general question and pass it on to the SLM to answer it.\n\nThis simple agent (or router) demonstrates the fundamental concept of using an SLM to make decisions about processing different types of user inputs. It shows both the power of SLMs for natural language understanding and their limitations in structured tasks.\n\nLimitations and Considerations\nThis agent seems to resolve our problem, but it has several limitations that are common when working with SLMs:\n\nJSON Parsing Issues: SLMs don’t always perfectly format JSON responses as requested. The code includes error handling for this.\nClassification Reliability: The SLM might not always correctly classify the query, especially with ambiguous questions.\nNumber Extraction: The SLM might extract numbers incorrectly or miss them entirely.\nError Handling: Robust error handling is essential when working with SLMs because their outputs can be unpredictable.\nLatency: Significant latency is involved in making multiple calls to the SLM. For example, for the above simple agent, the latency was about 50s when using the llama3.2:3B on a Raspberry Pi 5.\n\nHere, you can see the SLM latency (simple query) per device (in tokens/s):\n\n\n\n\n\n\n\n\n\nModel\nRaspi 5 (Cortex A-76)\nPC (i7)\nMac (M1 Pro)\n\n\n\n\nGemma3:4b\n3.8\n8.7\n39\n\n\nLlama3.2:3b\n5.5\n12\n63\n\n\nLlama3.2:1b\n7.5\n19.5\n111\n\n\nGemma3:1b\n12\n22.45\n91\n\n\n\n\nIn my simple tests, the 1B models struggled to classify the tasks correctly. The the 3B and 4B models worked fine\n\n\n\nImprovements\nTo create a more robust agent, we can, for example:\n\nExpand Capabilities: Add support for more operations (addition, subtraction, division).\nBetter Error Handling: Improve fallback mechanisms when the SLM fails to extract numbers or classify correctly.\nModel Preloading: Initialize the model at startup to reduce latency.\nAdding Regex Fallbacks: Use regular expressions as a fallback to extract numbers when the SLM fails.\nContext Preservation: Maintain conversation context for multi-turn interactions.\n\nA more robust script can be used with the above improvements. The diagram shows how it would work:\n\n\n\nimage-20250322113135882\n\n\nThe diagram illustrates the key components of the system:\n\nInitialization:\n\nThe system starts by initializing both models in parallel threads\nThis prevents cold starts and reduces latency\n\nQuery Processing Flow:\n\nUser input is first sent to a classification step\nA model (llama3.2:3B) determines if it’s a calculation or a general question (we can choose a different model here).\nIf it’s a calculation:\n\nThe system extracts the operation type and numbers\nNumbers are converted from strings to floats\nThe appropriate calculation is performed\nResults are formatted with comma separators (e.g., 1,234,567.89)\n\nIf it’s a general question:\n\nThe query is sent to the main model (llama3.2:3b) for answering (we can choose a different model here)\n\n\nOptimizations (highlighted in the subgraph):\n\nPersistent HTTP session for connection reuse\nKeep-alive parameter to prevent model unloading\nSimplified classification prompt for faster processing\nUsing a smaller model for the classification task\nRule-based fallback logic if the model classification fails\n\n\nThe main performance improvements come from:\n\nKeeping models loaded in memory\nUsing connection pooling\nSimplifying the classification task\nUsing a smaller model for classification\nInitializing models in parallel\n\nThis approach maintains the intelligent classification capability while significantly reducing execution time compared to the original implementation.\nRuning the script 3-ollama-calculator-agent.py, we get correct results with reduced latency of about 60%.\n\n\n\n\nGeneral Knowledge Router\nRemember when we asked our SLM: Who won the 2024 Summer Olympics men's 100m sprint final? We could not receive an answer because the modes were trained with information previously in late 2023.\nTo solve this issue, let’s build a more advanced agent to classify whether it should use its knowledge to answer a question or fetch updated information from the Internet. This addresses a key limitation of Small Language Models: their knowledge cutoff date.\nThe general architecture of our agent will be similar to the calculator, but now, we will use a web search API as a tool.\nThis agent addresses a critical limitation of SLMs - their knowledge cutoff date - by determining when to use the model’s built-in knowledge versus when to search for up-to-date information from the web.\n\nHow it works:\nUses SLM for Classification: Relies entirely on the SLM to determine whether a query needs web search or can be answered from the model’s knowledge.\nProvides Date Context: This section supplies the current date to help the SLM make informed decisions about whether information is outdated.\nIntegrates Tavily Search: Uses Tavily’s powerful search API to find relevant information for queries that need external data.\nHandles Timeouts: Includes fallback mechanisms when the model takes too long to respond.\nMaintains Source Attribution: Clearly indicates to the user whether the answer comes from the model’s knowledge or web search.\nLet’s run the script: 4-ollama-search-agent.py\nBut first, we should install the required libraries:\npip install requests\npip install tavily-python\nReplace \"tvly-YOUR_API_KEY\" with your actual Tavily API key.\n\nWhy Tavily is Superior for This Use Case\n\nBuilt for RAG: Tavily is specifically designed for retrieval-augmented generation, making it perfect for our knowledge router.\nHigh-Quality Results: It prioritizes reputable sources and provides context-relevant results.\nBuilt-in Summarization: The API can provide an AI-generated summary of search results, giving an additional layer of processing before your SLM.\nSimple Integration: Clean API with straightforward responses that are easy to parse.\nGenerous Free Tier: 1,000 free searches is plenty for testing and personal use.\n\n\nRuning the script and entering with the same questions that could not be answered before, we now have: Noah Lyles won the men's 100m sprint final at the 2024 Summer Olympics. He set a new personal best time of 9.79 seconds. This victory marked the United States' first win in the event since 2004.\n\nWhen the user enters a common-knowledge question, the agent will send it directly to the SLM. For example, if the user asks, \"Who is Albert Einstein?\", we get:"
  },
  {
    "objectID": "raspi/advancing_adgeai/adv_edgeai.html#improving-agent-reliability",
    "href": "raspi/advancing_adgeai/adv_edgeai.html#improving-agent-reliability",
    "title": "Advancing EdgeAI: Beyond Basic SLMs",
    "section": "Improving Agent Reliability",
    "text": "Improving Agent Reliability\nThere are several ways to enhance an agent’s reliability. One is to implement effective, approved, structured function calling, which makes agents’ responses more consistent and predictable.\n\n1. Function Calling with Pydantic\nIn the SLM chapter, we explored function calling when we created an app where the user enters a country’s name and gets, as an output, the distance in km from the capital city of such a country and the app’s location.\n\nOnce the user enters a country name, the model will return the name of its capital city (as a string) and the latitude and longitude of such city (in float). Using those coordinates, the app used a simple Python library (haversine) to calculate the distance between those 2 points.\nThe critical library used was Pydantic (and instructor), a robust data validation and settings management library engineered by Python to enhance the robustness and reliability of our codebase. In short, Pydantic helps ensure that the model’s response will always be consistent.\nFunction calling can improve an agent’s reliability by ensuring structured outputs and clear tool selection logic. Here’s a generic template about how we can implement it :\nimport time\nfrom haversine import haversine\nfrom pydantic import BaseModel, Field\nfrom ollama import chat\n\nMODEL = 'llama3.2:3B'   # The name of the model to be used\nmylat = -33.33          # Latitude of Santiago de Chile\nmylon = -70.51          # Longitude of Santiago de Chile\n\nclass CityCoord(BaseModel):\n    city: str = Field(..., description=\"Name of the city\")\n    lat: float = Field(..., description=\"Decimal Latitude of the city\")\n    lon: float = Field(..., description=\"Decimal Longitude of the city\")\n\ndef calc_dist(country, model=MODEL):\n    \n    start_time = time.perf_counter()  # Start timing\n\n    # Ask Ollama for structured data\n    response = chat(\n        model=model,\n        messages=[{\n            \"role\": \"user\",\n            \"content\": f\"Return the capital city of {country}, \\\n             with its decimal latitude and longitude.\"\n        }],\n        format=CityCoord.model_json_schema(),  # Structured JSON format\n        options={\"temperature\": 0}\n    )\n\n    resp = CityCoord.model_validate_json(response.message.content)\n\n    distance = haversine((mylat, mylon), (resp.lat, resp.lon), unit='km')\n    \n    end_time = time.perf_counter()  # End timing\n    elapsed_time = end_time - start_time  # Calculate elapsed time\n    \n    print(f\"\\nSantiago de Chile is about {int(round(distance, -1)):,} \\\nkilometers away from {resp.city}.\") \n    print(f\"[INFO] ==&gt; {MODEL}: {elapsed_time:.1f} seconds\")\n    \n    \n# Test\ncalc_dist('france')\ncalc_dist('colombia')\ncalc_dist('united states')\n\n\n\n2. Response Validation\nResponse validation is crucial to developing and deploying AI agents powered by language models. Here are key points regarding LLM validation for agents: Types of Validation\n\nResponse Relevancy: Determines if the LLM output addresses the input informatively and concisely.\nPrompt Alignment: Check if the LLM output follows instructions from the prompt template.\nCorrectness: Assesses factual accuracy based on ground truth.\nHallucination Detection: Identifies fake or made-up information in LLM outputs.\n\nAdding validation prevents incorrect or harmful responses, and here, we can test it with a simple script:\nimport ollama\nimport json\n\ndef validate_response(query, response):\n    \"\"\"Validate that the response is appropriate for the query\"\"\"\n    validation_prompt = f\"\"\"\n    User query: {query}\n    Generated response: {response}\n    \n    Evaluate if this response:\n    1. Directly addresses the user's query\n    2. Is factually accurate to the best of your knowledge\n    3. Is helpful and complete\n    \n    Respond in the following JSON format:\n    {{\n        \"valid\": true/false,\n        \"reason\": \"Explanation if invalid\",\n        \"score\": 0-10\n    }}\n    \"\"\"\n    \n    try:\n        validation = ollama.generate(\n            model=\"llama3.2:3b\",  \n            prompt=validation_prompt\n        )\n        \n        result = json.loads(validation['response'])\n        return result\n    except Exception as e:\n        print(f\"Error during validation: {e}\")\n        return {\"valid\": False, \"reason\": \"Validation error\", \"score\": 0}\n\n# Test\nquery = \"What is the Raspberry Pi 5?\"\nresponse = \"It is a pie created with raspberry and cooked in an oven\"\nvalidation = validate_response(query, response)\nprint(validation)"
  },
  {
    "objectID": "raspi/advancing_adgeai/adv_edgeai.html#retrieval-augmented-generation-rag",
    "href": "raspi/advancing_adgeai/adv_edgeai.html#retrieval-augmented-generation-rag",
    "title": "Advancing EdgeAI: Beyond Basic SLMs",
    "section": "Retrieval-Augmented Generation (RAG)",
    "text": "Retrieval-Augmented Generation (RAG)\nRAG systems enhance Small Language Models (SLMs) by providing relevant information from external sources before generation. This is particularly valuable for edge devices with limited model sizes, as it allows them to access knowledge beyond their training data without increasing the model size.\n\nUnderstanding RAG\nIn a basic interaction between a user and a language model, the user asks a question, which is sent as a prompt to the model. The model generates a response based solely on its pre-trained knowledge. In a RAG process, there’s an additional step between the user’s question and the model’s response. The user’s question triggers a retrieval process from a knowledge base.\n\nThe RAG process consists of these key steps:\n\nQuery Processing: When a user asks a question, the system converts it into an embedding (a numerical representation).\nDocument Retrieval: The system searches a knowledge base for documents with similar embeddings.\nContext Enhancement: Relevant documents are retrieved and combined with the original query.\nGeneration: The SLM generates a response using both the query and the retrieved context.\n\n\n\nImplementing a Naive RAG System\nWe will develop two crucial components (scripts) of an RAG system:\n\nCreating the Vector Database (10-Create-Persistent-Vector-Database.py). This script builds a knowledge base by:\n\nLoading documents from PDFs and URLs\nSplitting them into manageable chunks\nCreating embeddings for each chunk\nStoring these embeddings in a vector database (Chroma)\n\nQuerying the Database (20-Query-the-Persistent-RAG-Database.py). This script:\n\nLoads the saved vector database\nAccepts user queries\nRetrieves relevant documents based on query similarity\nCombines documents with the query in a prompt\nGenerates a response using the SLM\n\n\n\n\n\nInstalation\nOnce inside an environment, install the required libraries:\npip install -U 'langchain-chroma'\npip install -U langchain\npip install -U langchain-community\npip install -U langchain-ollama\npip install -U langchain-text-splitter\npip install -U langchain-community pypdf\npip install tiktoken\npip install -U langsmith\nLet’s examine how these components work together to implement a RAG system on edge devices.\n\n\nKey Components of the Naive RAG System\n\nDocument Processing\n\ndef create_vectorstore():\n    # Load documents from PDFs and URLs\n    docs_list = []\n    # [Document loading code]\n    \n    # Split documents into chunks\n    text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n        chunk_size=300, chunk_overlap=30\n    )\n    doc_splits = text_splitter.split_documents(docs_list)\n    \n    # Create embeddings and store in vector database\n    embedding_function = OllamaEmbeddings(model=\"nomic-embed-text\")\n    vectorstore = Chroma.from_documents(\n        documents=doc_splits,\n        collection_name=\"rag-edgeai-eng-chroma\",\n        embedding=embedding_function,\n        persist_directory=PERSIST_DIRECTORY\n    )\n    \n    # Persist to disk\n    vectorstore.persist()\nThis function processes our documents (chunk size of 300 with an overlap of 30), creating a searchable knowledge base. Notice we’re using OllamaEmbeddings with the nomic-embed-text model, which can run efficiently on edge devices like the Raspberry Pi.\n\nQuery Processing and Retrieval\n\ndef answer_question(question, retriever):\n    \"\"\"Generate an answer using the RAG system\"\"\"\n    start_time = time.time()\n    \n    print(f\"Question: {question}\")\n    print(\"Retrieving documents...\")\n    docs = retriever.invoke(question)\n    docs_content = \"\\n\\n\".join(doc.page_content for doc in docs)\n    print(f\"Retrieved {len(docs)} document chunks\")\n    \n    print(\"Generating answer...\")\n\n    # Using new LangSmith client and pull_prompt\n    client = Client()\n    rag_prompt = client.pull_prompt(\"rlm/rag-prompt\")\n\n    # Compose the RAG chain\n    if isinstance(rag_prompt, str):\n        rag_prompt = ChatPromptTemplate.from_template(rag_prompt)\n\n    rag_chain = rag_prompt | llm | StrOutputParser()\n    answer = rag_chain.invoke({\"context\": docs_content, \"question\": question})\n    \n    end_time = time.time()\n    latency = end_time - start_time\n    print(f\"Response latency: {latency:.2f} seconds using model: {local_llm}\")\n    \n    return answer\nThis function retrieves relevant documents based on the query and combines them with a specialized RAG prompt to generate a response. The RAG prompt is particularly important as it tells the model how to use the context documents to answer the question.\n\nSLM Integration\n\n# Initialize the LLM\nlocal_llm = \"llama3.2:3b\"\nllm = ChatOllama(model=local_llm, temperature=0)\nWe’re using Ollama to run the SLM locally on our edge device, in this case using the 3B parameter version of Llama 3.2.\n\n\nAdvantages of RAG for Edge AI\nUsing RAG on edge devices offers several significant advantages:\n\nKnowledge Extension: RAG allows small models to access knowledge beyond their training data, effectively extending their capabilities without increasing model size.\nReduced Hallucination: By providing factual context, RAG significantly reduces the likelihood of SLMs generating incorrect information.\nUp-to-date Information: Unlike the fixed knowledge in a model’s weights, RAG knowledge bases can be updated regularly with new information.\nDomain Specialization: RAG can make general SLMs perform like domain specialists by providing domain-specific knowledge bases.\nResource Efficiency: RAG allows smaller models (which require less memory and computation) to achieve performance comparable to much larger models.\n\n\n\nOptimizing RAG for Edge Devices\nWhen implementing RAG on resource-constrained edge devices like the Raspberry Pi, consider these optimizations:\n\nChunk Size: Smaller chunks (300-500 tokens) reduce memory usage during retrieval and generation.\nRetrieval Limits: Limit the number of retrieved documents (k=3 to 5) to reduce context size.\nEmbedding Model Selection: Choose lightweight embedding models like nomic-embed-text (137M parameters) or all-minilm (23M parameters).\nPersistent Storage: As shown in our examples, using persistent storage prevents recomputing embeddings every time that the RAG system is initiated.\nQuery Optimization: Implement query preprocessing to improve retrieval accuracy while reducing computational load.\n\ndef optimize_query(query):\n    \"\"\"Optimize the query for better retrieval results\"\"\"\n    # Remove filler words, focus on key terms\n    stop_words = {\"and\", \"or\", \"the\", \"a\", \"an\", \"in\", \"on\", \"at\", \"to\", \n                  \"for\", \"with\"}\n    terms = [term for term in query.lower().split() if term not in stop_words]\n    return \" \".join(terms)\n\n\nApplication: Enhanced Weather Station with RAG\nBuilding on our advanced weather station (see the chapter “Experimenting with SLMs for IoT Control”), we can, for example, integrate RAG to provide more contextual responses about weather conditions and historical patterns:\n\ndef weather_station_with_rag(retriever, model=\"llama3.2:3b\"):\n    # Get current sensor readings\n    temp_dht, humidity, temp_bmp, pressure, button_state = collect_data()\n    \n    # Formulate a query for the RAG system based on current readings\n    query = f\"Analysis of temperature {temp_dht}°C, humidity {humidity}%, \\\n    and pressure {pressure}hPa\"\n    \n    # Retrieve relevant context\n    docs = retriever.invoke(query)\n    context = \"\\n\\n\".join(doc.page_content for doc in docs)\n    \n    # Create a prompt that combines current readings with retrieved context\n    prompt = f\"\"\"\n    Current Weather Station Data:\n    - Temperature (DHT22): {temp_dht:.1f}°C\n    - Humidity: {humidity:.1f}%\n    - Pressure: {pressure:.2f}hPa\n    \n    Reference Information:\n    {context}\n    \n    Based on current readings and the reference information, provide:\n    1. An analysis of current weather conditions\n    2. What these conditions typically indicate\n    3. Recommendations for any actions needed\n    \"\"\"\n    \n    # Generate response using SLM\n    llm = ChatOllama(model=model, temperature=0)\n    response = llm.invoke(prompt)\n    \n    return response.content\nThis function enhances our weather station by providing context-aware responses incorporating current sensor readings and relevant information from our knowledge base. This is only an example. To use it, we should have “Weather Reference Data,” which we do not currently have. Instead, let’s create a general RAG system specializing in Edge AI Engineering.\n\n\nUsing the RAG System for Edge AI Engineering\nFor our RAG system, we will create a database with all chapters alheady written for the EdgeAI Engineering book (chapters as URLs) and a PDF Wevolver 2025 Edge AI Technology Report.\n# PDF documents to include\npdf_paths = [\"./data/2025_Edge_AI_Technology_Report.pdf\"]\n\n# Define URLs for document sources\nurls = [\n    \"https://mjrovai.github.io/EdgeML_Made_Ease_ebook/raspi\\\n    /object_detection/object_detection.html\",\n    \"https://mjrovai.github.io/EdgeML_Made_Ease_ebook/raspi/image_classification\\\n    /image_classification.html\",\n    \"https://mjrovai.github.io/EdgeML_Made_Ease_ebook/raspi/setup/setup.html\",\n    \"https://mjrovai.github.io/EdgeML_Made_Ease_ebook/raspi/counting_objects_yolo\\\n    /counting_objects_yolo.html\",\n    \"https://mjrovai.github.io/EdgeML_Made_Ease_ebook/raspi/llm/llm.html\",\n    \"https://mjrovai.github.io/EdgeML_Made_Ease_ebook/raspi/vlm/vlm.html\",\n    \"https://mjrovai.github.io/EdgeML_Made_Ease_ebook/raspi/physical_comp\\\n    /RPi_Physical_Computing.html\",\n    \"https://mjrovai.github.io/EdgeML_Made_Ease_ebook/raspi/iot/slm_iot.html\",\n]\nUsing the RAG system is straightforward. First, ensure you’ve created the vector database:\n# Run once to create the database\npython 10-Create-Persistent-Vector-Database.py\n\nThen, interact with the system through queries:\n# Start the interactive query interface\npython 20-Query-the-Persistent-RAG-Database.py\nExample interactions:\nYour question: What is edge AI?\n\nGenerating answer...\n\nQuestion: what is EdgeAI?\nRetrieving documents...\nRetrieved 4 document chunks\nGenerating answer...\nResponse latency: 165.72 seconds using model: llama3.2:3b\n\nANSWER:\n==================================================\nEdgeAI refers to the application of artificial intelligence (AI) at the edge \nof a network, typically in real-time applications such as IoT sensors, industrial \nrobots, and smart cameras. The Edge AI ecosystem includes edge devices, edge \nservers, and cloud platforms that work together to enable low-latency AI \ninferencing and processing of data on-site without relying on continuous cloud connectivity. Edge AI technologies aim to solve the world's biggest challenges \nin AI by leveraging energy-efficient, affordable, and scalable solutions for machine\nlearning and advanced edge computing.\n==================================================\n\nThose responses demonstrate how RAG enhances the SLM’s response with specific information from our knowledge base about Edge AI applications on Raspberry Pi. One issue that should be addressed is the latency.\nTo reduce latency, we can use for embedding, the all-minilm model which is much smaller (23M parameters vs. 137M for nomic-embed-text) and creates 384-dimensional embeddings instead of 768, significantly reducing computation time.\nAlso, smaller chunks can be helpful but have some disadvantages. For example, let’s say that we can use a small chunk size (100 tokens with 50 overlap). Here are some considerations:\n\nAdvantages\n\nMemory Efficiency: Smaller chunks require less memory during retrieval and processing, which is beneficial for resource-constrained devices like the Raspberry Pi.\nMore Granular Retrieval: Smaller chunks can potentially provide more precise matches to specific questions, especially for targeted queries about very specific details.\nReduced Context Window Usage: SLMs have limited context windows; smaller chunks allow you to include more distinct pieces of information while staying within these limits.\n\n\n\nDisadvantages\n\nLoss of Context: 100 tokens is approximately 75-80 words, which is often insufficient to capture complete concepts or explanations. Many paragraphs and technical descriptions require more space to convey their full meaning.\nIncreased Vector Store Size: More chunks mean more embeddings to store, potentially increasing the overall size of your vector database.\nFragmented Information: With such small chunks, related information will be split across multiple chunks, making it harder for the model to synthesize coherent answers.\n\n\n\n\nTesting Different Models and Chunk Sizes\nA good practice would be to experiment with different chunk sizes and embedding models and measure:\n\nRetrieval Quality: Are the retrieved chunks relevant to our queries?\nAnswer Accuracy: Does the SLM generate correct and comprehensive answers?\nMemory Usage: Is the system staying within the memory constraints of our device?\nResponse Time: How does chunk size affect latency?\n\nWe can create a simple benchmarking function to have one embedding model defined test the best chunk size:\ndef benchmark_chunk_sizes(document_list, \n                          query_list, \n                          sizes=[(100, 50), (300, 30), (500, 50), (1000, 100)]):\n    \"\"\"Test different chunk sizes and measure performance\"\"\"\n    results = {}\n    \n    for chunk_size, overlap in sizes:\n        print(f\"Testing chunk_size={chunk_size}, overlap={overlap}\")\n        \n        # Create splitter with current settings\n        text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n            chunk_size=chunk_size, chunk_overlap=overlap\n        )\n        \n        # Split documents\n        start_time = time.time()\n        doc_splits = text_splitter.split_documents(document_list)\n        split_time = time.time() - start_time\n        \n        # Create embeddings and store\n        embedding_function = OllamaEmbeddings(model=\"nomic-embed-text\")\n        temp_db_path = f\"temp_db_{chunk_size}_{overlap}\"\n        \n        start_time = time.time()\n        vectorstore = Chroma.from_documents(\n            documents=doc_splits,\n            collection_name=\"benchmark\",\n            embedding=embedding_function,\n            persist_directory=temp_db_path\n        )\n        db_time = time.time() - start_time\n        \n        # Create retriever\n        retriever = vectorstore.as_retriever(k=3)\n        \n        # Test queries\n        query_times = []\n        for query in query_list:\n            start_time = time.time()\n            docs = retriever.invoke(query)\n            query_time = time.time() - start_time\n            query_times.append(query_time)\n        \n        # Store results\n        results[(chunk_size, overlap)] = {\n            \"num_chunks\": len(doc_splits),\n            \"splitting_time\": split_time,\n            \"db_creation_time\": db_time,\n            \"avg_query_time\": sum(query_times) / len(query_times),\n            \"max_query_time\": max(query_times),\n            \"min_query_time\": min(query_times)\n        }\n        \n        # Clean up temporary DB\n        shutil.rmtree(temp_db_path)\n    \n    return results\nRegarding the query side, some optimations can also reduce the latency at the edge. Let’s modify the previous script, with:\n\nDirect Ollama API Calls: Bypasses the LangChain abstraction layer for embedding and LLM generation to reduce overhead.\nEmbedding Caching: Uses lru_cache to prevent recalculating embeddings for repeated queries.\nPreloading Models: Initializes models at startup to avoid cold-start latency.\nOptimized Retriever Settings: Uses minimal k-value (2) and adds a score threshold to filter out irrelevant matches.\nReduced Dependency Usage: Removes unnecessary imports and simplifies the pipeline.\nConcurrent Processing: Uses ThreadPoolExecutor for batch document embedding (when needed).\nEarly Termination: Checks for empty document results before running the LLM.\nSimplified Prompt: Uses a more concise prompt template focused on getting direct answers.\nFixed Seed: Uses a consistent seed for the LLM to reduce variability in response times.\n\nThis optimized version (25-optimized_RAG_query.py) significantly reduces the latency compared to our original implementation while maintaining compatibility with our existing nomic-embed-text vector database and chunk size (300/30).\n\nThe direct Ollama API approach removes several layers of abstraction in the LangChain implementations.\n\nWe can see latency improvements from 2 minutes down to approximately 50-110 seconds, depending on the complexity of the queries.\n\nIn the next section, we’ll explore how RAG can be combined with our agent architecture to create even more powerful edge AI systems."
  },
  {
    "objectID": "raspi/advancing_adgeai/adv_edgeai.html#advanced-agentic-rag-system",
    "href": "raspi/advancing_adgeai/adv_edgeai.html#advanced-agentic-rag-system",
    "title": "Advancing EdgeAI: Beyond Basic SLMs",
    "section": "Advanced Agentic RAG System",
    "text": "Advanced Agentic RAG System\nWe can significantly enhance traditional RAG implementations by incorporating some of the modules discussed earlier, such as intelligent routing, validation feedback loops, and explicit knowledge gap identification. This will provide more reliable and transparent answers for users querying document-based knowledge bases.\nFor example, let’s enhance the last RAG system created on the Edge AI Engineering dataset so that the agent can use tools, such as a calculator for arithmetic calculations.\n\nNote that any tool could be used here; the calculator is only a simple example to demonstrate the concept.\n\nWhen the user asks a question, the system first determines if it needs to use a tool or the RAG approach. For knowledge queries, the RAG system enhances the response with information from the database. The system then validates the answer quality, and if it’s not sufficient, tries again with an improved prompt. In cases where questions fall outside the database’s scope, the system will clearly inform the user rather than attempting to generate potentially misleading answers.\n\nSystem Architecture\n\n\n\nimage-20250322113245945\n\n\nThe system functions through several key components:\n\nQuery Router\n\nAnalyzes incoming queries to determine if they’re calculations or knowledge queries\nCan use the same model for the response generator or a lightweight model to reduce overhead\nImplements rule-based fallbacks for robust classification\n\nDocument Retriever\n\nConnects to a persistent vector database (Chroma)\nUses semantic embeddings to find relevant documents\nReturns contextually similar content for knowledge generation\n\nResponse Generator\n\nCreates answers based on retrieved documents\nImplements a two-stage approach with validation and improvement\nAdds appropriate disclaimers when information is insufficient\n\nValidation Engine\n\nEvaluates answer quality using structured criteria\nAssigns a numerical score to each generated response\nTriggers enhancement processes when quality is insufficient\n\nInteractive Interface\n\nProvides user-friendly interaction with clear quality indicators\nSupports model switching and verbosity control\nOffers guidance for improving query outcomes\n\n\n\n\nKey Workflow\nThe system follows this high-level workflow:\n\nUser submits a query\nRouter determines the query type (calculation vs. knowledge)\nFor calculations:\n\nExtract operation and numbers\nCompute and return result\n\nFor knowledge queries:\n\nRetrieve relevant documents\nGenerate initial answer with RAG\nValidate response quality\nIf quality is low, attempt enhancement with improved prompt\nIf still insufficient, add disclaimer about knowledge gaps\n\nReturn final answer with quality metrics\n\n\n\nImportant Code Sections\n\nQuery Routing\ndef route_query(query: str) -&gt; Dict[str, Any]:\n    \"\"\"Determine if the query is a calculation, otherwise use RAG\"\"\"\n    if VERBOSE:\n        print(f\"Routing query: {query}\")\n    \n    # Check for calculation keywords\n    calc_terms = [\"+\", \"add\", \"plus\", \"sum\", \"-\", \"subtract\", \"minus\", \n                  \"difference\", \"*\", \"×\", \"multiply\", \"times\", \"product\", \n                  \"/\", \"÷\", \"divide\",\n                  \"division\", \"quotient\"]\n    \n    # Simple rule-based detection for calculations\n    is_calc = any(term in query.lower() for term in calc_terms) and \\\n    re.search(r'\\d+', query)\n    \n    if is_calc:\n        # Use smaller, faster model for operation and number extraction\n        # ...extraction logic here...\n        return route_info\n    \n    # For everything else, use RAG\n    return {\"type\": \"rag\", \"reasoning\": \"Non-calculation query, using RAG\"}\n\n\nEnhanced RAG with Feedback Loop\n# First RAG attempt with standard prompt\nanswer = get_answer_with_rag(query, documents, llm)\nprocessing_type = \"rag_standard\"\n\n# Validate the response quality\nvalidation = validate_response(llm, query, answer)\nvalidation_score = validation.get(\"score\", 5)\n\n# If validation score is low, try again with enhanced prompt\nif validation_score &lt; 7:\n    if VERBOSE:\n        print(f\"First RAG attempt validation score: {validation_score}/10. \\\n        Trying enhanced prompt.\")\n    \n    # Second RAG attempt with enhanced prompt\n    enhanced_context = \"\\n\\n\".join(documents)\n    enhanced_prompt = f\"\"\"\n    I need a more detailed and accurate answer to the following question:\n    \n    {query}\n    \n    The previous answer wasn't satisfactory. Let me provide you with \\\n    relevant information:\n    \n    {enhanced_context}\n    \n    Based strictly on this information, provide a comprehensive answer.\n    Focus specifically on addressing the user's question with precise \\\n    information from the provided context.\n    If the information doesn't fully answer the question, clearly state \\\n    what you can determine\n    from the available information and what remains unknown.\n    \"\"\"\n    # ... process enhanced response ...\n\n\nKnowledge Gap Handling\n# If still low quality after enhancement, add a note\nif improved_score &lt; 6:\n    processing_type = \"rag_insufficient_info\"\n    information_gap_note = (\n        \"\\n\\nNote: The information in my knowledge base may be incomplete on this\"\n        \"topic. I've provided the best answer based on available information, but\" \n        \"there might be gaps or additional details that would provide a more \"\n        \"complete answer.\"\n    )\n    answer = answer + information_gap_note\n\n\n\nDetailed Workflow Diagram\n\n\n\n\n\nExamples\nRun the script 30-advanced_agentic_rag.py:\n\nSimple Calculation\n\n\n\nFirst Pass Rag\n\n\n\nMore Complex Queries\n\n\n\nQueries outside of the database scope:"
  },
  {
    "objectID": "raspi/advancing_adgeai/adv_edgeai.html#fine-tuning-slms-for-edge-deployment",
    "href": "raspi/advancing_adgeai/adv_edgeai.html#fine-tuning-slms-for-edge-deployment",
    "title": "Advancing EdgeAI: Beyond Basic SLMs",
    "section": "Fine-Tuning SLMs for Edge Deployment",
    "text": "Fine-Tuning SLMs for Edge Deployment\nFine-tuning can adapt models to specific domains or tasks, improving performance for targeted applications.\n\nPreparing for Fine-Tuning\n# Example dataset for fine-tuning a weather response model\ntraining_data = [\n    {\"input\": \"What's the weather in London?\", \n     \"output\": \"I need to check London's current weather. Please use the \\\n     weather tool.\"},\n    {\"input\": \"Is it going to rain tomorrow in Paris?\", \n     \"output\": \"To answer about tomorrow's weather in Paris, \\\n     I need to use the weather tool.\"},\n    {\"input\": \"Will it be sunny this weekend in Tokyo?\", \n     \"output\": \"To predict Tokyo's weekend weather, \\\n     I should use the weather tool.\"}\n]\n\n# Format data for fine-tuning\nformatted_data = []\nfor item in training_data:\n    formatted_data.append({\n        \"prompt\": item[\"input\"],\n        \"response\": item[\"output\"]\n    })\n\n# Save formatted data to a file\nimport json\nwith open(\"weather_finetune_data.json\", \"w\") as f:\n    json.dump(formatted_data, f)\n\n\nSetting Up a Fine-Tuning Process\nFine-tuning on edge devices is typically impractical due to resource constraints. Instead, fine-tune on a more powerful machine and deploy the result to the edge:\n\nThis is a conceptual example - actual implementation depends on the framework\n\ndef prepare_for_finetuning(data_path, output_path):\n    \"\"\"\n      Prepare a model for fine-tuning \n      (run this on a more powerful machine)\n    \"\"\"\n    # This is a conceptual example \n    print(f\"Fine-tuning model using data from {data_path}\")\n    print(f\"Fine-tuned model will be saved to {output_path}\")\n    \n    # The process would typically involve:\n    # 1. Loading the base model\n    # 2. Loading and preprocessing the training data\n    # 3. Setting up training parameters (learning rate, epochs, etc.)\n    # 4. Running the fine-tuning process\n    # 5. Evaluating the fine-tuned model\n    # 6. Saving and optimizing for edge deployment\n    \n    # For Ollama, we would create a custom model definition (Modelfile)\n    modelfile = f\"\"\"\n    FROM llama3.2:1b\n    \n    # Fine-tuning settings would go here\n    PARAMETER temperature 0.7\n    PARAMETER top_p 0.9\n    PARAMETER top_k 40\n    \n    # Custom system prompt for the specific domain\n    SYSTEM You are a specialized assistant for weather-related questions.\n    \"\"\"\n    \n    with open(output_path, \"w\") as f:\n        f.write(modelfile)\n    \n    print(\"Model preparation complete. Next steps:\")\n    print(\"1. Run fine-tuning on a powerful machine\")\n    print(\"2. Optimize the resulting model for edge deployment\")\n    print(\"3. Deploy to your Raspberry Pi\")\n\n\nReal implementation: Supervised Fine-Tuning (SFT)\nSupervised fine tuning (SFT) is a method to improve and customize pre-trained LLMs. It involves retraining base models on a smaller dataset of instructions and answers. The main goal is to transform a basic model that predicts text into an assistant that can follow instructions and answer questions. SFT can also enhance the model’s overall performance, add new knowledge, or adapt it to specific tasks and domains\nBefore considering SFT, it is recommended to try prompt engineering techniques like few-shot prompting or retrieval augmented generation (RAG), as discussed previously. In practice, these methods can solve many problems without fine-tuning. If this approach doesn’t meet our objectives (regarding quality, cost, latency, etc.), then SFT becomes a viable option when instruction data is available. SFT also offers benefits like additional control and customizability to create personalized LLMs.\nHowever, SFT has limitations. It works best when leveraging knowledge already present in the base model. Learning completely new information, like an unknown language, can be challenging and lead to more frequent hallucinations. For new domains unknown to the base model, it is recommended that it be continuously pre-trained on a raw dataset first.\nOn the opposite end of the spectrum, instruct models (i.e., already fine-tuned models) can be very close to our requirements. By providing chosen and rejected samples for a small set of instructions (between 100 and 1000 samples), we can force the LLM to behave as we need.\nThe easiest way to finetune an SLM is by using Unsloth. The three most popular SFT techniques are full fine-tuning, LoRA, and QLoRA.\n\nFor details, see: Fine-tune Llama 3.1 Ultra-Efficiently with Unsloth.\n\nFor example, using this link, it is possible to find several notebooks with the steps to finetune SLMs—for instance, the Gemma 3:1B.\nThe fine-tuned model can be saved on HF Hub or locally as GGUF, and to run a GGUF model locally, we can use Ollama, as shown below:\n\nDownload the finetuned GGUF model\nCreate a Modelfile in the home directory:\n\ncd ~\nnano Modelfile\n\nIn the Modelfile, specify the path to the GGUF file:\n\nFROM ~/Downloads/your-model-name.gguf\nPARAMETER temperature 1.0\nPARAMETER top_p 0.95\nPARAMETER top_k 64\n\nSave the Modelfile and exit the editor.\nCreate the loadable model in Ollama:\n\nollama create your-model-name -f Modelfile\n\nThe model name here can be anything.\n\n\nWe can now use the model through as we have done with the llama3.2:3B in this chapter.."
  },
  {
    "objectID": "raspi/advancing_adgeai/adv_edgeai.html#conclusion",
    "href": "raspi/advancing_adgeai/adv_edgeai.html#conclusion",
    "title": "Advancing EdgeAI: Beyond Basic SLMs",
    "section": "Conclusion",
    "text": "Conclusion\nThis chapter has explored comprehensive strategies for overcoming the inherent limitations of Small Language Models in edge computing environments. By implementing techniques ranging from optimized prompting strategies to sophisticated agent architectures and knowledge integration systems, we’ve demonstrated that it’s possible to significantly enhance the capabilities of edge AI systems without requiring more powerful hardware or cloud connectivity.\nThe techniques presented—chain-of-thought prompting, task decomposition, function calling, response validation, and RAG—form a toolkit that edge AI engineers can apply individually or in combination to address specific challenges. Each approach offers unique advantages: prompting techniques improve reasoning capabilities with minimal overhead, agent architectures enable SLMs to perform actions beyond text generation, and RAG systems dramatically expand an SLM’s knowledge without increasing model size.\nOur practical implementations on the Raspberry Pi showcase that these enhancements are not merely theoretical but can be deployed in real-world edge scenarios. From the simple calculator agent to the more sophisticated knowledge router and RAG-enabled question answering system, these examples provide templates that developers can adapt to their specific application requirements.\nThe true power of these techniques emerges when they’re strategically combined. An agent architecture with RAG capabilities, enhanced by chain-of-thought reasoning and validated with a feedback loop, creates an edge AI system that approaches the capabilities of much larger models while maintaining the advantages of edge deployment—privacy preservation, reduced latency, and operation without internet connectivity.\nAs edge AI continues to evolve, these techniques will become increasingly important in bridging the gap between the limited resources available on edge devices and the growing expectations for AI capabilities. By thoughtfully applying these approaches, developers can create intelligent systems that process data locally, respect user privacy, and operate reliably in diverse environments.\nThe future of edge AI lies not necessarily in deploying ever-larger models but in developing more innovative systems that combine efficient models with intelligent architectures, contextual knowledge integration, and robust validation mechanisms. By mastering these techniques, edge AI practitioners can create solutions that are not just technologically impressive but genuinely useful and trustworthy in addressing real-world challenges."
  },
  {
    "objectID": "raspi/advancing_adgeai/adv_edgeai.html#resources",
    "href": "raspi/advancing_adgeai/adv_edgeai.html#resources",
    "title": "Advancing EdgeAI: Beyond Basic SLMs",
    "section": "Resources",
    "text": "Resources\nThe scripts used in this chapter can be found here: Advancing EdgeAI Scripts"
  },
  {
    "objectID": "weekly_labs.html#week-1-introduction-and-setup",
    "href": "weekly_labs.html#week-1-introduction-and-setup",
    "title": "Edge AI Engineering - Weekly Labs",
    "section": "Week 1: Introduction and Setup",
    "text": "Week 1: Introduction and Setup\n\nLab 1: Raspberry Pi Configuration\nObjectives:\n\nInstall Raspberry Pi OS using Raspberry Pi Imager\nConfigure basic settings (hostname, SSH, WiFi)\nLearn essential Linux commands\nManage files between your computer and Raspberry Pi\n\nInstructions:\n\nDownload Raspberry Pi Imager on your computer\nConfigure OS settings (enable SSH, set hostname, WiFi credentials)\nBoot your Raspberry Pi and confirm connectivity\nLearn how to use SSH for remote access\nTransfer files using SCP or FileZilla\nUpdate your Raspberry Pi OS (sudo apt update && sudo apt upgrade)\nPractice basic Linux commands (ls, cd, mkdir, cp, mv)\n\nDeliverable: Screenshot showing successful SSH connection to your Raspberry Pi\n\n\nLab 2: Development Environment Setup\nObjectives:\n\nSet up Python environment for development\nConfigure remote development tools\nInstall essential libraries\nTest camera functionality\n\nInstructions:\n\nInstall Python essentials: pip install jupyter matplotlib numpy pillow\nConfigure Jupyter Notebook for remote access:\npip install jupyter\njupyter notebook --generate-config\njupyter notebook --ip=0.0.0.0 --no-browser\nConnect the camera module (USB or CSI) to your Raspberry Pi\nTest camera functionality using command-line tools\nWrite a simple Python script to capture an image\n\nDeliverable: A simple Python script that captures and displays an image from your camera and a Screenshot showing a successful image capture"
  },
  {
    "objectID": "weekly_labs.html#week-2-image-classification-fundamentals",
    "href": "weekly_labs.html#week-2-image-classification-fundamentals",
    "title": "Edge AI Engineering - Weekly Labs",
    "section": "Week 2: Image Classification Fundamentals",
    "text": "Week 2: Image Classification Fundamentals\n\nLab 3: Working with Pre-trained Models\nObjectives:\n\nInstall TensorFlow Lite runtime\nDownload and run MobileNet V2 model\nProcess and classify images\nUnderstand model inputs and outputs\n\nInstructions:\n\nInstall TensorFlow Lite runtime: pip install tflite_runtime\nDownload MobileNet V2 model:\nwget https://storage.googleapis.com/download.tensorflow.org/models/tflite_11_05_08/mobilenet_v2_1.0_224_quant.tgztar xzf mobilenet_v2_1.0_224_quant.tgz\nDownload labels file\nCreate a Python script that:\n\nLoads the TFLite model\nProcesses input images to 224x224 format\nRuns inference on test images\nDisplays top-5 predicted classes with confidence scores\n\n\nDeliverable: Python script that successfully classifies sample images with MobileNet V2 and a Screenshot showing a successful result\n\n\nLab 4: Custom Dataset Creation\nObjectives:\n\nCreate a simple custom dataset using Raspberry Pi camera\nOrganize images into classes\nPrepare dataset for model training\n\nInstructions:\n\nCreate a web interface for image capture:\n\nUse Flask to create a simple web server\nSet up camera preview and capture functionality\nSave captured images with appropriate filenames\n\nCapture at least 50 images per class for 3 classes\nOrganize the dataset into an appropriate directory structure\nDocument your dataset creation process\n\nDeliverable: Structured dataset with at least 3 classes and 50 images per class"
  },
  {
    "objectID": "weekly_labs.html#week-3-custom-image-classification",
    "href": "weekly_labs.html#week-3-custom-image-classification",
    "title": "Edge AI Engineering - Weekly Labs",
    "section": "Week 3: Custom Image Classification",
    "text": "Week 3: Custom Image Classification\n\nLab 5: Edge Impulse Model Training\nObjectives:\n\nCreate an Edge Impulse project\nUpload and process the dataset\nDesign and train a transfer learning model\nEvaluate model performance\n\nInstructions:\n\nCreate an Edge Impulse account and a new project\nUpload your custom dataset\nCreate an impulse design:\n\nSet image size to 160x160\nUse Transfer Learning for feature extraction\n\nGenerate features for all images\nTrain model using MobileNet V2\nAnalyze model performance (accuracy, confusion matrix)\nTest model on validation data\n\nDeliverable: Edge Impulse project link and screenshot of model performance metrics\n\n\nLab 6: Model Deployment to Raspberry Pi\nObjectives:\n\nExport trained model to TFLite format\nDeploy model to Raspberry Pi\nCreate a real-time inference application\nOptimize inference speed\n\nInstructions:\n\nExport model as TensorFlow Lite (.tflite)\nTransfer the model to Raspberry Pi\nCreate a Python application that:\n\nCaptures live images from the camera\nPreprocesses images for the model\nRuns inference and displays results\nShows confidence scores\n\nImplement a web interface for real-time classification\n\nDeliverable: Python script for real-time image classification with your custom model and a Screenshot showing a successful result"
  },
  {
    "objectID": "weekly_labs.html#week-4-object-detection-fundamentals",
    "href": "weekly_labs.html#week-4-object-detection-fundamentals",
    "title": "Edge AI Engineering - Weekly Labs",
    "section": "Week 4: Object Detection Fundamentals",
    "text": "Week 4: Object Detection Fundamentals\n\nLab 7: Pre-trained Object Detection\nObjectives:\n\nUnderstand object detection architecture\nRun pre-trained SSD-MobileNet model\nProcess detection outputs\nVisualize detected objects\n\nInstructions:\n\nDownload the pre-trained SSD-MobileNet V1 model\nCreate a Python script that:\n\nLoads the model and labels\nPreprocesses input images\nRuns inference\nExtracts bounding boxes, classes, and scores\nImplements Non-Maximum Suppression (NMS)\nVisualizes detections with bounding boxes\n\nTest on various images with multiple objects\n\nDeliverable: Python script that performs and visualizes object detection on test images and a Screenshot showing a successful result.\n\n\nLab 8: EfficientDet and FOMO Models\nObjectives:\n\nCompare different object detection architectures\nImplement EfficientDet and FOMO models\nAnalyze performance differences\nUnderstand trade-offs between models\n\nInstructions:\n\nDownload the EfficientDet Lite0 model\nImplement inference with EfficientDet\nCompare with SSD-MobileNet implementation\nLearn about FOMO (Faster Objects, More Objects)\nAnalyze trade-offs in accuracy vs. speed\nMeasure inference time on Raspberry Pi\n\nDeliverable: Comparison report of SSD-MobileNet vs. EfficientDet with performance metrics and visualized results"
  },
  {
    "objectID": "weekly_labs.html#week-5-custom-object-detection",
    "href": "weekly_labs.html#week-5-custom-object-detection",
    "title": "Edge AI Engineering - Weekly Labs",
    "section": "Week 5: Custom Object Detection",
    "text": "Week 5: Custom Object Detection\n\nLab 9: Dataset Creation and Annotation\nObjectives:\n\nCreate an object detection dataset\nLearn annotation techniques\nPrepare dataset for model training\n\nInstructions:\n\nCapture at least 100 images containing objects to detect\nUpload images to Roboflow or a similar annotation tool\nCreate bounding box annotations for each object\nApply data augmentation (rotation, brightness adjustment)\nExport dataset in YOLO format\nDocument the annotation process\n\nDeliverable: Annotated dataset with at least 2 object classes and 100 total images\n\n\nLab 10: Training Models in Edge Impulse\nObjectives:\n\nUpload annotated dataset to Edge Impulse\nTrain SSD MobileNet object detection model\nEvaluate model performance\nExport model for deployment\n\nInstructions:\n\nCreate a new Edge Impulse project for object detection\nUpload annotated dataset (train/test splits)\nCreate object detection impulse\nTrain SSD MobileNet model\nEvaluate model performance\nExport model as TensorFlow Lite\n\nDeliverable: Edge Impulse project link with trained object detection model and performance metrics"
  },
  {
    "objectID": "weekly_labs.html#week-6-advanced-object-detection",
    "href": "weekly_labs.html#week-6-advanced-object-detection",
    "title": "Edge AI Engineering - Weekly Labs",
    "section": "Week 6: Advanced Object Detection",
    "text": "Week 6: Advanced Object Detection\n\nLab 11: FOMO Model Training\nObjectives:\n\nUnderstanding FOMO architecture benefits\nTrain FOMO model on Edge Impulse\nCompare performance with SSD MobileNet\nDeploy optimized model to Raspberry Pi\n\nInstructions:\n\nCreate a new impulse in Edge Impulse using the same dataset\nTrain FOMO model instead of SSD MobileNet\nCompare inference speed and accuracy\nDeploy both models to Raspberry Pi\nCreate an application that can switch between models\nMeasure and document performance differences\n\nDeliverable: Python application that compares SSD MobileNet vs. FOMO performance in real-time\n\n\nLab 12: YOLO Implementation\nObjectives:\n\nInstall and configure Ultralytics YOLO\nConvert models to optimized NCNN format\nCreate real-time detection application\nImplement object counting\n\nInstructions:\n\nInstall Ultralytics: pip install ultralytics\nDownload and test YOLO (n) model\nExport model to NCNN format for optimization\nCreate Python script for real-time detection\nImplement object counting algorithm\nAdd visualization of counts over time\n\nDeliverable: Python application for real-time object detection and counting using YOLO"
  },
  {
    "objectID": "weekly_labs.html#week-7-object-counting-project",
    "href": "weekly_labs.html#week-7-object-counting-project",
    "title": "Edge AI Engineering - Weekly Labs",
    "section": "Week 7: Object Counting Project",
    "text": "Week 7: Object Counting Project\n\nLab 13: Custom YOLO Training\nObjectives:\n\nTrain YOLO on a custom dataset\nOptimize model for edge deployment\nCreate a complete application for object counting\n\nInstructions:\n\nTrain the YOLO model on your custom dataset\n\nUse Google Colab for training if needed\nSet appropriate hyperparameters\n\nExport optimized model for Raspberry Pi\nCreate a Python application that:\n\nCaptures video feed\nDetects objects using YOLO\nCounts objects over time\nLogs results to a database\n\n\nDeliverable: Complete object counting application with data logging\n\n\nLab 14: Fixed-Function AI Integration (Optional)\nObjectives:\n\nIntegrate multiple AI models into a single application\nCreate a dashboard for visualization\nOptimize application for long-term deployment\n\nInstructions:\n\nCreate an integration application that combines:\n\nObject detection capabilities\nClassification for detected objects\nCounting and tracking over time\n\nImplement a simple web dashboard for visualization\nAdd performance monitoring\nConfigure application for startup at boot\n\nDeliverable: Integrated application combining multiple AI capabilities with visualization dashboard"
  },
  {
    "objectID": "weekly_labs.html#week-8-introduction-to-generative-ai",
    "href": "weekly_labs.html#week-8-introduction-to-generative-ai",
    "title": "Edge AI Engineering - Weekly Labs",
    "section": "Week 8: Introduction to Generative AI",
    "text": "Week 8: Introduction to Generative AI\n\nLab 15: Raspberry Pi Configuration for SLMs\nObjectives:\n\nOptimize Raspberry Pi for running Small Language Models\nInstall an active cooling solution\nConfigure memory and swap\nInstall essential libraries\n\nInstructions:\n\nInstall active cooling solution on Raspberry Pi 5\nOptimize system configuration:\n\nIncrease swap memory: sudo dphys-swapfile swapoff, edit /etc/dphys-swapfile\nSet CONF_SWAPSIZE to 2048\nsudo dphys-swapfile setup && sudo dphys-swapfile swapon\n\nInstall dependencies:\nsudo apt update\nsudo apt install build-essential python3-dev\n\nDeliverable: Screenshot showing system configuration with increased swap and temperature monitor during stress test\n\n\nLab 16: Ollama Installation and Testing\nObjectives:\n\nInstall Ollama framework\nPull and test Small Language Models\nBenchmark model performance\nMonitor resource usage\n\nInstructions:\n\nInstall Ollama:\ncurl -fsSL https://ollama.com/install.sh | sh\nPull different models (such as):\nollama pull llama3.2:1b\nollama pull gemma:2b\nollama pull phi3:latest\nRun a basic inference test with each model\nMeasure and compare:\n\nLoad time\nInference speed (tokens/sec)\nMemory usage\nTemperature\n\n\nDeliverable: Benchmark report comparing performance metrics of different SLM models on your Raspberry Pi"
  },
  {
    "objectID": "weekly_labs.html#week-9-slm-python-integration",
    "href": "weekly_labs.html#week-9-slm-python-integration",
    "title": "Edge AI Engineering - Weekly Labs",
    "section": "Week 9: SLM Python Integration",
    "text": "Week 9: SLM Python Integration\n\nLab 17: Ollama Python Library\nObjectives:\n\nUse the Ollama Python library\nCreate interactive applications\nProcess SLM responses programmatically\nHandle multiple conversation turns\n\nInstructions:\n\nInstall Ollama Python library: pip install ollama\nCreate Python script to:\n\nConnect to Ollama API\nSend prompts to models\nProcess and format responses\nHandle conversation context\n\nImplement proper error handling\nCreate a simple interactive CLI application\n\nDeliverable: Python script demonstrating Ollama library usage with conversation handling\n\n\nLab 18: Function Calling and Structured Outputs\nObjectives:\n\nImplement function calling with SLMs\nCreate applications with structured outputs\nBuild validation mechanisms\nHandle image inputs\n\nInstructions:\n\nInstall required libraries:\npip install pydantic instructor openai\nCreate Pydantic models for structured outputs\nImplement function calling with an instructor:\nclient = instructor.patch(    OpenAI(base_url=\"http://localhost:11434/v1\", api_key=\"ollama\"),    mode=instructor.Mode.JSON,)\nBuild distance calculator application using SLM for city/country recognition\nAdd image input processing\n\nDeliverable: Python application that uses function calling for structured interaction with SLMs"
  },
  {
    "objectID": "weekly_labs.html#week-10-retrieval-augmented-generation",
    "href": "weekly_labs.html#week-10-retrieval-augmented-generation",
    "title": "Edge AI Engineering - Weekly Labs",
    "section": "Week 10: Retrieval-Augmented Generation",
    "text": "Week 10: Retrieval-Augmented Generation\n\nLab 19: RAG Fundamentals\nObjectives:\n\nUnderstand RAG architecture\nCreate vector database\nImplement embedding generation\nBuild simple RAG system\n\nInstructions:\n\nInstall required libraries:\npip install langchain chromadb\nCreate a simple dataset with text documents\nImplement document splitting and chunking\nGenerate embeddings using Ollama\nStore embeddings in ChromaDB\nCreate query system\n\nDeliverable: Python implementation of a basic RAG system with simple text documents\n\n\nLab 20: Advanced RAG\nObjectives:\n\nOptimize RAG for edge devices\nImplement more efficient retrieval\nCreate a specialized knowledge base\nBuild validation mechanisms\n\nInstructions:\n\nCreate a specialized knowledge base (e.g., technical documentation)\nImplement optimized embedding generation\nFine-tune retrieval parameters\nAdd response validation\nCreate a persistent vector store\nBenchmark performance\n\nDeliverable: Optimized RAG implementation with specialized knowledge base and performance analysis"
  },
  {
    "objectID": "weekly_labs.html#week-11-vision-language-models",
    "href": "weekly_labs.html#week-11-vision-language-models",
    "title": "Edge AI Engineering - Weekly Labs",
    "section": "Week 11: Vision-Language Models",
    "text": "Week 11: Vision-Language Models\n\nLab 21: Florence-2 Setup\nObjectives:\n\nInstall Florence-2 model\nConfigure environment\nRun basic inference tests\nUnderstand model capabilities\n\nInstructions:\n\nInstall required dependencies:\npip install transformers torch torchvision torchaudio\npip install timm einops\npip install autodistill-florence-2\nDownload model and test basic functionality\nRun image captioning test\nMeasure performance (memory usage, inference time)\n\nDeliverable: Python script demonstrating basic Florence-2 functionality with performance metrics\n\n\nLab 22: Vision Tasks with Florence-2\nObjectives:\n\nImplement various vision tasks\nCreate applications for captioning, detection, grounding\nOptimize performance\nCombine tasks\n\nInstructions:\n\nImplement image captioning:\n\nBasic caption generation\nDetailed caption generation\n\nImplement object detection:\n\nBounding box visualization\nMultiple object detection\n\nImplement visual grounding:\n\nHighlight specific objects based on text prompts\n\nCreate segmentation application\nMeasure the performance of each task\n\nDeliverable: Python application demonstrating multiple vision tasks with Florence-2 and performance analysis"
  },
  {
    "objectID": "weekly_labs.html#week-12-physical-computing-basics",
    "href": "weekly_labs.html#week-12-physical-computing-basics",
    "title": "Edge AI Engineering - Weekly Labs",
    "section": "Week 12: Physical Computing Basics",
    "text": "Week 12: Physical Computing Basics\n\nLab 23: Sensor and Actuator Integration\nObjectives:\n\nConnect digital sensors\nRead environmental data\nControl LEDs and actuators\nCreate a data collection system\n\nInstructions:\n\nConnect hardware components:\n\nDHT22 temperature/humidity sensor\nBMP280 pressure sensor\nLEDs (red, yellow, green)\nPush button\n\nInstall required libraries:\npip install adafruit-circuitpython-dht adafruit-circuitpython-bmp280\nCreate a Python script to read sensor data\nImplement LED control based on conditions\nCreate visualization of sensor data\n\nDeliverable: Python application for reading sensor data and controlling actuators with visualization\n\n\nLab 24: Jupyter Notebook Integration\nObjectives:\n\nUse Jupyter Notebook for physical computing\nCreate interactive widgets\nVisualize sensor data in real-time\nControl actuators from a notebook\n\nInstructions:\n\nInstall ipywidgets: pip install ipywidgets\nCreate a Jupyter Notebook for sensor data collection\nImplement interactive widgets for control\nCreate real-time visualization\nBuild a dashboard with multiple data views\n\nDeliverable: Jupyter Notebook with interactive widgets for sensor monitoring and actuator control"
  },
  {
    "objectID": "weekly_labs.html#week-13-slm-physical-computing-integration",
    "href": "weekly_labs.html#week-13-slm-physical-computing-integration",
    "title": "Edge AI Engineering - Weekly Labs",
    "section": "Week 13: SLM-Physical Computing Integration",
    "text": "Week 13: SLM-Physical Computing Integration\n\nLab 25: Basic SLM Analysis\nObjectives:\n\nIntegrate SLMs with sensor data\nCreate analysis application\nImplement decision-making logic\nControl actuators based on SLM responses\n\nInstructions:\n\nCreate a Python application that:\n\nCollects sensor data\nFormats data for SLM prompt\nSends prompt to model\nParses response\nControls actuators based on response\n\nImplement multiple analysis modes\nAdd error handling for SLM responses\n\nDeliverable: Python application integrating SLMs with physical sensors and actuators\n\n\nLab 26: SLM-IoT Control System\nObjectives:\n\nCreate a complete IoT monitoring system\nImplement natural language interaction\nAdd data logging and analysis\nCreate web interface\n\nInstructions:\n\nBuild a complete system with:\n\nSensor data collection\nSLM-based analysis\nNatural language command processing\nData logging to the database\nWeb interface for interaction\n\nImplement multiple SLM models\nAdd historical data analysis\nCreate visualization dashboard\n\nDeliverable: Complete IoT monitoring system with SLM integration and web interface"
  },
  {
    "objectID": "weekly_labs.html#week-14-advanced-edge-ai-techniques",
    "href": "weekly_labs.html#week-14-advanced-edge-ai-techniques",
    "title": "Edge AI Engineering - Weekly Labs",
    "section": "Week 14: Advanced Edge AI Techniques",
    "text": "Week 14: Advanced Edge AI Techniques\n\nLab 27: Building Agents\nObjectives:\n\nCreate agent architecture\nImplement tool usage\nBuild decision-making system\nHandle complex tasks\n\nInstructions:\n\nImplement calculator agent:\n\nCreate a query routing system\nImplement tool functions\nBuild decision-making logic\n\nCreate knowledge router:\n\nImplement web search integration\nBuild classification system\nHandle time-based queries\n\nMeasure and optimize performance\n\nDeliverable: Python implementation of agent architecture with tool usage and decision routing\n\n\nLab 28: Advanced Prompting and Validation\nObjectives:\n\nImplement chain-of-thought prompting\nCreate few-shot learning examples\nBuild task decomposition system\nImplement response validation\n\nInstructions:\n\nCreate examples for different prompting strategies\nImplement chain-of-thought framework\nBuild few-shot learning templates\nCreate a task decomposition system\nImplement validation mechanisms\nCompare the effectiveness of different strategies\n\nDeliverable: Python implementation demonstrating different prompting strategies with performance comparison"
  },
  {
    "objectID": "weekly_labs.html#week-15-final-project-integration",
    "href": "weekly_labs.html#week-15-final-project-integration",
    "title": "Edge AI Engineering - Weekly Labs",
    "section": "Week 15: Final Project Integration",
    "text": "Week 15: Final Project Integration\n\nLab 29: Agentic RAG System\nObjectives:\n\nCombine agent architecture with RAG\nCreate a complete knowledge system\nImplement advanced validation\nBuild query optimization\n\nInstructions:\n\nCreate a complete agentic RAG system:\n\nBuild knowledge database\nImplement agent architecture\nAdd tool functions\nCreate validation mechanisms\nOptimize retrieval\n\nTest with complex queries\nMeasure performance\nCreate visualization of system components\n\nDeliverable: Complete agentic RAG system with documentation and performance analysis\n\n\nLab 30: Final Project\nObjectives:\n\nDesign and implement a comprehensive Edge AI system\nCombine multiple techniques\nCreate complete documentation\nPresent project\n\nInstructions:\n\nDesign final project combining:\n\nComputer vision capabilities\nSLM integration\nPhysical computing\nAdvanced techniques (RAG, agents, etc.)\n\nImplement complete system\nCreate documentation\nMeasure performance\nPrepare presentation\n\nDeliverable: Complete final project with documentation, code, and presentation"
  },
  {
    "objectID": "weekly_labs.html#hardware-requirements",
    "href": "weekly_labs.html#hardware-requirements",
    "title": "Edge AI Engineering - Weekly Labs",
    "section": "Hardware Requirements",
    "text": "Hardware Requirements\n\nBasic Setup (Weeks 1-7)\n\nRaspberry Pi Zero 2W or Pi 5\nMicroSD card (32GB+)\nCamera module (USB webcam or Pi camera)\nPower supply\n\n\n\nGenerative AI (Weeks 8-15)\n\nRaspberry Pi 5 (8GB RAM recommended)\nActive cooling solution\nMicroSD card (64GB+ recommended)\n\n\n\nPhysical Computing (Weeks 12-15)\n\nDHT22 temperature/humidity sensor\nBMP280 pressure/temperature sensor\nLEDs (red, yellow, green)\nPush button\nResistors (4.7kΩ, 330Ω)\nJumper wires\nBreadboard"
  },
  {
    "objectID": "weekly_labs.html#software-requirements",
    "href": "weekly_labs.html#software-requirements",
    "title": "Edge AI Engineering - Weekly Labs",
    "section": "Software Requirements",
    "text": "Software Requirements\n\nDevelopment Environment\n\nRaspberry Pi OS (64-bit)\nPython 3.9+\nJupyter Notebook\nSSH client\n\n\n\nComputer Vision and DL\n\nTensorFlow Lite runtime\nOpenCV\nEdge Impulse\nUltralytics\n\n\n\nGenerative AI\n\nOllama\nTransformers\nPytorch\nChromaDB\nLangChain\nPydantic\nInstructor\n\n\n\nPhysical Computing\n\nGPIO Zero\nAdafruit CircuitPython libraries"
  },
  {
    "objectID": "weekly_labs.html#assessment-criteria",
    "href": "weekly_labs.html#assessment-criteria",
    "title": "Edge AI Engineering - Weekly Labs",
    "section": "Assessment Criteria",
    "text": "Assessment Criteria\nEach lab will be evaluated based on:\n\nFunctionality (40%): Does the implementation work as specified?\nCode Quality (20%): Is the code well-structured, documented, and efficient?\nDocumentation (20%): Are the process and results documented?\nAnalysis (20%): Is there a thoughtful analysis of results and performance?\n\nThe final project will be evaluated based on:\n\nIntegration (30%): How well different components are integrated\nInnovation (20%): Novel approaches or applications\nImplementation (30%): Overall quality and functionality\nPresentation (20%): Clear explanation and demonstration"
  },
  {
    "objectID": "weekly_labs.html#tips-for-success",
    "href": "weekly_labs.html#tips-for-success",
    "title": "Edge AI Engineering - Weekly Labs",
    "section": "Tips for Success",
    "text": "Tips for Success\n\nStart Early: These labs build on each other. Falling behind makes later labs more difficult.\nDocument As You Go: Take notes, screenshots, and document issues/solutions.\nOptimize Resources: SLMs and VLMs require careful resource management.\nCollaborate: Discuss approaches with classmates while ensuring individual work.\nBackup Regularly: Create backups of your SD card after significant progress.\nMeasure Performance: Always benchmark and optimize your implementations.\nAsk Questions: If you’re stuck, ask for help early rather than falling behind."
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "TinyML4D\nTinyML Made Easy, an eBook collection of a series of Hands-On tutorials, is part of the TinyML4D, an initiative to make Embedded Machine Learning (TinyML) education available to everyone, explicitly enabling innovative solutions for the unique challenges Developing Countries face."
  },
  {
    "objectID": "references.html#to-learn-more",
    "href": "references.html#to-learn-more",
    "title": "References",
    "section": "To learn more:",
    "text": "To learn more:\n\nOnline Courses\n\nHarvard School of Engineering and Applied Sciences - CS249r: Tiny Machine Learning\nProfessional Certificate in Tiny Machine Learning (TinyML) – edX/Harvard\nIntroduction to Embedded Machine Learning - Coursera/Edge Impulse\nComputer Vision with Embedded Machine Learning - Coursera/Edge Impulse\nUNIFEI-IESTI01 TinyML: “Machine Learning for Embedding Devices”\n\n\n\nBooks\n\n“Python for Data Analysis” by Wes McKinney\n“Deep Learning with Python” by François Chollet - GitHub Notebooks\n“TinyML” by Pete Warden and Daniel Situnayake\n“TinyML Cookbook 2nd Edition” by Gian Marco Iodice\n“Technical Strategy for AI Engineers, In the Era of Deep Learning” by Andrew Ng\n“AI at the Edge” book by Daniel Situnayake and Jenny Plunkett\n“XIAO: Big Power, Small Board” by Lei Feng and Marcelo Rovai\n“Machine Learning Systems” by Vijay Janapa Reddi\n\n\n\nProjects Repository\n\nEdge Impulse Expert Network"
  },
  {
    "objectID": "about_the_author.html",
    "href": "about_the_author.html",
    "title": "About the author",
    "section": "",
    "text": "Marcelo Rovai, a Brazilian living in Chile, is a recognized figure in engineering and technology education. He holds the title of Professor Honoris Causa from the Federal University of Itajubá (UNIFEI), Brazil. His educational background includes an Engineering degree from UNIFEI and a specialization from the Polytechnic School of São Paulo University (POLI/USP). Further enhancing his expertise, he earned an MBA from IBMEC (INSPER) and a Master’s in Data Science from the Universidad del Desarrollo (UDD) in Chile.\nWith a career spanning several high-profile technology companies, including AVIBRAS Airspace, AT&T, NCR, and IGT, where he served as Vice President for Latin America, he brings industry experience to his academic endeavors. He is a prolific writer on electronics-related topics and shares his knowledge through open platforms like Hackster.io.\nIn addition to his professional pursuits, he is dedicated to educational outreach, serving as a volunteer professor at UNIFEI and engaging with the TinyML4D group and the EDGE AIP– the Academia-Industry Partnership of EDGEAI Foundation as a Co-Chair, promoting TinyML education in developing countries. His work underscores a commitment to leveraging technology for societal advancement.\nLinkedIn profile: https://www.linkedin.com/in/marcelo-jose-rovai-brazil-chile/\nLectures, books, papers, and tutorials: https://github.com/Mjrovai/TinyML4D"
  }
]