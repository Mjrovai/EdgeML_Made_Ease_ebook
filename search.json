[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "EdgeML Made Easy",
    "section": "",
    "text": "Preface\nIn the rapidly evolving landscape of technology, the convergence of artificial intelligence and edge computing stands as one of the most exciting frontiers. This intersection promises to revolutionize how we interact with the world around us, bringing intelligence and decision-making capabilities directly to the devices we use every day. At the heart of this revolution lies the Raspberry Pi, a powerful yet accessible single-board computer that has democratized computing and now stands poised to do the same for edge AI.\nThe journey to this book began with a simple question: How can we make advanced machine learning accessible to everyone, not just those with access to powerful cloud resources or specialized hardware? The answer we found was the Raspberry Pi, which is the size of a credit card.\n“EdgeML Made Easy: Hands-On with the Raspberry Pi” is born out of a passion for technology and a belief in its power to solve real-world problems. It represents countless hours of experimentation, learning, and teaching, distilled into a format that we hope will inspire and empower you to explore the fascinating world of edge AI.\nThis book is not just about theory or abstract concepts. It’s about getting your hands dirty, writing code, training models, and seeing your creations come to life. We’ve designed each chapter to blend foundational knowledge and practical application, always with an eye toward what’s possible on the Raspberry Pi platform.\nFrom the compact Raspberry Pi Zero to the more powerful Pi 5, we explore how these incredible devices can become the brains of intelligent systems—recognizing images, understanding speech, detecting objects, and even running small language models. Each project in this book is a stepping stone, building your skills and confidence as you progress.\nBut beyond the technical skills, we hope this book instills something more valuable – a sense of curiosity and possibility. The field of edge AI is still in its infancy, with new applications and techniques emerging daily. By mastering the fundamentals presented here, you’ll be well-equipped to explore these frontiers, perhaps even pushing the boundaries of what’s possible on edge devices.\nWhether you’re a student looking to understand the practical applications of AI, a professional seeking to expand your skill set, or an enthusiast eager to bring intelligence to your projects, we hope this book serves as both a guide and an inspiration.\nAs you embark on this journey, remember that every expert was once a beginner. The learning path is filled with challenges and moments of joy and discovery. Embrace both, and let your creativity guide you.\nThank you for joining us on this exciting adventure into edge machine learning. Let’s begin exploring what’s possible when we bring AI to the edge, one Raspberry Pi at a time.\nHappy coding, and may your models always converge!\nProf. Marcelo Rovai August, 2024"
  },
  {
    "objectID": "Acknowledgements.html",
    "href": "Acknowledgements.html",
    "title": "Acknowledgments",
    "section": "",
    "text": "We extend our deepest gratitude to the entire TinyML4D Academic Network, comprised of distinguished professors, researchers, and professionals. Notable contributions from Marco Zennaro, Ermanno Petrosemoli, Brian Plancher, José Alberto Ferreira, Jesus Lopez, Diego Mendez, Shawn Hymel, Dan Situnayake, Pete Warden, and Laurence Moroney have been instrumental in advancing our understanding of Embedded Machine Learning (TinyML).\nSpecial commendation is reserved for Professor Vijay Janapa Reddi of Harvard University. His steadfast belief in the transformative potential of open-source communities, coupled with his invaluable guidance and teachings, has served as a beacon for our efforts from the very beginning.\nAcknowledging these individuals, we pay tribute to the collective wisdom and dedication that have enriched this field and our work.\n\nIllustrative images on the e-book and chapter’s covers generated by OpenAI’s DALL-E via ChatGPT"
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "Introduction",
    "section": "",
    "text": "In the rapidly evolving landscape of technology, the convergence of artificial intelligence and edge computing is revolutionizing how we interact with and understand the world around us. At the forefront of this transformation is the Raspberry Pi, a powerful yet accessible single-board computer that has become a cornerstone for innovators, educators, and hobbyists alike.\n“EdgeML Made Easy: Hands-On with the Raspberry Pi” is designed to bridge the gap between complex machine learning concepts and practical, real-world applications using the Raspberry Pi platform. This book is your guide to harnessing the power of edge AI, bringing sophisticated machine learning capabilities directly to where data is generated and actions are taken.\nWhy Raspberry Pi for Edge ML?\nThe Raspberry Pi, with its compact form factor, robust processing capabilities, and vibrant community support, offers an ideal platform for exploring and implementing edge machine learning solutions. Unlike more constrained microcontrollers, the Raspberry Pi provides:\n\nSufficient computational power to run complex ML models\nA full-fledged operating system, enabling easier development and deployment\nExtensive connectivity options, facilitating integration with various sensors and actuators\nA rich ecosystem of libraries and tools optimized for machine learning tasks\n\nThis book will explore leveraging these advantages to implement cutting-edge ML applications, from image classification and object detection to pose estimation and natural language processing.\nWhat You’ll Learn\nThis hands-on guide will take you through the entire process of developing ML applications on the Raspberry Pi:\n\nSetting up your Raspberry Pi for ML development\nCollecting and preparing data for various ML tasks\nTraining and optimizing models for edge deployment\nImplementing real-time inference on the Raspberry Pi\nBuilding practical projects that combine multiple ML techniques\n\nWhether you’re a student, an educator, a maker, or a professional looking to expand your skills, this book provides the knowledge and practical experience needed to bring your ML ideas to life on the Raspberry Pi platform.\nEmpowering Innovation at the Edge\nBy the end of this journey, you’ll be equipped with the skills to create intelligent, responsive systems that can see, understand, and interact with their environment. From smart cameras and voice assistants to industrial automation and IoT solutions, the possibilities are limited only by your imagination.\nJoin us as we explore the exciting intersection of machine learning and edge computing, and discover how the Raspberry Pi can become your gateway to innovating at the edge. Let’s embark on this journey to make edge ML accessible, practical, and, above all, impactful in solving real-world challenges."
  },
  {
    "objectID": "about_book.html",
    "href": "about_book.html",
    "title": "About this Book",
    "section": "",
    "text": "This book is part of the open book Machine Learning Systems, which we invite you to read.\n“EdgeML Made Easy: Hands-On with the Raspberry Pi” is an accessible, practical guide designed to empower readers with the knowledge and skills needed to implement machine learning at the edge using the Raspberry Pi platform. This book is part of the open-source Machine Learning Systems initiative, which aims to democratize AI education and application.\nKey Features:\n\nPractical Approach: Each chapter is built around hands-on projects demonstrating real-world applications of edge ML on Raspberry Pi.\nProgressive Learning: The book starts with fundamental concepts and gradually progresses to more advanced topics, ensuring a smooth learning curve for readers of various skill levels.\nRaspberry Pi Focus: All examples and projects are optimized for various Raspberry Pi models, including the Pi Zero 2W, Pi 4, and Pi 5, highlighting each model’s unique capabilities.\nComprehensive Coverage: From image processing and computer vision to natural language processing and sensor data analysis, this book covers various ML applications relevant to edge computing.\nOpen-Source Tools: We emphasize using open-source frameworks and libraries, such as Edge Impulse Studio, TensorFlow Lite, OpenCV, and PyTorch, ensuring accessibility and continuity in your learning journey.\nResource Optimization: Learn techniques to optimize ML models for the constrained resources of edge devices, balancing performance with efficiency.\nDeployment Ready: Gain insights into best practices for deploying and maintaining ML models on Raspberry Pi in production environments.\n\nWho This Book Is For:\n\nStudents and educators in computer science, engineering, and related fields\nHobbyists and makers interested in adding AI capabilities to their projects\nProfessionals looking to implement edge AI solutions in various industries.\nResearchers exploring the intersection of IoT, edge computing, and machine learning\n\nPrerequisites:\nWhile this book is designed to be accessible to a broad audience, readers will benefit from:\n\nBasic familiarity with Python programming\nFundamental understanding of machine learning concepts\nExperience with Raspberry Pi or similar single-board computers (helpful but not required)\n\nStructure of the Book:\nThe book is divided into chapters, each focusing on a specific aspect of edge ML on Raspberry Pi. Every chapter includes:\n\nTheoretical background to understand the concepts\nStep-by-step tutorials for implementing ML models\nPractical projects that apply the learned techniques\nTips for troubleshooting and optimizing performance\nSuggestions for further exploration and experimentation\n\nBy the end of this book, you’ll have a solid foundation in implementing various ML applications on Raspberry Pi, and the confidence to tackle your own edge AI projects. Whether you’re looking to create smart home devices, develop intelligent monitoring systems, or explore the frontiers of edge computing, “EdgeML Made Easy” will be your trusted companion on this exciting journey."
  },
  {
    "objectID": "raspi/setup/setup.html#introduction",
    "href": "raspi/setup/setup.html#introduction",
    "title": "Setup",
    "section": "Introduction",
    "text": "Introduction\nThe Raspberry Pi is a powerful and versatile single-board computer that has become an essential tool for engineers across various disciplines. Developed by the Raspberry Pi Foundation, these compact devices offer a unique combination of affordability, computational power, and extensive GPIO (General Purpose Input/Output) capabilities, making them ideal for prototyping, embedded systems development, and advanced engineering projects.\n\nKey Features\n\nComputational Power: Despite their small size, Raspberry Pis offers significant processing capabilities, with the latest models featuring multi-core ARM processors and up to 8GB of RAM.\nGPIO Interface: The 40-pin GPIO header allows direct interaction with sensors, actuators, and other electronic components, facilitating hardware-software integration projects.\nExtensive Connectivity: Built-in Wi-Fi, Bluetooth, Ethernet, and multiple USB ports enable diverse communication and networking projects.\nLow-Level Hardware Access: Raspberry Pis provides access to interfaces like I2C, SPI, and UART, allowing for detailed control and communication with external devices.\nReal-Time Capabilities: With proper configuration, Raspberry Pis can be used for soft real-time applications, making them suitable for control systems and signal processing tasks.\nPower Efficiency: Low power consumption enables battery-powered and energy-efficient designs, especially in models like the Pi Zero.\n\n\n\nRaspberry Pi Models (covered in this book)\n\nRaspberry Pi Zero 2 W (Raspi-Zero):\n\nIdeal for: Compact embedded systems\nKey specs: 1GHz single-core CPU (ARM Cortex-A53), 512MB RAM, minimal power consumption\n\nRaspberry Pi 5 (Raspi-5):\n\nIdeal for: More demanding applications such as edge computing, computer vision, and edgeAI applications, including LLMs.\nKey specs: 2.4GHz quad-core CPU (ARM Cortex A-76), up to 8GB RAM, PCIe interface for expansions\n\n\n\n\nEngineering Applications\n\nEmbedded Systems Design: Develop and prototype embedded systems for real-world applications.\nIoT and Networked Devices: Create interconnected devices and explore protocols like MQTT, CoAP, and HTTP/HTTPS.\nControl Systems: Implement feedback control loops, PID controllers, and interface with actuators.\nComputer Vision and AI: Utilize libraries like OpenCV and TensorFlow Lite for image processing and machine learning at the edge.\nData Acquisition and Analysis: Collect sensor data, perform real-time analysis, and create data logging systems.\nRobotics: Build robot controllers, implement motion planning algorithms, and interface with motor drivers.\nSignal Processing: Perform real-time signal analysis, filtering, and DSP applications.\nNetwork Security: Set up VPNs, firewalls, and explore network penetration testing.\n\nThis tutorial will guide you through setting up the most common Raspberry Pi models, enabling you to start on your machine learning project quickly. We’ll cover hardware setup, operating system installation, and initial configuration, focusing on preparing your Pi for Machine Learning applications."
  },
  {
    "objectID": "raspi/setup/setup.html#hardware-overview",
    "href": "raspi/setup/setup.html#hardware-overview",
    "title": "Setup",
    "section": "Hardware Overview",
    "text": "Hardware Overview\n\nRaspberry Pi Zero 2W\n\n\nProcessor: 1GHz quad-core 64-bit Arm Cortex-A53 CPU\nRAM: 512MB SDRAM\nWireless: 2.4GHz 802.11 b/g/n wireless LAN, Bluetooth 4.2, BLE\nPorts: Mini HDMI, micro USB OTG, CSI-2 camera connector\nPower: 5V via micro USB port\n\n\n\nRaspberry Pi 5\n\n\nProcessor:\n\nPi 5: Quad-core 64-bit Arm Cortex-A76 CPU @ 2.4GHz\nPi 4: Quad-core Cortex-A72 (ARM v8) 64-bit SoC @ 1.5GHz\n\nRAM: 2GB, 4GB, or 8GB options (8GB recommended for AI tasks)\nWireless: Dual-band 802.11ac wireless, Bluetooth 5.0\nPorts: 2 × micro HDMI ports, 2 × USB 3.0 ports, 2 × USB 2.0 ports, CSI camera port, DSI display port\nPower: 5V DC via USB-C connector (3A)"
  },
  {
    "objectID": "raspi/setup/setup.html#installing-the-operating-system",
    "href": "raspi/setup/setup.html#installing-the-operating-system",
    "title": "Setup",
    "section": "Installing the Operating System",
    "text": "Installing the Operating System\n\nThe Operating System (OS)\nAn operating system (OS) is fundamental software that manages computer hardware and software resources, providing standard services for computer programs. It is the core software that runs on a computer, acting as an intermediary between hardware and application software. The OS manages the computer’s memory, processes, device drivers, files, and security protocols.\n\nKey functions:\n\nProcess management: Allocating CPU time to different programs\nMemory management: Allocating and freeing up memory as needed\nFile system management: Organizing and keeping track of files and directories\nDevice management: Communicating with connected hardware devices\nUser interface: Providing a way for users to interact with the computer\n\nComponents:\n\nKernel: The core of the OS that manages hardware resources\nShell: The user interface for interacting with the OS\nFile system: Organizes and manages data storage\nDevice drivers: Software that allows the OS to communicate with hardware\n\n\nThe Raspberry Pi runs a specialized version of Linux designed for embedded systems. This operating system, typically a variant of Debian called Raspberry Pi OS (formerly Raspbian), is optimized for the Pi’s ARM-based architecture and limited resources.\n\nThe latest version of Raspberry Pi OS is based on Debian Bookworm.\n\nKey features:\n\nLightweight: Tailored to run efficiently on the Pi’s hardware.\nVersatile: Supports a wide range of applications and programming languages.\nOpen-source: Allows for customization and community-driven improvements.\nGPIO support: Enables interaction with sensors and other hardware through the Pi’s pins.\nRegular updates: Continuously improved for performance and security.\n\nEmbedded Linux on the Raspberry Pi provides a full-featured operating system in a compact package, making it ideal for projects ranging from simple IoT devices to more complex edge machine-learning applications. Its compatibility with standard Linux tools and libraries makes it a powerful platform for development and experimentation.\n\n\nInstallation\nTo use the Raspberry Pi, we will need an operating system. By default, Raspberry Pis checks for an operating system on any SD card inserted in the slot, so we should install an operating system using Raspberry Pi Imager.\nRaspberry Pi Imager is a tool for downloading and writing images on macOS, Windows, and Linux. It includes many popular operating system images for Raspberry Pi. We will also use the Imager to preconfigure credentials and remote access settings.\nFollow the steps to install the OS in your Raspi.\n\nDownload and install the Raspberry Pi Imager on your computer.\nInsert a microSD card into your computer (a 32GB SD card is recommended) .\nOpen Raspberry Pi Imager and select your Raspberry Pi model.\nChoose the appropriate operating system:\n\nFor Raspi-Zero: For example, you can select: Raspberry Pi OS Lite (64-bit).\n\n\n\nDue to its reduced SDRAM (512MB), the recommended OS for the Rasp Zero is the 32-bit version. However, to run some machine learning models, such as the YOLOv8 from Ultralitics, we should use the 64-bit version. Although Raspi-Zero can run a desktop, we will choose the LITE version (no Desktop) to reduce the RAM needed for regular operation.\n\n\nFor Raspi-5: We can select the full 64-bit version, which includes a desktop: Raspberry Pi OS (64-bit)\n\n\nSelect your microSD card as the storage device.\nClick on Next and then the gear icon to access advanced options.\nSet the hostname, the Raspi username and password, configure WiFi and enable SSH (Very important!)\n\n\n\nWrite the image to the microSD card.\n\n\nIn the examples here, we will use different hostnames: raspi, raspi-5, raspi-Zero, etc. You should replace by the one that you are using.\n\n\n\nInitial Configuration\n\nInsert the microSD card into your Raspberry Pi.\nConnect power to boot up the Raspberry Pi.\nPlease wait for the initial boot process to complete (it may take a few minutes).\n\n\nYou can find the most common Linux commands to be used with the Raspi here or here."
  },
  {
    "objectID": "raspi/setup/setup.html#remote-access",
    "href": "raspi/setup/setup.html#remote-access",
    "title": "Setup",
    "section": "Remote Access",
    "text": "Remote Access\n\nSSH Access\nThe easiest way to interact with the Rasp-Zero is via SSH (“Headless”). You can use a Terminal (MAC/Linux), PuTTy (Windows), or any other.\n\nFind your Raspberry Pi’s IP address (for example, check your router).\nOn your computer, open a terminal and connect via SSH:\nssh username@[raspberry_pi_ip_address]   \nAlternatively, if you do not have the IP address, you can try the following: shell ssh username@hostname.local for example, ssh mjrovai@rpi-5.local , ssh mjrovai@raspi.local , etc.\n\nWhen you see the prompt:\nmjrovai@rpi-5:~ $\nIt means that you are interacting remotely with your Raspi. It is a good practice to update/upgrade the system regularly. For that, you should run:\nsudo apt-get update\nsudo apt upgrade\nYou should confirm the Raspi IP address. On the terminal, you can use:\nhostname -I\n\n\n\n\nTo shut down the Raspi via terminal:\nWhen you want to turn off your Raspberry Pi, there are better ideas than just pulling the power cord. This is because the Raspi may still be writing data to the SD card, in which case merely powering down may result in data loss or, even worse, a corrupted SD card.\nFor safety shut down, use the command line:\nsudo shutdown -h now\n\nTo avoid possible data loss and SD card corruption, before removing the power, you should wait a few seconds after shutdown for the Raspberry Pi’s LED to stop blinking and go dark. Once the LED goes out, it’s safe to power down.\n\n\n\nTransfer Files between the Raspi and a computer\nTransferring files between the Raspi and our main computer can be done using a pen drive, directly on the terminal (with scp), or an FTP program over the network.\n\nUsing Secure Copy Protocol (scp):\n\nCopy files to your Raspberry Pi\nLet’s create a text file on our computer, for example, test.txt.\n\n\nYou can use any text editor. In the same terminal, an option is the nano.\n\nTo copy the file named test.txt from your personal computer to a user’s home folder on your Raspberry Pi, run the following command from the directory containing test.txt, replacing the &lt;username&gt; placeholder with the username you use to log in to your Raspberry Pi and the &lt;pi_ip_address&gt; placeholder with your Raspberry Pi’s IP address:\n$ scp test.txt &lt;username&gt;@&lt;pi_ip_address&gt;:~/\n\nNote that ~/ means that we will move the file to the ROOT of our Raspi. You can choose any folder in your Raspi. But you should create the folder before you run scp, since scp won’t create folders automatically.\n\nFor example, let’s transfer the file test.txt to the ROOT of my Raspi-zero, which has an IP of 192.168.4.210:\nscp test.txt mjrovai@192.168.4.210:~/\n\nI use a different shell profile to differentiate the terminals. The above action happens on your computer. Now, let’s go to our Raspi (using the SSH) and check if the file is there:\n\n\n\nCopy files from your Raspberry Pi\nTo copy a file named test.txt from a user’s home directory on a Raspberry Pi to the current directory on another computer, run the following command on your Host Computer:\n$ scp &lt;username&gt;@&lt;pi_ip_address&gt;:myfile.txt .\nFor example:\nOn the Raspi, let’s create a copy of the file with another name:\ncp test.txt test_2.txt\nAnd on the Host Computer (in my case, a Mac)\nscp mjrovai@192.168.4.210:test_2.txt .\n\n\n\n\nTransferring files using FTP\nTransferring files using FTP, such as FileZilla FTP Client, is also possible. Follow the instructions, install the program for your Desktop OS, and use the Raspi IP address as the Host. For example:\nsftp://192.168.4.210\nand enter your Raspi username and password. Pressing Quickconnect will open two windows, one for your host computer desktop (right) and another for the Raspi (left)."
  },
  {
    "objectID": "raspi/setup/setup.html#increasing-swap-memory",
    "href": "raspi/setup/setup.html#increasing-swap-memory",
    "title": "Setup",
    "section": "Increasing SWAP Memory",
    "text": "Increasing SWAP Memory\nUsing htop, a cross-platform interactive process viewer, you can easily monitor the resources running on your Raspi, such as the list of processes, the running CPUs, and the memory used in real-time. To lunch hop, enter with the command on the terminal:\nhtop\n\nRegarding memory, among the devices in the Raspberry Pi family, the Raspi-Zero has the smallest amount of SRAM (500MB), compared to a selection of 2GB to 8GB on the Raspis 4 or 5. For any Raspi, it is possible to increase the memory available to the system with “Swap.” Swap memory, also known as swap space, is a technique used in computer operating systems to temporarily store data from RAM (Random Access Memory) on the SD card when the physical RAM is fully utilized. This allows the operating system (OS) to continue running even when RAM is full, which can prevent system crashes or slowdowns.\nSwap memory benefits devices with limited RAM, such as the Raspi-Zero. Increasing swap can help run more demanding applications or processes, but it’s essential to balance this with the potential performance impact of frequent disk access.\nBy default, the Rapi-Zero’s SWAP (Swp) memory is only 100MB, which is very small for running some more complex and demanding Machine Learning applications (for example, YOLO). Let’s increase it to 2MB:\nFirst, turn off swap-file:\nsudo dphys-swapfile swapoff\nNext, you should open and change the file /etc/dphys-swapfile. For that, we will use the nano:\nsudo nano /etc/dphys-swapfile\nSearch for the CONF_SWAPSIZE variable (default is 200) and update it to 2000:\nCONF_SWAPSIZE=2000\nAnd save the file.\nNext, turn on the swapfile again and reboot the Rasp-zero:\nsudo dphys-swapfile setup\nsudo dphys-swapfile swapon\nsudo reboot\nWhen your device is rebooted (you should enter with the SSH again), you will realize that the maximum swap memory value shown on top is now something near 2GB (in my case, 1.95GB).\n\nTo keep the htop running, you should open another terminal window to interact continuously with your Raspi."
  },
  {
    "objectID": "raspi/setup/setup.html#installing-a-camera",
    "href": "raspi/setup/setup.html#installing-a-camera",
    "title": "Setup",
    "section": "Installing a Camera",
    "text": "Installing a Camera\nThe Raspi is an excellent device for computer vision applications; a camera is needed for it. We can install a standard USB webcam on the micro-USB port using a USB OTG adapter (Raspi-Zero and Rasp-5) or a camera module connected to the Raspi CSI (Camera Serial Interface) port.\n\nUSB Webcams generally have inferior quality to the camera modules that connect to the CSI port. They can also not be controlled using the raspistill and rasivid commands in the terminal or the picamera recording package in Python. Nevertheless, there may be reasons why you want to connect a USB camera to your Raspberry Pi, such as because of the benefit that it is much easier to set up multiple cameras with a single Raspberry Pi, long cables, or simply because you have such a camera on hand.\n\n\nInstalling a USB WebCam\n\nPower off the Raspi:\n\nsudo shutdown -h no\n\nConnect the USB Webcam (USB Camera Module 30fps,1280x720) to your Raspi (In this example, I am using the Raspi-Zero, but the instructions work for all Raspis).\n\n\n\nPower on again and run the SSH\nTo check if your USB camera is recognized, run:\n\nlsusb\nYou should see your camera listed in the output.\n\n\nTo take a test picture with your USB camera, use:\n\nfswebcam test_image.jpg\nThis will save an image named “test_image.jpg” in your current directory.\n\n\nSince we are using SSH to connect to our Rapsi, we must transfer the image to our main computer so we can view it. We can use FileZilla or SCP for this:\n\nOpen a terminal on your host computer and run:\nscp mjrovai@raspi-zero.local:~/test_image.jpg .\n\nReplace “mjrovai” with your username and “raspi-zero” with Pi’s hostname.\n\n\n\nIf the image quality isn’t satisfactory, you can adjust various settings; for example, define a resolution that is suitable for YOLO (640x640):\n\nfswebcam -r 640x640 --no-banner test_image_yolo.jpg\nThis captures a higher-resolution image without the default banner.\n\nAn ordinary USB Webcam can also be used:\n\nAnd verified using lsusb\n\n\nVideo Streaming\nFor stream video (which is more resource-intensive), we can install and use mjpg-streamer:\nFirst, install Git:\nsudo apt install git\nNow, we should install the necessary dependencies for mjpg-streamer, clone the repository, and proceed with the installation:\nsudo apt install cmake libjpeg62-turbo-dev\ngit clone https://github.com/jacksonliam/mjpg-streamer.git\ncd mjpg-streamer/mjpg-streamer-experimental\nmake\nsudo make install\nThen start the stream with:\nmjpg_streamer -i \"input_uvc.so\" -o \"output_http.so -w ./www\"\nWe can then access the stream by opening a web browser and navigating to:\nhttp://&lt;your_pi_ip_address&gt;:8080. In my case: http://192.168.4.210:8080\nWe should see a webpage with options to view the stream. Click on the link that says “Stream” or try accessing:\nhttp://&lt;raspberry_pi_ip_address&gt;:8080/?action=stream\n\n\n\n\nInstalling a Camera Module on the CSI port\nThere are now several Raspberry Pi camera modules. The original 5-megapixel model was releasedin 2013, followed by an 8-megapixel Camera Module 2, released in 2016. The latest camera model is the 12-megapixel Camera Module 3, released in 2023.\nThe original 5MP camera (Arducam OV5647) is no longer available from Raspberry Pi but can be found from several alternative suppliers. Below is an example of such a camera on a Raspi-Zero.\n\nHere is another example of a v2 Camera Module, which has a Sony IMX219 8-megapixel sensor:\n\nAny camera module will work on the Raspis, but for that, the onfiguration.txt file must be updated:\nsudo nano /boot/firmware/config.txt\nAt the bottom of the file, for example, to use the 5MP Arducam OV5647 camera, add the line:\ndtoverlay=ov5647,cam0\nOr for the v2 module, wich has the 8MP Sony IMX219 camera:\ndtoverlay=imx219,cam0\nSave the file (CTRL+O [ENTER] CRTL+X) and reboot the Raspi:\nSudo reboot\nAfter the boot, you can see if the camera is listed:\nlibcamera-hello --list-cameras\n\n\n\nlibcamerais an open-source software library that supports camera systems directly from the Linux operating system on Arm processors. It minimizes proprietary code running on the Broadcom GPU.\n\nLet’s capture a jpeg image with a resolution of 640 x 480 for testing and save it to a file named test_cli_camera.jpg\nrpicam-jpeg --output test_cli_camera.jpg --width 640 --height 480\nif we want to see the file saved, we should use ls -f, which lists all current directory content in long format. As before, we can use scp to view the image:"
  },
  {
    "objectID": "raspi/setup/setup.html#running-the-raspi-desktop-remotely",
    "href": "raspi/setup/setup.html#running-the-raspi-desktop-remotely",
    "title": "Setup",
    "section": "Running the Raspi Desktop remotely",
    "text": "Running the Raspi Desktop remotely\nWhile we’ve primarily interacted with the Raspberry Pi using terminal commands via SSH, we can access the whole graphical desktop environment remotely if we have installed the complete Raspberry Pi OS (for example, Raspberry Pi OS (64-bit). This can be particularly useful for tasks that benefit from a visual interface. To enable this functionality, we must set up a VNC (Virtual Network Computing) server on the Raspberry Pi. Here’s how to do it:\n\nEnable the VNC Server:\n\nConnect to your Raspberry Pi via SSH.\nRun the Raspberry Pi configuration tool by entering:\nsudo raspi-config\nNavigate to Interface Options using the arrow keys.\n\n\n\nSelect VNC and Yes to enable the VNC server.\n\n\n\nExit the configuration tool, saving changes when prompted.\n\n\nInstall a VNC Viewer on Your Computer:\n\nDownload and install a VNC viewer application on your main computer. Popular options include RealVNC Viewer, TightVNC, or VNC Viewer by RealVNC. We will install VNC Viewer by RealVNC.\n\nOnce installed, you should confirm the Raspi IP address. For example, on the terminal, you can use:\nhostname -I\n\nConnect to Your Raspberry Pi:\n\nOpen your VNC viewer application.\n\n\n\nEnter your Raspberry Pi’s IP address and hostname.\nWhen prompted, enter your Raspberry Pi’s username and password.\n\n\nThe Raspberry Pi 5 Desktop should appear on your computer monitor.\n\nAdjust Display Settings (if needed):\n\nOnce connected, adjust the display resolution for optimal viewing. This can be done through the Raspberry Pi’s desktop settings or by modifying the config.txt file.\nLet’s do it using the desktop settings. Reach the menu (the Raspberry Icon at the left upper corner) and select the best screen definition for your monitor:"
  },
  {
    "objectID": "raspi/setup/setup.html#updating-and-installing-software",
    "href": "raspi/setup/setup.html#updating-and-installing-software",
    "title": "Setup",
    "section": "Updating and Installing Software",
    "text": "Updating and Installing Software\n\nUpdate your system:\nsudo apt update && sudo apt upgrade -y\nInstall essential software:\nsudo apt install python3-pip -y\nEnable pip for Python projects:\nsudo rm /usr/lib/python3.11/EXTERNALLY-MANAGED"
  },
  {
    "objectID": "raspi/setup/setup.html#model-specific-considerations",
    "href": "raspi/setup/setup.html#model-specific-considerations",
    "title": "Setup",
    "section": "Model-Specific Considerations",
    "text": "Model-Specific Considerations\n\nRaspberry Pi Zero\n\nLimited processing power, best for lightweight projects\nUse headless setup (SSH) to conserve resources.\nConsider increasing swap space for memory-intensive tasks.\n\n\n\nRaspberry Pi 4 or 5\n\nSuitable for more demanding projects, including AI and machine learning.\nCan run full desktop environment smoothly.\nFor Pi 5, consider using an active cooler for temperature management during intensive tasks.\n\nRemember to adjust your project requirements based on the specific Raspberry Pi model you’re using. The Pi Zero is great for low-power, space-constrained projects, while the Pi 4/5 models are better suited for more computationally intensive tasks."
  },
  {
    "objectID": "raspi/image_classification/image_classification.html#introduction",
    "href": "raspi/image_classification/image_classification.html#introduction",
    "title": "Image Classification",
    "section": "Introduction",
    "text": "Introduction\nImage classification is a fundamental task in computer vision that involves categorizing an image into one of several predefined classes. It’s a cornerstone of artificial intelligence, enabling machines to interpret and understand visual information in a way that mimics human perception.\nImage classification refers to assigning a label or category to an entire image based on its visual content. This task is crucial in computer vision and has numerous applications across various industries. Image classification’s importance lies in its ability to automate visual understanding tasks that would otherwise require human intervention.\n\nApplications in Real-World Scenarios\nImage classification has found its way into numerous real-world applications, revolutionizing various sectors:\n\nHealthcare: Assisting in medical image analysis, such as identifying abnormalities in X-rays or MRIs.\nAgriculture: Monitoring crop health and detecting plant diseases through aerial imagery.\nAutomotive: Enabling advanced driver assistance systems and autonomous vehicles to recognize road signs, pedestrians, and other vehicles.\nRetail: Powering visual search capabilities and automated inventory management systems.\nSecurity and Surveillance: Enhancing threat detection and facial recognition systems.\nEnvironmental Monitoring: Analyzing satellite imagery for deforestation, urban planning, and climate change studies.\n\n\n\nAdvantages of Running Classification on Edge Devices like Raspberry Pi\nImplementing image classification on edge devices such as the Raspberry Pi offers several compelling advantages:\n\nLow Latency: Processing images locally eliminates the need to send data to cloud servers, significantly reducing response times.\nOffline Functionality: Classification can be performed without an internet connection, making it suitable for remote or connectivity-challenged environments.\nPrivacy and Security: Sensitive image data remains on the local device, addressing data privacy concerns and compliance requirements.\nCost-Effectiveness: Eliminates the need for expensive cloud computing resources, especially for continuous or high-volume classification tasks.\nScalability: Enables distributed computing architectures where multiple devices can work independently or in a network.\nEnergy Efficiency: Optimized models on dedicated hardware can be more energy-efficient than cloud-based solutions, which is crucial for battery-powered or remote applications.\nCustomization: Deploying specialized or frequently updated models tailored to specific use cases is more manageable.\n\nWe can create more responsive, secure, and efficient computer vision solutions by leveraging the power of edge devices like Raspberry Pi for image classification. This approach opens up new possibilities for integrating intelligent visual processing into various applications and environments.\nIn the following sections, we’ll explore how to implement and optimize image classification on the Raspberry Pi, harnessing these advantages to create powerful and efficient computer vision systems."
  },
  {
    "objectID": "raspi/image_classification/image_classification.html#setting-up-the-environment",
    "href": "raspi/image_classification/image_classification.html#setting-up-the-environment",
    "title": "Image Classification",
    "section": "Setting Up the Environment",
    "text": "Setting Up the Environment\n\nUpdating the Raspberry Pi\nFirst, ensure your Raspberry Pi is up to date:\nsudo apt update\nsudo apt upgrade -y\n\n\nInstalling Required Libraries\nInstall the necessary libraries for image processing and machine learning:\nsudo apt install python3-pip\nsudo rm /usr/lib/python3.11/EXTERNALLY-MANAGED\npip3 install --upgrade pip\n\n\nSetting up a Virtual Environment (Optional but Recommended)\nCreate a virtual environment to manage dependencies:\npython3 -m venv ~/tflite\nsource ~/tflite/bin/activate\n\n\nInstalling TensorFlow Lite\nWe are interested in performing inference, which refers to executing a TensorFlow Lite model on a device to make predictions based on input data. To perform an inference with a TensorFlow Lite model, we must run it through an interpreter. The TensorFlow Lite interpreter is designed to be lean and fast. The interpreter uses a static graph ordering and a custom (less-dynamic) memory allocator to ensure minimal load, initialization, and execution latency.\nWe’ll use the TensorFlow Lite runtime for Raspberry Pi, a simplified library for running machine learning models on mobile and embedded devices, without including all TensorFlow packages.\npip install tflite_runtime --no-deps\n\nThe wheel installed: tflite_runtime-2.14.0-cp311-cp311-manylinux_2_34_aarch64.whl\n\n\n\nInstalling Additional Python Libraries\nInstall required Python libraries for use with Image Classification:\nIf you have another version of Numpy installed, first uninstall it.\npip3 uninstall numpy\nInstall version 1.23.2, which is compatible with the tflite_runtime.\n pip3 install numpy==1.23.2\npip3 install Pillow matplotlib\n\n\nCreating a working directory:\nIf you are working on the Raspi-Zero with the minimum OS (No Desktop), you may not have a user-pre-defined directory tree (you can check it with ls. So, let’s create one:\nmkdir Documents\ncd Documents/\nmkdir TFLITE\ncd TFLITE/\nmkdir IMG_CLASS\ncd IMG_CLASS\nmkdir models\ncd models\n\nOn the Raspi-5, the /Documents should be there.\n\nGet a pre-trained Image Classification model:\nAn appropriate pre-trained model is crucial for successful image classification on resource-constrained devices like the Raspberry Pi. MobileNet is designed for mobile and embedded vision applications with a good balance between accuracy and speed. Versions: MobileNetV1, MobileNetV2, MobileNetV3. Let’s download the V2:\nwget https://storage.googleapis.com/download.tensorflow.org/models/\ntflite_11_05_08/mobilenet_v2_1.0_224_quant.tgz\n\ntar xzf mobilenet_v2_1.0_224_quant.tgz\nGet its labels:\nwget https://raw.githubusercontent.com/tensorflow/tensorflow/master/tensorflow/\nlite/java/demo/app/src/main/assets/labels_mobilenet_quant_v1_224.txt -O labels.txt\nIn the end, you should have the models in its directory:\n\n\nWe will only need the mobilenet_v2_1.0_224_quant.tflite model and the labels.txt. You can delete the other files.\n\n\n\nSetting up Jupyter Notebook (Optional)\nIf you prefer using Jupyter Notebook for development:\npip3 install jupyter\njupyter notebook --generate-config\nTo run Jupyter Notebook, run the command (change the IP address for yours):\njupyter notebook --ip=192.168.4.210 --no-browser\nOn the terminal, you can see the local URL address to open the notebook:\n\nYou can access it from another device by entering the Raspberry Pi’s IP address and the provided token in a web browser (you can copy the token from the terminal).\n\nDefine your working directory in the Raspi and create a new Python 3 notebook.\n\n\nVerifying the Setup\nTest your setup by running a simple Python script:\nimport tflite_runtime.interpreter as tflite\nimport numpy as np\nfrom PIL import Image\n\nprint(\"NumPy:\", np.__version__)\nprint(\"Pillow:\", Image.__version__)\n\n# Try to create a TFLite Interpreter\nmodel_path = \"./models/mobilenet_v2_1.0_224_quant.tflite\"\ninterpreter = tflite.Interpreter(model_path=model_path)\ninterpreter.allocate_tensors()\nprint(\"TFLite Interpreter created successfully!\")\nYou can create the Python script using nano on the terminal, saving it with CTRL+0 + ENTER + CTRL+X\n\nAnd run it with the command:\n\nOr you can run it directly on the Notebook:"
  },
  {
    "objectID": "raspi/image_classification/image_classification.html#making-inferences-with-mobilenet-v2",
    "href": "raspi/image_classification/image_classification.html#making-inferences-with-mobilenet-v2",
    "title": "Image Classification",
    "section": "Making inferences with Mobilenet V2",
    "text": "Making inferences with Mobilenet V2\nIn the last section, we set up the environment, including downloading a popular pre-trained model, Mobilenet V2, trained on ImageNet’s 224x224 images (1.2 million) for 1,001 classes (1,000 object categories plus 1 background). The model was converted to a compact 3.5MB TensorFlow Lite format, making it suitable for the limited storage and memory of a Raspberry Pi.\n\nLet’s start a new notebook to follow all the steps to classify one image:\nImport the needed libraries:\nimport time\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom PIL import Image\nimport tflite_runtime.interpreter as tflite\nLoad the TFLite model and allocate tensors:\nmodel_path = \"./models/mobilenet_v2_1.0_224_quant.tflite\"\ninterpreter = tflite.Interpreter(model_path=model_path)\ninterpreter.allocate_tensors()\nGet input and output tensors.\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\nInput details will give us information about how the model should be fed with an image. The shape of (1, 224, 224, 3) informs us that an image with dimensions (224x224x3) should be input one by one (Batch Dimension: 1).\n\nThe output details show that the inference will result in an array of 1,001 integer values. Those values result from the image classification, where each value is the probability of that specific label being related to the image.\n\nLet’s also inspect the dtype of input details of the model\ninput_dtype = input_details[0]['dtype']\ninput_dtype\ndtype('uint8')\nThis shows that the input image should be raw pixels (0 - 255).\nLet’s get a test image. You can transfer it from your computer or download one for testing. Let’s first create a folder under our working directory:\nmkdir images\ncd images\nwget https://upload.wikimedia.org/wikipedia/commons/3/3a/Cat03.jpg\nLet’s load and display the image:\n# Load he image\nimg_path = \"./images/Cat03.jpg\"\nimg = Image.open(img_path)\n\n# Display the image\nplt.figure(figsize=(8, 8))\nplt.imshow(img)\nplt.title(\"Original Image\")\nplt.show()\n\nWe can see the image size running the command:\nwidth, height = img.size\nThat shows us that the image is an RGB image with a width of 1600 and a height of 1600 pixels. So, to use our model, we should reshape it to (224, 224, 3) and add a batch dimension of 1, as defined in input details: (1, 224, 224, 3). The inference result, as shown in output details, will be an array with a 1001 size, as shown below:\n\nSo, let’s reshape the image, add the batch dimension, and see the result:\nimg = img.resize((input_details[0]['shape'][1], input_details[0]['shape'][2]))\ninput_data = np.expand_dims(img, axis=0\ninput_data.shape\nThe input_data shape is as expected: (1, 224, 224, 3)\nLet’s confirm the dtype of the input data:\ninput_data.dtype\ndtype('uint8')\nThe input data dtype is ‘uint8’, which is compatible with the dtype expected for the model.\nUsing the input_data, let’s run the interpreter and get the predictions (output):\ninterpreter.set_tensor(input_details[0]['index'], input_data)\ninterpreter.invoke()\npredictions = interpreter.get_tensor(output_details[0]['index'])[0]\nThe prediction is an array with 1001 elements. Let’s get the Top-5 indices where their elements have high values:\ntop_k_results = 5\ntop_k_indices = np.argsort(predictions)[::-1][:top_k_results]\ntop_k_indices \nThe top_k_indices is an array with 5 elements: array([283, 286, 282])\nSo, 283, 286, 282, 288, and 479 are the image’s most probable classes. Having the index, we must find to what class it appoints (such as car, cat, or dog). The text file downloaded with the model has a label associated with each index from 0 to 1,000. Let’s use a function to load the .txt file as a list:\ndef load_labels(filename):\n    with open(filename, 'r') as f:\n        return [line.strip() for line in f.readlines()]\nAnd get the list, printing the labels associated with the indexes:\nlabels_path = \"./models/labels.txt\"\nlabels = load_labels(labels_path)\n\nprint(labels[286])\nprint(labels[283])\nprint(labels[282])\nprint(labels[288])\nprint(labels[479])\nAs a result, we have:\nEgyptian cat\ntiger cat\ntabby\nlynx\ncarton\nAt least the four top indices are related to felines. The prediction content is the probability associated with each one of the labels. As we saw on output details, those values are quantized and should be dequantized and apply softmax.\nscale, zero_point = output_details[0]['quantization']\ndequantized_output = (predictions.astype(np.float32) - zero_point) * scale\nexp_output = np.exp(dequantized_output - np.max(dequantized_output))\nprobabilities = exp_output / np.sum(exp_output)\nLet’s print the top-5 probabilities:\nprint (probabilities[286])\nprint (probabilities[283])\nprint (probabilities[282])\nprint (probabilities[288])\nprint (probabilities[479])\n0.27741462\n0.3732285\n0.16919471\n0.10319158\n0.023410844\nFor clarity, let’s create a function to relate the labels with the probabilities:\nfor i in range(top_k_results):\n    print(\"\\t{:20}: {}%\".format(\n        labels[top_k_indices[i]],\n        (int(probabilities[top_k_indices[i]]*100))))\ntiger cat           : 37%\nEgyptian cat        : 27%\ntabby               : 16%\nlynx                : 10%\ncarton              : 2%\n\nDefine a general Image Classification function\nLet’s create a general function to give an image as input, and we get the Top-5 possible classes:\n\ndef image_classification(img_path, model_path, labels, top_k_results=5):\n    # load the image\n    img = Image.open(img_path)\n    plt.figure(figsize=(4, 4))\n    plt.imshow(img)\n    plt.axis('off')\n\n    # Load the TFLite model\n    interpreter = tflite.Interpreter(model_path=model_path)\n    interpreter.allocate_tensors()\n    \n    # Get input and output tensors\n    input_details = interpreter.get_input_details()\n    output_details = interpreter.get_output_details()\n\n    # Preprocess\n    img = img.resize((input_details[0]['shape'][1], \n                      input_details[0]['shape'][2]))\n    input_data = np.expand_dims(img, axis=0)\n\n    # Inference on Raspi-Zero\n    interpreter.set_tensor(input_details[0]['index'], input_data)\n    interpreter.invoke()\n    \n    # Obtain results and map them to the classes\n    predictions = interpreter.get_tensor(output_details[0]['index'])[0]\n\n    # Get indices of the top k results\n    top_k_indices = np.argsort(predictions)[::-1][:top_k_results]\n\n    # Get quantization parameters\n    scale, zero_point = output_details[0]['quantization']\n\n    # Dequantize the output and apply softmax\n    dequantized_output = (predictions.astype(np.float32) - zero_point) * scale\n    exp_output = np.exp(dequantized_output - np.max(dequantized_output))\n    probabilities = exp_output / np.sum(exp_output)\n\n    print(\"\\n\\t[PREDICTION]        [Prob]\\n\")\n    for i in range(top_k_results):\n        print(\"\\t{:20}: {}%\".format(\n            labels[top_k_indices[i]],\n            (int(probabilities[top_k_indices[i]]*100))))\n\nAnd loading some images for testing, we have:\n\n\n\nTesting with a model trained from scratch\nLet’s get a TFLite model trained from scratch. For that, you can follow the Notebook:\nCNN to classify Cifar-10 dataset\nIn the notebook, we trained a model using the CIFAR10 dataset, which contains 60,000 images from 10 classes of CIFAR (airplane, automobile, bird, cat, deer, dog, frog, horse, ship, and truck). CIFAR has 32x32 color images (3 color channels) where the objects are not centered and can have the object with a background, such as airplanes that might have a cloudy sky behind them! In short, small but real images.\nThe CNN trained model (cifar10_model.keras) had a size of 2.0MB. Using the TFLite Converter, the model cifar10.tflite became with 674MB (around 1/3 of the original size).\n\nOn the notebook Cifar 10 - Image Classification on a Raspi with TFLite (which can be run over the Raspi), we can follow the same steps we did with the mobilenet_v2_1.0_224_quant.tflite. Below are examples of images using the General Function for Image Classification on a Raspi-Zero, as shown in the last section.\n\n\n\nInstalling Picamera2\nPicamera2, a Python library for interacting with Raspberry Pi’s camera, is based on the libcamera camera stack, and the Raspberry Pi foundation maintains it. The Picamera2 library is supported on all Raspberry Pi models, from the Pi Zero to the RPi 5. It is already installed system-wide on the Raspi, but we should make it accessible within the virtual environment.\n\nFirst, activate the virtual environment if it’s not already activated:\nsource ~/tflite/bin/activate\nNow, let’s create a .pth file in your virtual environment to add the system site-packages path:\necho \"/usr/lib/python3/dist-packages\" &gt; $VIRTUAL_ENV/lib/python3.11/\nsite-packages/system_site_packages.pth\n\nNote: If your Python version differs, replace python3.11 with the appropriate version.\n\nAfter creating this file, try importing picamera2 in Python:\npython3\n&gt;&gt;&gt; import picamera2\n&gt;&gt;&gt; print(picamera2.__file__)\n\nThe above code will show the file location of the picamera2 module itself, proving that the library can be accessed from the environment.\n/home/mjrovai/tflite/lib/python3.11/site-packages/picamera2/__init__.py\nYou can also list the available cameras in the system:\n&gt;&gt;&gt; print(Picamera2.global_camera_info())\nIn my case, with a USB installed, I got:\n\nNow that we’ve confirmed picamera2 is working in the environment with an index 0, let’s try a simple Python script to capture an image from your USB camera:\nfrom picamera2 import Picamera2\nimport time\n\n# Initialize the camera\npicam2 = Picamera2() # default is index 0\n\n# Configure the camera\nconfig = picam2.create_still_configuration(main={\"size\": (640, 480)})\npicam2.configure(config)\n\n# Start the camera\npicam2.start()\n\n# Wait for the camera to warm up\ntime.sleep(2)\n\n# Capture an image\npicam2.capture_file(\"usb_camera_image.jpg\")\nprint(\"Image captured and saved as 'usb_camera_image.jpg'\")\n\n# Stop the camera\npicam2.stop()\nUse the Nano text editor, the Jupyter Notebook, or any other editor. Save this as a Python script (e.g., capture_image.py) and run it. This should capture an image from your camera and save it as “usb_camera_image.jpg” in the same directory as your script.\n\nIf the Jupyter is open, you can see the captured image on your computer. Otherwise, transfer the file from the Raspi to your computer.\n\n\nIf you are working with a Raspi-5 with a whole desktop, you can open the file directly on the device."
  },
  {
    "objectID": "raspi/image_classification/image_classification.html#image-classification-project",
    "href": "raspi/image_classification/image_classification.html#image-classification-project",
    "title": "Image Classification",
    "section": "Image Classification Project",
    "text": "Image Classification Project\nNow, we will develop a complete Image Classification project using the Edge Impulse Studio. As we did with the Movilinet V2, the trained and converted TFLite model will be used for inference.\n\nThe Goal\nThe first step in any ML project is to define its goal. In this case, it is to detect and classify two specific objects present in one image. For this project, we will use two small toys: a robot and a small Brazilian parrot (named Periquito). We will also collect images of a background where those two objects are absent.\n\n\n\nData Collection\nOnce we have defined our Machine Learning project goal, the next and most crucial step is collecting the dataset. We can use a phone for the image capture, but we will use the Raspi here. Let’s set up a simple web server on our Raspberry Pi to view the QVGA (320 x 240) captured images in a browser.\n\nFirst, let’s install Flask, a lightweight web framework for Python:\npip3 install flask\nLet’s create a new Python script combining image capture with a web server. We’ll call it get_img_data.py:\n\n\nfrom flask import Flask, Response, render_template_string, request, redirect, url_for\nfrom picamera2 import Picamera2\nimport io\nimport threading\nimport time\nimport os\nimport signal\n\napp = Flask(__name__)\n\n# Global variables\nbase_dir = \"dataset\"\npicam2 = None\nframe = None\nframe_lock = threading.Lock()\ncapture_counts = {}\ncurrent_label = None\nshutdown_event = threading.Event()\n\ndef initialize_camera():\n    global picam2\n    picam2 = Picamera2()\n    config = picam2.create_preview_configuration(main={\"size\": (320, 240)})\n    picam2.configure(config)\n    picam2.start()\n    time.sleep(2)  # Wait for camera to warm up\n\ndef get_frame():\n    global frame\n    while not shutdown_event.is_set():\n        stream = io.BytesIO()\n        picam2.capture_file(stream, format='jpeg')\n        with frame_lock:\n            frame = stream.getvalue()\n        time.sleep(0.1)  # Adjust as needed for smooth preview\n\ndef generate_frames():\n    while not shutdown_event.is_set():\n        with frame_lock:\n            if frame is not None:\n                yield (b'--frame\\r\\n'\n                       b'Content-Type: image/jpeg\\r\\n\\r\\n' + frame + b'\\r\\n')\n        time.sleep(0.1)  # Adjust as needed for smooth streaming\n\ndef shutdown_server():\n    shutdown_event.set()\n    if picam2:\n        picam2.stop()\n    # Give some time for other threads to finish\n    time.sleep(2)\n    # Send SIGINT to the main process\n    os.kill(os.getpid(), signal.SIGINT)\n\n@app.route('/', methods=['GET', 'POST'])\ndef index():\n    global current_label\n    if request.method == 'POST':\n        current_label = request.form['label']\n        if current_label not in capture_counts:\n            capture_counts[current_label] = 0\n        os.makedirs(os.path.join(base_dir, current_label), exist_ok=True)\n        return redirect(url_for('capture_page'))\n    return render_template_string('''\n        &lt;!DOCTYPE html&gt;\n        &lt;html&gt;\n        &lt;head&gt;\n            &lt;title&gt;Dataset Capture - Label Entry&lt;/title&gt;\n        &lt;/head&gt;\n        &lt;body&gt;\n            &lt;h1&gt;Enter Label for Dataset&lt;/h1&gt;\n            &lt;form method=\"post\"&gt;\n                &lt;input type=\"text\" name=\"label\" required&gt;\n                &lt;input type=\"submit\" value=\"Start Capture\"&gt;\n            &lt;/form&gt;\n        &lt;/body&gt;\n        &lt;/html&gt;\n    ''')\n\n@app.route('/capture')\ndef capture_page():\n    return render_template_string('''\n        &lt;!DOCTYPE html&gt;\n        &lt;html&gt;\n        &lt;head&gt;\n            &lt;title&gt;Dataset Capture&lt;/title&gt;\n            &lt;script&gt;\n                var shutdownInitiated = false;\n                function checkShutdown() {\n                    if (!shutdownInitiated) {\n                        fetch('/check_shutdown')\n                            .then(response =&gt; response.json())\n                            .then(data =&gt; {\n                                if (data.shutdown) {\n                                    shutdownInitiated = true;\n                                    document.getElementById('video-feed').src = '';\n                                    document.getElementById('shutdown-message')\n                                    .style.display = 'block';\n                                }\n                            });\n                    }\n                }\n                setInterval(checkShutdown, 1000);  // Check every second\n            &lt;/script&gt;\n        &lt;/head&gt;\n        &lt;body&gt;\n            &lt;h1&gt;Dataset Capture&lt;/h1&gt;\n            &lt;p&gt;Current Label: {{ label }}&lt;/p&gt;\n            &lt;p&gt;Images captured for this label: {{ capture_count }}&lt;/p&gt;\n            &lt;img id=\"video-feed\" src=\"{{ url_for('video_feed') }}\" width=\"640\" \n            height=\"480\" /&gt;\n            &lt;div id=\"shutdown-message\" style=\"display: none; color: red;\"&gt;\n                Capture process has been stopped. You can close this window.\n            &lt;/div&gt;\n            &lt;form action=\"/capture_image\" method=\"post\"&gt;\n                &lt;input type=\"submit\" value=\"Capture Image\"&gt;\n            &lt;/form&gt;\n            &lt;form action=\"/stop\" method=\"post\"&gt;\n                &lt;input type=\"submit\" value=\"Stop Capture\" \n                style=\"background-color: #ff6666;\"&gt;\n            &lt;/form&gt;\n            &lt;form action=\"/\" method=\"get\"&gt;\n                &lt;input type=\"submit\" value=\"Change Label\" \n                style=\"background-color: #ffff66;\"&gt;\n            &lt;/form&gt;\n        &lt;/body&gt;\n        &lt;/html&gt;\n    ''', label=current_label, capture_count=capture_counts.get(current_label, 0))\n\n@app.route('/video_feed')\ndef video_feed():\n    return Response(generate_frames(),\n                    mimetype='multipart/x-mixed-replace; boundary=frame')\n\n@app.route('/capture_image', methods=['POST'])\ndef capture_image():\n    global capture_counts\n    if current_label and not shutdown_event.is_set():\n        capture_counts[current_label] += 1\n        timestamp = time.strftime(\"%Y%m%d-%H%M%S\")\n        filename = f\"image_{timestamp}.jpg\"\n        full_path = os.path.join(base_dir, current_label, filename)\n        \n        picam2.capture_file(full_path)\n    \n    return redirect(url_for('capture_page'))\n\n@app.route('/stop', methods=['POST'])\ndef stop():\n    summary = render_template_string('''\n        &lt;!DOCTYPE html&gt;\n        &lt;html&gt;\n        &lt;head&gt;\n            &lt;title&gt;Dataset Capture - Stopped&lt;/title&gt;\n        &lt;/head&gt;\n        &lt;body&gt;\n            &lt;h1&gt;Dataset Capture Stopped&lt;/h1&gt;\n            &lt;p&gt;The capture process has been stopped. You can close this window.&lt;/p&gt;\n            &lt;p&gt;Summary of captures:&lt;/p&gt;\n            &lt;ul&gt;\n            {% for label, count in capture_counts.items() %}\n                &lt;li&gt;{{ label }}: {{ count }} images&lt;/li&gt;\n            {% endfor %}\n            &lt;/ul&gt;\n        &lt;/body&gt;\n        &lt;/html&gt;\n    ''', capture_counts=capture_counts)\n    \n    # Start a new thread to shutdown the server\n    threading.Thread(target=shutdown_server).start()\n    \n    return summary\n\n@app.route('/check_shutdown')\ndef check_shutdown():\n    return {'shutdown': shutdown_event.is_set()}\n\nif __name__ == '__main__':\n    initialize_camera()\n    threading.Thread(target=get_frame, daemon=True).start()\n    app.run(host='0.0.0.0', port=5000, threaded=True)\n\n\nRun this script:\npython3 get_img_data.py\nAccess the web interface:\n\nOn the Raspberry Pi itself (if you have a GUI): Open a web browser and go to http://localhost:5000\nFrom another device on the same network: Open a web browser and go to http://&lt;raspberry_pi_ip&gt;:5000 (Replace &lt;raspberry_pi_ip&gt; with your Raspberry Pi’s IP address). For example: http://192.168.4.210:5000/\n\n\nThis Python script creates a web-based interface for capturing and organizing image datasets using a Raspberry Pi and its camera. It’s handy for machine learning projects that require labeled image data.\n\nKey Features:\n\nWeb Interface: Accessible from any device on the same network as the Raspberry Pi.\nLive Camera Preview: This shows a real-time feed from the camera.\nLabeling System: Allows users to input labels for different categories of images.\nOrganized Storage: Automatically saves images in label-specific subdirectories.\nPer-Label Counters: Keeps track of how many images are captured for each label.\nSummary Statistics: Provides a summary of captured images when stopping the capture process.\n\n\n\nMain Components:\n\nFlask Web Application: Handles routing and serves the web interface.\nPicamera2 Integration: Controls the Raspberry Pi camera.\nThreaded Frame Capture: Ensures smooth live preview.\nFile Management: Organizes captured images into labeled directories.\n\n\n\nKey Functions:\n\ninitialize_camera(): Sets up the Picamera2 instance.\nget_frame(): Continuously captures frames for the live preview.\ngenerate_frames(): Yields frames for the live video feed.\nshutdown_server(): Sets the shutdown event, stops the camera, and shuts down the Flask server\nindex(): Handles the label input page.\ncapture_page(): Displays the main capture interface.\nvideo_feed(): Shows a live preview to position the camera\ncapture_image(): Saves an image with the current label.\nstop(): Stops the capture process and displays a summary.\n\n\n\nUsage Flow:\n\nStart the script on your Raspberry Pi.\nAccess the web interface from a browser.\nEnter a label for the images you want to capture and press Start Capture.\n\n\n\nUse the live preview to position the camera.\nClick Capture Image to save images under the current label.\n\n\n\nChange labels as needed for different categories, selecting Change Label.\nClick Stop Capture when finished to see a summary.\n\n\n\n\nTechnical Notes:\n\nThe script uses threading to handle concurrent frame capture and web serving.\nImages are saved with timestamps in their filenames for uniqueness.\nThe web interface is responsive and can be accessed from mobile devices.\n\n\n\nCustomization Possibilities:\n\nAdjust image resolution in the initialize_camera() function. Here we used QVGA (320X240).\nModify the HTML templates for a different look and feel.\nAdd additional image processing or analysis steps in the capture_image() function.\n\n\n\nNumber of samples on Dataset:\nGet around 60 images from each category (periquito, robot and background). Try to capture different angles, backgrounds, and light conditions. On the Raspi, we will end with a folder named dataset, witch contains 3 sub-folders periquito, robot, and background. one for each class of images.\nYou can use Filezilla to transfer the created dataset to your main computer."
  },
  {
    "objectID": "raspi/image_classification/image_classification.html#training-the-model-with-edge-impulse-studio",
    "href": "raspi/image_classification/image_classification.html#training-the-model-with-edge-impulse-studio",
    "title": "Image Classification",
    "section": "Training the model with Edge Impulse Studio",
    "text": "Training the model with Edge Impulse Studio\nWe will use the Edge Impulse Studio to train our model. Go to the Edge Impulse Page, enter your account credentials, and create a new project:\n\n\nHere, you can clone a similar project: Raspi - Img Class.\n\n\nDataset\nWe will walk through four main steps using the EI Studio (or Studio). These steps are crucial in preparing our model for use on the Raspi: Dataset, Impulse, Tests, and Deploy (on the Edge Device, in this case, the Raspi).\n\nRegarding the Dataset, it is essential to point out that our Original Dataset, captured with the Raspi, will be split into Training, Validation, and Test. The Test Set will be separated from the beginning and reserved for use only in the Test phase after training. The Validation Set will be used during training.\n\nOn Studio, follow the steps to upload the captured data:\n\nGo to the Data acquisition tab, and in the UPLOAD DATA section, upload the files from your computer in the chosen categories.\nLeave to the Studio the splitting of the original dataset into train and test and choose the label about\nRepeat the procedure for all three classes. At the end, you should see your “raw data” in the Studio:\n\n\nThe Studio allows you to explore your data, showing a complete view of all the data in your project. You can clear, inspect, or change labels by clicking on individual data items. In our case, a straightforward project, the data seems OK."
  },
  {
    "objectID": "raspi/image_classification/image_classification.html#the-impulse-design",
    "href": "raspi/image_classification/image_classification.html#the-impulse-design",
    "title": "Image Classification",
    "section": "The Impulse Design",
    "text": "The Impulse Design\nIn this phase, we should define how to:\n\nPre-process our data, which consists of resizing the individual images and determining the color depth to use (be it RGB or Grayscale) and\nSpecify a Model. In this case, it will be the Transfer Learning (Images) to fine-tune a pre-trained MobileNet V2 image classification model on our data. This method performs well even with relatively small image datasets (around 180 images in our case).\n\nTransfer Learning with MobileNet offers a streamlined approach to model training, which is especially beneficial for resource-constrained environments and projects with limited labeled data. MobileNet, known for its lightweight architecture, is a pre-trained model that has already learned valuable features from a large dataset (ImageNet).\n\nBy leveraging these learned features, we can train a new model for your specific task with fewer data and computational resources and achieve competitive accuracy.\n\nThis approach significantly reduces training time and computational cost, making it ideal for quick prototyping and deployment on embedded devices where efficiency is paramount.\nGo to the Impulse Design Tab and create the impulse, defining an image size of 160x160 and squashing them (squared form, without cropping). Select Image and Transfer Learning blocks. Save the Impulse.\n\n\nImage Pre-Processing\nAll the input QVGA/RGB565 images will be converted to 76,800 features (160x160x3).\n\nPress Save parameters and select Generate features in the next tab.\n\n\nModel Design\nMobileNet is a family of efficient convolutional neural networks designed for mobile and embedded vision applications. The key features of MobileNet are:\n\nLightweight: Optimized for mobile devices and embedded systems with limited computational resources.\nSpeed: Fast inference times, suitable for real-time applications.\nAccuracy: Maintains good accuracy despite its compact size.\n\nMobileNetV2, introduced in 2018, improves the original MobileNet architecture. Key features include:\n\nInverted Residuals: Inverted residual structures are used where shortcut connections are made between thin bottleneck layers.\nLinear Bottlenecks: Removes non-linearities in the narrow layers to prevent the destruction of information.\nDepth-wise Separable Convolutions: Continues to use this efficient operation from MobileNetV1.\n\nIn our project, we will do a Transfer Learning with the MobileNetV2 160x160 1.0, which means that the images used for training (and future inference) should have an input Size of 160x160 pixels and a Width Multiplier of 1.0 (full width, not reduced). This configuration balances between model size, speed, and accuracy.\n\n\nModel Training\nAnother valuable deep learning technique is Data Augmentation. Data augmentation improves the accuracy of machine learning models by creating additional artificial data. A data augmentation system makes small, random changes to the training data during the training process (such as flipping, cropping, or rotating the images).\nLooking under the hood, here you can see how Edge Impulse implements a data Augmentation policy on your data:\n# Implements the data augmentation policy\ndef augment_image(image, label):\n    # Flips the image randomly\n    image = tf.image.random_flip_left_right(image)\n\n    # Increase the image size, then randomly crop it down to\n    # the original dimensions\n    resize_factor = random.uniform(1, 1.2)\n    new_height = math.floor(resize_factor * INPUT_SHAPE[0])\n    new_width = math.floor(resize_factor * INPUT_SHAPE[1])\n    image = tf.image.resize_with_crop_or_pad(image, new_height, new_width)\n    image = tf.image.random_crop(image, size=INPUT_SHAPE)\n\n    # Vary the brightness of the image\n    image = tf.image.random_brightness(image, max_delta=0.2)\n\n    return image, label\nExposure to these variations during training can help prevent your model from taking shortcuts by “memorizing” superficial clues in your training data, meaning it may better reflect the deep underlying patterns in your dataset.\nThe final dense layer of our model will have 0 neurons with a 10% dropout for overfitting prevention. Here is the Training result:\n\nThe result is excellent, with a reasonable 35ms of latency (for a Rasp-4), which should result in around 30 fps (frames per second) during inference. A Raspi-Zero should be slower, and the Rasp-5, faster.\n\n\nTrading off: Accuracy versus speed\nIf faster inference is needed, we should train the model using smaller alphas (0.35, 0.5, and 0.75) or even reduce the image input size, trading with accuracy. However, reducing the input image size and decreasing the alpha (width multiplier) can speed up inference for MobileNet V2, but they have different trade-offs. Let’s compare:\n\nReducing Image Input Size:\n\nPros:\n\nSignificantly reduces the computational cost across all layers.\nDecreases memory usage.\nIt often provides a substantial speed boost.\n\nCons:\n\nIt may reduce the model’s ability to detect small features or fine details.\nIt can significantly impact accuracy, especially for tasks requiring fine-grained recognition.\n\n\nReducing Alpha (Width Multiplier):\n\nPros:\n\nReduces the number of parameters and computations in the model.\nMaintains the original input resolution, potentially preserving more detail.\nIt can provide a good balance between speed and accuracy.\n\nCons:\n\nIt may not speed up inference as dramatically as reducing input size.\nIt can reduce the model’s capacity to learn complex features.\n\nComparison:\n\nSpeed Impact:\n\nReducing input size often provides a more substantial speed boost because it reduces computations quadratically (halving both width and height reduces computations by about 75%).\nReducing alpha provides a more linear reduction in computations.\n\nAccuracy Impact:\n\nReducing input size can severely impact accuracy, especially when detecting small objects or fine details.\nReducing alpha tends to have a more gradual impact on accuracy.\n\nModel Architecture:\n\nChanging input size doesn’t alter the model’s architecture.\nChanging alpha modifies the model’s structure by reducing the number of channels in each layer.\n\n\nRecommendation:\n\nIf our application doesn’t require detecting tiny details and can tolerate some loss in accuracy, reducing the input size is often the most effective way to speed up inference.\nReducing alpha might be preferable if maintaining the ability to detect fine details is crucial or if you need a more balanced trade-off between speed and accuracy.\nFor best results, you might want to experiment with both:\n\nTry MobileNet V2 with input sizes like 160x160 or 92x92\nExperiment with alpha values like 1.0, 0.75, 0.5 or 0.35.\n\nAlways benchmark the different configurations on your specific hardware and with your particular dataset to find the optimal balance for your use case.\n\n\nRemember, the best choice depends on your specific requirements for accuracy, speed, and the nature of the images you’re working with. It’s often worth experimenting with combinations to find the optimal configuration for your particular use case.\n\n\n\nModel Testing\nNow, you should take the data set aside at the start of the project and run the trained model using it as input. Again, the result is excellent (92.22%).\n\n\nDeploying the model\nAs we did in the previous section, we can deploy the trained model as .tflite and use Raspi to run it using Python.\nOn the Dashboard tab, go to Transfer learning model (int8 quantized) and click on the download icon:\n\n\nLet’s also download the float32 version for comparasion\n\nTransfer the model from your computer to the Raspi (./models), for example, using FileZilla. Also, capture some images for inference (./images).\nImport the needed libraries:\nimport time\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom PIL import Image\nimport tflite_runtime.interpreter as tflite\nDefine the paths and labels:\nimg_path = \"./images/robot.jpg\"\nmodel_path = \"./models/ei-raspi-img-class-int8-quantized-model.tflite\"\nlabels = ['background', 'periquito', 'robot']\n\nNote that the models trained on the Edge Impulse Studio will output values with index 0, 1, 2, etc., where the actual labels will follow an alphabetic order.\n\nLoad the model, allocate the tensors, and get the input and output tensor details:\n# Load the TFLite model\ninterpreter = tflite.Interpreter(model_path=model_path)\ninterpreter.allocate_tensors()\n\n# Get input and output tensors\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\nOne important difference to note is that the dtype of the input details of the model is now int8, which means that the input values go from -128 to +127, while each pixel of our image goes from 0 to 256. This means that we should pre-process the image to match it. We can check here:\ninput_dtype = input_details[0]['dtype']\ninput_dtype\nnumpy.int8\nSo, let’s open the image and show it:\nimg = Image.open(img_path)\nplt.figure(figsize=(4, 4))\nplt.imshow(img)\nplt.axis('off')\nplt.show()\n\nAnd perform the pre-processing:\nscale, zero_point = input_details[0]['quantization']\nimg = img.resize((input_details[0]['shape'][1], \n                  input_details[0]['shape'][2]))\nimg_array = np.array(img, dtype=np.float32) / 255.0\nimg_array = (img_array / scale + zero_point).clip(-128, 127).astype(np.int8)\ninput_data = np.expand_dims(img_array, axis=0)\nChecking the input data, we can verify that the input tensor is compatible with what is expected by the model:\ninput_data.shape, input_data.dtype\n((1, 160, 160, 3), dtype('int8'))\nNow, it is time to perform the inference. Let’s also calculate the latency of the model:\n# Inference on Raspi-Zero\nstart_time = time.time()\ninterpreter.set_tensor(input_details[0]['index'], input_data)\ninterpreter.invoke()\nend_time = time.time()\ninference_time = (end_time - start_time) * 1000  # Convert to milliseconds\nprint (\"Inference time: {:.1f}ms\".format(inference_time))\nThe model will take around 125ms to perform the inference in the Raspi-Zero, which is 3 to 4 times longer than a Raspi-5.\nNow, we can get the output labels and probabilities. It is also important to note that the model trained on the Edge Impulse Studio has a softmax in its output (different from the original Movilenet V2), and we should use the model’s raw output as the “probabilities.”\n# Obtain results and map them to the classes\npredictions = interpreter.get_tensor(output_details[0]['index'])[0]\n\n# Get indices of the top k results\ntop_k_results=3\ntop_k_indices = np.argsort(predictions)[::-1][:top_k_results]\n\n# Get quantization parameters\nscale, zero_point = output_details[0]['quantization']\n\n# Dequantize the output\ndequantized_output = (predictions.astype(np.float32) - zero_point) * scale\nprobabilities = dequantized_output\n\nprint(\"\\n\\t[PREDICTION]        [Prob]\\n\")\nfor i in range(top_k_results):\n    print(\"\\t{:20}: {:.2f}%\".format(\n        labels[top_k_indices[i]],\n        probabilities[top_k_indices[i]] * 100))\n\nLet’s modify the function created before so that we can handle different type of models:\n\ndef image_classification(img_path, model_path, labels, top_k_results=3, \n                         apply_softmax=False):\n    # Load the image\n    img = Image.open(img_path)\n    plt.figure(figsize=(4, 4))\n    plt.imshow(img)\n    plt.axis('off')\n\n    # Load the TFLite model\n    interpreter = tflite.Interpreter(model_path=model_path)\n    interpreter.allocate_tensors()\n\n    # Get input and output tensors\n    input_details = interpreter.get_input_details()\n    output_details = interpreter.get_output_details()\n\n    # Preprocess\n    img = img.resize((input_details[0]['shape'][1], \n                      input_details[0]['shape'][2]))\n    \n    input_dtype = input_details[0]['dtype']\n    \n    if input_dtype == np.uint8:\n        input_data = np.expand_dims(np.array(img), axis=0)\n    elif input_dtype == np.int8:\n        scale, zero_point = input_details[0]['quantization']\n        img_array = np.array(img, dtype=np.float32) / 255.0\n        img_array = (img_array / scale + zero_point).clip(-128, 127).astype(np.int8)\n        input_data = np.expand_dims(img_array, axis=0)\n    else:  # float32\n        input_data = np.expand_dims(np.array(img, dtype=np.float32), axis=0) / 255.0\n\n    # Inference on Raspi-Zero\n    start_time = time.time()\n    interpreter.set_tensor(input_details[0]['index'], input_data)\n    interpreter.invoke()\n    end_time = time.time()\n    inference_time = (end_time - start_time) * 1000  # Convert to milliseconds\n\n    # Obtain results\n    predictions = interpreter.get_tensor(output_details[0]['index'])[0]\n\n    # Get indices of the top k results\n    top_k_indices = np.argsort(predictions)[::-1][:top_k_results]\n\n    # Handle output based on type\n    output_dtype = output_details[0]['dtype']\n    if output_dtype in [np.int8, np.uint8]:\n        # Dequantize the output\n        scale, zero_point = output_details[0]['quantization']\n        predictions = (predictions.astype(np.float32) - zero_point) * scale\n    \n    if apply_softmax:\n        # Apply softmax\n        exp_preds = np.exp(predictions - np.max(predictions))\n        probabilities = exp_preds / np.sum(exp_preds)\n    else:\n        probabilities = predictions\n\n    print(\"\\n\\t[PREDICTION]        [Prob]\\n\")\n    for i in range(top_k_results):\n        print(\"\\t{:20}: {:.1f}%\".format(\n            labels[top_k_indices[i]],\n            probabilities[top_k_indices[i]] * 100))\n    print (\"\\n\\tInference time: {:.1f}ms\".format(inference_time))\n\nAnd test it with different images and the int8 quantized model (160x160 alpha =1.0).\n\nLet’s download a smaller model, such as the one trained for the Nicla Vision Lab (int8 quantized model (96x96 alpha = 0.1), as a test. We can use the same function:\n\nThe model lost some accuracy, but it is still OK once our model does not look for many details. Regarding latency, we are around ten times faster on the Rasp-Zero."
  },
  {
    "objectID": "raspi/image_classification/image_classification.html#live-image-classification",
    "href": "raspi/image_classification/image_classification.html#live-image-classification",
    "title": "Image Classification",
    "section": "Live Image Classification",
    "text": "Live Image Classification\nLet’s develop an app to capture images with the USB camera in real time, showing its classification.\nUsing the nano on the terminal, save the code below, such as img_class_live_infer.py.\n\nfrom flask import Flask, Response, render_template_string, request, jsonify\nfrom picamera2 import Picamera2\nimport io\nimport threading\nimport time\nimport numpy as np\nfrom PIL import Image\nimport tflite_runtime.interpreter as tflite\nfrom queue import Queue\n\napp = Flask(__name__)\n\n# Global variables\npicam2 = None\nframe = None\nframe_lock = threading.Lock()\nis_classifying = False\nconfidence_threshold = 0.8\nmodel_path = \"./models/ei-raspi-img-class-int8-quantized-model.tflite\"\nlabels = ['background', 'periquito', 'robot']\ninterpreter = None\nclassification_queue = Queue(maxsize=1)\n\ndef initialize_camera():\n    global picam2\n    picam2 = Picamera2()\n    config = picam2.create_preview_configuration(main={\"size\": (320, 240)})\n    picam2.configure(config)\n    picam2.start()\n    time.sleep(2)  # Wait for camera to warm up\n\ndef get_frame():\n    global frame\n    while True:\n        stream = io.BytesIO()\n        picam2.capture_file(stream, format='jpeg')\n        with frame_lock:\n            frame = stream.getvalue()\n        time.sleep(0.1)  # Capture frames more frequently\n\ndef generate_frames():\n    while True:\n        with frame_lock:\n            if frame is not None:\n                yield (b'--frame\\r\\n'\n                       b'Content-Type: image/jpeg\\r\\n\\r\\n' + frame + b'\\r\\n')\n        time.sleep(0.1)\n\ndef load_model():\n    global interpreter\n    if interpreter is None:\n        interpreter = tflite.Interpreter(model_path=model_path)\n        interpreter.allocate_tensors()\n    return interpreter\n\ndef classify_image(img, interpreter):\n    input_details = interpreter.get_input_details()\n    output_details = interpreter.get_output_details()\n\n    img = img.resize((input_details[0]['shape'][1], \n                      input_details[0]['shape'][2]))\n    input_data = np.expand_dims(np.array(img), axis=0)\\\n                             .astype(input_details[0]['dtype'])\n\n    interpreter.set_tensor(input_details[0]['index'], input_data)\n    interpreter.invoke()\n\n    predictions = interpreter.get_tensor(output_details[0]['index'])[0]\n    # Handle output based on type\n    output_dtype = output_details[0]['dtype']\n    if output_dtype in [np.int8, np.uint8]:\n        # Dequantize the output\n        scale, zero_point = output_details[0]['quantization']\n        predictions = (predictions.astype(np.float32) - zero_point) * scale\n    return predictions\n\ndef classification_worker():\n    interpreter = load_model()\n    while True:\n        if is_classifying:\n            with frame_lock:\n                if frame is not None:\n                    img = Image.open(io.BytesIO(frame))\n            predictions = classify_image(img, interpreter)\n            max_prob = np.max(predictions)\n            if max_prob &gt;= confidence_threshold:\n                label = labels[np.argmax(predictions)]\n            else:\n                label = 'Uncertain'\n            classification_queue.put({'label': label, \n                                      'probability': float(max_prob)})\n        time.sleep(0.1)  # Adjust based on your needs\n\n@app.route('/')\ndef index():\n    return render_template_string('''\n        &lt;!DOCTYPE html&gt;\n        &lt;html&gt;\n        &lt;head&gt;\n            &lt;title&gt;Image Classification&lt;/title&gt;\n            &lt;script \n                src=\"https://code.jquery.com/jquery-3.6.0.min.js\"&gt;\n            &lt;/script&gt;\n            &lt;script&gt;\n                function startClassification() {\n                    $.post('/start');\n                    $('#startBtn').prop('disabled', true);\n                    $('#stopBtn').prop('disabled', false);\n                }\n                function stopClassification() {\n                    $.post('/stop');\n                    $('#startBtn').prop('disabled', false);\n                    $('#stopBtn').prop('disabled', true);\n                }\n                function updateConfidence() {\n                    var confidence = $('#confidence').val();\n                    $.post('/update_confidence', {confidence: confidence});\n                }\n                function updateClassification() {\n                    $.get('/get_classification', function(data) {\n                        $('#classification').text(data.label + ': ' \n                        + data.probability.toFixed(2));\n                    });\n                }\n                $(document).ready(function() {\n                    setInterval(updateClassification, 100);  \n                    // Update every 100ms\n                });\n            &lt;/script&gt;\n        &lt;/head&gt;\n        &lt;body&gt;\n            &lt;h1&gt;Image Classification&lt;/h1&gt;\n            &lt;img src=\"{{ url_for('video_feed') }}\" width=\"640\" height=\"480\" /&gt;\n            &lt;br&gt;\n            &lt;button id=\"startBtn\" onclick=\"startClassification()\"&gt;\n            Start Classification&lt;/button&gt;\n            &lt;button id=\"stopBtn\" onclick=\"stopClassification()\" disabled&gt;\n            Stop Classification&lt;/button&gt;\n            &lt;br&gt;\n            &lt;label for=\"confidence\"&gt;Confidence Threshold:&lt;/label&gt;\n            &lt;input type=\"number\" id=\"confidence\" name=\"confidence\" min=\"0\" \n            max=\"1\" step=\"0.1\" value=\"0.8\" onchange=\"updateConfidence()\"&gt;\n            &lt;br&gt;\n            &lt;div id=\"classification\"&gt;Waiting for classification...&lt;/div&gt;\n        &lt;/body&gt;\n        &lt;/html&gt;\n    ''')\n\n@app.route('/video_feed')\ndef video_feed():\n    return Response(generate_frames(),\n                    mimetype='multipart/x-mixed-replace; boundary=frame')\n\n@app.route('/start', methods=['POST'])\ndef start_classification():\n    global is_classifying\n    is_classifying = True\n    return '', 204\n\n@app.route('/stop', methods=['POST'])\ndef stop_classification():\n    global is_classifying\n    is_classifying = False\n    return '', 204\n\n@app.route('/update_confidence', methods=['POST'])\ndef update_confidence():\n    global confidence_threshold\n    confidence_threshold = float(request.form['confidence'])\n    return '', 204\n\n@app.route('/get_classification')\ndef get_classification():\n    if not is_classifying:\n        return jsonify({'label': 'Not classifying', 'probability': 0})\n    try:\n        result = classification_queue.get_nowait()\n    except Queue.Empty:\n        result = {'label': 'Processing', 'probability': 0}\n    return jsonify(result)\n\nif __name__ == '__main__':\n    initialize_camera()\n    threading.Thread(target=get_frame, daemon=True).start()\n    threading.Thread(target=classification_worker, daemon=True).start()\n    app.run(host='0.0.0.0', port=5000, threaded=True)\n\nOn the terminal, run:\npython3 img_class_live_infer.py\nAnd access the web interface:\n\nOn the Raspberry Pi itself (if you have a GUI): Open a web browser and go to http://localhost:5000\nFrom another device on the same network: Open a web browser and go to http://&lt;raspberry_pi_ip&gt;:5000 (Replace &lt;raspberry_pi_ip&gt; with your Raspberry Pi’s IP address). For example: http://192.168.4.210:5000/\n\nHere are some screenshots of the app running on an external desktop\n\nHere, you can see the app running on the YouTube:\n\nThe code creates a web application for real-time image classification using a Raspberry Pi, its camera module, and a TensorFlow Lite model. The application uses Flask to serve a web interface where is possible to view the camera feed and see live classification results.\n\nKey Components:\n\nFlask Web Application: Serves the user interface and handles requests.\nPiCamera2: Captures images from the Raspberry Pi camera module.\nTensorFlow Lite: Runs the image classification model.\nThreading: Manages concurrent operations for smooth performance.\n\n\n\nMain Features:\n\nLive camera feed display\nReal-time image classification\nAdjustable confidence threshold\nStart/Stop classification on demand\n\n\n\nCode Structure:\n\nImports and Setup:\n\nFlask for web application\nPiCamera2 for camera control\nTensorFlow Lite for inference\nThreading and Queue for concurrent operations\n\nGlobal Variables:\n\nCamera and frame management\nClassification control\nModel and label information\n\nCamera Functions:\n\ninitialize_camera(): Sets up the PiCamera2\nget_frame(): Continuously captures frames\ngenerate_frames(): Yields frames for the web feed\n\nModel Functions:\n\nload_model(): Loads the TFLite model\nclassify_image(): Performs inference on a single image\n\nClassification Worker:\n\nRuns in a separate thread\nContinuously classifies frames when active\nUpdates a queue with the latest results\n\nFlask Routes:\n\n/: Serves the main HTML page\n/video_feed: Streams the camera feed\n/start and /stop: Controls classification\n/update_confidence: Adjusts the confidence threshold\n/get_classification: Returns the latest classification result\n\nHTML Template:\n\nDisplays camera feed and classification results\nProvides controls for starting/stopping and adjusting settings\n\nMain Execution:\n\nInitializes camera and starts necessary threads\nRuns the Flask application\n\n\n\n\nKey Concepts:\n\nConcurrent Operations: Using threads to handle camera capture and classification separately from the web server.\nReal-time Updates: Frequent updates to the classification results without page reloads.\nModel Reuse: Loading the TFLite model once and reusing it for efficiency.\nFlexible Configuration: Allowing users to adjust the confidence threshold on the fly.\n\n\n\nUsage:\n\nEnsure all dependencies are installed.\nRun the script on a Raspberry Pi with a camera module.\nAccess the web interface from a browser using the Raspberry Pi’s IP address.\nStart classification and adjust settings as needed."
  },
  {
    "objectID": "raspi/image_classification/image_classification.html#conclusion",
    "href": "raspi/image_classification/image_classification.html#conclusion",
    "title": "Image Classification",
    "section": "Conclusion:",
    "text": "Conclusion:\nImage classification has emerged as a powerful and versatile application of machine learning, with significant implications for various fields, from healthcare to environmental monitoring. This chapter has demonstrated how to implement a robust image classification system on edge devices like the Raspi-Zero and Rasp-5, showcasing the potential for real-time, on-device intelligence.\nWe’ve explored the entire pipeline of an image classification project, from data collection and model training using Edge Impulse Studio to deploying and running inferences on a Raspi. The process highlighted several key points:\n\nThe importance of proper data collection and preprocessing for training effective models.\nThe power of transfer learning, allowing us to leverage pre-trained models like MobileNet V2 for efficient training with limited data.\nThe trade-offs between model accuracy and inference speed, especially crucial for edge devices.\nThe implementation of real-time classification using a web-based interface, demonstrating practical applications.\n\nThe ability to run these models on edge devices like the Raspi opens up numerous possibilities for IoT applications, autonomous systems, and real-time monitoring solutions. It allows for reduced latency, improved privacy, and operation in environments with limited connectivity.\nAs we’ve seen, even with the computational constraints of edge devices, it’s possible to achieve impressive results in terms of both accuracy and speed. The flexibility to adjust model parameters, such as input size and alpha values, allows for fine-tuning to meet specific project requirements.\nLooking forward, the field of edge AI and image classification continues to evolve rapidly. Advances in model compression techniques, hardware acceleration, and more efficient neural network architectures promise to further expand the capabilities of edge devices in computer vision tasks.\nThis project serves as a foundation for more complex computer vision applications and encourages further exploration into the exciting world of edge AI and IoT. Whether it’s for industrial automation, smart home applications, or environmental monitoring, the skills and concepts covered here provide a solid starting point for a wide range of innovative projects."
  },
  {
    "objectID": "raspi/image_classification/image_classification.html#resources",
    "href": "raspi/image_classification/image_classification.html#resources",
    "title": "Image Classification",
    "section": "Resources",
    "text": "Resources\n\nDataset Example\nSetup Test Notebook on a Raspi\nImage Classification Notebook on a Raspi\nCNN to classify Cifar-10 dataset at CoLab\nCifar 10 - Image Classification on a Raspi\nPython Scripts\nEdge Impulse Project"
  },
  {
    "objectID": "raspi/object_detection/object_detection.html#coming-soon.",
    "href": "raspi/object_detection/object_detection.html#coming-soon.",
    "title": "Object Detection",
    "section": "Coming soon.",
    "text": "Coming soon."
  },
  {
    "objectID": "raspi/counting_objects_yolo/counting_objects_yolo.html#introduction",
    "href": "raspi/counting_objects_yolo/counting_objects_yolo.html#introduction",
    "title": "Counting objects with YOLO",
    "section": "Introduction",
    "text": "Introduction\nAt the Federal University of Itajuba in Brazil, with the master’s student José Anderson Reis and Professor José Alberto Ferreira Filho, we are exploring a project that delves into the intersection of technology and nature. This tutorial will review our first steps and share our observations on deploying YOLOv8, a cutting-edge machine learning model, on the compact and efficient Raspberry Pi Zero 2W (Raspi-Zero). We aim to estimate the number of bees entering and exiting their hive—a task crucial for beekeeping and ecological studies.\nWhy is this important? Bee populations are vital indicators of environmental health, and their monitoring can provide essential data for ecological research and conservation efforts. However, manual counting is labor-intensive and prone to errors. By leveraging the power of embedded machine learning, or tinyML, we automate this process, enhancing accuracy and efficiency.\n\n\n\nimg\n\n\nThis tutorial will cover setting up the Raspberry Pi, integrating a camera module, optimizing and deploying YOLOv8 for real-time image processing, and analyzing the data gathered."
  },
  {
    "objectID": "raspi/counting_objects_yolo/counting_objects_yolo.html#installing-and-using-ultralytics-yolov8",
    "href": "raspi/counting_objects_yolo/counting_objects_yolo.html#installing-and-using-ultralytics-yolov8",
    "title": "Counting objects with YOLO",
    "section": "Installing and using Ultralytics YOLOv8",
    "text": "Installing and using Ultralytics YOLOv8\nUltralytics YOLOv8, is a version of the acclaimed real-time object detection and image segmentation model, YOLO. YOLOv8 is built on cutting-edge advancements in deep learning and computer vision, offering unparalleled performance in terms of speed and accuracy. Its streamlined design makes it suitable for various applications and easily adaptable to different hardware platforms, from edge devices to cloud APIs.\nLet’s start installing the Ultarlytics packages for local inference on the Rasp-Zero:\n\nUpdate the packages list, install pip, and upgrade to the latest:\n\nsudo apt update\nsudo apt install python3-pip -y\npip install -U pip\n\nInstall the ultralytics pip package with optional dependencies:\n\npip install ultralytics[export]\n\nReboot the device:\n\nsudo reboot\n\nTesting the YOLO\nAfter the Rasp-Zero booting, let’s create a directory for working with YOLO and change the current location to it::\nmkdir Documents/YOLO\ncd Documents/YOLO\nLet’s run inference on an image that will be downloaded from the Ultralytics website, using the YOLOV8n model (the smallest in the family) at the Terminal (CLI):\nyolo predict model='yolov8n' source='https://ultralytics.com/images/bus.jpg'\nThe inference result will appear in the terminal. In the image (bus.jpg), 4 persons, 1 bus, and 1 stop signal were detected:\n\nAlso, we got a message that Results saved to runs/detect/predict4. Inspecting that directory, we can see a new image saved (bus.jpg). Let’s download it from the Rasp-Zero to our desktop for inspection:\n\nSo, the Ultrayitics YOLO is correctly installed on our Rasp-Zero.\n\n\nExport Model to NCNN format\nAn issue is the high latency for this inference, 7.6 s, even with the smaller model of the family (YOLOv8n). This is a reality of deploying computer vision models on edge devices with limited computational power, such as the Rasp-Zero. One alternative is to use a format optimized for optimal performance. This ensures that even devices with limited processing power can handle advanced computer vision tasks well.\nOf all the model export formats supported by Ultralytics, the NCNN is a high-performance neural network inference computing framework optimized for mobile platforms. From the beginning of the design, NCNN was deeply considerate about deployment and use on mobile phones and did not have third-party dependencies. It is cross-platform and runs faster than all known open-source frameworks (such as TFLite).\nNCNN delivers the best inference performance when working with Raspberry Pi devices. NCNN is highly optimized for mobile embedded platforms (such as ARM architecture).\nSo, let’s convert our model and rerun the inference:\n\nExport a YOLOv8n PyTorch model to NCNN format, creating: ‘/yolov8n_ncnn_model’\n\nyolo export model=yolov8n.pt format=ncnn \n\nRun inference with the exported model (now the source could be the bus.jpg image that was downloaded from the website to the current directory on the last inference):\n\nyolo predict model='./yolov8n_ncnn_model' source='bus.jpg'\nNow, we can see that the latency was reduced by half.\n\n\n\nTalking about the YOLO Model\nThe YOLO (You Only Look Once) model is a highly efficient and widely used object detection algorithm known for its real-time processing capabilities. Unlike traditional object detection systems that repurpose classifiers or localizers to perform detection, YOLO frames the detection problem as a single regression task. This innovative approach enables YOLO to simultaneously predict multiple bounding boxes and their class probabilities from full images in one evaluation, significantly boosting its speed.\n\nKey Features:\n\nSingle Network Architecture:\n\nYOLO employs a single neural network to process the entire image. This network divides the image into a grid and, for each grid cell, directly predicts bounding boxes and associated class probabilities. This end-to-end training improves speed and simplifies the model architecture.\n\nReal-Time Processing:\n\nOne of YOLO’s standout features is its ability to perform object detection in real time. Depending on the version and hardware, YOLO can process images at high frames per second (FPS). This makes it ideal for applications requiring quick and accurate object detection, such as video surveillance, autonomous driving, and live sports analysis.\n\nEvolution of Versions:\n\nOver the years, YOLO has undergone significant improvements, from YOLOv1 to the latest YOLOv10. Each iteration has introduced enhancements in accuracy, speed, and efficiency. YOLOv8, for instance, incorporates advancements in network architecture, improved training methodologies, and better support for various hardware, ensuring a more robust performance.\nAlthough YOLOv10 is the family’s newest member with an encouraging performance based on its paper, it was just released (May 2024) and is not fully integrated with the Ultralitycs library. Conversely, the precision-recall curve analysis suggests that YOLOv8 generally outperforms YOLOv9, capturing a higher proportion of true positives while minimizing false positives more effectively (for more details, see this article). So, this work is based on the YOLOv8n.\n\n\nAccuracy and Efficiency:\n\nWhile early versions of YOLO traded off some accuracy for speed, recent versions have made substantial strides in balancing both. The newer models are faster and more accurate, detecting small objects (such as bees) and performing well on complex datasets.\n\nWide Range of Applications:\n\nYOLO’s versatility has led to its adoption in numerous fields. It is used in traffic monitoring systems to detect and count vehicles, security applications to identify potential threats, and agricultural technology to monitor crops and livestock. Its application extends to any domain requiring efficient and accurate object detection.\n\nCommunity and Development:\n\nYOLO continues to evolve and is supported by a strong community of developers and researchers (being the YOLOv8 very strong). Open-source implementations and extensive documentation have made it accessible for customization and integration into various projects. Popular deep learning frameworks like Darknet, TensorFlow, and PyTorch support YOLO, further broadening its applicability.\nUltralitics YOLOv8 can not only Detect (our case here) but also Segment and Pose models pre-trained on the COCO dataset and YOLOv8 Classify models pre-trained on the ImageNet dataset. Track mode is available for all Detect, Segment, and Pose models.\n\n\n\nIn this tutorial, we leverage the power of YOLOv8 exported to NCNN format to estimate the number of bees at a beehive entrance using a Raspberry Pi Zero 2W (Rasp-Zero) in real-time. This setup demonstrates the practicality and effectiveness of deploying advanced machine learning models on edge devices for real-time environmental monitoring.\n\n\n\nExploring YOLO with Python\nTo start, let’s call the Python Interpreter so we can explore how the YOLO model works, line by line:\npython\n\nNow, we should call the YOLO library from Ultralitics and load the model:\nfrom ultralytics import YOLO\nmodel = YOLO('yolov8n_ncnn_model')\nNext, run inference over an image (let’s use again bus.jpg):\nimg = 'bus.jpg'\nresult = model.predict(img, save=True, imgsz=640, conf=0.5, iou=0.3)\n\nWe can verify that the result is the same as the one we get running the inference at the terminal level (CLI).\nimage 1/1 /home/mjrovai/Documents/YOLO/bus.jpg: 640x640 3 persons, 1 bus, 4048.5ms\nSpeed: 635.7ms preprocess, 4048.5ms inference, 33897.6ms postprocess per image at shape (1, 3, 640, 640)\n\nResults saved to runs/detect/predict7 \nBut, we are interested in analyzing the “result” content.\nFor example, we can see result[0].boxes.data, showing us the main inference result, which is a tensor shape (4, 6). Each line is one of the objects detected, being the 4 first columns, the bounding boxes coordinates, the 5th, the confidence, and the 6th, the class (in this case, 0: person and 5: bus):\n\nWe can access several inference results separately, as the inference time, and have it printed in a better format:\ninference_time = int(result[0].speed['inference'])\nprint(f\"Inference Time: {inference_time} ms\")\nOr we can have the total number of objects detected:\nprint(f'Number of objects: {len (result[0].boxes.cls)}')\n\nWith Python, we can create a detailed output that meets our needs. In our final project, we will run a Python script at once rather than manually entering it line by line in the interpreter.\nFor that, let’s use nano as our text editor. First, we should create an empty Python script named, for example, yolov8_tests.py:\nnano yolov8_tests.py\nEnter with the code lines:\nfrom ultralytics import YOLO\n\n# Load the YOLOv8 model\nmodel = YOLO('yolov8n_ncnn_model')\n\n# Run inference\nimg = 'bus.jpg'\nresult = model.predict(img, save=False, imgsz=640, conf=0.5, iou=0.3)\n\n# print the results\ninference_time = int(result[0].speed['inference'])\nprint(f\"Inference Time: {inference_time} ms\")\nprint(f'Number of objects: {len (result[0].boxes.cls)}')\n\nAnd enter with the commands: [CTRL+O] + [ENTER] +[CTRL+X] to save the Python script.\nRun the script:\npython yolov8_tests.py\n\nWe can verify again that the result is precisely the same as when we run the inference at the terminal level (CLI) and with the built-in Python interpreter.\n\nNote about the Latency:\nThe process of calling the YOLO library and loading the model for inference for the first time takes a long time, but the inferences after that will be much faster. For example, the first single inference took 3 to 4 seconds, but after that, the inference time is reduced to less than 1 second."
  },
  {
    "objectID": "raspi/counting_objects_yolo/counting_objects_yolo.html#estimating-the-number-of-bees",
    "href": "raspi/counting_objects_yolo/counting_objects_yolo.html#estimating-the-number-of-bees",
    "title": "Counting objects with YOLO",
    "section": "Estimating the number of Bees",
    "text": "Estimating the number of Bees\nFor our project at the university, we are preparing to collect a dataset of bees at the entrance of a beehive using the same camera connected to the Rasp-Zero. The images should be collected every 10 seconds. With the Arducam OV5647, the horizontal Field of View (FoV) is 53.5o, which means that a camera positioned at the top of a standard Hive (46 cm) will capture all of its entrance (about 47 cm).\n\n\nDataset\nThe dataset collection is the most critical phase of the project and should take several weeks or months. For this tutorial, we will use a public dataset: “Sledevic, Tomyslav (2023), “[Labeled dataset for bee detection and direction estimation on beehive landing boards,” Mendeley Data, V5, doi: 10.17632/8gb9r2yhfc.5”\nThe original dataset has 6,762 images (1920 x 1080), and around 8% of them (518) have no bees (only background). This is very important with Object Detection, where we should keep around 10% of the dataset with only background (without any objects to be detected).\nThe images contain from zero to up to 61 bees:\n\nWe downloaded the dataset (images and annotations) and uploaded it to Roboflow. There, you should create a free account and start a new project, for example, (“Bees_on_Hive_landing_boards”):\n\n\nWe will not enter details about the Roboflow process once many tutorials are available.\n\nOnce the project is created and the dataset is uploaded, you should review the annotations using the “Auto-Label” Tool. Note that all images with only a background should be saved w/o any annotations. At this step, you can also add additional images.\n\nOnce all images are annotated, you should split them into training, validation, and testing.\n\n\n\nPre-Processing\nThe last step with the dataset is preprocessing to generate a final version for training. The Yolov8 model can be trained with 640 x 640 pixels (RGB) images. Let’s resize all images and generate augmented versions of each image (augmentation) to create new training examples from which our model can learn.\nFor augmentation, we will rotate the images (+/-15o) and vary the brightness and exposure.\n\nThis will create a final dataset of 16,228 images.\n\nNow, you should export the model in a YOLOv8 format. You can download a zipped version of the dataset to your desktop or get a downloaded code to be used with a Jupyter Notebook:\n\nAnd that is it! We are prepared to start our training using Google Colab.\n\nThe pre-processed dataset can be found at the Roboflow site.\n\n\n\nTraining YOLOv8 on a Customized Dataset\nFor training, let’s adapt one of the public examples available from Ultralitytics and run it on Google Colab:\n\nyolov8_bees_on_hive_landing_board.ipynb [Open In Colab]\n\n\nCritical points on the Notebook:\n\nRun it with GPU (the NVidia T4 is free)\nInstall Ultralytics using PIP.\n\nNow, you can import the YOLO and upload your dataset to the CoLab, pasting the Download code that you get from Roboflow. Note that your dataset will be mounted under /content/datasets/:\n\n\n\nIt is important to verify and change, if needed, the file data.yaml with the correct path for the images:\n\nnames:\n- bee\nnc: 1\nroboflow:\n  license: CC BY 4.0\n  project: bees_on_hive_landing_boards\n  url: https://universe.roboflow.com/marcelo-rovai-riila/bees_on_hive_landing_boards/dataset/1\n  version: 1\n  workspace: marcelo-rovai-riila\ntest: /content/datasets/Bees_on_Hive_landing_boards-1test/images\ntrain: /content/datasets/Bees_on_Hive_landing_boards-1/train/images\nval: /content/datasets/Bees_on_Hive_landing_boards-1/valid/images\n\nDefine the main hyperparameters that you want to change from default, for example:\nMODEL = 'yolov8n.pt'\nIMG_SIZE = 640\nEPOCHS = 25 # For a final project, you should consider at least 100 epochs \nRun the training (using CLI):\n!yolo task=detect mode=train model={MODEL} data={dataset.location}/data.yaml epochs={EPOCHS} imgsz={IMG_SIZE} plots=True \n\n\n​ The model took 2.7 hours to train and has an excellent result (mAP50 of 0.984). At the end of the training, all results are saved in the folder listed, for example: /runs/detect/train3/. There, you can find, for example, the confusion matrix and the metrics curves per epoch.\n\n\nNote that the trained model (best.pt) is saved in the folder /runs/detect/train3/weights/. Now, you should validade the trained model with the valid/images.\n\n!yolo task=detect mode=val model={HOME}/runs/detect/train3/weights/best.pt data={dataset.location}/data.yaml\n​ The results were similar to training.\n\nNow, we should perform inference on the images left aside for testing\n\n!yolo task=detect mode=predict model={HOME}/runs/detect/train3/weights/best.pt conf=0.25 source={dataset.location}/test/images save=True\nThe inference results are saved in the folder runs/detect/predict. Let’s see some of them:\n\nWe can also perform inference with a completely new and complex image from another beehive with a different background (the beehive of Professor Maurilio of our University). The results were great (but not perfect and with a lower confidence score). The model found 41 bees.\n\n\nThe last thing to do is export the train, validation, and test results for your Drive at Google. To do so, you should mount your drive.\nfrom google.colab import drive\ndrive.mount('/content/gdrive')\nand copy the content of /runs folder to a folder that you should create in your Drive, for example:\n!scp -r /content/runs '/content/gdrive/MyDrive/10_UNIFEI/Bee_Project/YOLO/bees_on_hive_landing'"
  },
  {
    "objectID": "raspi/counting_objects_yolo/counting_objects_yolo.html#inference-with-the-trained-model-using-the-rasp-zero",
    "href": "raspi/counting_objects_yolo/counting_objects_yolo.html#inference-with-the-trained-model-using-the-rasp-zero",
    "title": "Counting objects with YOLO",
    "section": "Inference with the trained model, using the Rasp-Zero",
    "text": "Inference with the trained model, using the Rasp-Zero\nUsing the FileZilla FTP, let’s transfer the best.pt to our Rasp-Zero (before the transfer, you may change the model name, for example, bee_landing_640_best.pt).\nThe first thing to do is convert the model to an NCNN format:\nyolo export model=bee_landing_640_best.pt format=ncnn \nAs a result, a new converted model, bee_landing_640_best_ncnn_model is created in the same directory.\nLet’s create a folder to receive some test images (under Documents/YOLO/:\nmkdir test_images\nUsing the FileZilla FTP, let’s transfer a few images from the test dataset to our Rasp-Zero:\n\nLet’s use the Python Interpreter:\npython\nAs before, we will import the YOLO library and define our converted model to detect bees:\nfrom ultralytics import YOLO\nmodel = YOLO('bee_landing_640_best_ncnn_model')\nNow, let’s define an image and call the inference (we will save the image result this time to external verification):\nimg = 'test_images/15_bees.jpg'\nresult = model.predict(img, save=True, imgsz=640, conf=0.2, iou=0.3)\nThe inference result is saved on the variable result, and the processed image on runs/detect/predict9\n\nUsing FileZilla FTP, we can send the inference result to our Desktop for verification:\n\nlet’s go over the other images, analyzing the number of objects (bees) found:\n\nDepending on the confidence, we can have some false positives or negatives. But in general, with a model trained based on the smaller base model of the YOLOv8 family (YOLOv8n) and also converted to NCNN, the result is pretty good, running on an Edge device such as the Rasp-Zero. Also, note that the inference latency is around 730ms.\nFor example, by running the inference on Maurilio-bee.jpeg, we can find 40 bees. During the test phase on Colab, 41 bees were found (we only missed one here.)"
  },
  {
    "objectID": "raspi/counting_objects_yolo/counting_objects_yolo.html#considerations-about-the-post-processing",
    "href": "raspi/counting_objects_yolo/counting_objects_yolo.html#considerations-about-the-post-processing",
    "title": "Counting objects with YOLO",
    "section": "Considerations about the Post-Processing",
    "text": "Considerations about the Post-Processing\nOur final project should be very simple in terms of code. We will use the camera to capture an image every 10 seconds. As we did in the previous section, the captured image should be the input for the trained and converted model. We should get the number of bees for each image and save it in a database (for example, timestamp: number of bees).\nWe can do it with a single Python script or use a Linux system timer, like cron, to periodically capture images every 10 seconds and have a separate Python script to process these images as they are saved. This method can be particularly efficient in managing system resources and can be more robust against potential delays in image processing.\n\nSetting Up the Image Capture with cron\nFirst, we should set up a cron job to use the rpicam-jpeg command to capture an image every 10 seconds.\n\nEdit the crontab:\n\nOpen the terminal and type crontab -e to edit the cron jobs.\ncron normally doesn’t support sub-minute intervals directly, so we should use a workaround like a loop or watch for file changes.\n\nCreate a Bash Script (capture.sh):\n\nImage Capture: This bash script captures images every 10 seconds using rpicam-jpeg, a command that is part of the raspijpeg tool. This command lets us control the camera and capture JPEG images directly from the command line. This is especially useful because we are looking for a lightweight and straightforward method to capture images without the need for additional libraries like Picamera or external software. The script also saves the captured image with a timestamp.\n\n#!/bin/bash\n# Script to capture an image every 10 seconds\n\nwhile true\ndo\n  DATE=$(date +\"%Y-%m-%d_%H%M%S\")\n  rpicam-jpeg --output test_images/$DATE.jpg --width 640 --height 640\n  sleep 10\ndone\n\nWe should make the script executable with chmod +x capture.sh.\nThe script must start at boot or use a @reboot entry in cron to start it automatically.\n\n\n\n\nSetting Up the Python Script for Inference\nImage Processing: The Python script continuously monitors the designated directory for new images, processes each new image using the YOLOv8 model, updates the database with the count of detected bees, and optionally deletes the image to conserve disk space.\nDatabase Updates: The results, along with the timestamps, are saved in an SQLite database. For that, a simple option is to use sqlite3.\nIn short, we need to write a script that continuously monitors the directory for new images, processes them using a YOLO model, and then saves the results to a SQLite database. Here’s how we can create and make the script executable:\n#!/usr/bin/env python3\nimport os\nimport time\nimport sqlite3\nfrom datetime import datetime\nfrom ultralytics import YOLO\n\n# Constants and paths\nIMAGES_DIR = 'test_images/'\nMODEL_PATH = 'bee_landing_640_best_ncnn_model'\nDB_PATH = 'bee_count.db'\n\ndef setup_database():\n    \"\"\" Establishes a database connection and creates the table if it doesn't exist. \"\"\"\n    conn = sqlite3.connect(DB_PATH)\n    cursor = conn.cursor()\n    cursor.execute('''\n        CREATE TABLE IF NOT EXISTS bee_counts\n        (timestamp TEXT, count INTEGER)\n    ''')\n    conn.commit()\n    return conn\n\ndef process_image(image_path, model, conn):\n    \"\"\" Processes an image to detect objects and logs the count to the database. \"\"\"\n    result = model.predict(image_path, save=False, imgsz=640, conf=0.2, iou=0.3, verbose=False)\n    num_bees = len(result[0].boxes.cls) \n    timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n    cursor = conn.cursor()\n    cursor.execute(\"INSERT INTO bee_counts (timestamp, count) VALUES (?, ?)\", (timestamp, num_bees))\n    conn.commit()\n    print(f'Processed {image_path}: Number of bees detected = {num_bees}')\n\ndef monitor_directory(model, conn):\n    \"\"\" Monitors the directory for new images and processes them as they appear. \"\"\"\n    processed_files = set()\n    while True:\n        try:\n            files = set(os.listdir(IMAGES_DIR))\n            new_files = files - processed_files\n            for file in new_files:\n                if file.endswith('.jpg'):\n                    full_path = os.path.join(IMAGES_DIR, file)\n                    process_image(full_path, model, conn)\n                    processed_files.add(file)\n            time.sleep(1)  # Check every second\n        except KeyboardInterrupt:\n            print(\"Stopping...\")\n            break\n\ndef main():\n    conn = setup_database()\n    model = YOLO(MODEL_PATH)\n    monitor_directory(model, conn)\n    conn.close()\n\nif __name__ == \"__main__\":\n    main()\nThe python script must be executable, for that:\n\nSave the script: For example, as process_images.py.\nChange file permissions to make it executable:\nchmod +x process_images.py\nRun the script directly from the command line:\n./process_images.py\n\nWe should consider keeping the script running even after closing the terminal; for that, we can use nohup or screen:\nnohup ./process_images.py &\nor\nscreen -S bee_monitor\n./process_images.py\nNote that we are capturing images with their own timestamp and then log a separate timestamp for when the inference results are saved to the database. This approach can be beneficial for the following reasons:\n\nAccuracy in Data Logging:\n\nCapture Timestamp: The timestamp associated with each image capture represents the exact moment the image was taken. This is crucial for applications where precise timing of events (like bee activity) is important for analysis.\nInference Timestamp: This timestamp indicates when the image was processed and the results were recorded in the database. This can differ from the capture time due to processing delays or if the image processing is batched or queued.\n\nPerformance Monitoring:\n\nHaving separate timestamps allows us to monitor the performance and efficiency of your image processing pipeline. We can measure the delay between image capture and result logging, which helps optimize the system for real-time processing needs.\n\nTroubleshooting and Audit:\n\nSeparate timestamps provide a better audit trail and troubleshooting data. If there are issues with the image processing or data recording, having distinct timestamps can help isolate whether delays or problems occurred during capture, processing, or logging.\n\n\n\n\nScript For Reading the SQLite Database\nHere is an example of a code to retrieve the data from the database:\n#!/usr/bin/env python3\nimport sqlite3\n\ndef main():\n    db_path = 'bee_count.db'\n    conn = sqlite3.connect(db_path)\n    cursor = conn.cursor()\n    query = \"SELECT * FROM bee_counts\"\n    cursor.execute(query)\n    data = cursor.fetchall()\n    for row in data:\n        print(f\"Timestamp: {row[0]}, Number of bees: {row[1]}\")\n    conn.close()\n\nif __name__ == \"__main__\":\n    main()\n\n\nAdding Environment data\nBesides bee counting, environmental data, such as temperature and humidity, are essential for monitoring the bee-have health. Using a Rasp-Zero, it is straightforward to add a digital sensor such as the DHT-22 to get this data.\n\nEnvironmental data will be part of our final project. If you want to know more about connecting sensors to a Raspberry Pi and, even more, how to save the data to a local database and send it to the web, follow this tutorial: From Data to Graph: A Web Journey With Flask and SQLite."
  },
  {
    "objectID": "raspi/counting_objects_yolo/counting_objects_yolo.html#conclusion",
    "href": "raspi/counting_objects_yolo/counting_objects_yolo.html#conclusion",
    "title": "Counting objects with YOLO",
    "section": "Conclusion",
    "text": "Conclusion\nIn this tutorial, we have thoroughly explored integrating the YOLOv8 model with a Raspberry Pi Zero 2W to address the practical and pressing task of counting (or better, “estimating”) bees at a beehive entrance. Our project underscores the robust capability of embedding advanced machine learning technologies within compact edge computing devices, highlighting their potential impact on environmental monitoring and ecological studies.\nThis tutorial provides a step-by-step guide to the practical deployment of the YOLOv8 model. We demonstrate a tangible example of a real-world application by optimizing it for edge computing in terms of efficiency and processing speed (using NCNN format). This not only serves as a functional solution but also as an instructional tool for similar projects.\nThe technical insights and methodologies shared in this tutorial are the basis for the complete work to be developed at our university in the future. We envision further development, such as integrating additional environmental sensing capabilities and refining the model’s accuracy and processing efficiency. Implementing alternative energy solutions like the proposed solar power setup will expand the project’s sustainability and applicability in remote or underserved locations."
  },
  {
    "objectID": "raspi/counting_objects_yolo/counting_objects_yolo.html#resources",
    "href": "raspi/counting_objects_yolo/counting_objects_yolo.html#resources",
    "title": "Counting objects with YOLO",
    "section": "Resources",
    "text": "Resources\n\nThe Dataset paper, Notebooks, and PDF version are in the Project repository."
  },
  {
    "objectID": "raspi/llm/llm.html#coming-soon.",
    "href": "raspi/llm/llm.html#coming-soon.",
    "title": "Small Language Models (SLM)",
    "section": "Coming soon.",
    "text": "Coming soon."
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "TinyML4D\nTinyML Made Easy, an eBook collection of a series of Hands-On tutorials, is part of the TinyML4D, an initiative to make Embedded Machine Learning (TinyML) education available to everyone, explicitly enabling innovative solutions for the unique challenges Developing Countries face."
  },
  {
    "objectID": "references.html#to-learn-more",
    "href": "references.html#to-learn-more",
    "title": "References",
    "section": "To learn more:",
    "text": "To learn more:\n\nOnline Courses\n\nHarvard School of Engineering and Applied Sciences - CS249r: Tiny Machine Learning\nProfessional Certificate in Tiny Machine Learning (TinyML) – edX/Harvard\nIntroduction to Embedded Machine Learning - Coursera/Edge Impulse\nComputer Vision with Embedded Machine Learning - Coursera/Edge Impulse\nUNIFEI-IESTI01 TinyML: “Machine Learning for Embedding Devices”\n\n\n\nBooks\n\n“Python for Data Analysis” by Wes McKinney\n“Deep Learning with Python” by François Chollet - GitHub Notebooks\n“TinyML” by Pete Warden and Daniel Situnayake\n“TinyML Cookbook 2nd Edition” by Gian Marco Iodice\n“Technical Strategy for AI Engineers, In the Era of Deep Learning” by Andrew Ng\n“AI at the Edge” book by Daniel Situnayake and Jenny Plunkett\n“XIAO: Big Power, Small Board” by Lei Feng and Marcelo Rovai\n“MACHINE LEARNING SYSTEMS for TinyML” by a collaborative effort\n\n\n\nProjects Repository\n\nEdge Impulse Expert Network"
  },
  {
    "objectID": "about_the_author.html",
    "href": "about_the_author.html",
    "title": "About the author",
    "section": "",
    "text": "Marcelo Rovai, a Brazilian living in Chile, is a recognized figure in engineering and technology education. He holds the title of Professor Honoris Causa from the Federal University of Itajubá (UNIFEI), Brazil. His educational background includes an Engineering degree from UNIFEI and a specialization from the Polytechnic School of São Paulo University (USP). Further enhancing his expertise, he earned an MBA from IBMEC (INSPER) and a Master’s in Data Science from the Universidad del Desarrollo (UDD) in Chile.\nWith a career spanning several high-profile technology companies such as AVIBRAS Airspace, ATT, NCR, and IGT, where he served as Vice President for Latin America, he brings a wealth of industry experience to his academic endeavors. He is a prolific writer on electronics-related topics and shares his knowledge through open platforms like Hackster.io.\nIn addition to his professional pursuits, he is dedicated to educational outreach, serving as a volunteer professor at UNIFEI and engaging with the TinyML4D group as a Co-Chair, promoting TinyML education in developing countries. His work underscores a commitment to leveraging technology for societal advancement.\nLinkedIn profile: https://www.linkedin.com/in/marcelo-jose-rovai-brazil-chile/\nLectures, books, papers, and tutorials: https://github.com/Mjrovai/TinyML4D"
  }
]