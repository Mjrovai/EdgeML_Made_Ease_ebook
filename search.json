[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Edge AI Engineering",
    "section": "",
    "text": "Preface\nIn the rapidly evolving landscape of technology, the convergence of artificial intelligence and edge computing stands as one of the most exciting frontiers. This intersection promises to revolutionize how we interact with the world around us, bringing intelligence and decision-making capabilities directly to the devices we use every day. At the heart of this revolution lies the Raspberry Pi, a powerful yet accessible single-board computer (SBC) that has democratized computing and now stands poised to do the same for edge AI.\nThis book, which serves as the official textbook for IESTI05 Edge AI Engineering at the Federal University of Itajubá (UNIFEI) in Brazil, embodies both a passion for technology and a conviction in its capacity to address real-world problems. While developed to support UNIFEI’s engineering curriculum, the content is designed to be valuable for all learners, whether in academic settings or pursuing independent study.\n“Edge AI Engineering: Hands-on with the Raspberry Pi” is not just about theory or abstract concepts. It’s about getting your hands dirty, writing code, training models, and seeing your creations come to life. Each chapter blends foundational knowledge with practical application, focusing on what’s possible with the Raspberry Pi platform.\nFrom the compact Raspberry Pi Zero to the more powerful Pi 5, we explore how these incredible devices can become the brains of intelligent systems—recognizing images, understanding speech, detecting objects, and even running small language models. Each project serves as a stepping stone, building your skills and confidence as you progress.\nBeyond the technical skills, this book aims to instill something more valuable – a sense of curiosity and possibility. The field of edge AI is still in its infancy, with new applications and techniques emerging daily. By mastering the fundamentals presented here, you’ll be well-equipped to explore these frontiers, perhaps even pushing the boundaries of what’s possible on edge devices.\nWhether you’re a student seeking to understand AI’s practical applications, a professional looking to expand your skill set, or an enthusiast eager to add intelligence to your projects, we hope this book serves as both a guide and an inspiration.\nAs you embark on this journey, remember that every expert was once a beginner. The learning path is filled with challenges and moments of joy and discovery. Embrace both, and let your creativity guide you.\nThank you for joining us on this exciting adventure into edge machine learning. Let’s begin exploring what’s possible when we bring AI to the edge, one Raspberry Pi at a time.\nHappy coding, and may your models always converge!\nProf. Marcelo Rovai\nBrazil, September 2025"
  },
  {
    "objectID": "Acknowledgements.html",
    "href": "Acknowledgements.html",
    "title": "Acknowledgments",
    "section": "",
    "text": "I extend my deepest gratitude to the entire TinyML4D Academic Network, comprised of distinguished professors, researchers, and professionals. Notable contributions from Marco Zennaro, Ermanno Petrosemoli, Brian Plancher, José Alberto Ferreira, Jesus Lopez, Diego Mendez, Shawn Hymel, Dan Situnayake, Pete Warden, and Laurence Moroney have been instrumental in advancing our understanding of Embedded Machine Learning (TinyML) and Edge AI.\nSpecial commendation is reserved for Professor Vijay Janapa Reddi of Harvard University. His steadfast belief in the transformative potential of open-source communities, coupled with his invaluable guidance and teachings, has served as a beacon and a cornerstone for our efforts from the beginning.\nAcknowledging these individuals, we pay tribute to the collective wisdom and dedication that have enriched this field and our work.\n\n\nGoogle ImageFX and OpenAI’s DALL-E generated illustrations of some of the images on the book and chapter covers. Claude Sonnet helped with code and text reviews."
  },
  {
    "objectID": "intro.html#edge-ai-engineering",
    "href": "intro.html#edge-ai-engineering",
    "title": "Introduction",
    "section": "Edge AI Engineering",
    "text": "Edge AI Engineering\nIn today’s rapidly evolving technological landscape, the convergence of artificial intelligence and edge computing represents one of the most promising frontiers of innovation. Edge AI—the practice of running AI algorithms locally on hardware devices rather than in the cloud—transforms how we interact with technology daily, enabling more responsive, private, and efficient intelligent systems.\nThis book, “Edge AI Engineering: Hands-on with the Raspberry Pi,” is your practical guide to this exciting field. We’ll explore fixed-function AI (reactive systems that process specific inputs) and generative AI (proactive systems that create new content) through hands-on projects using the versatile and accessible Raspberry Pi platform.\n\nWhy Edge AI Matters\nTraditional AI deployment often relies on cloud infrastructure, requiring constant connectivity and introducing latency. Edge AI addresses these limitations by bringing intelligence directly to where data is generated and actions occur. This approach offers several compelling advantages:\n\nReduced latency: Process data locally for near-instantaneous responses\nEnhanced privacy: Keep sensitive information on your device rather than sending it to remote servers\nNetwork independence: Maintain functionality even without internet connectivity\nLower bandwidth usage: Process data locally, sending only relevant results when needed\nEnergy efficiency: Optimize processing for resource-constrained environments\n\n\n\nThe Raspberry Pi Advantage\nThe Raspberry Pi, with its combination of affordability, processing capability, and extensive GPIO options, provides an ideal platform for exploring Edge AI concepts. From the compact Raspberry Pi Zero 2W to the more powerful Pi 5, these devices offer:\n\nSufficient computational power for running optimized AI models\nA complete Linux-based operating system for straightforward development\nExtensive connectivity options for integrating with sensors and actuators\nA vibrant community and ecosystem of libraries and tools\nAn accessible entry point for students, hobbyists, and professionals alike\n\n\n\nWhat You’ll Learn\nThis book takes a progressive approach to Edge AI engineering, starting with foundational concepts and building toward more advanced applications:\n\nEssential setup and configuration: Prepare your Raspberry Pi for Edge AI development\nComputer vision applications: Implement image classification and object detection systems\nSmall Language Models (SLMs): Run and optimize language models directly on your Raspberry Pi\nVision-Language Models: Explore multimodal AI with Florence-2\nPhysical computing integration: Connect AI systems with sensors and actuators\nAdvanced optimization techniques: Enhance model performance through methods like RAG, agents, and function calling\n\nEach chapter includes detailed explanations, step-by-step instructions, and practical projects demonstrating real-world applications of Edge AI concepts.\n\n\nWho This Book Is For\nWhether you’re a student exploring AI for the first time, an educator developing a curriculum, a maker building innovative projects, or a professional seeking to expand your skills, this book provides the knowledge and hands-on experience needed to implement Edge AI solutions on the Raspberry Pi platform successfully.\nJoin us on this journey to the edge of AI innovation, where we’ll bridge theory and practice through engaging, accessible projects that demonstrate the transformative potential of intelligent edge computing."
  },
  {
    "objectID": "about_book.html#key-features",
    "href": "about_book.html#key-features",
    "title": "About this Book",
    "section": "Key Features",
    "text": "Key Features\n\nProgressive Learning Path: The book structure follows a natural progression from basic to advanced concepts, beginning with foundational computer vision applications and advancing to generative AI techniques.\nModel-Specific Optimizations: Each chapter provides targeted guidance for different Raspberry Pi models, helping you maximize performance whether using a Pi Zero 2W or Pi 5.\nOpen-Source Foundation: We emphasize accessible tools and frameworks, including Edge Impulse Studio, TensorFlow Lite, PyTorch, Transformers, and Ollama, ensuring you can continue your learning journey with widely available resources.\nPractical Problem-Solving: Rather than abstract exercises, each project addresses real-world challenges that demonstrate the practical value of Edge AI.\nResource Optimization Techniques: Learn essential strategies for deploying AI on resource-constrained devices, balancing performance needs with hardware limitations.\nCross-Domain Applications: Explore implementations spanning computer vision, natural language processing, and physical computing, showcasing the versatility of Edge AI."
  },
  {
    "objectID": "about_book.html#structure-and-organization",
    "href": "about_book.html#structure-and-organization",
    "title": "About this Book",
    "section": "Structure and Organization",
    "text": "Structure and Organization\nThe book is organized into two main sections:\n\nFixed Function AI (Computer Vision): Chapters covering image classification, object detection, and specialized applications like object counting.\nGenerative AI (Language and Vision Models): Chapters exploring Small Language Models, Vision-Language Models, physical computing integration, and advanced optimization techniques.\n\nEach chapter follows a consistent format that includes:\n\nConceptual background and theory\nStep-by-step implementation guides\nPractical projects with complete code\nPerformance optimization strategies\nIdeas for further exploration"
  },
  {
    "objectID": "about_book.html#prerequisites",
    "href": "about_book.html#prerequisites",
    "title": "About this Book",
    "section": "Prerequisites",
    "text": "Prerequisites\nWhile designed to be accessible, readers will benefit from:\n\nBasic Python programming knowledge\nFamiliarity with Linux command-line basics\nElementary understanding of machine learning concepts\nPrevious experience with Raspberry Pi (helpful but not required)\n\nBy completing this book, you’ll possess the skills to design, implement, and optimize Edge AI applications across a wide range of use cases, leveraging the unique capabilities of the Raspberry Pi platform to bring intelligence to the edge."
  },
  {
    "objectID": "Classification_of_AI_Applications.html#fixed-function-ai-vs.-generative-ai",
    "href": "Classification_of_AI_Applications.html#fixed-function-ai-vs.-generative-ai",
    "title": "Classification of AI Applications",
    "section": "Fixed Function AI vs. Generative AI",
    "text": "Fixed Function AI vs. Generative AI\nAI applications can be broadly categorized into two approaches that represent different capabilities, interaction models, and implementation strategies:\n\nFixed Function AI (Reactive)\nFixed Function AI, or Reactive AI, operates by analyzing specific inputs according to predetermined patterns and rules and then producing consistent outputs for given scenarios. These systems:\n\nRespond to specific triggers: They activate only when presented with particular inputs.\nFollow defined patterns: Their behavior is predictable and consistent.\nExcel at structured tasks: They perform exceptionally well at classification, detection, and pattern recognition\nOperate within boundaries: Their capabilities are limited to their specific programming.\n\nIn the first part of this book (Chapters 2-4), we explore fixed-function AI through computer vision applications:\n\nImage classification for identifying objects in images\nObject detection for locating and labeling multiple objects\nSpecialized detection applications like counting objects\n\nThese applications demonstrate how edge devices can deliver reliable, efficient AI in constrained environments, focusing on specific, well-defined tasks.\n\n\nGenerative AI (Proactive)\nGenerative AI, also known as Proactive AI, represents a fundamental shift in capability. These systems can:\n\nCreate new content: They generate novel text, images, or solutions.\nUnderstand context: They interpret and respond to nuanced situations.\nEngage in dialogue: They maintain contextual awareness across interactions.\nAdapt to novel scenarios: They apply knowledge to situations beyond their explicit training.\n\nThe second part of this book (Chapters 5-9) explores Generative AI at the edge:\n\nSmall Language Models that bring conversational AI to edge devices\nVision-Language Models that combine visual and textual understanding\nPhysical computing integration that connects AI to the real world\nAdvanced techniques to enhance edge AI capabilities\n\nThis progression from Fixed Function to Generative AI mirrors the evolution of artificial intelligence itself—from specialized systems designed for specific tasks to more flexible, creative systems capable of addressing a broader range of challenges.\n\n\nSummary Table\n\n\n\n\n\n\n\n\n\nAI Type\nCore Focus\nExample Applications\nTypical Use Cases\n\n\n\n\nFixed Function (Reactive)\nData analysis, assessment, automation\nFraud detection, spam filters, image recognition\nBanking, healthcare diagnostics, security systems\n\n\nGenerative (Proactive)\nContent creation, anticipation, dialogue\nChatGPT, DALL·E, predictive maintenance, smart assistants\nContent creation, customer support, design, automation\n\n\n\n\n\nConclusion\n\nFixed Function (Reactive) AI is ideal for applications requiring efficiency, predictability, and low resource use, where tasks are well-defined and do not require creative output or adaptation.\nGenerative (Proactive) AI is suited for scenarios demanding creativity, personalization, and anticipatory actions. It enables richer, more human-like interactions and innovative solutions across industries."
  },
  {
    "objectID": "Classification_of_AI_Applications.html#the-edge-ai-advantage",
    "href": "Classification_of_AI_Applications.html#the-edge-ai-advantage",
    "title": "Classification of AI Applications",
    "section": "The Edge AI Advantage",
    "text": "The Edge AI Advantage\nBoth Fixed Function and Generative AI gain unique benefits when deployed at the edge:\n\nReduced latency: Processing happens locally, eliminating network delays\nEnhanced privacy: Sensitive data remains on-device\nOperational reliability: Systems function regardless of network connectivity\nResource efficiency: Optimized models utilize limited hardware effectively\n\nBy understanding these fundamental classifications, you’ll realize how different AI approaches serve distinct purposes and how each can be effectively implemented on edge devices like the Raspberry Pi.\nAs we progress through this book, this classification framework will help contextualize each project and technique, connecting individual implementations to broader AI concepts and applications."
  },
  {
    "objectID": "raspi/setup/setup.html#introduction",
    "href": "raspi/setup/setup.html#introduction",
    "title": "Setup",
    "section": "Introduction",
    "text": "Introduction\nThe Raspberry Pi is a powerful and versatile single-board computer that has become an essential tool for engineers across various disciplines. Developed by the Raspberry Pi Foundation, these compact devices offer a unique combination of affordability, computational power, and extensive GPIO (General Purpose Input/Output) capabilities, making them ideal for prototyping, embedded systems development, and advanced engineering projects.\n\nKey Features\n\nComputational Power: Despite their small size, Raspberry Pis offer significant processing capabilities, with the latest models featuring multi-core ARM processors and up to 8GB of RAM.\nGPIO Interface: The 40-pin GPIO header allows direct interaction with sensors, actuators, and other electronic components, facilitating hardware-software integration projects.\nExtensive Connectivity: Built-in Wi-Fi, Bluetooth, Ethernet, and multiple USB ports enable diverse communication and networking projects.\nLow-Level Hardware Access: Raspberry Pis provides access to interfaces like I2C, SPI, and UART, allowing for detailed control and communication with external devices.\nReal-Time Capabilities: With proper configuration, Raspberry Pis can be used for soft real-time applications, making them suitable for control systems and signal processing tasks.\nPower Efficiency: Low power consumption enables battery-powered and energy-efficient designs, especially in models like the Pi Zero.\n\n\n\nRaspberry Pi Models (covered in this book)\n\nRaspberry Pi Zero 2 W (Raspi-Zero):\n\nIdeal for: Compact embedded systems\nKey specs: 1GHz single-core CPU (ARM Cortex-A53), 512MB RAM, minimal power consumption (usually, around 600mW in Idle. Also, the power-off (“zombie current”) is very low, around 45mA (225mW), which is significantly less than full-size Raspberry Pi models.\n\nRaspberry Pi 5 (Raspi-5):\n\nIdeal for more demanding applications, such as edge computing, computer vision, and edgeAI applications, including LLMs.\nKey specs: 2.4GHz quad-core CPU (ARM Cortex A-76), up to 8GB RAM, PCIe interface for expansions, higher power consumption (usually, around 3-3.5W in Idle)\n\n\n\n\nEngineering Applications\n\nEmbedded Systems Design: Develop and prototype embedded systems for real-world applications.\nIoT and Networked Devices: Create interconnected devices and explore protocols like MQTT, CoAP, and HTTP/HTTPS.\nControl Systems: Implement feedback control loops, PID controllers, and interface with actuators.\nComputer Vision and AI: Utilize libraries like OpenCV and TensorFlow Lite for image processing and machine learning at the edge.\nData Acquisition and Analysis: Collect sensor data, perform real-time analysis, and create data logging systems.\nRobotics: Build robot controllers, implement motion planning algorithms, and interface with motor drivers.\nSignal Processing: Perform real-time signal analysis, filtering, and DSP applications.\nNetwork Security: Set up VPNs, firewalls, and explore network penetration testing.\n\nThis tutorial will guide you through setting up the most common Raspberry Pi models, enabling you to start on your machine learning project quickly. We’ll cover hardware setup, operating system installation, and initial configuration, focusing on preparing your Pi for Machine Learning applications."
  },
  {
    "objectID": "raspi/setup/setup.html#hardware-overview",
    "href": "raspi/setup/setup.html#hardware-overview",
    "title": "Setup",
    "section": "Hardware Overview",
    "text": "Hardware Overview\n\nRaspberry Pi Zero 2W\n\n\nProcessor: 1GHz quad-core 64-bit Arm Cortex-A53 CPU\nRAM: 512MB SDRAM\nWireless: 2.4GHz 802.11 b/g/n wireless LAN, Bluetooth 4.2, BLE\nPorts: Mini HDMI, micro USB OTG, CSI-2 camera connector\nPower: 5V/2.5A via micro USB port 12.5W Micro USB power supply\n\n\n\nRaspberry Pi 5\n\n\nProcessor:\n\nPi 5: Quad-core 64-bit Arm Cortex-A76 CPU @ 2.4GHz\nPi 4: Quad-core Cortex-A72 (ARM v8) 64-bit SoC @ 1.5GHz\n\nRAM: 2GB, 4GB, or 8GB options (8GB recommended for AI tasks)\nWireless: Dual-band 802.11ac wireless (2.4 GHz and 5 GHz), Bluetooth 5.0\nPorts: 2 × micro HDMI ports, 2 × USB 3.0 ports, 2 × USB 2.0 ports, CSI camera port, DSI display port\nPower: 5V/5A, 5V/3A limits peripherals to 600mA, via USB-C connector 27W USB-C power supply\n\n\nIn the labs, we will use different names to address the Raspberry Pi: Raspi, Raspi-5, Raspi-Zero, etc. Usually, Raspi or Raspberry Pi is used when the instructions or comments apply to every model."
  },
  {
    "objectID": "raspi/setup/setup.html#installing-the-operating-system",
    "href": "raspi/setup/setup.html#installing-the-operating-system",
    "title": "Setup",
    "section": "Installing the Operating System",
    "text": "Installing the Operating System\n\nThe Operating System (OS)\nAn operating system (OS) is essential software that manages computer hardware and software resources, providing standard services for computer programs. It is the core software that runs on a computer, serving as an intermediary between hardware and application software. The OS oversees the computer’s memory, processes, device drivers, files, and security protocols.\n\nKey functions:\n\nProcess management: Allocating CPU time to different programs\nMemory management: Allocating and freeing up memory as needed\nFile system management: Organizing and keeping track of files and directories\nDevice management: Communicating with connected hardware devices\nUser interface: Providing a way for users to interact with the computer\n\nComponents:\n\nKernel: The core of the OS that manages hardware resources\nShell: The user interface for interacting with the OS\nFile system: Organizes and manages data storage\nDevice drivers: Software that allows the OS to communicate with hardware\n\n\nThe Raspberry Pi runs a specialized version of Linux designed for embedded systems. This operating system, typically a variant of Debian called Raspberry Pi OS (formerly Raspbian), is optimized for the Pi’s ARM-based architecture and limited resources.\n\nThe latest version of Raspberry Pi OS is based on Debian Bookworm.\n\nKey features:\n\nLightweight: Tailored to run efficiently on the Pi’s hardware.\nVersatile: Supports a wide range of applications and programming languages.\nOpen-source: Allows for customization and community-driven improvements.\nGPIO support: Enables interaction with sensors and other hardware through the Pi’s pins.\nRegular updates: Continuously improved for performance and security.\n\nEmbedded Linux on the Raspberry Pi provides a full-featured operating system in a compact package, making it ideal for projects ranging from simple IoT devices to more complex edge machine-learning applications. Its compatibility with standard Linux tools and libraries makes it a powerful platform for development and experimentation.\n\n\nInstallation\nTo use the Raspberry Pi, we will need an operating system. By default, Raspberry Pis checks for an operating system on any SD card inserted in the slot, so we should install an operating system using Raspberry Pi Imager.\nRaspberry Pi Imager is a tool for downloading and writing images on macOS, Windows, and Linux. It includes many popular operating system images for Raspberry Pi. We will also use the Imager to preconfigure credentials and remote access settings.\nFollow the steps to install the OS on your Raspberry Pi.\n\nDownload and install the Raspberry Pi Imager on your computer.\nInsert a microSD card into your computer (a 32GB SD card is recommended) .\nOpen Raspberry Pi Imager and select your Raspberry Pi model.\nChoose the appropriate operating system:\n\nFor Raspi-Zero: For example, you can select: Raspberry Pi OS Lite (64-bit).\n\n\n\nDue to its reduced SDRAM (512MB), the recommended OS for the Raspberry Pi Zero is the 32-bit version. However, to run some machine learning models, such as the YOLOv8 from Ultralitics, we should use the 64-bit version. Although Raspi-Zero can run a desktop, we will choose the LITE version (no Desktop) to reduce the RAM needed for regular operation.\n\n\nFor Raspi-5: We can select the full 64-bit version, which includes a desktop: Raspberry Pi OS (64-bit)\n\n\nSelect your microSD card as the storage device.\nClick on Next and then the gear icon to access advanced options.\nSet the hostname, the Raspberry Pi username and password, configure WiFi, and enable SSH (Very important!)\n\n\n\nWrite the image to the microSD card.\n\n\nIn the examples here, we will use different hostnames depending on the device used: raspi, raspi-5, raspi-Zero, etc. Please replace it with the one you’re currently using.\n\n\n\nInitial Configuration\n\nInsert the microSD card into your Raspberry Pi.\nConnect power to boot up the Raspberry Pi. For the Rasp-5, use a 5V/3A (or 5A) external Power supply. For the Rasp-Zero, use a 5V/2.5A external Power supply (Optionally, for light uses, it is possible to use the computer’s USB to power on the Rasp-Zero.)\nPlease wait for the initial boot process to complete (it may take a few minutes).\n\n\nYou can find the most common Linux commands to be used with the Raspberry Pi here or here."
  },
  {
    "objectID": "raspi/setup/setup.html#remote-access",
    "href": "raspi/setup/setup.html#remote-access",
    "title": "Setup",
    "section": "Remote Access",
    "text": "Remote Access\n\nSSH Access\nThe easiest way to interact with the Raspi-Zero is via SSH (“Headless”). You can use a Terminal (MAC/Linux), PuTTy (Windows), or any other.\n\nThe Raspberry Pi and the notebook should be on the same WiFi network. Note that the Raspberry Pi 5 is dual-band 802.11ac Wi-Fi (2.4 GHz and 5 GHz), but the Raspberry Pi Zero 2W is only 2.4GHz.\n\n\nFi nd your Raspberry Pi’s IP address (for example, check your router).\nOn your computer, open a terminal and connect via SSH:\nssh username@[raspberry_pi_ip_address]   \nAlternatively, if you do not have the IP address, you can try the following: bash ssh username@hostname.local for example, ssh mjrovai@rpi-5.local , ssh mjrovai@raspi.local , etc.\n\nWhen you see the prompt:\nmjrovai@rpi-5:~ $\nIt means that you are interacting remotely with your Raspberry Pi. It is a good practice to update/upgrade the system regularly. For that, you should run:\nsudo apt-get update\nsudo apt upgrade\nYou should confirm the Raspberry Pi IP address. On the terminal, you can use:\nhostname -I\n\n\n\n\nTo shut down the Raspi via terminal:\nWhen you want to turn off your Raspberry Pi, there are better ideas than just pulling the power cord. This is because the Raspi may still be writing data to the SD card, in which case merely powering down may result in data loss or, even worse, a corrupted SD card.\nFor a safety shutdown, use the command line:\nsudo shutdown -h now\n\nTo avoid possible data loss and SD card corruption, before removing the power, you should wait a few seconds after shutdown for the Raspberry Pi’s LED to stop blinking and go dark. Once the LED goes out, it’s safe to power down.\n\n\n\nTransfer Files between the Raspberry Pi and a computer\nTransferring files between the Raspberry Pi and our main computer can be done using a pen drive, directly on the terminal (with scp), or an FTP program over the network.\n\nUsing Secure Copy Protocol (scp):\n\nCopy files to your Raspberry Pi\nLet’s create a text file on our computer, for example, test.txt.\n\n\nYou can use any text editor. In the same terminal, an option is the nano.\n\nTo copy the file named test.txt from your personal computer to a user’s home folder on your Raspberry Pi, run the following command from the directory containing test.txt, replacing the &lt;username&gt; placeholder with the username you use to log in to your Raspberry Pi and the &lt;pi_ip_address&gt; placeholder with your Raspberry Pi’s IP address:\n$ scp test.txt &lt;username&gt;@&lt;pi_ip_address&gt;:~/\n\nNote that ~/ means that we will move the file to the ROOT of our Raspi. You can choose any folder in your Raspi. But you should create the folder before you run scp, since scp won’t create folders automatically.\n\nFor example, let’s transfer the file test.txt to the ROOT of my Raspi-zero, which has an IP of 192.168.4.210:\nscp test.txt mjrovai@192.168.4.210:~/\n\nI use a different profile to differentiate the terminals. The above action happens on your computer. Now, let’s go to our Raspi (using the SSH) and check if the file is there:\n\n\n\nCopy files from your Raspberry Pi\nTo copy a file named test.txt from a user’s home directory on a Raspberry Pi to the current directory on another computer, run the following command on your Host Computer:\n$ scp &lt;username&gt;@&lt;pi_ip_address&gt;:myfile.txt .\nFor example:\nOn the Raspi, let’s create a copy of the file with another name:\ncp test.txt test_2.txt\nAnd on the Host Computer (in my case, a Mac)\nscp mjrovai@192.168.4.210:test_2.txt .\n\n\n\n\nTransferring files using FTP\nTransferring files using FTP, such as FileZilla FTP Client, is also possible. Follow the instructions to install the program on your Desktop, then use the Raspberry Pi’s IP address as the Host. For example:\nsftp://192.168.4.210\nEnter your Raspberry Pi username and password. Pressing Quickconnect will open two windows, one for your host computer desktop (right) and another for the Raspberry Pi (left)."
  },
  {
    "objectID": "raspi/setup/setup.html#increasing-swap-memory",
    "href": "raspi/setup/setup.html#increasing-swap-memory",
    "title": "Setup",
    "section": "Increasing SWAP Memory",
    "text": "Increasing SWAP Memory\nUsing htop, a cross-platform interactive process viewer, you can easily monitor the resources running on your Raspberry Pi, such as the list of processes, the running CPUs, and the memory used in real-time. To lunch hop, enter with the command on the terminal:\nhtop\n\nRegarding memory, among the devices in the Raspberry Pi family, the Raspi-Zero has the smallest amount of SRAM (500MB), compared to a selection of 2GB to 8GB on the Raspberry Pi 4 or 5. For any Raspberry Pi, it is possible to increase the memory available to the system with “Swap.” Swap memory, also known as swap space, is a technique used in computer operating systems to temporarily store data from RAM (Random Access Memory) on the SD card when the physical RAM is fully utilized. This allows the operating system (OS) to continue running even when RAM is full, which can prevent system crashes or slowdowns.\nSwap memory benefits devices with limited RAM, such as the Raspberry Pi Zero. Increasing swap can help run more demanding applications or processes, but it’s essential to balance this with the potential performance impact of frequent disk access.\nBy default, the Rapi-Zero’s SWAP (Swp) memory is only 100MB, which is very small for running some more complex and demanding Machine Learning applications (for example, YOLO). Let’s increase it to 2MB:\nFirst, turn off swap-file:\nsudo dphys-swapfile swapoff\nNext, open and modify the file /etc/dphys-swapfile. For that, we will use the nano:\nsudo nano /etc/dphys-swapfile\nSearch for the CONF_SWAPSIZE variable (default is 200) and update it to 2000:\nCONF_SWAPSIZE=2000\nAnd save the file.\nNext, turn on the swapfile again and reboot the Raspi-zero:\nsudo dphys-swapfile setup\nsudo dphys-swapfile swapon\nsudo reboot\nWhen your device is rebooted (you should enter with the SSH again), you will realize that the maximum swap memory value shown on top is now something near 2GB (in my case, 1.95GB).\n\nTo keep the htop running, you should open another terminal window to interact continuously with your Raspi."
  },
  {
    "objectID": "raspi/setup/setup.html#installing-a-camera",
    "href": "raspi/setup/setup.html#installing-a-camera",
    "title": "Setup",
    "section": "Installing a Camera",
    "text": "Installing a Camera\nThe Raspi is an excellent device for computer vision applications, requiring a camera. We can install a standard USB webcam on the micro-USB port using a USB OTG adapter (Raspi-Zero and Raspi-5) or a camera module connected to the Raspi CSI (Camera Serial Interface) port.\n\nUSB Webcams generally have inferior quality to the camera modules that connect to the CSI port. They can also not be controlled using the raspistill and rasivid commands in the terminal or the picamera recording package in Python. Nevertheless, there may be reasons why you want to connect a USB camera to your Raspberry Pi, such as because of the benefit that it is much easier to set up multiple cameras with a single Raspberry Pi, long cables, or simply because you have such a camera on hand.\n\n\nInstalling a Camera Module on the CSI port\nThere are now several Raspberry Pi camera modules. The original 5-megapixel model was releasedin 2013, followed by an 8-megapixel Camera Module 2, released in 2016. The latest camera model is the 12-megapixel Camera Module 3, released in 2023.\nThe original 5MP camera (Arducam OV5647) is no longer available from Raspberry Pi but can be found from several alternative suppliers. Below is an example of such a camera on a Raspi-Zero.\n\nHere is another example of a v2 Camera Module, which has a Sony IMX219 8-megapixel sensor:\n\nFirst, try to list the installed cameras:\nrpicam-hello --list-cameras\nIf the command is not recognized, install\nsudo apt install libcamera-apps\nTry to list the installed camera again. If Ok, you should see something like:\n\nAny camera module will work on the Raspberry Pi, but for that, it is possible that the config.txt file must be updated:\nsudo nano /boot/firmware/config.txt\nAt the bottom of the file, for example, to use the 5MP Arducam OV5647 camera, add the line:\ndtoverlay=ov5647,cam0\nOr for the v2 module, wich has the 8MP Sony IMX219 camera:\ndtoverlay=imx219,cam0\nSave the file (CTRL+O [ENTER] CRTL+X) and reboot the Raspi:\nSudo reboot\nAfter the boot, you can see if the camera is listed:\nrpicam-hello --list-cameras\n\n\n\nlibcamerais an open-source software library that supports camera systems directly from the Linux operating system on Arm processors. It minimizes proprietary code running on the Broadcom GPU.\n\nLet’s capture a JPEG image with a resolution of 640 x 480 for testing and save it to a file named test_cli_camera.jpg\nrpicam-jpeg --output test_cli_camera.jpg --width 640 --height 480\nTo view the saved file, we should use ls -f, which lists all the current directory content in long format. As before, we can use scp to view the image:\n\nAlternatively, you can transfer it to your desktop using FileZilla.\n\n\nInstalling a USB WebCam\n\nPower off the Raspi:\n\nsudo shutdown -h no\n\nConnect the USB Webcam (USB Camera Module 30fps, 1280x720) to your Raspberry Pi (in this example, I am using the Raspberry Pi Zero, but the instructions work for all Raspberry Pis).\n\n\n\nPower on again and run the SSH\nTo check if your USB camera is recognized, run:\n\nlsusb\nYou should see your camera listed in the output.\n\n\nTo take a test picture with your USB camera, use:\n\nfswebcam test_image.jpg\nThis will save an image named “test_image.jpg” in your current directory.\n\n\nSince we are using SSH to connect to our Rapsi, we must transfer the image to our main computer so we can view it. We can use FileZilla or SCP for this:\n\nOpen a terminal on your host computer and run:\nscp mjrovai@raspi-zero.local:~/test_image.jpg .\n\nReplace “mjrovai” with your username and “raspi-zero” with Pi’s hostname.\n\n\n\nIf the image quality isn’t satisfactory, you can adjust various settings; for example, define a resolution that is suitable for YOLO (640x640):\n\nfswebcam -r 640x640 --no-banner test_image_yolo.jpg\nThis captures a higher-resolution image without the default banner.\n\nAn ordinary USB Webcam can also be used:\n\nAnd verified using lsusb\n\n\nVideo Streaming\nFor stream video (which is more resource-intensive), we can install and use mjpg-streamer:\nFirst, install Git:\nsudo apt install git\nNow, we should install the necessary dependencies for mjpg-streamer, clone the repository, and proceed with the installation:\nsudo apt install cmake libjpeg62-turbo-dev\ngit clone https://github.com/jacksonliam/mjpg-streamer.git\ncd mjpg-streamer/mjpg-streamer-experimental\nmake\nsudo make install\nThen start the stream with:\nmjpg_streamer -i \"input_uvc.so\" -o \"output_http.so -w ./www\"\nWe can then access the stream by opening a web browser and navigating to:\nhttp://&lt;your_pi_ip_address&gt;:8080. In my case: http://192.168.4.210:8080\nWe should see a webpage with options to view the stream. Click on the link that says “Stream” or try accessing:\nhttp://&lt;raspberry_pi_ip_address&gt;:8080/?action=stream"
  },
  {
    "objectID": "raspi/setup/setup.html#running-the-raspi-desktop-remotely",
    "href": "raspi/setup/setup.html#running-the-raspi-desktop-remotely",
    "title": "Setup",
    "section": "Running the Raspi Desktop remotely",
    "text": "Running the Raspi Desktop remotely\nWhile we’ve primarily interacted with the Raspberry Pi using terminal commands via SSH, we can access the whole graphical desktop environment remotely if we have installed the complete Raspberry Pi OS (for example, Raspberry Pi OS (64-bit). This can be particularly useful for tasks that benefit from a visual interface. To enable this functionality, we must set up a VNC (Virtual Network Computing) server on the Raspberry Pi. Here’s how to do it:\n\nEnable the VNC Server:\n\nConnect to your Raspberry Pi via SSH.\nRun the Raspberry Pi configuration tool by entering:\nsudo raspi-config\nNavigate to Interface Options using the arrow keys.\n\n\n\nSelect VNC and Yes to enable the VNC server.\n\n\n\nExit the configuration tool, saving changes when prompted.\n\n\nInstall a VNC Viewer on Your Computer:\n\nDownload and install a VNC viewer application on your main computer. Popular options include RealVNC Viewer, TightVNC, or VNC Viewer by RealVNC. We will install VNC Viewer by RealVNC.\n\nOnce installed, you should confirm the Raspi IP address. For example, on the terminal, you can use:\nhostname -I\n\nConnect to Your Raspberry Pi:\n\nOpen your VNC viewer application.\n\n\n\nEnter your Raspberry Pi’s IP address and hostname.\nWhen prompted, enter your Raspberry Pi’s username and password.\n\n\nThe Raspberry Pi 5 Desktop should appear on your computer monitor.\n\nAdjust Display Settings (if needed):\n\nOnce connected, adjust the display resolution for optimal viewing. This can be done through the Raspberry Pi’s desktop settings or by modifying the config.txt file.\nLet’s do it using the desktop settings. Reach the menu (the Raspberry Icon at the left upper corner) and select the best screen definition for your monitor:"
  },
  {
    "objectID": "raspi/setup/setup.html#updating-and-installing-software",
    "href": "raspi/setup/setup.html#updating-and-installing-software",
    "title": "Setup",
    "section": "Updating and Installing Software",
    "text": "Updating and Installing Software\n\nUpdate your system:\nsudo apt update && sudo apt upgrade -y\nInstall essential software:\nsudo apt install python3-pip -y\nEnable pip for Python projects:\nsudo rm /usr/lib/python3.11/EXTERNALLY-MANAGED"
  },
  {
    "objectID": "raspi/setup/setup.html#model-specific-considerations",
    "href": "raspi/setup/setup.html#model-specific-considerations",
    "title": "Setup",
    "section": "Model-Specific Considerations",
    "text": "Model-Specific Considerations\n\nRaspberry Pi Zero (Raspi-Zero)\n\nLimited processing power, best for lightweight projects\nIt is better to use a headless setup (SSH) to conserve resources.\nConsider increasing swap space for memory-intensive tasks.\nIt can be used for Image Classification and Object Detection Labs, but not for the LLM (SLM).\n\n\n\nRaspberry Pi 4 or 5 (Raspi-4 or Raspi-5)\n\nSuitable for more demanding projects, including AI and machine learning.\nIt can run the whole desktop environment smoothly.\nRaspi-4 can be used for Image Classification and Object Detection Labs, but will not work well with LLMs (SLM).\nFor Raspi-5, consider using an active cooler for temperature management during intensive tasks, as in the LLMs (SLMs) lab.\n\nRemember to adjust your project requirements based on the specific Raspberry Pi model you’re using. The Raspi-Zero is great for low-power, space-constrained projects, while the Raspi-4 or 5 models are better suited for more computationally intensive tasks."
  },
  {
    "objectID": "raspi/image_classification/image_classification_fund.html#introduction",
    "href": "raspi/image_classification/image_classification_fund.html#introduction",
    "title": "Image Classification Fundamentals",
    "section": "Introduction",
    "text": "Introduction\nImage classification is a fundamental task in computer vision that involves categorizing an image into one of several predefined classes. It’s a cornerstone of artificial intelligence, enabling machines to interpret and understand visual information in a way that mimics human perception.\nImage classification refers to assigning a label or category to an entire image based on its visual content. This task is crucial in computer vision and has numerous applications across various industries. Image classification’s importance lies in its ability to automate visual understanding tasks that would otherwise require human intervention.\n\nApplications in Real-World Scenarios\nImage classification has found its way into numerous real-world applications, revolutionizing various sectors:\n\nHealthcare: Assisting in medical image analysis, such as identifying abnormalities in X-rays or MRIs.\nAgriculture: Monitoring crop health and detecting plant diseases through aerial imagery.\nAutomotive: Enabling advanced driver assistance systems and autonomous vehicles to recognize road signs, pedestrians, and other vehicles.\nRetail: Powering visual search capabilities and automated inventory management systems.\nSecurity and Surveillance: Enhancing threat detection and facial recognition systems.\nEnvironmental Monitoring: Analyzing satellite imagery for deforestation, urban planning, and climate change studies.\n\n\n\nAdvantages of Running Classification on Edge Devices like Raspberry Pi\nImplementing image classification on edge devices such as the Raspberry Pi offers several compelling advantages:\n\nLow Latency: Processing images locally eliminates the need to send data to cloud servers, significantly reducing response times.\nOffline Functionality: Classification can be performed without an internet connection, making it suitable for remote or connectivity-challenged environments.\nPrivacy and Security: Sensitive image data remains on the local device, addressing data privacy concerns and compliance requirements.\nCost-Effectiveness: Eliminates the need for expensive cloud computing resources, especially for continuous or high-volume classification tasks.\nScalability: Enables distributed computing architectures where multiple devices can work independently or in a network.\nEnergy Efficiency: Optimized models on dedicated hardware can be more energy-efficient than cloud-based solutions, which is crucial for battery-powered or remote applications.\nCustomization: Deploying specialized or frequently updated models tailored to specific use cases is more manageable.\n\nWe can create more responsive, secure, and efficient computer vision solutions by leveraging the power of edge devices like Raspberry Pi for image classification. This approach opens up new possibilities for integrating intelligent visual processing into various applications and environments.\nIn the following sections, we’ll explore how to implement and optimize image classification on the Raspberry Pi, harnessing these advantages to create powerful and efficient computer vision systems."
  },
  {
    "objectID": "raspi/image_classification/image_classification_fund.html#setting-up-the-environment",
    "href": "raspi/image_classification/image_classification_fund.html#setting-up-the-environment",
    "title": "Image Classification Fundamentals",
    "section": "Setting Up the Environment",
    "text": "Setting Up the Environment\n\nUpdating the Raspberry Pi\nFirst, ensure your Raspberry Pi is up to date:\nsudo apt update\nsudo apt upgrade -y\nsudo reboot # Reboot to ensure all updates take effect\n\n\nInstalling Required Sytem Level Libraries\nInstall Python tools and camera libraries\nsudo apt install -y python3-pip python3-venv python3-picamera2\nsudo apt install -y libcamera-dev libcamera-tools libcamera-apps\nPicamera2, a Python library for interacting with Raspberry Pi’s camera, is based on the libcamera camera stack, and the Raspberry Pi Foundation maintains it. The Picamera2 library is supported on all Raspberry Pi models, from the Pi Zero to the Pi 5.\nTesting picamera2 Instalation\n\n\n\nSetting up a Virtual Environment\nCreate a virtual environment with access to system packages to manage dependencies:\npython3 -m venv ~/tflite_env --system-site-packages\nActivate the environment:\nsource ~/tflite_env/bin/activate\nTo exit the virtual environment, use:\ndeactivate\n\n\nInstall Python Packages (inside Virtual Environment)\n\nEnsure you’re in the virtual environment (venv)\n\npip install numpy pillow  # Image processing\npip install matplotlib  # For displaying images\npip install opencv-python  # Computer vision\nVerify installation\npip list | grep -E \"(numpy|pillow|opencv|picamera)\"\n\nThe proven Numpy version should be 1.24.2. If you have another version installed, first uninstall it.\npip uninstall numpy\nInstall version 1.24.2, which is compatible with the tflite_runtime and picamera.\n pip3 install numpy==1.24.2\n\nSystem vs pip Package Installation Rule\nUse sudo apt install (outside of venv) for:\n\nSystem-level dependencies and libraries\nHardware interface libraries (as a camera)\nDevelopment headers and build tools\nAnything that needs to interface directly with hardware\n\nUse pip install (inside venv) for:\n\nPure Python packages\nApplication-specific libraries\nPackages that don’t need system-level access\n\n\nRule of thumb: Use sudo apt install only for system dependencies and hardware interfaces. Use pip install (without sudo) inside an activated virtual environment for everything else. Inside the vent, PIP or PIP3 are the same.\nThe virtual environment will automatically see both the system packages and the pip-installed packages thanks to the --system-site-packages flag.\n\n\n\n\nSetting up Jupyter Notebook\nLet’s set up Jupyter Notebook optimized for headless Raspberry Pi camera work and development:\npip install jupyter jupyterlab notebook\njupyter notebook --generate-config\nTo run Jupyter Notebook, run the command (change the IP address for yours):\njupyter notebook --ip=192.168.4.210 --no-browser\nOn the terminal, you can see the local URL address to open the notebook:\n\nYou can access it from another device by entering the Raspberry Pi’s IP address and the provided token in a web browser (you can copy the token from the terminal).\n\nDefine the working directory in the Raspi and create a new Python 3 notebook. For example:\ncd Documents\nmkdir Python\n\nIt is possible to create folders directly in the Jupyter Notebook\n\nCreate a new Notebook and test the code below:\nImport Libraries\nimport time\nimport numpy as np\nfrom PIL import Image\nimport matplotlib.pyplot as plt\nfrom picamera2 import Picamera2\nLoad an image from the internet, for example (note that it is possible to run a command line from the Notebook, using ! before the command:\n!wget https://upload.wikimedia.org/wikipedia/commons/3/3a/Cat03.jpg\nAn image (Cat03.jpg will be downloaded to the current directory.\nLoad and show the image:\nimg_path = \"Cat03.jpg\"\nimg = Image.open(img_path)\n\n# Display the image\nplt.figure(figsize=(6, 6))\nplt.imshow(img)\nplt.title(\"Original Image\")\nplt.show()\nWe can see the image displayed on the Notebook:\n\nNow, let’s use the camera to capture a local image:\nfrom picamera2 import Picamera2\nimport time\n\n# Initialize camera\npicam2 = Picamera2()\npicam2.start()\n\n# Wait for camera to warm up\ntime.sleep(2)\n\n# Capture image\npicam2.capture_file(\"class3_test.jpg\")\nprint(\"Image captured: class3_test.jpg\")\n\n# Stop camera\npicam2.stop()\npicam2.close()\nAnd use a similar code as before to show it (adapting the img_path and title):"
  },
  {
    "objectID": "raspi/image_classification/image_classification_fund.html#installing-tensorflow-lite",
    "href": "raspi/image_classification/image_classification_fund.html#installing-tensorflow-lite",
    "title": "Image Classification Fundamentals",
    "section": "Installing TensorFlow Lite",
    "text": "Installing TensorFlow Lite\nWe are interested in performing inference, which refers to executing a TensorFlow Lite model on a device to make predictions based on input data. To perform an inference with a TensorFlow Lite model, we must run it through an interpreter. The TensorFlow Lite interpreter is designed to be lean and fast. The interpreter uses a static graph ordering and a custom (less-dynamic) memory allocator to ensure minimal load, initialization, and execution latency.\nWe’ll use the TensorFlow Lite runtime for Raspberry Pi, a simplified library for running machine learning models on mobile and embedded devices, without including all TensorFlow packages.\npip install tflite-runtime\n\nThe wheel installed: tflite_runtime-2.14.0-cp311-cp311-manylinux_2_34_aarch64.whl\n\nReboot the system:\nsudo reboot\nVerify instalation:\npip list | grep -E \"(tflite-runtime)\"\nWe should see as a response: tflite-runtime      2.14.0.\n\nCreating a working directory:\nIf you are working on the Raspi-Zero with the minimum OS (No Desktop), you do not have a user-pre-defined directory tree (you can check it with ls. So, let’s create one:\nmkdir Documents\ncd Documents/\nmkdir TFLITE\ncd TFLITE/\nmkdir IMG_CLASS\ncd IMG_CLASS\nmkdir models\ncd models\n\nOn the Raspi-5, the /Documents should be there.\n\nGet a pre-trained Image Classification model:\nAn appropriate pre-trained model is crucial for successful image classification on resource-constrained devices like the Raspberry Pi. MobileNet is designed for mobile and embedded vision applications with a good balance between accuracy and speed. Versions: MobileNetV1, MobileNetV2, MobileNetV3. Let’s download the V2:\nwget https://storage.googleapis.com/download.tensorflow.org/models/tflite_11_05_08/mobilenet_v2_1.0_224_quant.tgz\n\ntar xzf mobilenet_v2_1.0_224_quant.tgz\nGet its labels:\nwget https://raw.githubusercontent.com/Mjrovai/EdgeML-with-Raspberry-Pi/refs/heads/main/IMG_CLASS/models/labels.txt\nIn the end, you should have the models in its directory:\n\n\nWe will only need the mobilenet_v2_1.0_224_quant.tflite model and the labels.txt. We can delete the other files.\n\n\n\nVerifying the Setup\nLet’s test our setup by running a simple Python script:\nimport tflite_runtime.interpreter as tflite\nimport numpy as np\nfrom PIL import Image\n\nprint(\"NumPy:\", np.__version__)\nprint(\"Pillow:\", Image.__version__)\n\n# Try to create a TFLite Interpreter\nmodel_path = \"./models/mobilenet_v2_1.0_224_quant.tflite\"\ninterpreter = tflite.Interpreter(model_path=model_path)\ninterpreter.allocate_tensors()\nprint(\"TFLite Interpreter created successfully!\")\nWe can create the Python script using nano on the terminal, saving it with CTRL+0 + ENTER + CTRL+X\n\nAnd run it with the command:\npython setup_test.py\n\nOr you can run it directly on the Notebook:"
  },
  {
    "objectID": "raspi/image_classification/image_classification_fund.html#making-inferences-with-mobilenet-v2",
    "href": "raspi/image_classification/image_classification_fund.html#making-inferences-with-mobilenet-v2",
    "title": "Image Classification Fundamentals",
    "section": "Making inferences with Mobilenet V2",
    "text": "Making inferences with Mobilenet V2\nIn the last section, we set up the environment, including downloading a popular pre-trained model, Mobilenet V2, trained on ImageNet’s 224x224 images (1.2 million) for 1,001 classes (1,000 object categories plus 1 background). The model was converted to a compact 3.5MB TensorFlow Lite format, making it suitable for the limited storage and memory of a Raspberry Pi.\n\nIn the IMG_CLASS working directory, let’s start a new notebook to follow all the steps to classify one image:\nImport the needed libraries:\nimport time\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom PIL import Image\nimport tflite_runtime.interpreter as tflite\nLoad the TFLite model and allocate tensors:\nmodel_path = \"./models/mobilenet_v2_1.0_224_quant.tflite\"\ninterpreter = tflite.Interpreter(model_path=model_path)\ninterpreter.allocate_tensors()\nGet input and output tensors.\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\nInput details will give us information about how the model should be fed with an image. The shape of (1, 224, 224, 3) informs us that an image with dimensions (224x224x3) should be input one by one (Batch Dimension: 1).\n\nThe output details show that the inference will result in an array of 1,001 integer values. Those values result from the image classification, where each value is the probability of that specific label being related to the image.\n\nLet’s also inspect the dtype of input details of the model\ninput_dtype = input_details[0]['dtype']\ninput_dtype\ndtype('uint8')\nThis shows that the input image should be raw pixels (0 - 255).\nLet’s get a test image. We can either transfer it from our computer or download one for testing, as we did before. Let’s first create a folder under our working directory:\nmkdir images\ncd images\nwget https://upload.wikimedia.org/wikipedia/commons/3/3a/Cat03.jpg\nLet’s load and display the image:\n# Load he image\nimg_path = \"./images/Cat03.jpg\"\nimg = Image.open(img_path)\n\n# Display the image\nplt.figure(figsize=(8, 8))\nplt.imshow(img)\nplt.title(\"Original Image\")\nplt.show()\n\nWe can see the image size running the command:\nwidth, height = img.size\nThat shows us that the image is an RGB image with a width of 1600 and a height of 1600 pixels. To use our model, we should reshape it to (224, 224, 3) and add a batch dimension of 1, as defined in the input details: (1, 224, 224, 3). The inference result, as shown in the output details, will be an array of size 1001, as shown below:\n\nSo, let’s reshape the image, add the batch dimension, and see the result:\nimg = img.resize((input_details[0]['shape'][1], input_details[0]['shape'][2]))\ninput_data = np.expand_dims(img, axis=0)\ninput_data.shape\nThe input_data shape is as expected: (1, 224, 224, 3)\nLet’s confirm the dtype of the input data:\ninput_data.dtype\ndtype('uint8')\nThe input data dtype is ‘uint8’, which is compatible with the dtype expected for the model.\nUsing the input_data, let’s run the interpreter and get the predictions (output):\ninterpreter.set_tensor(input_details[0]['index'], input_data)\ninterpreter.invoke()\npredictions = interpreter.get_tensor(output_details[0]['index'])[0]\nThe prediction is an array with 1001 elements. Let’s get the Top-5 indices where their elements have high values:\ntop_k_results = 5\ntop_k_indices = np.argsort(predictions)[::-1][:top_k_results]\ntop_k_indices \nThe top_k_indices is an array with 5 elements: array([283, 286, 282])\nSo, 283, 286, 282, 288, and 479 are the image’s most probable classes. Having the index, we must find to what class it appoints (such as car, cat, or dog). The text file downloaded with the model has a label associated with each index from 0 to 1,000. Let’s use a function to load the .txt file as a list:\ndef load_labels(filename):\n    with open(filename, 'r') as f:\n        return [line.strip() for line in f.readlines()]\nAnd get the list, printing the labels associated with the indexes:\nlabels_path = \"./models/labels.txt\"\nlabels = load_labels(labels_path)\n\nprint(labels[286])\nprint(labels[283])\nprint(labels[282])\nprint(labels[288])\nprint(labels[479])\nAs a result, we have:\nEgyptian cat\ntiger cat\ntabby\nlynx\ncarton\nAt least four of the top indices are related to felines. The prediction content is the probability associated with each one of the labels. As we saw in the output details, those values are quantized and should be dequantized:\nscale, zero_point = output_details[0]['quantization']\ndequantized_output = (predictions.astype(np.float32) - zero_point) * scale\ndequantized_output\narray([-0.8900329, -0.4944627,  1.285603 , ..., -1.9778508,  1.5822806,\n        4.549057 ], dtype=float32)\nThe output (positive and negative numbers) shows that the output probably does not have a Softmax. Checking the model documentation (https://arxiv.org/abs/1801.04381v4): MobileNet V2 typically doesn’t include a softmax layer at the output. It usually ends with a 1x1 convolution followed by average pooling and a fully connected layer. So, for getting the probabilities (0 to 1), we should apply Softmax:\nexp_output = np.exp(dequantized_output - np.max(dequantized_output))\nprobabilities = exp_output / np.sum(exp_output)\nLet’s print the top-5 probabilities:\nprint (probabilities[286])\nprint (probabilities[283])\nprint (probabilities[282])\nprint (probabilities[288])\nprint (probabilities[479])\n0.27741462\n0.3732285\n0.16919471\n0.10319158\n0.023410844\nFor clarity, let’s create a function to relate the labels to the probabilities:\nfor i in range(top_k_results):\n    print(\"\\t{:20}: {}%\".format(\n        labels[top_k_indices[i]],\n        (int(probabilities[top_k_indices[i]]*100))))\ntiger cat           : 37%\nEgyptian cat        : 27%\ntabby               : 16%\nlynx                : 10%\ncarton              : 2%\n\nDefine a general Image Classification function\nLet’s create a general function to give an image as input, and we get the Top-5 possible classes:\n\ndef image_classification(img_path, model_path, labels, top_k_results=5):\n    # load the image\n    img = Image.open(img_path)\n    plt.figure(figsize=(4, 4))\n    plt.imshow(img)\n    plt.axis('off')\n\n    # Load the TFLite model\n    interpreter = tflite.Interpreter(model_path=model_path)\n    interpreter.allocate_tensors()\n    \n    # Get input and output tensors\n    input_details = interpreter.get_input_details()\n    output_details = interpreter.get_output_details()\n    \n    # Preprocess\n    img = img.resize((input_details[0]['shape'][1], \n                      input_details[0]['shape'][2]))\n    input_data = np.expand_dims(img, axis=0)\n    \n    # Inference on Raspi-Zero\n    interpreter.set_tensor(input_details[0]['index'], input_data)\n    interpreter.invoke()\n    \n    # Obtain results and map them to the classes\n    predictions = interpreter.get_tensor(output_details[0]['index'])[0]\n    \n    # Get indices of the top k results\n    top_k_indices = np.argsort(predictions)[::-1][:top_k_results]\n    \n    # Get quantization parameters\n    scale, zero_point = output_details[0]['quantization']\n    \n    # Dequantize the output and apply softmax\n    dequantized_output = (predictions.astype(np.float32) - zero_point) * scale\n    exp_output = np.exp(dequantized_output - np.max(dequantized_output))\n    probabilities = exp_output / np.sum(exp_output)\n    \n    print(\"\\n\\t[PREDICTION]        [Prob]\\n\")\n    for i in range(top_k_results):\n        print(\"\\t{:20}: {}%\".format(\n            labels[top_k_indices[i]],\n            (int(probabilities[top_k_indices[i]]*100))))\n\nAnd loading some images for testing, we have:\n\n\n\nTesting the classification with the Camera\nLet’s modify the Python script used before to capture an image from the camera (size: 224x224), saving it in the images folder:\nfrom picamera2 import Picamera2\nimport time\n\ndef capture_image(image_path):\n    \n  # Initialize camera\n  picam2 = Picamera2() # default is index 0\n\n  # Configure the camera\n  config = picam2.create_still_configuration(main={\"size\": (224, 224)})\n  picam2.configure(config)\n  picam2.start()\n\n  # Wait for camera to warm up\n  time.sleep(2)\n\n  # Capture image\n  picam2.capture_file(image_path)\n  print(\"Image captured: \"+\"image_path\")\n\n  # Stop camera\n  picam2.stop()\n  picam2.close()\nNow, let’s capture an image and sent it for inference:\nimg_path = './images/cam_img_test.jpg'\nmodel_path = \"./models/mobilenet_v2_1.0_224_quant.tflite\"\nlabels = load_labels(\"./models/labels.txt\")\ncapture_image(img_path)\nimage_classification(img_path, model_path, labels, top_k_results=5)\n\n\n\nExploring a Model Trained from Zero\nLet’s get a TFLite model trained from scratch. For that, we can follow the Notebook:\nCNN to classify Cifar-10 dataset\nIn the notebook, we trained a model using the CIFAR10 dataset, which contains 60,000 images from 10 classes of CIFAR (airplane, automobile, bird, cat, deer, dog, frog, horse, ship, and truck). CIFAR has 32x32 color images (3 color channels) where the objects are not centered and can have the object with a background, such as airplanes that might have a cloudy sky behind them! In short, small but real images.\nThe CNN trained model (cifar10_model.keras) had a size of 2.0MB. Using the TFLite Converter, the model cifar10.tflite became with 674MB (around 1/3 of the original size).\n\nRuning the notebook 20_Cifar_10_Image_Classification.ipynb with the trained model: cifar10.tflite, and following the same steps as we did with the MobileNet model, we can see that the inference result is inferior in terms of probability when compared with the MobileNetV2.\nBelow are examples of images using the General Function for Image Classification on a Raspberry Pi Zero, as shown in the last section."
  },
  {
    "objectID": "raspi/image_classification/image_classification_fund.html#conclusion",
    "href": "raspi/image_classification/image_classification_fund.html#conclusion",
    "title": "Image Classification Fundamentals",
    "section": "Conclusion:",
    "text": "Conclusion:\nThis chapter has established a solid foundation for understanding and implementing image classification on Raspberry Pi devices using Python and TensorFlow Lite. Throughout this journey, we have explored the essential components that make edge-based computer vision both practical and powerful.\nWe began by understanding the theoretical foundations of image classification and its real-world applications across diverse sectors, from healthcare to environmental monitoring. The advantages of running classification on edge devices like the Raspberry Pi—including low latency, offline functionality, enhanced privacy, and cost-effectiveness—make it an attractive solution for many practical applications.\nThe hands-on experience of setting up the development environment provided crucial insights into the requirements and constraints of embedded systems. We successfully configured TensorFlow Lite runtime, installed essential Python libraries, and established a working directory structure that serves as the foundation for computer vision projects.\nWorking with the pre-trained MobileNet V2 model demonstrated several key concepts:\n\nModel Architecture Understanding: We explored how pre-trained models like MobileNet V2 are optimized for mobile and embedded applications, achieving an excellent balance between accuracy and computational efficiency.\nQuantization Benefits: The 3.5MB quantized model showed how compression techniques make sophisticated neural networks feasible on resource-constrained devices without significant accuracy loss.\nInference Pipeline: We implemented the complete inference workflow, from image preprocessing (resizing to 224×224, handling data types) to post-processing (dequantization, softmax application, and top-k prediction extraction).\nPerformance Considerations: The chapter highlighted the importance of understanding model input/output specifications, memory management, and the trade-offs between accuracy and speed on edge devices.\n\nThe practical implementation using cameras connected to a Raspberry Pi demonstrated the seamless integration between hardware and software components. The ability to capture images directly from the device and perform real-time classification showcases the potential for autonomous and IoT applications.\nKey technical achievements include:\n\nSuccessfully setting up TensorFlow Lite runtime and dependencies\nImplementing proper image preprocessing pipelines\nUnderstanding quantized model operations and dequantization processes\nCreating reusable functions for image classification tasks\nIntegrating camera capture with inference workflows\n\nThis foundational knowledge prepares us for more advanced topics, including custom model training and deployment. The skills developed here—understanding model architectures, implementing inference pipelines, and working with embedded Python environments—are transferable to a wide range of computer vision applications.\nThe chapter serves as a stepping stone toward building more sophisticated AI systems on edge devices, demonstrating that powerful computer vision capabilities are accessible even on modest hardware platforms when properly optimized and implemented."
  },
  {
    "objectID": "raspi/image_classification/image_classification_fund.html#resources",
    "href": "raspi/image_classification/image_classification_fund.html#resources",
    "title": "Image Classification Fundamentals",
    "section": "Resources",
    "text": "Resources\n\nDataset Example\nSetup Test Notebook on a Raspi\nImage Classification Notebook on a Raspi\nCNN to classify Cifar-10 dataset at CoLab\nCifar 10 - Image Classification on a Raspi\nPython Scripts"
  },
  {
    "objectID": "raspi/image_classification/custom_image_classification_project.html#sec-image-classification-image-classification-project-d575",
    "href": "raspi/image_classification/custom_image_classification_project.html#sec-image-classification-image-classification-project-d575",
    "title": "Custom Image Classification Project",
    "section": "Image Classification Project",
    "text": "Image Classification Project\nIn this chapter, we will develop a complete Image Classification project using the Edge Impulse Studio. As we did with the MobiliNet V2, the trained and converted TFLite model will be used for inference using a Python script.\nHere a typical ML workflow that we will use in our project:\n\n\n\n\n\n\nThe Goal\nThe first step in any ML project is to define its goal. In this case, it is to detect and classify two specific objects present in one image. For this project, we will use two small toys: a robot and a small Brazilian parrot (named Periquito). We will also collect images of a background where those two objects are absent.\n \n\n\nData Collection\nOnce we have defined our Machine Learning project goal, the next and most crucial step is collecting the dataset. We can use a phone for the image capture, but we will use the Raspi here. Let’s set up a simple web server on our Raspberry Pi to view the QVGA (320 x 240) captured images in a browser.\n\nFirst, let’s install Flask, a lightweight web framework for Python:\npip3 install flask\nGo to the working folder (IMG_CLASS) and create a new Python script combining image capture with a web server. We’ll call it get_img_data.py:\n\n\nfrom flask import Flask, Response, render_template_string,\n                  request, redirect, url_for\nfrom picamera2 import Picamera2\nimport io\nimport threading\nimport time\nimport os\nimport signal\n\napp = Flask(__name__)\n\n# Global variables\nbase_dir = \"dataset\"\npicam2 = None\nframe = None\nframe_lock = threading.Lock()\ncapture_counts = {}\ncurrent_label = None\nshutdown_event = threading.Event()\n\ndef initialize_camera():\n    global picam2\n    picam2 = Picamera2()\n    config = picam2.create_preview_configuration(\n             main={\"size\": (320, 240)}\n    )\n    picam2.configure(config)\n    picam2.start()\n    time.sleep(2)  # Wait for camera to warm up\n\ndef get_frame():\n    global frame\n    while not shutdown_event.is_set():\n        stream = io.BytesIO()\n        picam2.capture_file(stream, format='jpeg')\n        with frame_lock:\n            frame = stream.getvalue()\n        time.sleep(0.1)  # Adjust as needed for smooth preview\n\ndef generate_frames():\n    while not shutdown_event.is_set():\n        with frame_lock:\n            if frame is not None:\n                yield (b'--frame\\r\\n'\n                       b'Content-Type: image/jpeg\\r\\n\\r\\n' +\n                                       frame + b'\\r\\n')\n        time.sleep(0.1)  # Adjust as needed for smooth streaming\n\ndef shutdown_server():\n    shutdown_event.set()\n    if picam2:\n        picam2.stop()\n    # Give some time for other threads to finish\n    time.sleep(2)\n    # Send SIGINT to the main process\n    os.kill(os.getpid(), signal.SIGINT)\n\n@app.route('/', methods=['GET', 'POST'])\ndef index():\n    global current_label\n    if request.method == 'POST':\n        current_label = request.form['label']\n        if current_label not in capture_counts:\n            capture_counts[current_label] = 0\n        os.makedirs(os.path.join(base_dir, current_label),\n                                 exist_ok=True)\n        return redirect(url_for('capture_page'))\n    return render_template_string('''\n        &lt;!DOCTYPE html&gt;\n        &lt;html&gt;\n        &lt;head&gt;\n            &lt;title&gt;Dataset Capture - Label Entry&lt;/title&gt;\n        &lt;/head&gt;\n        &lt;body&gt;\n            &lt;h1&gt;Enter Label for Dataset&lt;/h1&gt;\n            &lt;form method=\"post\"&gt;\n                &lt;input type=\"text\" name=\"label\" required&gt;\n                &lt;input type=\"submit\" value=\"Start Capture\"&gt;\n            &lt;/form&gt;\n        &lt;/body&gt;\n        &lt;/html&gt;\n    ''')\n\n@app.route('/capture')\ndef capture_page():\n    return render_template_string('''\n        &lt;!DOCTYPE html&gt;\n        &lt;html&gt;\n        &lt;head&gt;\n            &lt;title&gt;Dataset Capture&lt;/title&gt;\n            &lt;script&gt;\n                var shutdownInitiated = false;\n                function checkShutdown() {\n                    if (!shutdownInitiated) {\n                        fetch('/check_shutdown')\n                            .then(response =&gt; response.json())\n                            .then(data =&gt; {\n                                if (data.shutdown) {\n                                    shutdownInitiated = true;\n                                    document.getElementById(\n                                          'video-feed').src = '';\n                                    document.getElementById(\n                                          'shutdown-message')\n                                    .style.display = 'block';\n                                }\n                            });\n                    }\n                }\n                setInterval(checkShutdown, 1000); // Check\n                                                     every second\n            &lt;/script&gt;\n        &lt;/head&gt;\n        &lt;body&gt;\n            &lt;h1&gt;Dataset Capture&lt;/h1&gt;\n            &lt;p&gt;Current Label: {{ label }}&lt;/p&gt;\n            &lt;p&gt;Images captured for this label: {{ capture_count\n                                                  }}&lt;/p&gt;\n            &lt;img id=\"video-feed\" src=\"{{ url_for('video_feed')\n                                         }}\" width=\"640\"\n            height=\"480\" /&gt;\n            &lt;div id=\"shutdown-message\" style=\"display: none;\n                                              color: red;\"&gt;\n                Capture process has been stopped.\n                You can close this window.\n            &lt;/div&gt;\n            &lt;form action=\"/capture_image\" method=\"post\"&gt;\n                &lt;input type=\"submit\" value=\"Capture Image\"&gt;\n            &lt;/form&gt;\n            &lt;form action=\"/stop\" method=\"post\"&gt;\n                &lt;input type=\"submit\" value=\"Stop Capture\"\n                style=\"background-color: #ff6666;\"&gt;\n            &lt;/form&gt;\n            &lt;form action=\"/\" method=\"get\"&gt;\n                &lt;input type=\"submit\" value=\"Change Label\"\n                style=\"background-color: #ffff66;\"&gt;\n            &lt;/form&gt;\n        &lt;/body&gt;\n        &lt;/html&gt;\n    ''', label=current_label, capture_count=capture_counts.get(\n                                            current_label, 0))\n\n@app.route('/video_feed')\ndef video_feed():\n    return Response(generate_frames(),\n                    mimetype='multipart/x-mixed-replace;\n                    boundary=frame')\n\n@app.route('/capture_image', methods=['POST'])\ndef capture_image():\n    global capture_counts\n    if current_label and not shutdown_event.is_set():\n        capture_counts[current_label] += 1\n        timestamp = time.strftime(\"%Y%m%d-%H%M%S\")\n        filename = f\"image_{timestamp}.jpg\"\n        full_path = os.path.join(base_dir, current_label,\n                                 filename)\n\n        picam2.capture_file(full_path)\n\n    return redirect(url_for('capture_page'))\n\n@app.route('/stop', methods=['POST'])\ndef stop():\n    summary = render_template_string('''\n        &lt;!DOCTYPE html&gt;\n        &lt;html&gt;\n        &lt;head&gt;\n            &lt;title&gt;Dataset Capture - Stopped&lt;/title&gt;\n        &lt;/head&gt;\n        &lt;body&gt;\n            &lt;h1&gt;Dataset Capture Stopped&lt;/h1&gt;\n            &lt;p&gt;The capture process has been stopped.\n               You can close this window.&lt;/p&gt;\n            &lt;p&gt;Summary of captures:&lt;/p&gt;\n            &lt;ul&gt;\n            {% for label, count in capture_counts.items() %}\n                &lt;li&gt;{{ label }}: {{ count }} images&lt;/li&gt;\n            {% endfor %}\n            &lt;/ul&gt;\n        &lt;/body&gt;\n        &lt;/html&gt;\n    ''', capture_counts=capture_counts)\n\n    # Start a new thread to shutdown the server\n    threading.Thread(target=shutdown_server).start()\n\n    return summary\n\n@app.route('/check_shutdown')\ndef check_shutdown():\n    return {'shutdown': shutdown_event.is_set()}\n\nif __name__ == '__main__':\n    initialize_camera()\n    threading.Thread(target=get_frame, daemon=True).start()\n    app.run(host='0.0.0.0', port=5000, threaded=True)\n\n\nRun this script:\n\n    python3 get_img_data.py\n\nAccess the web interface:\n\nOn the Raspberry Pi itself (if you have a GUI): Open a web browser and go to http://localhost:5000\nFrom another device on the same network: Open a web browser and go to http://&lt;raspberry_pi_ip&gt;:5000 (Replace &lt;raspberry_pi_ip&gt; with your Raspberry Pi’s IP address). For example: http://192.168.4.210:5000/\n\n\nThis Python script creates a web-based interface for capturing and organizing image datasets using a Raspberry Pi and its camera. It’s handy for machine learning projects that require labeled image data.\n\nKey Features:\n\nWeb Interface: Accessible from any device on the same network as the Raspberry Pi.\nLive Camera Preview: This shows a real-time feed from the camera.\nLabeling System: Allows users to input labels for different categories of images.\nOrganized Storage: Automatically saves images in label-specific subdirectories.\nPer-Label Counters: Keeps track of how many images are captured for each label.\nSummary Statistics: Provides a summary of captured images when stopping the capture process.\n\n\n\nMain Components:\n\nFlask Web Application: Handles routing and serves the web interface.\nPicamera2 Integration: Controls the Raspberry Pi camera.\nThreaded Frame Capture: Ensures smooth live preview.\nFile Management: Organizes captured images into labeled directories.\n\n\n\nKey Functions:\n\ninitialize_camera(): Sets up the Picamera2 instance.\nget_frame(): Continuously captures frames for the live preview.\ngenerate_frames(): Yields frames for the live video feed.\nshutdown_server(): Sets the shutdown event, stops the camera, and shuts down the Flask server\nindex(): Handles the label input page.\ncapture_page(): Displays the main capture interface.\nvideo_feed(): Shows a live preview to position the camera\ncapture_image(): Saves an image with the current label.\nstop(): Stops the capture process and displays a summary.\n\n\n\nUsage Flow:\n\nStart the script on your Raspberry Pi.\nAccess the web interface from a browser.\nEnter a label for the images you want to capture and press Start Capture.\n\n \n\nUse the live preview to position the camera.\nClick Capture Image to save images under the current label.\n\n \n\nChange labels as needed for different categories, selecting Change Label.\nClick Stop Capture when finished to see a summary.\n\n \n\n\nTechnical Notes:\n\nThe script uses threading to handle concurrent frame capture and web serving.\nImages are saved with timestamps in their filenames for uniqueness.\nThe web interface is responsive and can be accessed from mobile devices.\n\n\n\nCustomization Possibilities:\n\nAdjust image resolution in the initialize_camera() function. Here we used QVGA \\((320\\times 240)\\).\nModify the HTML templates for a different look and feel.\nAdd additional image processing or analysis steps in the capture_image() function.\n\n\n\nNumber of samples on Dataset:\nGet around 60 images from each category (periquito, robot and background). Try to capture different angles, backgrounds, and light conditions.\nOn the Raspi, we will end with a folder named dataset, which contains three sub-folders: periquito, robot, and background, one for each class of images.\nYou can use Filezilla to transfer the created dataset to your main computer."
  },
  {
    "objectID": "raspi/image_classification/custom_image_classification_project.html#sec-image-classification-training-model-edge-impulse-studio-671f",
    "href": "raspi/image_classification/custom_image_classification_project.html#sec-image-classification-training-model-edge-impulse-studio-671f",
    "title": "Custom Image Classification Project",
    "section": "Training the model with Edge Impulse Studio",
    "text": "Training the model with Edge Impulse Studio\nWe will use the Edge Impulse Studio to train our model. Go to the Edge Impulse Page, enter your account credentials, and create a new project:\n \n\nHere, you can clone a similar project: Raspi - Img Class.\n\n\nDataset\nWe will walk through four main steps using the EI Studio (or Studio). These steps are crucial in preparing our model for use on the Raspi: Dataset, Impulse, Tests, and Deploy (on the Edge Device, in this case, the Raspi).\n\nRegarding the Dataset, it is essential to point out that our Original Dataset, captured with the Raspi, will be split into Training, Validation, and Test. The Test Set will be separated from the beginning and reserved for use only in the Test phase after training. The Validation Set will be used during training.\n\nOn Studio, follow the steps to upload the captured data:\n\nGo to the Data acquisition tab, and in the UPLOAD DATA section, upload the files from your computer in the chosen categories.\nLeave to the Studio the splitting of the original dataset into train and test and choose the label about\nRepeat the procedure for all three classes. At the end, you should see your “raw data” in the Studio:\n\n \nThe Studio allows you to explore your data, showing a complete view of all the data in your project. You can clear, inspect, or change labels by clicking on individual data items. In our case, a straightforward project, the data seems OK."
  },
  {
    "objectID": "raspi/image_classification/custom_image_classification_project.html#sec-image-classification-impulse-design-cfb1",
    "href": "raspi/image_classification/custom_image_classification_project.html#sec-image-classification-impulse-design-cfb1",
    "title": "Custom Image Classification Project",
    "section": "The Impulse Design",
    "text": "The Impulse Design\nIn this phase, we should define how to:\n\nPre-process our data, which consists of resizing the individual images and determining the color depth to use (be it RGB or Grayscale) and\nSpecify a Model. In this case, it will be the Transfer Learning (Images) to fine-tune a pre-trained MobileNet V2 image classification model on our data. This method performs well even with relatively small image datasets (around 180 images in our case).\n\nTransfer Learning with MobileNet offers a streamlined approach to model training, which is especially beneficial for resource-constrained environments and projects with limited labeled data. MobileNet, known for its lightweight architecture, is a pre-trained model that has already learned valuable features from a large dataset (ImageNet).\n \nBy leveraging these learned features, we can train a new model for your specific task with fewer data and computational resources and achieve competitive accuracy.\n \nThis approach significantly reduces training time and computational cost, making it ideal for quick prototyping and deployment on embedded devices where efficiency is paramount.\nGo to the Impulse Design Tab and create the impulse, defining an image size of \\(160\\times 160\\) and squashing them (squared form, without cropping). Select Image and Transfer Learning blocks. Save the Impulse.\n \n\nImage Pre-Processing\nAll the input QVGA/RGB565 images will be converted to 76,800 features \\((160\\times 160\\times 3)\\).\n\n\n\n\n\nPress Save parameters and select Generate features in the next tab.\n\n\nModel Design\nMobileNet is a family of efficient convolutional neural networks designed for mobile and embedded vision applications. The key features of MobileNet are:\n\nLightweight: Optimized for mobile devices and embedded systems with limited computational resources.\nSpeed: Fast inference times, suitable for real-time applications.\nAccuracy: Maintains good accuracy despite its compact size.\n\nMobileNetV2, introduced in 2018, improves the original MobileNet architecture. Key features include:\n\nInverted Residuals: Inverted residual structures are used where shortcut connections are made between thin bottleneck layers.\nLinear Bottlenecks: Removes non-linearities in the narrow layers to prevent the destruction of information.\nDepth-wise Separable Convolutions: Continues to use this efficient operation from MobileNetV1.\n\nIn our project, we will do a Transfer Learning with the MobileNetV2 160x160 1.0, which means that the images used for training (and future inference) should have an input Size of \\(160\\times 160\\) pixels and a Width Multiplier of 1.0 (full width, not reduced). This configuration balances between model size, speed, and accuracy.\n\n\nModel Training\nAnother valuable deep learning technique is Data Augmentation. Data augmentation improves the accuracy of machine learning models by creating additional artificial data. A data augmentation system makes small, random changes to the training data during the training process (such as flipping, cropping, or rotating the images).\nLooking under the hood, here you can see how Edge Impulse implements a data Augmentation policy on your data:\n# Implements the data augmentation policy\ndef augment_image(image, label):\n    # Flips the image randomly\n    image = tf.image.random_flip_left_right(image)\n\n    # Increase the image size, then randomly crop it down to\n    # the original dimensions\n    resize_factor = random.uniform(1, 1.2)\n    new_height = math.floor(resize_factor * INPUT_SHAPE[0])\n    new_width = math.floor(resize_factor * INPUT_SHAPE[1])\n    image = tf.image.resize_with_crop_or_pad(image, new_height,\n                                             new_width)\n    image = tf.image.random_crop(image, size=INPUT_SHAPE)\n\n    # Vary the brightness of the image\n    image = tf.image.random_brightness(image, max_delta=0.2)\n\n    return image, label\nExposure to these variations during training can help prevent your model from taking shortcuts by “memorizing” superficial clues in your training data, meaning it may better reflect the deep underlying patterns in your dataset.\nThe final dense layer of our model will have 0 neurons with a 10% dropout for overfitting prevention. Here is the Training result:\n \nThe result is excellent, with a reasonable 35 ms of latency (for a Raspi-4), which should result in around 30 fps (frames per second) during inference. A Raspi-Zero should be slower, and the Raspi-5, faster.\n\n\nTrading off: Accuracy versus speed\nIf faster inference is needed, we should train the model using smaller alphas (0.35, 0.5, and 0.75) or even reduce the image input size, trading with accuracy. However, reducing the input image size and decreasing the alpha (width multiplier) can speed up inference for MobileNet V2, but they have different trade-offs. Let’s compare:\n\nReducing Image Input Size:\n\nPros:\n\nSignificantly reduces the computational cost across all layers.\nDecreases memory usage.\nIt often provides a substantial speed boost.\n\nCons:\n\nIt may reduce the model’s ability to detect small features or fine details.\nIt can significantly impact accuracy, especially for tasks requiring fine-grained recognition.\n\n\nReducing Alpha (Width Multiplier):\n\nPros:\n\nReduces the number of parameters and computations in the model.\nMaintains the original input resolution, potentially preserving more detail.\nIt can provide a good balance between speed and accuracy.\n\nCons:\n\nIt may not speed up inference as dramatically as reducing input size.\nIt can reduce the model’s capacity to learn complex features.\n\nComparison:\n\nSpeed Impact:\n\nReducing input size often provides a more substantial speed boost because it reduces computations quadratically (halving both width and height reduces computations by about 75%).\nReducing alpha provides a more linear reduction in computations.\n\nAccuracy Impact:\n\nReducing input size can severely impact accuracy, especially when detecting small objects or fine details.\nReducing alpha tends to have a more gradual impact on accuracy.\n\nModel Architecture:\n\nChanging input size doesn’t alter the model’s architecture.\nChanging alpha modifies the model’s structure by reducing the number of channels in each layer.\n\n\nRecommendation:\n\nIf our application doesn’t require detecting tiny details and can tolerate some loss in accuracy, reducing the input size is often the most effective way to speed up inference.\nReducing alpha might be preferable if maintaining the ability to detect fine details is crucial or if you need a more balanced trade-off between speed and accuracy.\nFor best results, you might want to experiment with both:\n\nTry MobileNet V2 with input sizes like \\(160\\times 160\\) or \\(92\\times 92\\)\nExperiment with alpha values like 1.0, 0.75, 0.5 or 0.35.\n\nAlways benchmark the different configurations on your specific hardware and with your particular dataset to find the optimal balance for your use case.\n\n\nRemember, the best choice depends on your specific requirements for accuracy, speed, and the nature of the images you’re working with. It’s often worth experimenting with combinations to find the optimal configuration for your particular use case.\n\n\n\nModel Testing\nNow, you should take the data set aside at the start of the project and run the trained model using it as input. Again, the result is excellent (92.22%).\n\n\nDeploying the model\nAs we did in the previous section, we can deploy the trained model as .tflite and use Raspi to run it using Python.\nOn the Dashboard tab, go to Transfer learning model (int8 quantized) and click on the download icon:\n \n\nLet’s also download the float32 version for comparison\n\nTransfer the models from your computer to the Raspi (./models), for example, using FileZilla. Also, capture some images for inference and save them in (./images), or use the images in the ./dataset folder.\nLet’s remember what we did in the last chapter:\nActivate the environment:\nsource ~/tflite_env/bin/activate\nRun a Jupyter Notebook, using the command:\njupyter notebook --ip=192.168.4.210 --no-browser\n\nChange the IP address for yours\n\nOpen a new notebook and enter with the code below:\nImport the needed libraries:\nimport time\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom PIL import Image\nimport tflite_runtime.interpreter as tflite\nDefine the paths and labels:\nimg_path = \"./images/robot.jpg\"\nmodel_path = \"./models/ei-raspi-img-class-int8-quantized-\\\n                model.tflite\"\nlabels = ['background', 'periquito', 'robot']\n\nNote that the models trained on the Edge Impulse Studio will output values with index 0, 1, 2, etc., where the actual labels will follow an alphabetic order.\n\nLoad the model, allocate the tensors, and get the input and output tensor details:\n# Load the TFLite model\ninterpreter = tflite.Interpreter(model_path=model_path)\ninterpreter.allocate_tensors()\n\n# Get input and output tensors\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\nOne important difference to note is that the dtype of the input details of the model is now int8, which means that the input values go from –128 to +127, while each pixel of our image goes from 0 to 255. This means that we should pre-process the image to match it. We can check here:\ninput_dtype = input_details[0]['dtype']\ninput_dtype\nnumpy.int8\nSo, let’s open the image and show it:\nimg = Image.open(img_path)\nplt.figure(figsize=(4, 4))\nplt.imshow(img)\nplt.axis('off')\nplt.show()\n \nAnd perform the pre-processing:\nscale, zero_point = input_details[0]['quantization']\nimg = img.resize((input_details[0]['shape'][1],\n                  input_details[0]['shape'][2]))\nimg_array = np.array(img, dtype=np.float32) / 255.0\nimg_array = (\n    (img_array / scale + zero_point)\n    .clip(-128, 127)\n    .astype(np.int8)\n)\ninput_data = np.expand_dims(img_array, axis=0)\nChecking the input data, we can verify that the input tensor is compatible with what is expected by the model:\ninput_data.shape, input_data.dtype\n((1, 160, 160, 3), dtype('int8'))\nNow, it is time to perform the inference. Let’s also calculate the latency of the model:\n# Inference on Raspi-Zero\nstart_time = time.time()\ninterpreter.set_tensor(input_details[0]['index'], input_data)\ninterpreter.invoke()\nend_time = time.time()\ninference_time = (end_time - start_time) * 1000 # Convert\n                                                # to milliseconds\nprint (\"Inference time: {:.1f}ms\".format(inference_time))\nThe model will take around 125ms to perform the inference in the Raspi-Zero, which is 3 to 4 times longer than a Raspi-5.\nNow, we can get the output labels and probabilities. It is also important to note that the model trained on the Edge Impulse Studio has a softmax activation function in its output (different from the original Movilenet V2), and we can use the model’s raw output as the “probabilities.”\n# Obtain results and map them to the classes\npredictions = interpreter.get_tensor(output_details[0]\n                                     ['index'])[0]\n\n# Get indices of the top k results\ntop_k_results=3\ntop_k_indices = np.argsort(predictions)[::-1][:top_k_results]\n\n# Get quantization parameters\nscale, zero_point = output_details[0]['quantization']\n\n# Dequantize the output\ndequantized_output = (predictions.astype(np.float32) -\n                      zero_point) * scale\nprobabilities = dequantized_output\n\nprint(\"\\n\\t[PREDICTION]        [Prob]\\n\")\nfor i in range(top_k_results):\n    print(\"\\t{:20}: {:.2f}%\".format(\n        labels[top_k_indices[i]],\n        probabilities[top_k_indices[i]] * 100))\n \nLet’s modify the function created before so that we can handle different type of models:\n\ndef image_classification(img_path, model_path, labels,\n                         top_k_results=3, apply_softmax=False):\n    # Load the image\n    img = Image.open(img_path)\n    plt.figure(figsize=(4, 4))\n    plt.imshow(img)\n    plt.axis('off')\n\n    # Load the TFLite model\n    interpreter = tflite.Interpreter(model_path=model_path)\n    interpreter.allocate_tensors()\n    \n    # Get input and output tensors\n    input_details = interpreter.get_input_details()\n    output_details = interpreter.get_output_details()\n    \n    # Preprocess\n    img = img.resize((input_details[0]['shape'][1],\n                      input_details[0]['shape'][2]))\n    \n    input_dtype = input_details[0]['dtype']\n    \n    if input_dtype == np.uint8:\n        input_data = np.expand_dims(np.array(img), axis=0)\n    elif input_dtype == np.int8:\n        scale, zero_point = input_details[0]['quantization']\n        img_array = np.array(img, dtype=np.float32) / 255.0\n        img_array = (\n            img_array / scale\n            + zero_point\n        ).clip(-128, 127).astype(np.int8)\n        input_data = np.expand_dims(img_array, axis=0)\n    else:  # float32\n        input_data = np.expand_dims(\n            np.array(img, dtype=np.float32),\n            axis=0\n        ) / 255.0\n    \n    # Inference on Raspi-Zero\n    start_time = time.time()\n    interpreter.set_tensor(input_details[0]['index'], input_data)\n    interpreter.invoke()\n    end_time = time.time()\n    inference_time = (end_time -\n                        start_time\n                      ) * 1000 # Convert to milliseconds\n    \n    # Obtain results\n    predictions = interpreter.get_tensor(output_details[0]\n                                         ['index'])[0]\n    \n    # Get indices of the top k results\n    top_k_indices = np.argsort(predictions)[::-1][:top_k_results]\n    \n    # Handle output based on type\n    output_dtype = output_details[0]['dtype']\n    if output_dtype in [np.int8, np.uint8]:\n        # Dequantize the output\n        scale, zero_point = output_details[0]['quantization']\n        predictions = (predictions.astype(np.float32) -\n                       zero_point) * scale\n    \n    if apply_softmax:\n        # Apply softmax\n        exp_preds = np.exp(predictions - np.max(predictions))\n        probabilities = exp_preds / np.sum(exp_preds)\n    else:\n        probabilities = predictions\n    \n    print(\"\\n\\t[PREDICTION]        [Prob]\\n\")\n    for i in range(top_k_results):\n        print(\"\\t{:20}: {:.1f}%\".format(\n            labels[top_k_indices[i]],\n            probabilities[top_k_indices[i]] * 100))\n    print (\"\\n\\tInference time: {:.1f}ms\".format(inference_time))\n\nAnd test it with different images and the int8 quantized model (160x160 alpha =1.0).\n \nLet’s download a smaller model, such as the one trained for the Nicla Vision Lab (int8 quantized model, 96x96, alpha = 0.1), as a test. We can use the same function:\n \nThe model lost some accuracy, but it is still OK once our model does not look for many details. Regarding latency, we are around ten times faster on the Raspi-Zero."
  },
  {
    "objectID": "raspi/image_classification/custom_image_classification_project.html#sec-image-classification-live-image-classification-f123",
    "href": "raspi/image_classification/custom_image_classification_project.html#sec-image-classification-live-image-classification-f123",
    "title": "Custom Image Classification Project",
    "section": "Live Image Classification",
    "text": "Live Image Classification\nLet’s develop an app that captures images with the camera in real-time and displays their classification.\nUsing the nano on the terminal, save the code below, such as img_class_live_infer.py.\n\nfrom flask import Flask, Response, render_template_string,\n                  request, jsonify\nfrom picamera2 import Picamera2\nimport io\nimport threading\nimport time\nimport numpy as np\nfrom PIL import Image\nimport tflite_runtime.interpreter as tflite\nfrom queue import Queue\n\napp = Flask(__name__)\n\n# Global variables\npicam2 = None\nframe = None\nframe_lock = threading.Lock()\nis_classifying = False\nconfidence_threshold = 0.8\nmodel_path = \"./models/ei-raspi-img-class-int8-quantized-\\\n                model.tflite\"\nlabels = ['background', 'periquito', 'robot']\ninterpreter = None\nclassification_queue = Queue(maxsize=1)\n\ndef initialize_camera():\n    global picam2\n    picam2 = Picamera2()\n    config = picam2.create_preview_configuration(\n        main={\"size\": (320, 240)}\n    )\n    picam2.configure(config)\n    picam2.start()\n    time.sleep(2)  # Wait for camera to warm up\n\ndef get_frame():\n    global frame\n    while True:\n        stream = io.BytesIO()\n        picam2.capture_file(stream, format='jpeg')\n        with frame_lock:\n            frame = stream.getvalue()\n        time.sleep(0.1)  # Capture frames more frequently\n\ndef generate_frames():\n    while True:\n        with frame_lock:\n            if frame is not None:\n                yield (\n                   b'--frame\\r\\n'\n                   b'Content-Type: image/jpeg\\r\\n\\r\\n'\n                   + frame + b'\\r\\n'\n                )\n        time.sleep(0.1)\n\ndef load_model():\n    global interpreter\n    if interpreter is None:\n        interpreter = tflite.Interpreter(model_path=model_path)\n        interpreter.allocate_tensors()\n    return interpreter\n\ndef classify_image(img, interpreter):\n    input_details = interpreter.get_input_details()\n    output_details = interpreter.get_output_details()\n\n    img = img.resize((input_details[0]['shape'][1],\n                      input_details[0]['shape'][2]))\n    input_data = np.expand_dims(np.array(img), axis=0)\\\n                             .astype(input_details[0]['dtype'])\n    \n    interpreter.set_tensor(input_details[0]['index'], input_data)\n    interpreter.invoke()\n    \n    predictions = interpreter.get_tensor(output_details[0]\n                                         ['index'])[0]\n    # Handle output based on type\n    output_dtype = output_details[0]['dtype']\n    if output_dtype in [np.int8, np.uint8]:\n        # Dequantize the output\n        scale, zero_point = output_details[0]['quantization']\n        predictions = (predictions.astype(np.float32) -\n                       zero_point) * scale\n    return predictions\n\ndef classification_worker():\n    interpreter = load_model()\n    while True:\n        if is_classifying:\n            with frame_lock:\n                if frame is not None:\n                    img = Image.open(io.BytesIO(frame))\n            predictions = classify_image(img, interpreter)\n            max_prob = np.max(predictions)\n            if max_prob &gt;= confidence_threshold:\n                label = labels[np.argmax(predictions)]\n            else:\n                label = 'Uncertain'\n            classification_queue.put({\n                 'label': label,\n                 'probability': float(max_prob)\n            })\n        time.sleep(0.1)  # Adjust based on your needs\n\n@app.route('/')\ndef index():\n   return render_template_string('''\n      &lt;!DOCTYPE html&gt;\n      &lt;html&gt;\n      &lt;head&gt;\n          &lt;title&gt;Image Classification&lt;/title&gt;\n          &lt;script\n            src=\"https://code.jquery.com/jquery-3.6.0.min.js\"&gt;\n          &lt;/script&gt;\n          &lt;script&gt;\n              function startClassification() {\n                  $.post('/start');\n                  $('#startBtn').prop('disabled', true);\n                  $('#stopBtn').prop('disabled', false);\n              }\n              function stopClassification() {\n                  $.post('/stop');\n                  $('#startBtn').prop('disabled', false);\n                  $('#stopBtn').prop('disabled', true);\n              }\n              function updateConfidence() {\n                  var confidence = $('#confidence').val();\n                  $.post('/update_confidence',\n                         {confidence: confidence}\n                        );\n              }\n              function updateClassification() {\n                  $.get('/get_classification', function(data) {\n                    $('#classification').text(data.label + ': '\n                    + data.probability.toFixed(2));\n                  });\n              }\n              $(document).ready(function() {\n                  setInterval(updateClassification, 100);\n                  // Update every 100ms\n              });\n          &lt;/script&gt;\n      &lt;/head&gt;\n      &lt;body&gt;\n          &lt;h1&gt;Image Classification&lt;/h1&gt;\n          &lt;img src=\"{{ url_for('video_feed') }}\"\n               width=\"640\"\n               height=\"480\" /&gt;\n\n          &lt;br&gt;\n          &lt;button id=\"startBtn\"\n                  onclick=\"startClassification()\"&gt;\n            Start Classification\n          &lt;/button&gt;\n    \n          &lt;button id=\"stopBtn\"\n                  onclick=\"stopClassification()\"\n                  disabled&gt;\n            Stop Classification\n          &lt;/button&gt;\n    \n          &lt;br&gt;\n          &lt;label for=\"confidence\"&gt;Confidence Threshold:&lt;/label&gt;\n          &lt;input type=\"number\"\n                 id=\"confidence\"\n                 name=\"confidence\"\n                 min=\"0\" max=\"1\"\n                 step=\"0.1\"\n                 value=\"0.8\"\n                 onchange=\"updateConfidence()\" /&gt;\n    \n          &lt;br&gt;\n          &lt;div id=\"classification\"&gt;\n             Waiting for classification...\n          &lt;/div&gt;\n    \n      &lt;/body&gt;\n      &lt;/html&gt;\n   ''')\n\n@app.route('/video_feed')\ndef video_feed():\n    return Response(\n       generate_frames(),\n       mimetype='multipart/x-mixed-replace; boundary=frame'\n    )\n\n@app.route('/start', methods=['POST'])\ndef start_classification():\n    global is_classifying\n    is_classifying = True\n    return '', 204\n\n@app.route('/stop', methods=['POST'])\ndef stop_classification():\n    global is_classifying\n    is_classifying = False\n    return '', 204\n\n@app.route('/update_confidence', methods=['POST'])\ndef update_confidence():\n    global confidence_threshold\n    confidence_threshold = float(request.form['confidence'])\n    return '', 204\n\n@app.route('/get_classification')\ndef get_classification():\n    if not is_classifying:\n        return jsonify({'label': 'Not classifying',\n                       'probability': 0})\n    try:\n        result = classification_queue.get_nowait()\n    except Queue.Empty:\n        result = {'label': 'Processing', 'probability': 0}\n    return jsonify(result)\n\nif __name__ == '__main__':\n    initialize_camera()\n    threading.Thread(target=get_frame, daemon=True).start()\n    threading.Thread(target=classification_worker,\n                     daemon=True).start()\n    app.run(host='0.0.0.0', port=5000, threaded=True)\n\nOn the terminal, run:\npython3 img_class_live_infer.py\nAnd access the web interface:\n\nOn the Raspberry Pi itself (if you have a GUI): Open a web browser and go to http://localhost:5000\nFrom another device on the same network: Open a web browser and go to http://&lt;raspberry_pi_ip&gt;:5000 (Replace &lt;raspberry_pi_ip&gt; with your Raspberry Pi’s IP address). For example: http://192.168.4.210:5000/\n\nHere are some screenshots of the app running on an external desktop\n \nHere, you can see the app running on the YouTube:\n\nThe code creates a web application for real-time image classification using a Raspberry Pi, its camera module, and a TensorFlow Lite model. The application uses Flask to serve a web interface where is possible to view the camera feed and see live classification results.\n\nKey Components:\n\nFlask Web Application: Serves the user interface and handles requests.\nPiCamera2: Captures images from the Raspberry Pi camera module.\nTensorFlow Lite: Runs the image classification model.\nThreading: Manages concurrent operations for smooth performance.\n\n\n\nMain Features:\n\nLive camera feed display\nReal-time image classification\nAdjustable confidence threshold\nStart/Stop classification on demand\n\n\n\nCode Structure:\n\nImports and Setup:\n\nFlask for web application\nPiCamera2 for camera control\nTensorFlow Lite for inference\nThreading and Queue for concurrent operations\n\nGlobal Variables:\n\nCamera and frame management\nClassification control\nModel and label information\n\nCamera Functions:\n\ninitialize_camera(): Sets up the PiCamera2\nget_frame(): Continuously captures frames\ngenerate_frames(): Yields frames for the web feed\n\nModel Functions:\n\nload_model(): Loads the TFLite model\nclassify_image(): Performs inference on a single image\n\nClassification Worker:\n\nRuns in a separate thread\nContinuously classifies frames when active\nUpdates a queue with the latest results\n\nFlask Routes:\n\n/: Serves the main HTML page\n/video_feed: Streams the camera feed\n/start and /stop: Controls classification\n/update_confidence: Adjusts the confidence threshold\n/get_classification: Returns the latest classification result\n\nHTML Template:\n\nDisplays camera feed and classification results\nProvides controls for starting/stopping and adjusting settings\n\nMain Execution:\n\nInitializes camera and starts necessary threads\nRuns the Flask application\n\n\n\n\nKey Concepts:\n\nConcurrent Operations: Using threads to handle camera capture and classification separately from the web server.\nReal-time Updates: Frequent updates to the classification results without page reloads.\nModel Reuse: Loading the TFLite model once and reusing it for efficiency.\nFlexible Configuration: Allowing users to adjust the confidence threshold on the fly.\n\n\n\nUsage:\n\nEnsure all dependencies are installed.\nRun the script on a Raspberry Pi with a camera module.\nAccess the web interface from a browser using the Raspberry Pi’s IP address.\nStart classification and adjust settings as needed."
  },
  {
    "objectID": "raspi/image_classification/custom_image_classification_project.html#sec-image-classification-summary-9796",
    "href": "raspi/image_classification/custom_image_classification_project.html#sec-image-classification-summary-9796",
    "title": "Custom Image Classification Project",
    "section": "Summary:",
    "text": "Summary:\nImage classification has emerged as a powerful and versatile application of machine learning, with significant implications for various fields, from healthcare to environmental monitoring. This chapter has demonstrated how to implement a robust image classification system on edge devices like the Raspi-Zero and Raspi-5, showcasing the potential for real-time, on-device intelligence.\nWe’ve explored the entire pipeline of an image classification project, from data collection and model training using Edge Impulse Studio to deploying and running inferences on a Raspi. The process highlighted several key points:\n\nThe importance of proper data collection and preprocessing for training effective models.\nThe power of transfer learning, allowing us to leverage pre-trained models like MobileNet V2 for efficient training with limited data.\nThe trade-offs between model accuracy and inference speed, especially crucial for edge devices.\nThe implementation of real-time classification using a web-based interface, demonstrating practical applications.\n\nThe ability to run these models on edge devices like the Raspi opens up numerous possibilities for IoT applications, autonomous systems, and real-time monitoring solutions. It allows for reduced latency, improved privacy, and operation in environments with limited connectivity.\nAs we’ve seen, even with the computational constraints of edge devices, it’s possible to achieve impressive results in terms of both accuracy and speed. The flexibility to adjust model parameters, such as input size and alpha values, allows for fine-tuning to meet specific project requirements.\nLooking forward, the field of edge AI and image classification continues to evolve rapidly. Advances in model compression techniques, hardware acceleration, and more efficient neural network architectures promise to further expand the capabilities of edge devices in computer vision tasks.\nThis project serves as a foundation for more complex computer vision applications and encourages further exploration into the exciting world of edge AI and IoT. Whether it’s for industrial automation, smart home applications, or environmental monitoring, the skills and concepts covered here provide a solid starting point for a wide range of innovative projects."
  },
  {
    "objectID": "raspi/image_classification/custom_image_classification_project.html#sec-image-classification-resources-1d0e",
    "href": "raspi/image_classification/custom_image_classification_project.html#sec-image-classification-resources-1d0e",
    "title": "Custom Image Classification Project",
    "section": "Resources",
    "text": "Resources\n\nDataset Example\nPython Scripts\nEdge Impulse Project\nImage Classification Project - Edge Impulse Notebook"
  },
  {
    "objectID": "raspi/object_detection/object_detection_fundamentals.html#introduction",
    "href": "raspi/object_detection/object_detection_fundamentals.html#introduction",
    "title": "Object Detection: Fundamentals",
    "section": "Introduction",
    "text": "Introduction\nBuilding upon our exploration of image classification, we now turn our attention to a more advanced computer vision task: object detection. While image classification assigns a single label to an entire image, object detection goes further by identifying and locating multiple objects within a single image. This capability opens up many new applications and challenges, particularly in edge computing and IoT devices like the Raspberry Pi.\nObject detection combines the tasks of classification and localization. It not only determines what objects are present in an image but also pinpoints their locations by, for example, drawing bounding boxes around them. This added complexity makes object detection a more powerful tool for understanding visual scenes, but it also requires more sophisticated models and training techniques.\nIn edge AI, where we work with constrained computational resources, implementing efficient object detection models becomes crucial. The challenges we faced with image classification—balancing model size, inference speed, and accuracy—are even more pronounced in object detection. However, the rewards are also more significant, as object detection enables more nuanced and detailed analysis of visual data.\nSome applications of object detection on edge devices include:\n\nSurveillance and security systems\nAutonomous vehicles and drones\nIndustrial quality control\nWildlife monitoring\nAugmented reality applications\n\nAs we put our hands into object detection, we’ll build upon the concepts and techniques we explored in image classification. We’ll examine popular object detection architectures designed for efficiency, such as:\n\nSingle Stage Detectors, such as MobileNet and EfficientDet,\nFOMO (Faster Objects, More Objects), and\nYOLO (You Only Look Once).\n\n\nTo learn more about object detection models, follow the tutorial A Gentle Introduction to Object Recognition With Deep Learning.\n\nWe will explore those object detection models using:\n\nTensorFlow Lite Runtime (now changed to LiteRT),\nEdge Impulse Linux Python SDK and\nUltralitics\n\n\nThroughout this lab, we’ll cover the fundamentals of object detection and how it differs from image classification. We’ll also learn how to train, fine-tune, test, optimize, and deploy popular object detection architectures using a dataset created from scratch.\n\nObject Detection Fundamentals\nObject detection builds upon the foundations of image classification but extends its capabilities significantly. To understand object detection, it’s crucial first to recognize its key differences from image classification:\n\nImage Classification vs. Object Detection\nImage Classification:\n\nAssigns a single label to an entire image\nAnswers the question: “What is this image’s primary object or scene?”\nOutputs a single class prediction for the whole image\n\nObject Detection:\n\nIdentifies and locates multiple objects within an image\nAnswers the questions: “What objects are in this image, and where are they located?”\nOutputs multiple predictions, each consisting of a class label and a bounding box\n\nTo visualize this difference, let’s consider an example:\n\nThis diagram illustrates the critical difference: image classification provides a single label for the entire image, while object detection identifies multiple objects, their classes, and their locations within the image.\n\n\nKey Components of Object Detection\nObject detection systems typically consist of two main components:\n\nObject Localization: This component identifies the location of objects within the image. It typically outputs bounding boxes, rectangular regions encompassing each detected object.\nObject Classification: This component determines the class or category of each detected object, similar to image classification but applied to each localized region.\n\n\n\nChallenges in Object Detection\nObject detection presents several challenges beyond those of image classification:\n\nMultiple objects: An image may contain multiple objects of various classes, sizes, and positions.\nVarying scales: Objects can appear at different sizes within the image.\nOcclusion: Objects may be partially hidden or overlapping.\nBackground clutter: Distinguishing objects from complex backgrounds can be challenging.\nReal-time performance: Many applications require fast inference times, especially on edge devices.\n\n\n\nApproaches to Object Detection\nThere are two main approaches to object detection:\n\nTwo-stage detectors: These first propose regions of interest and then classify each region. Examples include R-CNN and its variants (Fast R-CNN, Faster R-CNN).\nSingle-stage detectors: These predict bounding boxes (or centroids) and class probabilities in one forward pass of the network. Examples include YOLO (You Only Look Once), EfficientDet, SSD (Single Shot Detector), and FOMO (Faster Objects, More Objects). These are often faster and more suitable for edge devices, such as the Raspberry Pi.\n\n\n\nEvaluation Metrics\nObject detection uses different metrics compared to image classification:\n\nIntersection over Union (IoU) is a metric used to evaluate the accuracy of an object detector. It measures the overlap between two bounding boxes: the Ground Truth box (the manually labeled correct box) and the Predicted box (the box generated by the object detection model). The IoU value is calculated by dividing the area of the Intersection (the overlapping area) by the area of the Union (the total area covered by both boxes). A higher IoU value indicates a better prediction.\n\n\n\nMean Average Precision (mAP) is a widely used metric for evaluating the performance of object detection models. It provides a single number that reflects a model’s ability to accurately both classify and localize objects. The “mean” in mAP refers to the average taken over all object classes in the dataset. The “average precision” (AP) is calculated for each class, and then these AP values are averaged to get the final mAP score. A high mAP score indicates that the model is excellent at identifying all objects and placing a tight-fitting, accurate bounding box around them.\n\n\n\nFrames Per Second (FPS): Measures detection speed, crucial for real-time applications on edge devices."
  },
  {
    "objectID": "raspi/object_detection/object_detection_fundamentals.html#pre-trained-object-detection-models-overview",
    "href": "raspi/object_detection/object_detection_fundamentals.html#pre-trained-object-detection-models-overview",
    "title": "Object Detection: Fundamentals",
    "section": "Pre-Trained Object Detection Models Overview",
    "text": "Pre-Trained Object Detection Models Overview\nAs we saw in the introduction, given an image or a video stream, an object detection model can identify which of a known set of objects might be present and provide information about their positions within the image.\n\nYou can test some common models online by visiting Object Detection - MediaPipe Studio\n\nOn Kaggle, we can find the most common pre-trained TFLite models to use with the Raspberry Pi, ssd_mobilenet_v1, and efficiendet. Those models were trained on the COCO (Common Objects in Context) dataset, with over 200,000 labeled images in 91 categories.\nDownload the models and upload them to the ./models folder on the Raspberry Pi.\n\nAlternatively, you can find the models and the COCO labels on GitHub.\n\nFor the first part of this lab, we will focus on a pre-trained 300x300 SSD-Mobilenet V1 model and compare it with the 320x320 EfficientDet-lite0, also trained using the COCO 2017 dataset. Both models were converted to a TensorFlow Lite format (4.2MB for the SSD Mobilenet and 4.6MB for the EfficientDet).\n\nSSD-Mobilenet V2 or V3 is recommended for transfer learning projects, but once the V1 TFLite model is publicly available, we will use it for this overview.\n\n\n\nThe model outputs up to ten detections per image, including bounding boxes, class IDs, and confidence scores.\n\n\nSetting Up the TFLite Environment\nWe should confirm the steps done on the last Hands-On Lab, Image Classification, as follows:\n\nUpdating the Raspberry Pi\nInstalling Required Libraries\nSetting up a Virtual Environment (Optional but Recommended)\n\nsource ~/tflite/bin/activate\n\nInstalling TensorFlow Lite Runtime\nInstalling Additional Python Libraries (inside the environment)\n\n\n\nCreating a Working Directory:\nConsidering that we have created the Documents/TFLITE folder in the last Lab, let’s now create the specific folders for this object detection lab:\ncd Documents/TFLITE/\nmkdir OBJ_DETECT\ncd OBJ_DETECT\nmkdir images\nmkdir models\ncd models\n\n\nInference and Post-Processing\n\nLet’s start a new notebook to follow all the steps to detect objects in an image:\nImport the needed libraries:\nimport time\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom PIL import Image\nimport tflite_runtime.interpreter as tflite\nLoad the TFLite model and allocate tensors:\nmodel_path = \"./models/ssd-mobilenet-v1-tflite-default-v1.tflite\"\ninterpreter = tflite.Interpreter(model_path=model_path)\ninterpreter.allocate_tensors()\nGet input and output tensors.\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\nInput details will inform us how the model should be fed with an image. The shape of (1, 300, 300, 3) with a dtype of uint8 tells us that a non-normalized (pixel value range from 0 to 255) image with dimensions (300x300x3) should be input one by one (Batch Dimension: 1).\n\nThe output details include not only the labels (“classes”) and probabilities (“scores”) but also the relative window position of the bounding boxes (“boxes”) about where the object is located on the image and the number of detected objects (“num_detections”). The output details also tell us that the model can detect a maximum of 10 objects in the image.\n\nSo, for the above example, using the same cat image used with the Image Classification Lab, looking for the output, we have a 76% probability of having found an object with a class ID of 16 on an area delimited by a bounding box of [0.028011084, 0.020121813, 0.9886069, 0.802299]. Those four numbers are related to ymin, xmin, ymax, and xmax, the box coordinates.\n\nConsidering that y ranges from the top (ymin) to the bottom (ymax) and x ranges from left (xmin) to right (xmax), we have, in fact, the coordinates of the top-left corner and the bottom-right one. With both edges and knowing the shape of the picture, it is possible to draw a rectangle around the object, as shown in the figure below:\n\nNext, we should find what class ID 16 means. Opening the file coco_labels.txt, as a list, each element has an associated index, and inspecting index 16, we get, as expected, cat. The probability is the value returned from the score.\nLet’s now upload some images with multiple objects on them for testing.\nimg_path = \"./images/cat_dog.jpeg\"\norig_img = Image.open(img_path)\n\n# Display the image\nplt.figure(figsize=(8, 8))\nplt.imshow(orig_img)\nplt.title(\"Original Image\")\nplt.show()\n\nBased on the input details, let’s pre-process the image, changing its shape and expanding its dimensions:\nimg = orig_img.resize((input_details[0]['shape'][1], \n                  input_details[0]['shape'][2]))\ninput_data = np.expand_dims(img, axis=0)\ninput_data.shape, input_data.dtype \nThe new input_data shape is(1, 300, 300, 3) with a dtype of uint8, which is compatible with what the model expects.\nUsing the input_data, let’s run the interpreter, measure the latency, and get the output:\nstart_time = time.time()\ninterpreter.set_tensor(input_details[0]['index'], input_data)\ninterpreter.invoke()\nend_time = time.time()\ninference_time = (end_time - start_time) * 1000  # Convert to milliseconds\nprint (\"Inference time: {:.1f}ms\".format(inference_time))\nWith a latency of around 800ms, we can get four distinct outputs:\nboxes = interpreter.get_tensor(output_details[0]['index'])[0] \nclasses = interpreter.get_tensor(output_details[1]['index'])[0]  \nscores = interpreter.get_tensor(output_details[2]['index'])[0]   \nnum_detections = int(interpreter.get_tensor(output_details[3]['index'])[0])\nOn a quick inspection, we can see that the model detected two objects with a score over 0.5:\nfor i in range(num_detections):\n    if scores[i] &gt; 0.5:  # Confidence threshold\n        print(f\"Object {i}:\")\n        print(f\"  Bounding Box: {boxes[i]}\")\n        print(f\"  Confidence: {scores[i]}\")\n        print(f\"  Class: {classes[i]}\")\n\nAnd we can also visualize the results:\nplt.figure(figsize=(12, 8))\nplt.imshow(orig_img)\nfor i in range(num_detections):\n    if scores[i] &gt; 0.5:  # Adjust threshold as needed\n        ymin, xmin, ymax, xmax = boxes[i]\n        (left, right, top, bottom) = (xmin * orig_img.width, \n                                      xmax * orig_img.width, \n                                      ymin * orig_img.height, \n                                      ymax * orig_img.height)\n        rect = plt.Rectangle((left, top), right-left, bottom-top, \n                             fill=False, color='red', linewidth=2)\n        plt.gca().add_patch(rect)\n        class_id = int(classes[i])\n        class_name = labels[class_id]\n        plt.text(left, top-10, f'{class_name}: {scores[i]:.2f}', \n                 color='red', fontsize=12, backgroundcolor='white')\n\n\nThe choice of the confidence threshold is crucial. For example, changing it to 0.2 will show false positives. A proper code should handle it.\n\n\n\nEfficientDet\nEfficientDet is not technically an SSD (Single Shot Detector) model, but it shares some similarities and builds upon ideas from SSD and other object detection architectures:\n\nEfficientDet:\n\nDeveloped by Google researchers in 2019\nUses EfficientNet as the backbone network\nEmploys a novel bi-directional feature pyramid network (BiFPN)\nIt uses compound scaling to scale the backbone network and the object detection components efficiently.\n\nSimilarities to SSD:\n\nBoth are single-stage detectors, meaning they perform object localization and classification in a single forward pass.\nBoth use multi-scale feature maps to detect objects at different scales.\n\nKey differences:\n\nBackbone: SSD typically uses VGG or MobileNet, while EfficientDet uses EfficientNet.\nFeature fusion: SSD uses a simple feature pyramid, while EfficientDet uses the more advanced BiFPN.\nScaling method: EfficientDet introduces compound scaling for all components of the network\n\nAdvantages of EfficientDet:\n\nGenerally achieves better accuracy-efficiency trade-offs than SSD and many other object detection models.\nMore flexible scaling enables a family of models with varying size-performance trade-offs.\n\n\nWhile EfficientDet is not an SSD model, it can be seen as an evolution of single-stage detection architectures, incorporating more advanced techniques to improve efficiency and accuracy. When using EfficientDet, we can expect similar output structures to SSD (e.g., bounding boxes and class scores).\n\nOn GitHub, you can find another notebook exploring the EfficientDet model that we did with SSD MobileNet."
  },
  {
    "objectID": "raspi/object_detection/object_detection_fundamentals.html#object-detection-on-a-live-stream",
    "href": "raspi/object_detection/object_detection_fundamentals.html#object-detection-on-a-live-stream",
    "title": "Object Detection: Fundamentals",
    "section": "Object Detection on a live stream",
    "text": "Object Detection on a live stream\nThe object detection models can also detect objects in real-time using a camera. The captured image should be the input for the trained and converted model. For the Raspberry Pi 4 or 5 with a desktop, OpenCV can capture the frames and display the inference result.\nHowever, even without a desktop, creating a live stream with a webcam to detect objects in real-time is also possible. For example, let’s start with the script developed for the Image Classification app and adapt it for a Real-Time Object Detection Web Application Using TensorFlow Lite and Flask.\nThis app version should work for any TFLite models.\n\nVerify if the model is in its correct folder, for example:\n\nmodel_path = \"./models/ssd-mobilenet-v1-tflite-default-v1.tflite\"\nDownload the Python script object_detection_app.py from GitHub.\nCheck the model and labels:\n\nAnd on the terminal, run:\npython object_detection_app.py\nAfter starting, you should receive the message on the terminal (the IP is from my Raspberry):\n\n* Running on http://192.168.4.210:5000\nPress CTRL+C to quit\n\nAnd access the web interface:\n\nOn the Raspberry Pi itself (if you have a GUI): Open a web browser and go to http://localhost:5000\nFrom another device on the same network: Open a web browser and go to http://&lt;raspberry_pi_ip&gt;:5000 (Replace &lt;raspberry_pi_ip&gt; with your Raspberry Pi’s IP address). For example: http://192.168.4.210:5000/\n\nHere is a screenshot of the app running on an external desktop\n\nLet’s see a technical description of the key modules used in the object detection application:\n\nTensorFlow Lite (tflite_runtime):\n\nPurpose: Efficient inference of machine learning models on edge devices.\nWhy: TFLite offers reduced model size and optimized performance compared to full TensorFlow, which is crucial for resource-constrained devices like Raspberry Pi. It supports hardware acceleration and quantization, further improving efficiency.\nKey functions: Interpreter for loading and running the model, get_input_details() and get_output_details() for interfacing with the model.\n\nFlask:\n\nPurpose: Lightweight web framework for building backend servers.\nWhy: Flask’s simplicity and flexibility make it ideal for rapidly developing and deploying web applications. It’s less resource-intensive than larger frameworks suitable for edge devices.\nKey components: route decorators for defining API endpoints, Response objects for streaming video, render_template_string for serving dynamic HTML.\n\nPicamera2:\n\nPurpose: Interface with the Raspberry Pi camera module.\nWhy: Picamera2 is the latest library for controlling Raspberry Pi cameras, offering improved performance and features over the original Picamera library.\nKey functions: create_preview_configuration() for setting up the camera, capture_file() for capturing frames.\n\nPIL (Python Imaging Library):\n\nPurpose: Image processing and manipulation.\nWhy: PIL provides a wide range of image processing capabilities. It’s used here to resize images, draw bounding boxes, and convert between image formats.\nKey classes: Image for loading and manipulating images, ImageDraw for drawing shapes and text on images.\n\nNumPy:\n\nPurpose: Efficient array operations and numerical computing.\nWhy: NumPy’s array operations are much faster than pure Python lists, which is crucial for efficiently processing image data and model inputs/outputs.\nKey functions: array() for creating arrays, expand_dims() for adding dimensions to arrays.\n\nThreading:\n\nPurpose: Concurrent execution of tasks.\nWhy: Threading enables simultaneous frame capture, object detection, and web server operation, which is crucial for maintaining real-time performance.\nKey components: Thread class creates separate execution threads, and Lock is used for thread synchronization.\n\nio.BytesIO:\n\nPurpose: In-memory binary streams.\nWhy: Allows efficient handling of image data in memory without needing temporary files, improving speed and reducing I/O operations.\n\ntime:\n\nPurpose: Time-related functions.\nWhy: Used for adding delays (time.sleep()) to control frame rate and for performance measurements.\n\njQuery (client-side):\n\nPurpose: Simplified DOM manipulation and AJAX requests.\nWhy: It makes it easy to update the web interface dynamically and communicate with the server without page reloads.\nKey functions: .get() and .post() for AJAX requests, DOM manipulation methods for updating the UI.\n\n\nRegarding the main app system architecture:\n\nMain Thread: Runs the Flask server, handling HTTP requests and serving the web interface.\nCamera Thread: Continuously captures frames from the camera.\nDetection Thread: Processes frames through the TFLite model for object detection.\nFrame Buffer: Shared memory space (protected by locks) storing the latest frame and detection results.\n\nAnd the app data flow, we can describe in short:\n\nCamera captures frame → Frame Buffer\nDetection thread reads from Frame Buffer → Processes through TFLite model → Updates detection results in Frame Buffer\nFlask routes access Frame Buffer to serve the latest frame and detection results\nWeb client receives updates via AJAX and updates UI\n\nThis architecture enables efficient, real-time object detection while maintaining a responsive web interface on a resource-constrained edge device, such as a Raspberry Pi. Threading and efficient libraries, such as TFLite and PIL, enable the system to process video frames in real-time, while Flask and jQuery provide a user-friendly way to interact with them.\nYou can test the app with another pre-processed model, such as the EfficientDet, by changing the app line:\nmodel_path = \"./models/lite-model_efficientdet_lite0_detection_metadata_1.tflite\"\n\nIf we want to use the app for the SSD-MobileNetV2 model, trained on Edge Impulse Studio with the “Box versus Wheel” dataset, the code should also be adapted depending on the input details, as we have explored in its notebook."
  },
  {
    "objectID": "raspi/object_detection/object_detection_fundamentals.html#conclusion",
    "href": "raspi/object_detection/object_detection_fundamentals.html#conclusion",
    "title": "Object Detection: Fundamentals",
    "section": "Conclusion",
    "text": "Conclusion\nThis lab has explored the implementation of object detection on edge devices like the Raspberry Pi, demonstrating the power and potential of running advanced computer vision tasks on resource-constrained hardware. We examined the object detection models SSD-MobileNet and EfficientDet, comparing their performance and trade-offs on edge devices.\nThe lab exemplified a real-time object detection web application, demonstrating how these models can be integrated into practical, interactive systems.\nThe ability to perform object detection on edge devices opens up numerous possibilities across various domains, including precision agriculture, industrial automation, quality control, smart home applications, and environmental monitoring. By processing data locally, these systems can offer reduced latency, improved privacy, and operation in environments with limited connectivity.\nLooking ahead, potential areas for further exploration include: - Using a custom dataset (labeled on Roboflow), walking through the process of training models using Edge Impulse Studio and Ultralytics, and deploying them on Raspberry Pi. - To improve inference speed on edge devices, explore various optimization methods, such as model quantization (TFLite int8) and format conversion (e.g., to NCNN). - Implementing multi-model pipelines for more complex tasks - Exploring hardware acceleration options for Raspberry Pi - Integrating object detection with other sensors for more comprehensive edge AI systems - Developing edge-to-cloud solutions that leverage both local processing and cloud resources\nObject detection on edge devices can create intelligent, responsive systems that bring the power of AI directly into the physical world, opening up new frontiers in how we interact with and understand our environment."
  },
  {
    "objectID": "raspi/object_detection/object_detection_fundamentals.html#resources",
    "href": "raspi/object_detection/object_detection_fundamentals.html#resources",
    "title": "Object Detection: Fundamentals",
    "section": "Resources",
    "text": "Resources\n\nSSD-MobileNet Notebook on a Raspi\nEfficientDet Notebook on a Raspi\nPython Scripts\nModels\n​"
  },
  {
    "objectID": "raspi/object_detection/custom_object_detection.html#object-detection-project",
    "href": "raspi/object_detection/custom_object_detection.html#object-detection-project",
    "title": "Custom Object Detection Project",
    "section": "Object Detection Project",
    "text": "Object Detection Project\nIn this chapter, we will develop a complete Object Detection project from data collection, labelling, training, and deployment. As we did with the Image Classification project, the trained and converted model will be used for inference.\n\nWe will use the same dataset to train 3 models: SSD-MobileNet V2, FOMO, and YOLO.\n\nThe Goal\nAll Machine Learning projects need to start with a goal. Let’s assume we are in an industrial facility and must sort and count wheels and special boxes.\n\nIn other words, we should perform a multi-label classification, where each image can have three classes:\n\nBackground (no objects)\nBox\nWheel\n\n\n\nRaw Data Collection\nOnce we have defined our Machine Learning project goal, the next and most crucial step is collecting the dataset. We can use a phone, the Raspi, or a mix to create the raw dataset (with no labels). Let’s use the simple web app on our Raspberry Pi to view the QVGA (320 x 240) captured images in a browser.\nFrom GitHub, get the Python script get_img_data.py and open it in the terminal:\npython3 get_img_data.py \nAccess the web interface:\n\nOn the Raspberry Pi itself (if you have a GUI): Open a web browser and go to http://localhost:5000\nFrom another device on the same network: Open a web browser and go to http://&lt;raspberry_pi_ip&gt;:5000 (Replace &lt;raspberry_pi_ip&gt; with your Raspberry Pi’s IP address). For example: http://192.168.4.210:5000/\n\nThe Python script creates a web-based interface for capturing and organizing image datasets using a Raspberry Pi and its camera. It’s handy for machine learning projects that require labeled image data or not, as in our case here.\nAccess the web interface from a browser, enter a generic label for the images you want to capture, and press Start Capture.\n\n\nNote that the images to be captured will have multiple labels that should be defined later.\n\nUse the live preview to position the camera and click Capture Image to save images under the current label (in this case, box-wheel.\n\nWhen we have enough images, we can press Stop Capture. The captured images are saved in the folder dataset/box-wheel:\n\n\nGet around 60 images. Try to capture different angles, backgrounds, and light conditions. Filezilla can transfer the created raw dataset to your main computer.\n\n\n\nLabeling Data\nThe next step in an Object Detect project is to create a labeled dataset. We should label the raw dataset images, creating bounding boxes around each picture’s objects (box and wheel). We can use labeling tools like LabelImg, CVAT, Roboflow, or even the Edge Impulse Studio. Once we have explored the Edge Impulse tool in other labs, let’s use Roboflow here.\n\nWe are using Roboflow (free version) here for two main reasons. 1) We can have auto-labeler, and 2) The annotated dataset is available in several formats and can be used both on Edge Impulse Studio (we will use it for MobileNet V2 and FOMO train) and on CoLab (YOLOv8 train), for example. Having the annotated dataset on Edge Impulse (Free account), it is not possible to use it for training on other platforms.\n\nWe should upload the raw dataset to Roboflow. Create a free account there and start a new project, for example, (“box-versus-wheel”).\n\n\nWe will not enter in deep details about the Roboflow process once many tutorials are available.\n\n\nAnnotate\nOnce the project is created and the dataset is uploaded, you can use the “Auto-Label” tool to make the annotations or do it manually.\n\nThe Label Assist tool can be handy to help on the labbeling process.\n\nNote that you should also upload images with only a background, which should be saved w/o any annotations using the Null Tool option.\n\nOnce all images are annotated, you should split them into training, validation, and testing.\n\n\n\nData Pre-Processing\nThe last step with the dataset is preprocessing to generate a final version for training. Let’s resize all images to 320x320 and generate augmented versions of each image (augmentation) to create new training examples from which our model can learn.\nFor augmentation, we will rotate the images (+/-15o), crop, and vary the brightness and exposure.\n\nAt the end of the process, we will have 153 images.\n\nNow, you should export the annotated dataset in a format that Edge Impulse, Ultralitics, and other frameworks/tools understand, for example, YOLOv8 (or v11). Let’s download a zipped version of the dataset to our desktop.\n\nHere, it is possible to review how the dataset was structured\n\nThere are 3 separate folders, one for each split (train/test/valid). For each of them, there are 2 subfolders, images, and labels. The pictures are stored as image_id.jpg and images_id.txt, where “image_id” is unique for every picture.\nThe labels file format will be class_id bounding box coordinates, where in our case, class_id will be 0 for box and 1 for wheel. The numerical id (o, 1, 2…) will follow the alphabetical order of the class name.\nThe data.yaml file has info about the dataset as the classes’ names (names: ['box', 'wheel']) following the YOLO format.\nAnd that’s it! We are ready to start training using the Edge Impulse Studio (as we will do in the following step), Ultralytics (as we will when discussing YOLO), or even training from scratch on CoLab (as we did with the Cifar-10 dataset on the Image Classification lab).\n\nThe pre-processed dataset can be found at the Roboflow site, or here:"
  },
  {
    "objectID": "raspi/object_detection/custom_object_detection.html#training-an-ssd-mobilenet-model-on-edge-impulse-studio",
    "href": "raspi/object_detection/custom_object_detection.html#training-an-ssd-mobilenet-model-on-edge-impulse-studio",
    "title": "Custom Object Detection Project",
    "section": "Training an SSD MobileNet Model on Edge Impulse Studio",
    "text": "Training an SSD MobileNet Model on Edge Impulse Studio\nGo to Edge Impulse Studio, enter your credentials at Login (or create an account), and start a new project.\n\nHere, you can clone the project developed for this hands-on lab: Raspi - Object Detection.\n\nOn the Project Dashboard tab, go down and on Project info, and for Labeling method select Bounding boxes (object detection)\n\nUploading the annotated data\nOn Studio, go to the Data acquisition tab, and on the UPLOAD DATA section, upload from your computer the raw dataset.\nWe can use the option Select a folder, choosing, for example, the folder train in your computer, which contains two sub-folders, images, and labels. Select the Image label format, “YOLO TXT”, upload into the caegory Training, and press Upload data.\n\nRepeat the process for the test data (upload both folders, test, and validation). At the end of the upload process, you should end with the annotated dataset of 153 images split in the train/test (84%/16%).\n\nNote that labels will be stored at the labels files 0 and 1 , which are equivalent to box and wheel.\n\n\n\n\nThe Impulse Design\nThe first thing to define when we enter the Create impulse step is to describe the target device for deployment. A pop-up window will appear. We will select Raspberry 4, an intermediary device between the Raspi-Zero and the Raspi-5.\n\nThis choice will not interfere with the training; it will only give us an idea about the latency of the model on that specific target.\n\n\nIn this phase, you should define how to:\n\nPre-processing consists of resizing the individual images. In our case, the images were pre-processed on Roboflow, to 320x320 , so let’s keep it. The resize will not matter here because the images are already squared. If you upload a rectangular image, squash it (squared form, without cropping). Afterward, you could define if the images are converted from RGB to Grayscale or not.\nDesign a Model, in this case, “Object Detection.”\n\n\n\n\nPreprocessing all dataset\nIn the section Image, select Color depth as RGB, and press Save parameters.\n\nThe Studio moves automatically to the next section, Generate features, where all samples will be pre-processed, resulting in 480 objects: 207 boxes and 273 wheels.\n\nThe feature explorer shows that all samples evidence a good separation after the feature generation.\n\n\nModel Design, Training, and Test\nFor training, we should select a pre-trained model. Let’s use the MobileNetV2 SSD FPN-Lite (320x320 only) .\n\nBase Network (MobileNetV2)\nDetection Network (Single Shot Detector or SSD)\nFeature Extractor (FPN-Lite)\n\nIt is a pre-trained object detection model designed to locate up to 10 objects within an image, outputting a bounding box for each object detected. The model is approximately 3.7 MB in size. It supports an RGB input at 320x320px.\n\nRegarding the training hyper-parameters, the model will be trained with:\n\nEpochs: 25\nBatch size: 32\nLearning Rate: 0.15.\n\nFor validation during training, 20% of the dataset (validation_dataset) will be spared.\n\nAs a result, the model ends with an overall precision score (based on COCO mAP) of 88.8%, higher than the result when using the test data (83.3%).\n\n\nDeploying the model\nWe have two ways to deploy our model:\n\nTFLite model, which lets deploy the trained model as .tflite for the Raspi to run it using Python.\nLinux (AARCH64), a binary for Linux (AARCH64), implements the Edge Impulse Linux protocol, which lets us run our models on any Linux-based development board, with SDKs for Python, for example. See the documentation for more information and setup instructions.\n\nLet’s deploy the TFLite model. On the Dashboard tab, go to Transfer learning model (int8 quantized) and click on the download icon:\n\n\nTransfer the model from your computer to the Raspi folder./models and capture or get some images for inference and save them in the folder ./images.\n\n\nInference and Post-Processing\nThe inference can be made as discussed in the Pre-Trained Object Detection Models Overview. Let’s start a new notebook to follow all the steps to detect cubes and wheels on an image.\nImport the needed libraries:\nimport time\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as patches\nfrom PIL import Image\nimport tflite_runtime.interpreter as tflite\nDefine the model path and labels:\nmodel_path = \"./models/ei-raspi-object-detection-SSD-MobileNetv2-320x0320-\\\nint8.lite\"\nlabels = ['box', 'wheel']\n\nRemember that the model will output the class ID as values (0 and 1), following an alphabetic order regarding the class names.\n\nLoad the model, allocate the tensors, and get the input and output tensor details:\n# Load the TFLite model\ninterpreter = tflite.Interpreter(model_path=model_path)\ninterpreter.allocate_tensors()\n\n# Get input and output tensors\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\nOne crucial difference to note is that the dtype of the input details of the model is now int8, which means that the input values go from -128 to +127, while each pixel of our raw image goes from 0 to 256. This means that we should pre-process the image to match it. We can check here:\ninput_dtype = input_details[0]['dtype']\ninput_dtype\nnumpy.int8\nSo, let’s open the image and show it:\n# Load the image\nimg_path = \"./images/box_2_wheel_2.jpg\"\norig_img = Image.open(img_path)\n\n# Display the image\nplt.figure(figsize=(6, 6))\nplt.imshow(orig_img)\nplt.title(\"Original Image\")\nplt.show()\n\nAnd perform the pre-processing:\nscale, zero_point = input_details[0]['quantization']\nimg = orig_img.resize((input_details[0]['shape'][1], \n                  input_details[0]['shape'][2]))\nimg_array = np.array(img, dtype=np.float32) / 255.0\nimg_array = (img_array / scale + zero_point).clip(-128, 127).astype(np.int8)\ninput_data = np.expand_dims(img_array, axis=0)\nChecking the input data, we can verify that the input tensor is compatible with what is expected by the model:\ninput_data.shape, input_data.dtype\n((1, 320, 320, 3), dtype('int8'))\nNow, it is time to perform the inference. Let’s also calculate the latency of the model:\n# Inference on Raspi-Zero\nstart_time = time.time()\ninterpreter.set_tensor(input_details[0]['index'], input_data)\ninterpreter.invoke()\nend_time = time.time()\ninference_time = (end_time - start_time) * 1000  # Convert to milliseconds\nprint (\"Inference time: {:.1f}ms\".format(inference_time))\nThe model will take around 600ms to perform the inference in the Raspi-Zero, which is around 5 times longer than a Raspi-5.\nNow, we can get the output classes of objects detected, its bounding boxes coordinates, and probabilities.\nboxes = interpreter.get_tensor(output_details[1]['index'])[0]  \nclasses = interpreter.get_tensor(output_details[3]['index'])[0]  \nscores = interpreter.get_tensor(output_details[0]['index'])[0]        \nnum_detections = int(interpreter.get_tensor(output_details[2]['index'])[0])\nfor i in range(num_detections):\n    if scores[i] &gt; 0.5:  # Confidence threshold\n        print(f\"Object {i}:\")\n        print(f\"  Bounding Box: {boxes[i]}\")\n        print(f\"  Confidence: {scores[i]}\")\n        print(f\"  Class: {classes[i]}\")\n\nFrom the results, we can see that 4 objects were detected: two with class ID 0 (box)and two with class ID 1 (wheel), what is correct!\nLet’s visualize the result for a threshold of 0.5\nthreshold = 0.5\nplt.figure(figsize=(6,6))\nplt.imshow(orig_img)\nfor i in range(num_detections):\n    if scores[i] &gt; threshold:  \n        ymin, xmin, ymax, xmax = boxes[i]\n        (left, right, top, bottom) = (xmin * orig_img.width, \n                                      xmax * orig_img.width, \n                                      ymin * orig_img.height, \n                                      ymax * orig_img.height)\n        rect = plt.Rectangle((left, top), right-left, bottom-top, \n                             fill=False, color='red', linewidth=2)\n        plt.gca().add_patch(rect)\n        class_id = int(classes[i])\n        class_name = labels[class_id]\n        plt.text(left, top-10, f'{class_name}: {scores[i]:.2f}', \n                 color='red', fontsize=12, backgroundcolor='white')\n\nBut what happens if we reduce the threshold to 0.3, for example?\n\nWe start to see false positives and multiple detections, where the model detects the same object multiple times with different confidence levels and slightly different bounding boxes.\nCommonly, sometimes, we need to adjust the threshold to smaller values to capture all objects, avoiding false negatives, which would lead to multiple detections.\nTo improve the detection results, we should implement Non-Maximum Suppression (NMS), which helps eliminate overlapping bounding boxes and keeps only the most confident detection.\nFor that, let’s create a general function named non_max_suppression(), with the role of refining object detection results by eliminating redundant and overlapping bounding boxes. It achieves this by iteratively selecting the detection with the highest confidence score and removing other significantly overlapping detections based on an Intersection over Union (IoU) threshold.\ndef non_max_suppression(boxes, scores, threshold):\n    # Convert to corner coordinates\n    x1 = boxes[:, 0]\n    y1 = boxes[:, 1]\n    x2 = boxes[:, 2]\n    y2 = boxes[:, 3]\n\n    areas = (x2 - x1 + 1) * (y2 - y1 + 1)\n    order = scores.argsort()[::-1]\n\n    keep = []\n    while order.size &gt; 0:\n        i = order[0]\n        keep.append(i)\n        xx1 = np.maximum(x1[i], x1[order[1:]])\n        yy1 = np.maximum(y1[i], y1[order[1:]])\n        xx2 = np.minimum(x2[i], x2[order[1:]])\n        yy2 = np.minimum(y2[i], y2[order[1:]])\n\n        w = np.maximum(0.0, xx2 - xx1 + 1)\n        h = np.maximum(0.0, yy2 - yy1 + 1)\n        inter = w * h\n        ovr = inter / (areas[i] + areas[order[1:]] - inter)\n\n        inds = np.where(ovr &lt;= threshold)[0]\n        order = order[inds + 1]\n\n    return keep\nHow it works:\n\nSorting: It starts by sorting all detections by their confidence scores, highest to lowest.\nSelection: It selects the highest-scoring box and adds it to the final list of detections.\nComparison: This selected box is compared with all remaining lower-scoring boxes.\nElimination: Any box that overlaps significantly (above the IoU threshold) with the selected box is eliminated.\nIteration: This process repeats with the next highest-scoring box until all boxes are processed.\n\nNow, we can define a more precise visualization function that will take into consideration an IoU threshold, detecting only the objects that were selected by the non_max_suppression function:\ndef visualize_detections(image, boxes, classes, scores, \n                         labels, threshold, iou_threshold):\n    if isinstance(image, Image.Image):\n        image_np = np.array(image)\n    else:\n        image_np = image\n\n    height, width = image_np.shape[:2]\n    \n    # Convert normalized coordinates to pixel coordinates\n    boxes_pixel = boxes * np.array([height, width, height, width])\n    \n    # Apply NMS\n    keep = non_max_suppression(boxes_pixel, scores, iou_threshold)\n    \n    # Set the figure size to 12x8 inches\n    fig, ax = plt.subplots(1, figsize=(12, 8))\n\n    ax.imshow(image_np)\n    \n    for i in keep:\n        if scores[i] &gt; threshold:\n            ymin, xmin, ymax, xmax = boxes[i]\n            rect = patches.Rectangle((xmin * width, ymin * height),\n                                     (xmax - xmin) * width,\n                                     (ymax - ymin) * height,\n                                     linewidth=2, edgecolor='r', facecolor='none')\n            ax.add_patch(rect)\n            class_name = labels[int(classes[i])]\n            ax.text(xmin * width, ymin * height - 10,\n                    f'{class_name}: {scores[i]:.2f}', color='red',\n                    fontsize=12, backgroundcolor='white')\n\n    plt.show()\nNow we can create a function that will call the others, performing inference on any image:\ndef detect_objects(img_path, conf=0.5, iou=0.5):\n    orig_img = Image.open(img_path)\n    scale, zero_point = input_details[0]['quantization']\n    img = orig_img.resize((input_details[0]['shape'][1], \n                      input_details[0]['shape'][2]))\n    img_array = np.array(img, dtype=np.float32) / 255.0\n    img_array = (img_array / scale + zero_point).clip(-128, 127).\\\n    astype(np.int8)\n    input_data = np.expand_dims(img_array, axis=0)\n    \n    # Inference on Raspi-Zero\n    start_time = time.time()\n    interpreter.set_tensor(input_details[0]['index'], input_data)\n    interpreter.invoke()\n    end_time = time.time()\n    inference_time = (end_time - start_time) * 1000  # Convert to ms\n    print (\"Inference time: {:.1f}ms\".format(inference_time))\n    \n    # Extract the outputs\n    boxes = interpreter.get_tensor(output_details[1]['index'])[0]  \n    classes = interpreter.get_tensor(output_details[3]['index'])[0]  \n    scores = interpreter.get_tensor(output_details[0]['index'])[0]        \n    num_detections = int(interpreter.get_tensor(output_details[2]['index'])[0])\n\n    visualize_detections(orig_img, boxes, classes, scores, labels, \n                         threshold=conf, \n                         iou_threshold=iou)\nNow, running the code, having the same image again with a confidence threshold of 0.3, but with a small IoU:\nimg_path = \"./images/box_2_wheel_2.jpg\"\ndetect_objects(img_path, conf=0.3,iou=0.05)"
  },
  {
    "objectID": "raspi/object_detection/custom_object_detection.html#training-a-fomo-model-at-edge-impulse-studio",
    "href": "raspi/object_detection/custom_object_detection.html#training-a-fomo-model-at-edge-impulse-studio",
    "title": "Custom Object Detection Project",
    "section": "Training a FOMO Model at Edge Impulse Studio",
    "text": "Training a FOMO Model at Edge Impulse Studio\nThe inference with the SSD MobileNet model worked well, but the latency was significantly high. The inference varied from 0.5 to 1.3 seconds on a Raspi-Zero, which means around or less than 1 FPS (1 frame per second). One alternative to speed up the process is to use FOMO (Faster Objects, More Objects).\nThis novel machine learning algorithm lets us count multiple objects and find their location in an image in real-time using up to 30x less processing power and memory than MobileNet SSD or YOLO. The main reason this is possible is that while other models calculate the object’s size by drawing a square around it (bounding box), FOMO ignores the size of the image, providing only the information about where the object is located in the image through its centroid coordinates.\n\nHow FOMO works?\nIn a typical object detection pipeline, the first stage is extracting features from the input image. FOMO leverages MobileNetV2 to perform this task. MobileNetV2 processes the input image to produce a feature map that captures essential characteristics, such as textures, shapes, and object edges, in a computationally efficient way.\n\nOnce these features are extracted, FOMO’s simpler architecture, focused on center-point detection, interprets the feature map to determine where objects are located in the image. The output is a grid of cells, where each cell represents whether or not an object center is detected. The model outputs one or more confidence scores for each cell, indicating the likelihood of an object being present.\nLet’s see how it works on an image.\nFOMO divides the image into blocks of pixels using a factor of 8. For the input of 96x96, the grid would be 12x12 (96/8=12). For a 160x160, the grid will be 20x20, and so on. Next, FOMO will run a classifier through each pixel block to calculate the probability that there is a box or a wheel in each of them and, subsequently, determine the regions that have the highest probability of containing the object (If a pixel block has no objects, it will be classified as background). From the overlap of the final region, the FOMO provides the coordinates (related to the image dimensions) of the centroid of this region.\n\nTrade-off Between Speed and Precision:\n\nGrid Resolution: FOMO uses a grid of fixed resolution, meaning each cell can detect if an object is present in that part of the image. While it doesn’t provide high localization accuracy, it makes a trade-off by being fast and computationally light, which is crucial for edge devices.\nMulti-Object Detection: Since each cell is independent, FOMO can detect multiple objects simultaneously in an image by identifying multiple centers.\n\n\n\nImpulse Design, new Training and Testing\nReturn to Edge Impulse Studio, and in the Experiments tab, create another impulse. Now, the input images should be 160x160 (this is the expected input size for MobilenetV2).\n\nOn the Image tab, generate the features and go to the Object detection tab.\nWe should select a pre-trained model for training. Let’s use the FOMO (Faster Objects, More Objects) MobileNetV2 0.35.\n\nRegarding the training hyper-parameters, the model will be trained with:\n\nEpochs: 30\nBatch size: 32\nLearning Rate: 0.001.\n\nFor validation during training, 20% of the dataset (validation_dataset) will be spared. We will not apply Data Augmentation for the remaining 80% (train_dataset) because our dataset was already augmented during the labeling phase at Roboflow.\nAs a result, the model ends with an overall F1 score of 93.3% with an impressive latency of 8ms (Raspi-4), around 60X less than we got with the SSD MovileNetV2.\n\n\nNote that FOMO automatically added a third label background to the two previously defined boxes (0) and wheels (1).\n\nOn the Model testing tab, we can see that the accuracy was 94%. Here is one of the test sample results:\n\n\nIn object detection tasks, accuracy is generally not the primary evaluation metric. Object detection involves classifying objects and providing bounding boxes around them, making it a more complex problem than simple classification. The issue is that we do not have the bounding box, only the centroids. In short, using accuracy as a metric could be misleading and may not provide a complete understanding of how well the model is performing.\n\n\n\nDeploying the model\nAs we did in the previous section, we can deploy the trained model as TFLite or Linux (AARCH64). Let’s do it now as Linux (AARCH64), a binary that implements the Edge Impulse Linux protocol.\nEdge Impulse for Linux models is delivered in .eim format. This executable contains our “full impulse” created in Edge Impulse Studio. The impulse consists of the signal processing block(s) and any learning and anomaly block(s) we added and trained. It is compiled with optimizations for our processor or GPU (e.g., NEON instructions on ARM cores), plus a straightforward IPC layer (over a Unix socket).\nAt the Deploy tab, select the option Linux (AARCH64), the int8model and press Build.\n\nThe model will be automatically downloaded to your computer.\nOn our Raspi, let’s create a new working area:\ncd ~\ncd Documents\nmkdir EI_Linux\ncd EI_Linux\nmkdir models\nmkdir images\nRename the model for easy identification:\nFor example, raspi-object-detection-linux-aarch64-FOMO-int8.eim and transfer it to the new Raspi folder./models and capture or get some images for inference and save them in the folder ./images.\n\n\nInference and Post-Processing\nThe inference will be made using the Linux Python SDK. This library lets us run machine learning models and collect sensor data on Linux machines using Python. The SDK is open source and hosted on GitHub: edgeimpulse/linux-sdk-python.\nLet’s set up a Virtual Environment for working with the Linux Python SDK\npython3 -m venv ~/eilinux\nsource ~/eilinux/bin/activate\nAnd Install the all the libraries needed:\nsudo apt-get update\nsudo apt-get install libatlas-base-dev libportaudio0 libportaudio2\nsudo apt-get installlibportaudiocpp0 portaudio19-dev\n\npip3 install edge_impulse_linux -i https://pypi.python.org/simple\npip3 install Pillow matplotlib pyaudio opencv-contrib-python\n\nsudo apt-get install portaudio19-dev\npip3 install pyaudio \npip3 install opencv-contrib-python\nPermit our model to be executable.\nchmod +x raspi-object-detection-linux-aarch64-FOMO-int8.eim\nInstall the Jupiter Notebook on the new environment\npip3 install jupyter\nRun a notebook locally (on the Raspi-4 or 5 with desktop)\njupyter notebook\nor on the browser on your computer:\njupyter notebook --ip=192.168.4.210 --no-browser\nLet’s start a new notebook by following all the steps to detect cubes and wheels on an image using the FOMO model and the Edge Impulse Linux Python SDK.\nImport the needed libraries:\nimport sys, time\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as patches\nfrom PIL import Image\nimport cv2\nfrom edge_impulse_linux.image import ImageImpulseRunner\nDefine the model path and labels:\nmodel_file = \"raspi-object-detection-linux-aarch64-int8.eim\"\nmodel_path = \"models/\"+ model_file # Trained ML model from Edge Impulse\nlabels = ['box', 'wheel']\n\nRemember that the model will output the class ID as values (0 and 1), following an alphabetic order regarding the class names.\n\nLoad and initialize the model:\n# Load the model file\nrunner = ImageImpulseRunner(model_path)\n\n# Initialize model\nmodel_info = runner.init()\nThe model_info will contain critical information about our model. However, unlike the TFLite interpreter, the EI Linux Python SDK library will now prepare the model for inference.\nSo, let’s open the image and show it (Now, for compatibility, we will use OpenCV, the CV Library used internally by EI. OpenCV reads the image as BGR, so we will need to convert it to RGB :\n# Load the image\nimg_path = \"./images/1_box_1_wheel.jpg\"\norig_img = cv2.imread(img_path)\nimg_rgb = cv2.cvtColor(orig_img, cv2.COLOR_BGR2RGB)\n\n# Display the image\nplt.imshow(img_rgb)\nplt.title(\"Original Image\")\nplt.show()\n\nNow we will get the features and the preprocessed image (cropped) using the runner:\nfeatures, cropped = runner.get_features_from_image_auto_studio_setings(img_rgb)\nAnd perform the inference. Let’s also calculate the latency of the model:\nres = runner.classify(features)\nLet’s get the output classes of objects detected, their bounding boxes centroids, and probabilities.\nprint('Found %d bounding boxes (%d ms.)' % (\n  len(res[\"result\"][\"bounding_boxes\"]), \n  res['timing']['dsp'] + res['timing']['classification']))\nfor bb in res[\"result\"][\"bounding_boxes\"]:\n    print('\\t%s (%.2f): x=%d y=%d w=%d h=%d' % (\n      bb['label'], bb['value'], bb['x'], \n      bb['y'], bb['width'], bb['height']))\nFound 2 bounding boxes (29 ms.)\n    1 (0.91): x=112 y=40 w=16 h=16\n    0 (0.75): x=48 y=56 w=8 h=8\nThe results show that two objects were detected: one with class ID 0 (box) and one with class ID 1 (wheel), which is correct!\nLet’s visualize the result (The threshold is 0.5, the default value set during the model testing on the Edge Impulse Studio).\nprint('\\tFound %d bounding boxes (latency: %d ms)' % (\n  len(res[\"result\"][\"bounding_boxes\"]), \n  res['timing']['dsp'] + res['timing']['classification']))\nplt.figure(figsize=(5,5))\nplt.imshow(cropped)\n\n# Go through each of the returned bounding boxes\nbboxes = res['result']['bounding_boxes']\nfor bbox in bboxes:\n\n    # Get the corners of the bounding box\n    left = bbox['x']\n    top = bbox['y']\n    width = bbox['width']\n    height = bbox['height']\n    \n    # Draw a circle centered on the detection\n    circ = plt.Circle((left+width//2, top+height//2), 5, \n                     fill=False, color='red', linewidth=3)\n    plt.gca().add_patch(circ)\n    class_id = int(bbox['label'])\n    class_name = labels[class_id]\n    plt.text(left, top-10, f'{class_name}: {bbox[\"value\"]:.2f}', \n              color='red', fontsize=12, backgroundcolor='white')\nplt.show()"
  },
  {
    "objectID": "raspi/object_detection/custom_object_detection.html#conclusion",
    "href": "raspi/object_detection/custom_object_detection.html#conclusion",
    "title": "Custom Object Detection Project",
    "section": "Conclusion",
    "text": "Conclusion\nThis chapter has explored the implementation of a custom object detector on edge devices, such as the Raspberry Pi, demonstrating the power and potential of running advanced computer vision tasks on resource-constrained hardware. We’ve covered several vital aspects:\n\nModel Comparison: We examined different object detection models, as SSD-MobileNet and FOMO, comparing their performance and trade-offs on edge devices.\nTraining and Deployment: Using a custom dataset of boxes and wheels (labeled on Roboflow), we walked through the process of training models with Edge Impulse Studio and Ultralytics and deploying them on a Raspberry Pi.\nOptimization Techniques: To improve inference speed on edge devices, we explored various optimization methods, such as model quantization (TFLite int8).\nPerformance Considerations: Throughout the lab, we discussed the balance between model accuracy and inference speed, a critical consideration for edge AI applications.\n\nAs discussed before, the ability to perform object detection on edge devices opens up numerous possibilities across various domains, including precision agriculture, industrial automation, quality control, smart home applications, and environmental monitoring. By processing data locally, these systems can offer reduced latency, improved privacy, and operation in environments with limited connectivity.\nLooking ahead, potential areas for further exploration include: - Implementing multi-model pipelines for more complex tasks - Exploring hardware acceleration options for Raspberry Pi - Integrating object detection with other sensors for more comprehensive edge AI systems - Developing edge-to-cloud solutions that leverage both local processing and cloud resources\nObject detection on edge devices can create intelligent, responsive systems that bring the power of AI directly into the physical world, opening up new frontiers in how we interact with and understand our environment."
  },
  {
    "objectID": "raspi/object_detection/custom_object_detection.html#resources",
    "href": "raspi/object_detection/custom_object_detection.html#resources",
    "title": "Custom Object Detection Project",
    "section": "Resources",
    "text": "Resources\n\nDataset (“Box versus Wheel”)\nFOMO - EI Linux Notebook on a Raspi\nEdge Impulse Project - SSD MobileNet and FOMO\nPython Scripts\nModels"
  },
  {
    "objectID": "raspi/object_detection/cv_yolo.html#exploring-a-yolo-models-using-ultralitics",
    "href": "raspi/object_detection/cv_yolo.html#exploring-a-yolo-models-using-ultralitics",
    "title": "Computer Vision Applications with YOLO",
    "section": "Exploring a YOLO Models using Ultralitics",
    "text": "Exploring a YOLO Models using Ultralitics\nIn this chapter, we will explore YOLOv8 and v11. Ultralytics YOLO (v8 and v11) are versions of the acclaimed real-time object detection and image segmentation model, YOLO. YOLOv8 and v11 are built on cutting-edge advancements in deep learning and computer vision, offering unparalleled performance in terms of speed and accuracy. Its streamlined design makes it suitable for various applications and easily adaptable to different hardware platforms, from edge devices to cloud APIs.\n\nTalking about the YOLO Model\nThe YOLO (You Only Look Once) model is a highly efficient and widely used object detection algorithm known for its real-time processing capabilities. Unlike traditional object detection systems that repurpose classifiers or localizers to perform detection, YOLO frames the detection problem as a single regression task. This innovative approach enables YOLO to simultaneously predict multiple bounding boxes and their class probabilities from full images in one evaluation, significantly boosting its speed.\n\nKey Features:\n\nSingle Network Architecture:\n\nYOLO employs a single neural network to process the entire image. This network divides the image into a grid and, for each grid cell, directly predicts bounding boxes and associated class probabilities. This end-to-end training improves speed and simplifies the model architecture.\n\nReal-Time Processing:\n\nOne of YOLO’s standout features is its ability to perform object detection in real-time. Depending on the version and hardware, YOLO can process images at high frames per second (FPS). This makes it ideal for applications requiring quick and accurate object detection, such as video surveillance, autonomous driving, and live sports analysis.\n\nEvolution of Versions:\n\nOver the years, YOLO has undergone significant improvements, from YOLOv1 to the latest YOLOv12. Each iteration has introduced enhancements in accuracy, speed, and efficiency. YOLOv8, for instance, incorporates advancements in network architecture, improved training methodologies, and better support for various hardware, ensuring a more robust performance.\nYOLOv11 offers substantial improvements in accuracy, speed, and parameter efficiency compared to prior versions such as YOLOv8 and YOLOv10, making it one of the most versatile and powerful real-time object detection models available as of 2025\n\n\nAccuracy and Efficiency:\n\nWhile early versions of YOLO traded off some accuracy for speed, recent versions have made substantial strides in balancing both. The newer models are faster and more accurate, detecting small objects (such as bees) and performing well on complex datasets.\n\nWide Range of Applications:\n\nYOLO’s versatility has led to its adoption in numerous fields. It is used in traffic monitoring systems to detect and count vehicles, security applications to identify potential threats and agricultural technology to monitor crops and livestock. Its application extends to any domain requiring efficient and accurate object detection.\n\nCommunity and Development:\n\nYOLO continues to evolve and is supported by a strong community of developers and researchers (being the YOLOv8 very strong). Open-source implementations and extensive documentation have made it accessible for customization and integration into various projects. Popular deep learning frameworks like Darknet, TensorFlow, and PyTorch support YOLO, further broadening its applicability.\n\nModel Capabilities\nYOLO models support multiple computer vision tasks:\n\nObject Detection: Identifying and localizing objects with bounding boxes\nInstance Segmentation: Pixel-level object segmentation\nPose Estimation: Human pose keypoint detection\nClassification: Image classification tasks\n\n\nUltralitics YOLO Detect, Segment, and Pose models pre-trained on the COCO dataset, and Classify on the ImageNet dataset.\n\nTrack mode is available for all Detect, Segment, and Pose models. The latest versions of YOLO can also perform OBB, which stands for Oriented Bounding Box, a rectangular box in computer vision that can rotate to match the orientation of an object within an image, providing a much tighter and more precise fit than traditional axis-aligned bounding boxes.\n\n\n\n\n\nAvailable Model Sizes\nYOLO offers several model variants optimized for different use cases, for example. The YOLOv8:\n\nYOLOv8n (Nano): Smallest model, fastest inference, lowest accuracy\nYOLOv8s (Small): Balanced performance for edge devices\nYOLOv8m (Medium): Higher accuracy, moderate computational requirements\nYOLOv8l (Large): High accuracy, requires more computational resources\nYOLOv8x (Extra Large): Highest accuracy, most computational intensive\n\n\nFor Raspberry Pi applications, YOLOv8n or YOLO11n are typically the best choice due to their optimized size and speed."
  },
  {
    "objectID": "raspi/object_detection/cv_yolo.html#installation",
    "href": "raspi/object_detection/cv_yolo.html#installation",
    "title": "Computer Vision Applications with YOLO",
    "section": "Installation",
    "text": "Installation\nOn our Raspi, let’s deactivate the current environment to create a new working area:\ndeactivate\ncd ~\ncd Documents/\nmkdir YOLO\ncd YOLO\nmkdir models\nmkdir images\nLet’s set up a Virtual Environment for working with the Ultralytics YOLO\npython3 -m venv ~/yolo --system-site-packages\nsource ~/yolo/bin/activate\nAnd install the Ultralytics packages for local inference on the Raspi\n\nUpdate the packages list, install pip, and upgrade to the latest:\n\nsudo apt update\nsudo apt install python3-pip -y\npip install -U pip\n\nInstall the ultralytics pip package with optional dependencies:\n\npip install ultralytics[export]\n\nReboot the device:\n\nsudo reboot\n\nTesting the YOLO\nAfter the Raspi booting, let’s activate the yolo env, go to the working directory,\nsource ~/yolo/bin/activate\ncd /Documents/YOLO\nAnd run inference on an image that will be downloaded from the Ultralytics website, using, for example, the YOLOV8n model (the smallest in the family) at the Terminal (CLI):\nyolo predict model='yolov8n' source='https://ultralytics.com/images/bus.jpg'\n\nNote that the first time we invoke a model, it will automatically be downloaded to the current directory.\n\nThe inference result will appear in the terminal. In the image (bus.jpg), 4 persons, 1 bus, and 1 stop signal were detected:\n\nAlso, we got a message that Results saved to runs/detect/predict. Inspecting that directory, we can see a new image saved (bus.jpg). Let’s download it from the Raspi to our desktop for inspection:\n\nSo, the Ultrayitics YOLO is correctly installed on our Raspberry Pi. Note that on the Raspberry Pi Zero, an issue is the high latency for this inference, which takes several seconds, even with the most compact model in the family (YOLOv8n).\n\nTesting with the YOLOv11\nThe procedure is the same as we did with version v8. As a comparison, we can see that the YOLOv11 is faster than the v8, but seems a little less precise, as it does not detect the “stop sign” as the v8."
  },
  {
    "objectID": "raspi/object_detection/cv_yolo.html#export-models-to-ncnn-format",
    "href": "raspi/object_detection/cv_yolo.html#export-models-to-ncnn-format",
    "title": "Computer Vision Applications with YOLO",
    "section": "Export Models to NCNN format",
    "text": "Export Models to NCNN format\nDeploying computer vision models on edge devices with limited computational power, such as the Raspberry Pi Zero, can cause latency issues. One alternative is to use a format optimized for optimal performance. This ensures that even devices with limited processing power can handle advanced computer vision tasks well.\nOf all the model export formats supported by Ultralytics, the NCNN is a high-performance neural network inference computing framework optimized for mobile platforms. From the beginning of the design, NCNN was deeply considerate of deployment and use on mobile phones, and it did not have third-party dependencies. It is cross-platform and runs faster than all known open-source frameworks (such as TFLite).\nNCNN delivers the best inference performance when working with Raspberry Pi devices. NCNN is highly optimized for mobile embedded platforms (such as ARM architecture).\nLet’s move the downloaded YOLO models to the ./models folder and thebus.jpg to ./images.\nAnd convert our models and rerun the inferences:\n\nExport the YOLO PyTorch models to NCNN format, creating: yolov8n_ncnn_model and yolo11n_ncnn_model\n\nyolo export model=./models/yolov8n.pt format=ncnn \nyolo export model=./models/yolo11n.pt format=ncnn\n\nRun inference with the exported models:\n\nyolo predict task=detect model='./models/yolov8n_ncnn_model' source='./images/bus.jpg'\nyolo predict task=detect model='./models/yolo11n_ncnn_model' source='./images/bus.jpg'\n\nThe first inference, when the model is loaded, typically has a high latency; however, from the second inference, it is possible to note that the inference time decreases.\n\nWe can now realize that neither model detects the “Stop Signal”, with YOLOv11 being the fastest. The optimized models are more rapid but also less accurate."
  },
  {
    "objectID": "raspi/object_detection/cv_yolo.html#exploring-yolo-with-python",
    "href": "raspi/object_detection/cv_yolo.html#exploring-yolo-with-python",
    "title": "Computer Vision Applications with YOLO",
    "section": "Exploring YOLO with Python",
    "text": "Exploring YOLO with Python\nTo start, let’s call the Python Interpreter so we can explore how the YOLO model works, line by line:\npython\nNow, we should call the YOLO library from Ultralitics and load the model:\nfrom ultralytics import YOLO\nmodel = YOLO('./models/yolov8n_ncnn_model')\nRun inference over an image (let’s use again bus.jpg):\nimg = './images/bus.jpg'\nresult = model.predict(img, save=True, imgsz=640, conf=0.5, iou=0.3)\n\nWe can verify that the result is almost identical to the one we get running the inference at the terminal level (CLI), except that the bus stop was not detected with the reduced NCNN model. Note that the latency was reduced.\nLet’s analyze the “result” content.\nFor example, we can see result[0].boxes.data, showing us the main inference result, which is a tensor with a shape of (4, 6). Each line is one of the objects detected, being the first four columns, the bounding boxes coordinates, the 5th, the confidence, and the 6th, the class (in this case, 0: person and 5: bus):\n\nWe can access several inference results separately, as the inference time, and have it printed in a better format:\ninference_time = int(result[0].speed['inference'])\nprint(f\"Inference Time: {inference_time} ms\")\nOr we can have the total number of objects detected:\nprint(f'Number of objects: {len (result[0].boxes.cls)}')\n\nWith Python, we can create a detailed output that meets our needs (See Model Prediction with Ultralytics YOLO for more details). Let’s run a Python script instead of manually entering it line by line in the interpreter, as shown below. Let’s use nano as our text editor. First, we should create an empty Python script named, for example, yolov8_tests.py:\nnano yolov8_tests.py\nEnter the code lines:\nfrom ultralytics import YOLO\n\n# Load the YOLOv8 model\nmodel = YOLO('./models/yolov8n_ncnn_model')\n\n# Run inference\nimg = './images/bus.jpg'\nresult = model.predict(img, save=False, imgsz=640, conf=0.5, iou=0.3)\n\n# print the results\ninference_time = int(result[0].speed['inference'])\nprint(f\"Inference Time: {inference_time} ms\")\nprint(f'Number of objects: {len (result[0].boxes.cls)}')\n\nAnd enter with the commands: [CTRL+O] + [ENTER] +[CTRL+X] to save the Python script.\nRun the script:\npython yolov8_tests.py\nThe result is the same as running the inference at the terminal level (CLI) and with the built-in Python interpreter.\n\nCalling the YOLO library and loading the model for inference for the first time takes a long time, but the inferences after that will be much faster. For example, the first single inference can take several seconds, but after that, the inference time should be reduced to less than 1 second.\n\n\nInference Arguments\nmodel.predict() accepts multiple arguments that can be passed at inference time to override defaults:\nInference arguments:\n\n\n\nArgument\nType\nDefault\nDescription\n\n\n\n\nsource\nstr\n'ultralytics/assets'\nSpecifies the data source for inference. Can be an image path, video file, directory, URL, or device ID for live feeds. Supports a wide range of formats and sources, enabling flexible application across different types of input.\n\n\nconf\nfloat\n0.25\nSets the minimum confidence threshold for detections. Objects detected with confidence below this threshold will be disregarded. Adjusting this value can help reduce false positives.\n\n\niou\nfloat\n0.7\nIntersection Over Union (IoU) threshold for Non-Maximum Suppression (NMS). Lower values result in fewer detections by eliminating overlapping boxes, useful for reducing duplicates.\n\n\nimgsz\nint or tuple\n640\nDefines the image size for inference. Can be a single integer 640 for square resizing or a (height, width) tuple. Proper sizing can improve detection accuracy and processing speed.\n\n\nrect\nbool\nTrue\nIf enabled, minimally pads the shorter side of the image until it’s divisible by stride to improve inference speed. If disabled, pads the image to a square during inference.\n\n\nhalf\nbool\nFalse\nEnables half-precision (FP16) inference, which can speed up model inference on supported GPUs with minimal impact on accuracy.\n\n\ndevice\nstr\nNone\nSpecifies the device for inference (e.g., cpu, cuda:0 or 0). Allows users to select between CPU, a specific GPU, or other compute devices for model execution.\n\n\nbatch\nint\n1\nSpecifies the batch size for inference (only works when the source is a directory, video file or .txt file). A larger batch size can provide higher throughput, shortening the total amount of time required for inference.\n\n\nmax_det\nint\n300\nMaximum number of detections allowed per image. Limits the total number of objects the model can detect in a single inference, preventing excessive outputs in dense scenes.\n\n\nvid_stride\nint\n1\nFrame stride for video inputs. Allows skipping frames in videos to speed up processing at the cost of temporal resolution. A value of 1 processes every frame, higher values skip frames.\n\n\nstream_buffer\nbool\nFalse\nDetermines whether to queue incoming frames for video streams. If False, old frames get dropped to accommodate new frames (optimized for real-time applications). If True, queues new frames in a buffer, ensuring no frames get skipped, but will cause latency if inference FPS is lower than stream FPS.\n\n\nvisualize\nbool\nFalse\nActivates visualization of model features during inference, providing insights into what the model is “seeing”. Useful for debugging and model interpretation.\n\n\naugment\nbool\nFalse\nEnables test-time augmentation (TTA) for predictions, potentially improving detection robustness at the cost of inference speed.\n\n\nagnostic_nms\nbool\nFalse\nEnables class-agnostic Non-Maximum Suppression (NMS), which merges overlapping boxes of different classes. Useful in multi-class detection scenarios where class overlap is common.\n\n\nclasses\nlist[int]\nNone\nFilters predictions to a set of class IDs. Only detections belonging to the specified classes will be returned. Useful for focusing on relevant objects in multi-class detection tasks.\n\n\nretina_masks\nbool\nFalse\nReturns high-resolution segmentation masks. The returned masks (masks.data) will match the original image size if enabled. If disabled, they have the image size used during inference.\n\n\nembed\nlist[int]\nNone\nSpecifies the layers from which to extract feature vectors or embeddings. Useful for downstream tasks like clustering or similarity search.\n\n\nproject\nstr\nNone\nName of the project directory where prediction outputs are saved if save is enabled.\n\n\nname\nstr\nNone\nName of the prediction run. Used for creating a subdirectory within the project folder, where prediction outputs are stored if save is enabled.\n\n\nstream\nbool\nFalse\nEnables memory-efficient processing for long videos or numerous images by returning a generator of Results objects instead of loading all frames into memory at once.\n\n\nverbose\nbool\nTrue\nControls whether to display detailed inference logs in the terminal, providing real-time feedback on the prediction process.\n\n\n\nVisualization arguments:\n\n\n\n\n\n\n\n\n\nArgument\nType\nDefault\nDescription\n\n\n\n\nshow\nbool\nFalse\nIf True, displays the annotated images or videos in a window. Useful for immediate visual feedback during development or testing.\n\n\nsave\nbool\nFalse or True\nEnables saving of the annotated images or videos to file. Useful for documentation, further analysis, or sharing results. Defaults to True when using CLI & False when used in Python.\n\n\nsave_frames\nbool\nFalse\nWhen processing videos, saves individual frames as images. Useful for extracting specific frames or for detailed frame-by-frame analysis.\n\n\nsave_txt\nbool\nFalse\nSaves detection results in a text file, following the format [class] [x_center] [y_center] [width] [height] [confidence]. Useful for integration with other analysis tools.\n\n\nsave_conf\nbool\nFalse\nIncludes confidence scores in the saved text files. Enhances the detail available for post-processing and analysis.\n\n\nsave_crop\nbool\nFalse\nSaves cropped images of detections. Useful for dataset augmentation, analysis, or creating focused datasets for specific objects.\n\n\nshow_labels\nbool\nTrue\nDisplays labels for each detection in the visual output. Provides immediate understanding of detected objects.\n\n\nshow_conf\nbool\nTrue\nDisplays the confidence score for each detection alongside the label. Gives insight into the model’s certainty for each detection.\n\n\nshow_boxes\nbool\nTrue\nDraws bounding boxes around detected objects. Essential for visual identification and location of objects in images or video frames.\n\n\nline_width\nNone or int\nNone\nSpecifies the line width of bounding boxes. If None, the line width is automatically adjusted based on the image size. Provides visual customization for clarity."
  },
  {
    "objectID": "raspi/object_detection/cv_yolo.html#exploring-other-computer-vision-applications",
    "href": "raspi/object_detection/cv_yolo.html#exploring-other-computer-vision-applications",
    "title": "Computer Vision Applications with YOLO",
    "section": "Exploring other Computer Vision Applications",
    "text": "Exploring other Computer Vision Applications\nLet’s set up Jupyter Notebook optimized for headless Raspberry Pi camera work and development:\npip install jupyter jupyterlab notebook\njupyter notebook --generate-config\nTo run Jupyter Notebook, run the command (change the IP address for yours):\njupyter notebook --ip=192.168.4.210 --no-browser\nOn the terminal, you can see the local URL address and its Token to open the notebook. Copy and paste it into the Browser.\n\nEnvironment Setup and Dependencies\nimport time\nimport numpy as np\nfrom PIL import Image\nfrom ultralytics import YOLO\nimport matplotlib.pyplot as plt\nHere we have all the necessary libraries, which we installed automatically when we installed Ultralytics.\n\nTime: Performance measurement and benchmarking\nNumPy: Numerical computations and array operations\nPIL (Python Imaging Library): Image loading and manipulation\nUltralytics YOLO: Core YOLO functionality\nMatplotlib: Visualization and plotting results\n\n\n\nModel Configuration and Loading\nmodel_path= \"./models/yolo11n.pt\"\ntask = \"detect\" \nverbose = False\n\nmodel = YOLO(model_path, task, verbose)\n\nModel Selection: YOLOv11n (nano) is chosen for its balance of speed and accuracy\nTask Specification: We will select detect, which in fact is the default for the model. But remember that YOLO supports multiple computer vision tasks, which will be explored later.\nVerbose Control: output model information during model initialization\n\n\n\nPerformance Characteristics\nLet’s open the previous bus image using PIL\nsource = Image.open(\"./images/bus.jpg\")\nAnd run an inference in the source:\nresults = model.predict(source, save=False, imgsz=640, conf=0.5, iou=0.3)\nFrom the inference results info, we can see that the first time an inference is run, the latency is greater.\n# First inference\n0: 640x480 4 persons, 1 bus, 7528.3ms\n\n# Second inference  \n0: 640x480 4 persons, 1 bus, 2822.1ms\nThe dramatic difference between the first inference (7.5s) and subsequent inferences (2.8s) illustrates:\n\nModel Loading Overhead: Initial inference includes model loading time\nOptimization Effects: Subsequent inferences benefit from cached optimizations\n\n\n\nResults Object Structure\nLet’s explore the YOLO’s output structure:\nresult = results[0]\n# - boxes, keypoints, masks, names\n# - orig_img, orig_shape, path\n# - speed metrics\n\n\nBounding Box Analysis\nresult.boxes.cls  # Class IDs: tensor([5., 0., 0., 0., 0.])\nresult.boxes.conf # Confidence scores\nresult.boxes.xyxy # Bounding box coordinates\n\nCoordinate Systems: On Result.boxes, we can get different bounding box formats (xyxy, xywh, normalized):\n\nxywh: Tensor with bounding box coordinates in center_x, center_y, width, height format, in pixels.\nxywhn: Normalized center_x, center_y, width, height, scaled to the image dimensions, values in .\nxyxy: Tensor of boxes as x1, y1, x2, y2 in pixels, representing the top-left and bottom-right corners.\nxyxyn: Normalized x1, y1, x2, y2, scaled by image width and height, values in .\n\n\n\n\n8. Visualization and Customization\nThe Ultralytics plot() can be customized to show as the detection result, for example, only the bounding boxes:\nim_bgr = result.plot(boxes=True, labels=False, conf=False)\nimg = Image.fromarray(im_bgr[..., ::-1])\n\nplt.figure(figsize=(6, 6))\nplt.imshow(img)\n#plt.axis('off')  # This turns off the axis numbers\nplt.title(\"YOLO Result\")\nplt.show()\n\nCustomization Options:\nThe plot() method in Ultralytics YOLO Results object accepts several arguments to control what is visualized on the image, including boxes, masks, keypoints, confidences, labels, and more. Common Arguments for plot()\n\nboxes (bool): Show/hide bounding boxes. Default is True.\nconf (bool): Show/hide confidence scores. Default is True.\nlabels (bool): Show/hide class labels. Default is True.\nmasks (bool): Show/hide segmentation masks (when available, e.g. in segment tasks).\nkpt_line (bool): Draw lines connecting pose keypoints (skeleton diagram). Default is True in pose tasks.\nline_width (int): Set annotation line thickness.\nfont_size (int): Set font size for text annotations.\nshow (bool): If True, immediately display the image (interactive environments)."
  },
  {
    "objectID": "raspi/object_detection/cv_yolo.html#exploring-other-computer-vision-tasks",
    "href": "raspi/object_detection/cv_yolo.html#exploring-other-computer-vision-tasks",
    "title": "Computer Vision Applications with YOLO",
    "section": "Exploring Other Computer Vision Tasks",
    "text": "Exploring Other Computer Vision Tasks\n\nInstance Segmentation\nmodel_path= \"./models/yolo11n-seg.pt\"\ntask = \"segment\"\n\nmodel = YOLO(model_path, task, verbose)\nNote that a specific variation of the model, for instance segmentation, will be downloaded. Now, lt’s use another image for testing:\nsource = Image.open(\"./images/beatles.jpg\")\nDisplay the image\nplt.figure(figsize=(6, 6))\nplt.imshow(source)\n#plt.axis('off')  # This turns off the axis numbers\nplt.title(\"Original Image\")\nplt.show()\n\nAnd run the inference:\nresults = model.predict(source, save=False)\nresult = results[0]\nDisplay the result:\nim_bgr = result.plot(boxes=False, conf=False, masks=True)\nimg = Image.fromarray(im_bgr[..., ::-1])\nplt.figure(figsize=(6, 6))\nplt.imshow(img)\nplt.axis('off')  # This turns off the axis numbers\nplt.title(\"YOLO Segmentation Result\")\nplt.show()\n\n\nPose Estimation\nDownload the model:\nmodel_path= \"./models/yolo11n-pose.pt\"\ntask = \"pose\"\n\nmodel = YOLO(model_path, task, verbose)\nRunning the Inference on the beatles image:\nsource = Image.open(\"./images/beatles.jpg\")\nresults = model.predict(source, save=False)\nresult = results[0]\nShowing the human pose keypoint detection and skeleton visualization.\nim_bgr = result.plot(boxes=False, conf=False, kpt_line=True)\nimg = Image.fromarray(im_bgr[..., ::-1])\nplt.figure(figsize=(6, 6))\nplt.imshow(img)\nplt.axis('off')  # This turns off the axis numbers\nplt.title(\"YOLO Pose Estimation Result\")\nplt.show()"
  },
  {
    "objectID": "raspi/object_detection/cv_yolo.html#training-yolo-on-a-customized-dataset",
    "href": "raspi/object_detection/cv_yolo.html#training-yolo-on-a-customized-dataset",
    "title": "Computer Vision Applications with YOLO",
    "section": "Training YOLO on a Customized Dataset",
    "text": "Training YOLO on a Customized Dataset\n\nObject Detection Project\nWe will now develop a customized object detection project from the data collected and labelled with Roboflow. The training and deployment will be done in Python using a CoLab and Ultralytics functions.\n\n\nWe will use with YOLO, the same dataset previously used to train the SSD-MobileNet V2 and FOMO models.\n\nAs a reminder, we are assuming we are in an industrial facility that must sort and count wheels and special boxes.\n\nEach image can have three classes:\n\nBackground (no objects)\nBox\nWheel\n\n\n\nThe Dataset\nReturn to our “Boxe versus Wheel” dataset, labeled on Roboflow. On the Download Dataset, instead of Download a zip to computer option done for training on Edge Impulse Studio, we will opt for Show download code. This option will open a pop-up window with a code snippet that should be pasted into our training notebook.\n\nFor training, let’s choose one model (let’s say YOLOv8) and adapt one of the publicly available examples from Ultralytics, then run it on Google Colab. Below, you can find my adaptation:\n\nYOLOv8 Box versus Wheel Dataset Training [Open In Colab]\n\n\nCritical points on the Notebook:\n\nRun it with GPU (the NVidia T4 is free)\nInstall Ultralytics using PIP.\n\nNow, you can import the YOLO and upload your dataset to the CoLab, pasting the Download code that we get from Roboflow. Note that our dataset will be mounted under /content/datasets/:\n\n\n\nIt is essential to verify and change the file data.yaml with the correct path for the images (copy the path on each images folder).\n\nnames:\n- box\n- wheel\nnc: 2\nroboflow:\n  license: CC BY 4.0\n  project: box-versus-wheel-auto-dataset\n  url: https://universe.roboflow.com/marcelo-rovai-riila/box-versus-wheel-auto-dataset/dataset/5\n  version: 5\n  workspace: marcelo-rovai-riila\ntest: /content/datasets/Box-versus-Wheel-auto-dataset-5/test/images\ntrain: /content/datasets/Box-versus-Wheel-auto-dataset-5/train/images\nval: /content/datasets/Box-versus-Wheel-auto-dataset-5/valid/images\n\nDefine the main hyperparameters that you want to change from default, for example:\nMODEL = 'yolov8n.pt'\nIMG_SIZE = 640\nEPOCHS = 25 # For a final project, you should consider at least 100 epochs \nRun the training (using CLI):\n!yolo task=detect mode=train model={MODEL} data={dataset.location}/data.yaml epochs={EPOCHS} imgsz={IMG_SIZE} plots=True \n\n\n​ The model took a few minutes to be trained and has an excellent result (mAP50 of 0.995). At the end of the training, all results are saved in the folder listed, for example: /runs/detect/train/. There, you can find, for example, the confusion matrix.\n\n\nNote that the trained model (best.pt) is saved in the folder /runs/detect/train/weights/. Now, you should validate the trained model with the valid/images.\n\n!yolo task=detect mode=val model={HOME}/runs/detect/train/weights/best.pt data={dataset.location}/data.yaml\n​ The results were similar to training.\n\nNow, we should perform inference on the images left aside for testing\n\n!yolo task=detect mode=predict model={HOME}/runs/detect/train/weights/best.pt conf=0.25 source={dataset.location}/test/images save=True\nThe inference results are saved in the folder runs/detect/predict. Let’s see some of them:\n\n\nIt is advised to export the train, validation, and test results for a Drive at Google. To do so, we should mount the drive.\nfrom google.colab import drive\ndrive.mount('/content/gdrive')\nand copy the content of /runs folder to a folder that you should create in your Drive, for example:\n!scp -r /content/runs '/content/gdrive/MyDrive/10_UNIFEI/Box_vs_Wheel_Project'\n\n\n\n\nInference with the trained model, using the Raspi\nDownload the trained model /runs/detect/train/weights/best.pt to your computer. Using the FileZilla FTP, let’s transfer the best.pt to the Raspi models folder (before the transfer, you may change the model name, for example, box_wheel_320_yolo.pt).\nUsing the FileZilla FTP, let’s transfer a few images from the test dataset to .\\YOLO\\images:\nLet’s return to the YOLO folder and use the Python Interpreter:\ncd ..\npython\nAs before, we will import the YOLO library and define our converted model to detect bees:\nfrom ultralytics import YOLO\nmodel = YOLO('./models/box_wheel_320_yolo.pt')\nNow, let’s define an image and call the inference (we will save the image result this time to external verification):\nimg = './images/1_box_1_wheel.jpg'\nresult = model.predict(img, save=True, imgsz=320, conf=0.5, iou=0.3)\nLet’s repeat for several images. The inference result is saved on the variable result, and the processed image on runs/detect/predict8\n\nUsing FileZilla FTP, we can send the inference result to our Desktop for verification:\n\nWe can see that the inference result is excellent! The model was trained based on the smaller base model of the YOLOv8 family (YOLOv8n). The issue is the latency, around 1 second (or 1 FPS on the Raspi-Zero). Of course, we can reduce this latency and convert the model to TFLite or NCNN.\n\nThe model trained with YOLO11, has similar result as the v8,"
  },
  {
    "objectID": "raspi/object_detection/cv_yolo.html#conclusion",
    "href": "raspi/object_detection/cv_yolo.html#conclusion",
    "title": "Computer Vision Applications with YOLO",
    "section": "Conclusion",
    "text": "Conclusion\nThis chapter has explored the YOLO model and the implementation of a custom object detector on a Raspberry Pi, demonstrating the power and potential of running advanced computer vision tasks on resource-constrained hardware. We’ve covered several vital aspects:\n\nModel Comparison: We examined different object detection models, including SSD-MobileNet, FOMO, and YOLO, comparing their performance and trade-offs on edge devices.\nTraining and Deployment: Using a custom dataset of boxes and wheels (labeled on Roboflow), we walked through the process of training models with Ultralytics and deploying them on a Raspberry Pi.\nOptimization Techniques: To improve inference speed on edge devices, we explored various optimization methods, such as format conversion (e.g., to NCNN).\nPerformance Considerations: Throughout the lab, we discussed the balance between model accuracy and inference speed, a critical consideration for edge AI applications.\n\nAS discussed before, the ability to perform object detection on edge devices opens up numerous possibilities across various domains, including precision agriculture, industrial automation, quality control, smart home applications, and environmental monitoring. By processing data locally, these systems can offer reduced latency, improved privacy, and operation in environments with limited connectivity.\nLooking ahead, potential areas for further exploration include: - Implementing multi-model pipelines for more complex tasks - Exploring hardware acceleration options for Raspberry Pi - Integrating object detection with other sensors for more comprehensive edge AI systems - Developing edge-to-cloud solutions that leverage both local processing and cloud resources\nObject detection on edge devices can create intelligent, responsive systems that bring the power of AI directly into the physical world, opening up new frontiers in how we interact with and understand our environment."
  },
  {
    "objectID": "raspi/object_detection/cv_yolo.html#resources",
    "href": "raspi/object_detection/cv_yolo.html#resources",
    "title": "Computer Vision Applications with YOLO",
    "section": "Resources",
    "text": "Resources\n\nDataset (“Box versus Wheel”)\nYOLOv8 Box versus Wheel Dataset Training on CoLab\nModel Predictions with Ultralytics YOLO\nPython Scripts\nModels"
  },
  {
    "objectID": "raspi/counting_objects_yolo/counting_objects_yolo.html#introduction",
    "href": "raspi/counting_objects_yolo/counting_objects_yolo.html#introduction",
    "title": "Counting objects with YOLO",
    "section": "Introduction",
    "text": "Introduction\nAt the Federal University of Itajuba in Brazil, with the master’s student José Anderson Reis and Professor José Alberto Ferreira Filho, we are exploring a project that delves into the intersection of technology and nature. This tutorial will review our first steps and share our observations on deploying YOLOv8, a cutting-edge machine learning model, on the compact and efficient Raspberry Pi Zero 2W (Raspi-Zero). We aim to estimate the number of bees entering and exiting their hive—a task crucial for beekeeping and ecological studies.\nWhy is this important? Bee populations are vital indicators of environmental health, and their monitoring can provide essential data for ecological research and conservation efforts. However, manual counting is labor-intensive and prone to errors. By leveraging the power of embedded machine learning, or tinyML, we automate this process, enhancing accuracy and efficiency.\n\n\n\nimg\n\n\nThis tutorial will cover setting up the Raspberry Pi, integrating a camera module, optimizing and deploying YOLOv8 for real-time image processing, and analyzing the data gathered."
  },
  {
    "objectID": "raspi/counting_objects_yolo/counting_objects_yolo.html#estimating-the-number-of-bees",
    "href": "raspi/counting_objects_yolo/counting_objects_yolo.html#estimating-the-number-of-bees",
    "title": "Counting objects with YOLO",
    "section": "Estimating the number of Bees",
    "text": "Estimating the number of Bees\nFor our project at the university, we are preparing to collect a dataset of bees at the entrance of a beehive using the same camera connected to the Raspberry Pi. The images should be collected every 10 seconds. With the Arducam OV5647, the horizontal Field of View (FoV) is 53.5o, which means that a camera positioned at the top of a standard Hive (46 cm) will capture all of its entrance (about 47 cm).\n\n\nDataset\nThe dataset collection is the most critical phase of the project and should take several weeks or months. For this tutorial, we will use a public dataset: “Sledevic, Tomyslav (2023), “[Labeled dataset for bee detection and direction estimation on beehive landing boards,” Mendeley Data, V5, doi: 10.17632/8gb9r2yhfc.5”\nThe original dataset has 6,762 images (1920 x 1080), and around 8% of them (518) have no bees (only background). This is very important with Object Detection, where we should keep around 10% of the dataset with only background (without any objects to be detected).\nThe images contain from zero to up to 61 bees:\n\nWe downloaded the dataset (images and annotations) and uploaded it to Roboflow. There, you should create a free account and start a new project, for example, (“Bees_on_Hive_landing_boards”):\n\n\nWe will not enter details about the Roboflow process once many tutorials are available.\n\nOnce the project is created and the dataset is uploaded, you should review the annotations using the “Auto-Label” Tool. Note that all images with only a background should be saved w/o any annotations. At this step, you can also add additional images.\n\nOnce all images are annotated, you should split them into training, validation, and testing.\n\n\n\nPre-Processing\nThe last step with the dataset is preprocessing to generate a final version for training. The Yolov8 model can be trained with 640 x 640 pixels (RGB) images. Let’s resize all images and generate augmented versions of each image (augmentation) to create new training examples from which our model can learn.\nFor augmentation, we will rotate the images (+/-15o) and vary the brightness and exposure.\n\nThis will create a final dataset of 16,228 images.\n\nNow, you should export the annotateddataset in a YOLOv8 format. You can download a zipped version of the dataset to your desktop or get a downloaded code to be used with a Jupyter Notebook:\n\nAnd that is it! We are prepared to start our training using Google Colab.\n\nThe pre-processed dataset can be found at the Roboflow site.\n\n\n\nTraining YOLOv8 on a Customized Dataset\nFor training, let’s adapt one of the public examples available from Ultralitytics and run it on Google Colab:\n\nyolov8_bees_on_hive_landing_board.ipynb [Open In Colab]\n\n\nCritical points on the Notebook:\n\nRun it with GPU (the NVidia T4 is free)\nInstall Ultralytics using PIP.\n\nNow, you can import the YOLO and upload your dataset to the CoLab, pasting the Download code that you get from Roboflow. Note that your dataset will be mounted under /content/datasets/:\n\n\n\nIt is important to verify and change, if needed, the file data.yaml with the correct path for the images:\n\nnames:\n- bee\nnc: 1\nroboflow:\n  license: CC BY 4.0\n  project: bees_on_hive_landing_boards\n  url: https://universe.roboflow.com/marcelo-rovai-riila/bees_on_hive_landing_boards/dataset/1\n  version: 1\n  workspace: marcelo-rovai-riila\ntest: /content/datasets/Bees_on_Hive_landing_boards-1test/images\ntrain: /content/datasets/Bees_on_Hive_landing_boards-1/train/images\nval: /content/datasets/Bees_on_Hive_landing_boards-1/valid/images\n\nDefine the main hyperparameters that you want to change from default, for example:\nMODEL = 'yolov8n.pt'\nIMG_SIZE = 640\nEPOCHS = 25 # For a final project, you should consider at least 100 epochs \nRun the training (using CLI):\n!yolo task=detect mode=train model={MODEL} data={dataset.location}/data.yaml epochs={EPOCHS} imgsz={IMG_SIZE} plots=True \n\n\n​ The model took 2.7 hours to train and has an excellent result (mAP50 of 0.984). At the end of the training, all results are saved in the folder listed, for example: /runs/detect/train3/. There, you can find, for example, the confusion matrix and the metrics curves per epoch.\n\n\nNote that the trained model (best.pt) is saved in the folder /runs/detect/train3/weights/. Now, you should validade the trained model with the valid/images.\n\n!yolo task=detect mode=val model={HOME}/runs/detect/train3/weights/best.pt data={dataset.location}/data.yaml\n​ The results were similar to training.\n\nNow, we should perform inference on the images left aside for testing\n\n!yolo task=detect mode=predict model={HOME}/runs/detect/train3/weights/best.pt conf=0.25 source={dataset.location}/test/images save=True\nThe inference results are saved in the folder runs/detect/predict. Let’s see some of them:\n\nWe can also perform inference with a completely new and complex image from another beehive with a different background (the beehive of Professor Maurilio of our University). The results were great (but not perfect and with a lower confidence score). The model found 41 bees.\n\n\nThe last thing to do is export the train, validation, and test results for your Drive at Google. To do so, you should mount your drive.\nfrom google.colab import drive\ndrive.mount('/content/gdrive')\nand copy the content of /runs folder to a folder that you should create in your Drive, for example:\n!scp -r /content/runs '/content/gdrive/MyDrive/10_UNIFEI/Bee_Project/YOLO/bees_on_hive_landing'"
  },
  {
    "objectID": "raspi/counting_objects_yolo/counting_objects_yolo.html#inference-with-the-trained-model-using-the-rasp-zero",
    "href": "raspi/counting_objects_yolo/counting_objects_yolo.html#inference-with-the-trained-model-using-the-rasp-zero",
    "title": "Counting objects with YOLO",
    "section": "Inference with the trained model, using the Rasp-Zero",
    "text": "Inference with the trained model, using the Rasp-Zero\nUsing the FileZilla FTP, let’s transfer the best.pt to our Rasp-Zero (before the transfer, you may change the model name, for example, bee_landing_640_best.pt).\nThe first thing to do is convert the model to an NCNN format:\nyolo export model=bee_landing_640_best.pt format=ncnn \nAs a result, a new converted model, bee_landing_640_best_ncnn_model is created in the same directory.\nLet’s create a folder to receive some test images (under Documents/YOLO/:\nmkdir test_images\nUsing the FileZilla FTP, let’s transfer a few images from the test dataset to our Rasp-Zero:\n\nLet’s use the Python Interpreter:\npython\nAs before, we will import the YOLO library and define our converted model to detect bees:\nfrom ultralytics import YOLO\nmodel = YOLO('bee_landing_640_best_ncnn_model')\nNow, let’s define an image and call the inference (we will save the image result this time to external verification):\nimg = 'test_images/15_bees.jpg'\nresult = model.predict(img, save=True, imgsz=640, conf=0.2, iou=0.3)\nThe inference result is saved on the variable result, and the processed image on runs/detect/predict9\n\nUsing FileZilla FTP, we can send the inference result to our Desktop for verification:\n\nlet’s go over the other images, analyzing the number of objects (bees) found:\n\nDepending on the confidence, we can have some false positives or negatives. But in general, with a model trained based on the smaller base model of the YOLOv8 family (YOLOv8n) and also converted to NCNN, the result is pretty good, running on an Edge device such as the Rasp-Zero. Also, note that the inference latency is around 730ms.\nFor example, by running the inference on Maurilio-bee.jpeg, we can find 40 bees. During the test phase on Colab, 41 bees were found (we only missed one here.)"
  },
  {
    "objectID": "raspi/counting_objects_yolo/counting_objects_yolo.html#considerations-about-the-post-processing",
    "href": "raspi/counting_objects_yolo/counting_objects_yolo.html#considerations-about-the-post-processing",
    "title": "Counting objects with YOLO",
    "section": "Considerations about the Post-Processing",
    "text": "Considerations about the Post-Processing\nOur final project should be very simple in terms of code. We will use the camera to capture an image every 10 seconds. As we did in the previous section, the captured image should be the input for the trained and converted model. We should get the number of bees for each image and save it in a database (for example, timestamp: number of bees).\nWe can do it with a single Python script or use a Linux system timer, like cron, to periodically capture images every 10 seconds and have a separate Python script to process these images as they are saved. This method can be particularly efficient in managing system resources and can be more robust against potential delays in image processing.\n\nSetting Up the Image Capture with cron\nFirst, we should set up a cron job to use the rpicam-jpeg command to capture an image every 10 seconds.\n\nEdit the crontab:\n\nOpen the terminal and type crontab -e to edit the cron jobs.\ncron normally doesn’t support sub-minute intervals directly, so we should use a workaround like a loop or watch for file changes.\n\nCreate a Bash Script (capture.sh):\n\nImage Capture: This bash script captures images every 10 seconds using rpicam-jpeg, a command that is part of the raspijpeg tool. This command lets us control the camera and capture JPEG images directly from the command line. This is especially useful because we are looking for a lightweight and straightforward method to capture images without the need for additional libraries like Picamera or external software. The script also saves the captured image with a timestamp.\n\n#!/bin/bash\n# Script to capture an image every 10 seconds\n\nwhile true\ndo\n  DATE=$(date +\"%Y-%m-%d_%H%M%S\")\n  rpicam-jpeg --output test_images/$DATE.jpg --width 640 --height 640\n  sleep 10\ndone\n\nWe should make the script executable with chmod +x capture.sh.\nThe script must start at boot or use a @reboot entry in cron to start it automatically.\n\n\n\n\nSetting Up the Python Script for Inference\nImage Processing: The Python script continuously monitors the designated directory for new images, processes each new image using the YOLOv8 model, updates the database with the count of detected bees, and optionally deletes the image to conserve disk space.\nDatabase Updates: The results, along with the timestamps, are saved in an SQLite database. For that, a simple option is to use sqlite3.\nIn short, we need to write a script that continuously monitors the directory for new images, processes them using a YOLO model, and then saves the results to a SQLite database. Here’s how we can create and make the script executable:\n#!/usr/bin/env python3\nimport os\nimport time\nimport sqlite3\nfrom datetime import datetime\nfrom ultralytics import YOLO\n\n# Constants and paths\nIMAGES_DIR = 'test_images/'\nMODEL_PATH = 'bee_landing_640_best_ncnn_model'\nDB_PATH = 'bee_count.db'\n\ndef setup_database():\n    \"\"\" \n      Establishes a database connection and creates the table \n      if it doesn't exist. \n    \"\"\"\n    conn = sqlite3.connect(DB_PATH)\n    cursor = conn.cursor()\n    cursor.execute('''\n        CREATE TABLE IF NOT EXISTS bee_counts\n        (timestamp TEXT, count INTEGER)\n    ''')\n    conn.commit()\n    return conn\n\ndef process_image(image_path, model, conn):\n    \"\"\" \n      Processes an image to detect objects and logs \n      the count to the database. \n    \"\"\"\n    result = model.predict(image_path, save=False, imgsz=640, conf=0.2, iou=0.3, verbose=False)\n    num_bees = len(result[0].boxes.cls) \n    timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n    cursor = conn.cursor()\n    cursor.execute(\"INSERT INTO bee_counts (timestamp, count) VALUES (?, ?)\", \n                   (timestamp, num_bees)\n                  )\n    conn.commit()\n    print(f'Processed {image_path}: Number of bees detected = {num_bees}')\n\ndef monitor_directory(model, conn):\n    \"\"\" \n      Monitors the directory for new images and processes \n      them as they appear. \n    \"\"\"\n    processed_files = set()\n    while True:\n        try:\n            files = set(os.listdir(IMAGES_DIR))\n            new_files = files - processed_files\n            for file in new_files:\n                if file.endswith('.jpg'):\n                    full_path = os.path.join(IMAGES_DIR, file)\n                    process_image(full_path, model, conn)\n                    processed_files.add(file)\n            time.sleep(1)  # Check every second\n        except KeyboardInterrupt:\n            print(\"Stopping...\")\n            break\n\ndef main():\n    conn = setup_database()\n    model = YOLO(MODEL_PATH)\n    monitor_directory(model, conn)\n    conn.close()\n\nif __name__ == \"__main__\":\n    main()\nThe python script must be executable, for that:\n\nSave the script: For example, as process_images.py.\nChange file permissions to make it executable:\nchmod +x process_images.py\nRun the script directly from the command line:\n./process_images.py\n\nWe should consider keeping the script running even after closing the terminal; for that, we can use nohup or screen:\nnohup ./process_images.py &\nor\nscreen -S bee_monitor\n./process_images.py\nNote that we are capturing images with their own timestamp and then log a separate timestamp for when the inference results are saved to the database. This approach can be beneficial for the following reasons:\n\nAccuracy in Data Logging:\n\nCapture Timestamp: The timestamp associated with each image capture represents the exact moment the image was taken. This is crucial for applications where precise timing of events (like bee activity) is important for analysis.\nInference Timestamp: This timestamp indicates when the image was processed and the results were recorded in the database. This can differ from the capture time due to processing delays or if the image processing is batched or queued.\n\nPerformance Monitoring:\n\nHaving separate timestamps allows us to monitor the performance and efficiency of your image processing pipeline. We can measure the delay between image capture and result logging, which helps optimize the system for real-time processing needs.\n\nTroubleshooting and Audit:\n\nSeparate timestamps provide a better audit trail and troubleshooting data. If there are issues with the image processing or data recording, having distinct timestamps can help isolate whether delays or problems occurred during capture, processing, or logging.\n\n\n\n\nScript For Reading the SQLite Database\nHere is an example of a code to retrieve the data from the database:\n#!/usr/bin/env python3\nimport sqlite3\n\ndef main():\n    db_path = 'bee_count.db'\n    conn = sqlite3.connect(db_path)\n    cursor = conn.cursor()\n    query = \"SELECT * FROM bee_counts\"\n    cursor.execute(query)\n    data = cursor.fetchall()\n    for row in data:\n        print(f\"Timestamp: {row[0]}, Number of bees: {row[1]}\")\n    conn.close()\n\nif __name__ == \"__main__\":\n    main()\n\n\nAdding Environment data\nBesides bee counting, environmental data, such as temperature and humidity, are essential for monitoring the bee-have health. Using a Rasp-Zero, it is straightforward to add a digital sensor such as the DHT-22 to get this data.\n\nEnvironmental data will be part of our final project. If you want to know more about connecting sensors to a Raspberry Pi and, even more, how to save the data to a local database and send it to the web, follow this tutorial: From Data to Graph: A Web Journey With Flask and SQLite."
  },
  {
    "objectID": "raspi/counting_objects_yolo/counting_objects_yolo.html#conclusion",
    "href": "raspi/counting_objects_yolo/counting_objects_yolo.html#conclusion",
    "title": "Counting objects with YOLO",
    "section": "Conclusion",
    "text": "Conclusion\nIn this tutorial, we have thoroughly explored integrating the YOLOv8 model with a Raspberry Pi Zero 2W to address the practical and pressing task of counting (or better, “estimating”) bees at a beehive entrance. Our project underscores the robust capability of embedding advanced machine learning technologies within compact edge computing devices, highlighting their potential impact on environmental monitoring and ecological studies.\nThis tutorial provides a step-by-step guide to the practical deployment of the YOLOv8 model. We demonstrate a tangible example of a real-world application by optimizing it for edge computing in terms of efficiency and processing speed (using NCNN format). This not only serves as a functional solution but also as an instructional tool for similar projects.\nThe technical insights and methodologies shared in this tutorial are the basis for the complete work to be developed at our university in the future. We envision further development, such as integrating additional environmental sensing capabilities and refining the model’s accuracy and processing efficiency. Implementing alternative energy solutions like the proposed solar power setup will expand the project’s sustainability and applicability in remote or underserved locations."
  },
  {
    "objectID": "raspi/counting_objects_yolo/counting_objects_yolo.html#resources",
    "href": "raspi/counting_objects_yolo/counting_objects_yolo.html#resources",
    "title": "Counting objects with YOLO",
    "section": "Resources",
    "text": "Resources\nThe Dataset paper, Notebooks, and PDF version are in the Project repository."
  },
  {
    "objectID": "raspi/llm/llm.html#introduction",
    "href": "raspi/llm/llm.html#introduction",
    "title": "Small Language Models (SLM)",
    "section": "Introduction",
    "text": "Introduction\nIn the fast-growing area of artificial intelligence, edge computing presents an opportunity to decentralize capabilities traditionally reserved for powerful, centralized servers. This lab explores the practical integration of small versions of traditional large language models (LLMs) into a Raspberry Pi 5, transforming this edge device into an AI hub capable of real-time, on-site data processing.\nAs large language models grow in size and complexity, Small Language Models (SLMs) offer a compelling alternative for edge devices, striking a balance between performance and resource efficiency. By running these models directly on Raspberry Pi, we can create responsive, privacy-preserving applications that operate even in environments with limited or no internet connectivity.\nThis lab will guide you through setting up, optimizing, and leveraging SLMs on Raspberry Pi. We will explore the installation and utilization of Ollama. This open-source framework allows us to run LLMs locally on our machines (our desktops or edge devices such as the Raspberry Pis or NVidia Jetsons). Ollama is designed to be efficient, scalable, and easy to use, making it a good option for deploying AI models such as Microsoft Phi, Google Gemma, Meta Llama, and LLaVa (Multimodal). We will integrate some of those models into projects using Python’s ecosystem, exploring their potential in real-world scenarios (or at least point in this direction)."
  },
  {
    "objectID": "raspi/llm/llm.html#setup",
    "href": "raspi/llm/llm.html#setup",
    "title": "Small Language Models (SLM)",
    "section": "Setup",
    "text": "Setup\nWe could use any Raspi model in the previous labs, but here, the choice must be the Raspberry Pi 5 (Raspi-5). It is a robust platform that substantially upgrades the last version 4, equipped with the Broadcom BCM2712, a 2.4GHz quad-core 64-bit Arm Cortex-A76 CPU featuring Cryptographic Extension and enhanced caching capabilities. It boasts a VideoCore VII GPU, dual 4Kp60 HDMI® outputs with HDR, and a 4Kp60 HEVC decoder. Memory options include 4GB and 8GB of high-speed LPDDR4X SDRAM, with 8GB being our choice to run SLMs. It also features expandable storage via a microSD card slot and a PCIe 2.0 interface for fast peripherals such as M.2 SSDs (Solid State Drives).\n\nFor real SSL applications, SSDs are a better option than SD cards.\n\nBy the way, as Alasdair Allan discussed, inferencing directly on the Raspberry Pi 5 CPU—with no GPU acceleration—is now on par with the performance of the Coral TPU.\n\nFor more info, please see the complete article: Benchmarking TensorFlow and TensorFlow Lite on Raspberry Pi 5.\n\nRaspberry Pi Active Cooler\nWe suggest installing an Active Cooler, a dedicated clip-on cooling solution for Raspberry Pi 5 (Raspi-5), for this lab. It combines an aluminum heatsink with a temperature-controlled blower fan to keep the Raspi-5 operating comfortably under heavy loads, such as running SLMs.\n\nThe Active Cooler has pre-applied thermal pads for heat transfer and is mounted directly to the Raspberry Pi 5 board using spring-loaded push pins. The Raspberry Pi firmware actively manages it: at 60°C, the blower’s fan will be turned on; at 67.5°C, the fan speed will be increased; and finally, at 75°C, the fan increases to full speed. The blower’s fan will spin down automatically when the temperature drops below these limits.\n\n\nTo prevent overheating, all Raspberry Pi boards begin to throttle the processor when the temperature reaches 80°Cand throttle even further when it reaches the maximum temperature of 85°C (more detail here)."
  },
  {
    "objectID": "raspi/llm/llm.html#generative-ai-genai",
    "href": "raspi/llm/llm.html#generative-ai-genai",
    "title": "Small Language Models (SLM)",
    "section": "Generative AI (GenAI)",
    "text": "Generative AI (GenAI)\nGenerative AI is an artificial intelligence system capable of creating new, original content across various mediums such as text, images, audio, and video. These systems learn patterns from existing data and use that knowledge to generate novel outputs that didn’t previously exist. Large Language Models (LLMs), Small Language Models (SLMs), and multimodal models can all be considered types of GenAI when used for generative tasks.\nGenAI provides the conceptual framework for AI-driven content creation, with LLMs serving as powerful general-purpose text generators. SLMs adapt this technology for edge computing, while multimodal models extend GenAI capabilities across different data types. Together, they represent a spectrum of generative AI technologies, each with its strengths and applications, collectively driving AI-powered content creation and understanding.\n\nLarge Language Models (LLMs)\nLarge Language Models (LLMs) are advanced artificial intelligence systems that understand, process, and generate human-like text. These models are characterized by their massive scale in terms of the amount of data they are trained on and the number of parameters they contain. Critical aspects of LLMs include:\n\nSize: LLMs typically contain billions of parameters. For example, GPT-3 has 175 billion parameters, while some newer models exceed a trillion parameters.\nTraining Data: They are trained on vast amounts of text data, often including books, websites, and other diverse sources, amounting to hundreds of gigabytes or even terabytes of text.\nArchitecture: Most LLMs use transformer-based architectures, which allow them to process and generate text by paying attention to different parts of the input simultaneously.\nCapabilities: LLMs can perform a wide range of language tasks without specific fine-tuning, including:\n\nText generation\nTranslation\nSummarization\nQuestion answering\nCode generation\nLogical reasoning\n\nFew-shot Learning: They can often understand and perform new tasks with minimal examples or instructions.\nResource-Intensive: Due to their size, LLMs typically require significant computational resources to run, often needing powerful GPUs or TPUs.\nContinual Development: The field of LLMs is rapidly evolving, with new models and techniques constantly emerging.\nEthical Considerations: The use of LLMs raises important questions about bias, misinformation, and the environmental impact of training such large models.\nApplications: LLMs are used in various fields, including content creation, customer service, research assistance, and software development.\nLimitations: Despite their power, LLMs can produce incorrect or biased information and lack true understanding or reasoning capabilities.\n\nWe must note that we use large models beyond text, calling them multi-modal models. These models integrate and process information from multiple types of input simultaneously. They are designed to understand and generate content across various forms of data, such as text, images, audio, and video.\nCertainly. Let’s define open and closed models in the context of AI and language models:\n\n\nClosed vs Open Models:\nClosed models, also called proprietary models, are AI models whose internal workings, code, and training data are not publicly disclosed. Examples: GPT-4 (by OpenAI), Claude (by Anthropic), Gemini (by Google).\nOpen models, also known as open-source models, are AI models whose underlying code, architecture, and often training data are publicly available and accessible. Examples: Gemma (by Google), LLaMA (by Meta) and Phi (by Microsoft)/\nOpen models are particularly relevant for running models on edge devices like Raspberry Pi as they can be more easily adapted, optimized, and deployed in resource-constrained environments. Still, it is crucial to verify their Licenses. Open models come with various open-source licenses that may affect their use in commercial applications, while closed models have clear, albeit restrictive, terms of service.\n\n\n\nAdapted from https://arxiv.org/pdf/2304.13712\n\n\n\n\nSmall Language Models (SLMs)\nIn the context of edge computing on devices like Raspberry Pi, full-scale LLMs are typically too large and resource-intensive to run directly. This limitation has driven the development of smaller, more efficient models, such as the Small Language Models (SLMs).\nSLMs are compact versions of LLMs designed to run efficiently on resource-constrained devices such as smartphones, IoT devices, and single-board computers like the Raspberry Pi. These models are significantly smaller in size and computational requirements than their larger counterparts while still retaining impressive language understanding and generation capabilities.\nKey characteristics of SLMs include:\n\nReduced parameter count: Typically ranging from a few hundred million to a few billion parameters, compared to two-digit billions in larger models.\nLower memory footprint: Requiring, at most, a few gigabytes of memory rather than tens or hundreds of gigabytes.\nFaster inference time: Can generate responses in milliseconds to seconds on edge devices.\nEnergy efficiency: Consuming less power, making them suitable for battery-powered devices.\nPrivacy-preserving: Enabling on-device processing without sending data to cloud servers.\nOffline functionality: Operating without an internet connection.\n\nSLMs achieve their compact size through various techniques such as knowledge distillation, model pruning, and quantization. While they may not match the broad capabilities of larger models, SLMs excel in specific tasks and domains, making them ideal for targeted applications on edge devices.\n\nWe will generally consider SLMs, language models with less than 5 billion parameters quantized to 4 bits.\n\nExamples of SLMs include compressed versions of models like Meta Llama, Microsoft PHI, and Google Gemma. These models enable a wide range of natural language processing tasks directly on edge devices, from text classification and sentiment analysis to question answering and limited text generation.\nFor more information on SLMs, the paper, LLM Pruning and Distillation in Practice: The Minitron Approach, provides an approach applying pruning and distillation to obtain SLMs from LLMs. And, SMALL LANGUAGE MODELS: SURVEY, MEASUREMENTS, AND INSIGHTS, presents a comprehensive survey and analysis of Small Language Models (SLMs), which are language models with 100 million to 5 billion parameters designed for resource-constrained devices."
  },
  {
    "objectID": "raspi/llm/llm.html#ollama",
    "href": "raspi/llm/llm.html#ollama",
    "title": "Small Language Models (SLM)",
    "section": "Ollama",
    "text": "Ollama\n\n\n\nollama logo\n\n\nOllama is an open-source framework that allows us to run language models (LMs), large or small, locally on our machines. Here are some critical points about Ollama:\n\nLocal Model Execution: Ollama enables running LMs on personal computers or edge devices such as the Raspi-5, eliminating the need for cloud-based API calls.\nEase of Use: It provides a simple command-line interface for downloading, running, and managing different language models.\nModel Variety: Ollama supports various LLMs, including Phi, Gemma, Llama, Mistral, and other open-source models.\nCustomization: Users can create and share custom models tailored to specific needs or domains.\nLightweight: Designed to be efficient and run on consumer-grade hardware.\nAPI Integration: Offers an API that allows integration with other applications and services.\nPrivacy-Focused: By running models locally, it addresses privacy concerns associated with sending data to external servers.\nCross-Platform: Available for macOS, Windows, and Linux systems (our case, here).\nActive Development: Regularly updated with new features and model support.\nCommunity-Driven: Benefits from community contributions and model sharing.\n\nTo learn more about what Ollama is and how it works under the hood, you should see this short video from Matt Williams, one of the founders of Ollama:\n\n\nMatt has an entirely free course about Ollama that we recommend: \n\n\nInstalling Ollama\nLet’s set up and activate a Virtual Environment for working with Ollama:\npython3 -m venv ~/ollama\nsource ~/ollama/bin/activate\nAnd run the command to install Ollama:\ncurl -fsSL https://ollama.com/install.sh | sh\nAs a result, an API will run in the background on 127.0.0.1:11434. From now on, we can run Ollama via the terminal. For starting, let’s verify the Ollama version, which will also tell us that it is correctly installed:\nollama -v\n\nOn the Ollama Library page, we can find the models Ollama supports. For example, by filtering by Most popular, we can see Meta Llama, Google Gemma, Microsoft Phi, LLaVa, etc.\n\n\nMeta Llama 3.2 1B/3B\n\nLet’s install and run our first small language model, Llama 3.2 1B (and 3B). The Meta Llama, 3.2 collections of multilingual large language models (LLMs), is a collection of pre-trained and instruction-tuned generative models in 1B and 3B sizes (text in/text out). The Llama 3.2 instruction-tuned text-only models are optimized for multilingual dialogue use cases, including agentic retrieval and summarization tasks.\nThe 1B and 3B models were pruned from the Llama 8B, and then logits from the 8B and 70B models were used as token-level targets (token-level distillation). Knowledge distillation was used to recover performance (they were trained with 9 trillion tokens). The 1B model has 1,24B, quantized to integer (Q8_0), and the 3B, 3.12B parameters, with a Q4_0 quantization, which ends with a size of 1.3 GB and 2GB, respectively. Its context window is 131,072 tokens.\n\nInstall and run the Model\nollama run llama3.2:1b\nRunning the model with the command before, we should have the Ollama prompt available for us to input a question and start chatting with the LLM model; for example,\n&gt;&gt;&gt; What is the capital of France?\nAlmost immediately, we get the correct answer:\nThe capital of France is Paris.\nUsing the option --verbose when calling the model will generate several statistics about its performance (The model will be polling only the first time we run the command).\n\nEach metric gives insights into how the model processes inputs and generates outputs. Here’s a breakdown of what each metric means:\n\nTotal Duration (2.620170326s): This is the complete time taken from the start of the command to the completion of the response. It encompasses loading the model, processing the input prompt, and generating the response.\nLoad Duration (39.947908ms): This duration indicates the time to load the model or necessary components into memory. If this value is minimal, it can suggest that the model was preloaded or that only a minimal setup was required.\nPrompt Eval Count (32 tokens): The number of tokens in the input prompt. In NLP, tokens are typically words or subwords, so this count includes all the tokens that the model evaluated to understand and respond to the query.\nPrompt Eval Duration (1.644773s): This measures the model’s time to evaluate or process the input prompt. It accounts for the bulk of the total duration, implying that understanding the query and preparing a response is the most time-consuming part of the process.\nPrompt Eval Rate (19.46 tokens/s): This rate indicates how quickly the model processes tokens from the input prompt. It reflects the model’s speed in terms of natural language comprehension.\nEval Count (8 token(s)): This is the number of tokens in the model’s response, which in this case was, “The capital of France is Paris.”\nEval Duration (889.941ms): This is the time taken to generate the output based on the evaluated input. It’s much shorter than the prompt evaluation, suggesting that generating the response is less complex or computationally intensive than understanding the prompt.\nEval Rate (8.99 tokens/s): Similar to the prompt eval rate, this indicates the speed at which the model generates output tokens. It’s a crucial metric for understanding the model’s efficiency in output generation.\n\nThis detailed breakdown can help understand the computational demands and performance characteristics of running SLMs like Llama on edge devices like the Raspberry Pi 5. It shows that while prompt evaluation is more time-consuming, the actual generation of responses is relatively quicker. This analysis is crucial for optimizing performance and diagnosing potential bottlenecks in real-time applications.\nLoading and running the 3B model, we can see the difference in performance for the same prompt;\n\nThe eval rate is lower, 5.3 tokens/s versus 9 tokens/s with the smaller model.\nWhen question about\n&gt;&gt;&gt; What is the distance between Paris and Santiago, Chile?\nThe 1B model answered 9,841 kilometers (6,093 miles), which is inaccurate, and the 3B model answered 7,300 miles (11,700 km), which is close to the correct (11,642 km).\nLet’s ask for the Paris’s coordinates:\n&gt;&gt;&gt; what is the latitude and longitude of Paris?\nThe latitude and longitude of Paris are 48.8567° N (48°55' \n42\" N) and 2.3510° E (2°22' 8\" E), respectively.\n\nBoth 1B and 3B models gave correct answers.\n\n\nGoogle Gemma 2 2B\nLet’s install Gemma 2, a high-performing and efficient model available in three sizes: 2B, 9B, and 27B. We will install Gemma 2 2B, a lightweight model trained with 2 trillion tokens that produces outsized results by learning from larger models through distillation. The model has 2.6 billion parameters and a Q4_0 quantization, which ends with a size of 1.6 GB. Its context window is 8,192 tokens.\n\nInstall and run the Model\nollama run gemma2:2b --verbose\nRunning the model with the command before, we should have the Ollama prompt available for us to input a question and start chatting with the LLM model; for example,\n&gt;&gt;&gt; What is the capital of France?\nAlmost immediately, we get the correct answer:\nThe capital of France is **Paris**. 🗼\nAnd it’ statistics.\n\nWe can see that Gemma 2:2B has around the same performance as Lama 3.2:3B, but having less parameters.\nOther examples:\n&gt;&gt;&gt; What is the distance between Paris and Santiago, Chile?\n\nThe distance between Paris, France and Santiago, Chile is \napproximately **7,000 miles (11,267 kilometers)**. \n\nKeep in mind that this is a straight-line distance, and actual \ntravel distance can vary depending on the chosen routes and any \nstops along the way. ✈️`\nAlso, a good response but less accurate than Llama3.2:3B.\n&gt;&gt;&gt; what is the latitude and longitude of Paris?\n\nYou got it! Here are the latitudes and longitudes of Paris, \nFrance:\n\n* **Latitude:** 48.8566° N (north)\n* **Longitude:** 2.3522° E (east) \n\nLet me know if you'd like to explore more about Paris or its \nlocation! 🗼🇫🇷 \nA good and accurate answer (a little more verbose than the Llama answers).\n\n\nMicrosoft Phi3.5 3.8B\nLet’s pull a bigger (but still tiny) model, the PHI3.5, a 3.8B lightweight state-of-the-art open model by Microsoft. The model belongs to the Phi-3 model family and supports 128K token context length and the languages: Arabic, Chinese, Czech, Danish, Dutch, English, Finnish, French, German, Hebrew, Hungarian, Italian, Japanese, Korean, Norwegian, Polish, Portuguese, Russian, Spanish, Swedish, Thai, Turkish and Ukrainian.\nThe model size, in terms of bytes, will depend on the specific quantization format used. The size can go from 2-bit quantization (q2_k) of 1.4GB (higher performance/lower quality) to 16-bit quantization (fp-16) of 7.6GB (lower performance/higher quality).\nLet’s run the 4-bit quantization (Q4_0), which will need 2.2GB of RAM, with an intermediary trade-off regarding output quality and performance.\nollama run phi3.5:3.8b --verbose\n\nYou can use run or pull to download the model. What happens is that Ollama keeps note of the pulled models, and once the PHI3 does not exist, before running it, Ollama pulls it.\n\nLet’s enter with the same prompt used before:\n&gt;&gt;&gt; What is the capital of France?\n\nThe capital of France is Paris. It' extradites significant \nhistorical, cultural, and political importance to the country as \nwell as being a major European city known for its art, fashion, \ngastronomy, and culture. Its influence extends beyond national \nborders, with millions of tourists visiting each year from around \nthe globe. The Seine River flows through Paris before it reaches \nthe broader English Channel at Le Havre. Moreover, France is one \nof Europe's leading economies with its capital playing a key role \n\n...\nThe answer was very “verbose”, let’s specify a better prompt:\n\nIn this case, the answer was still longer than we expected, with an eval rate of 2.25 tokens/s, more than double that of Gemma and Llama.\n\nChoosing the most appropriate prompt is one of the most important skills to be used with LLMs, no matter its size.\n\nWhen we asked the same questions about distance and Latitude/Longitude, we did not get a good answer for a distance of 13,507 kilometers (8,429 miles), but it was OK for coordinates. Again, it could have been less verbose (more than 200 tokens for each answer).\nWe can use any model as an assistant since their speed is relatively decent, but on September 24, the Llama2:3B is a better choice. You should try other models, depending on your needs. 🤗 Open LLM Leaderboard can give you an idea about the best models in size, benchmark, license, etc.\n\nThe best model to use is the one fit for your specific necessity. Also, take into consideration that this field evolves with new models every day,\n\n\n\nMultimodal Models\nMultimodal models are artificial intelligence (AI) systems that can process and understand information from multiple sources, such as images, text, audio, and video. In our context, multimodal LLMs can process various inputs, including text, images, and audio, as prompts and convert those prompts into various outputs, not just the source type.\nWe will work here with LLaVA-Phi-3, a fine-tuned LLaVA model from Phi 3 Mini 4k. It has strong performance benchmarks that are on par with the original LLaVA (Large Language and Vision Assistant) model.\nThe LLaVA-Phi-3 is an end-to-end trained large multimodal model designed to understand and generate content based on visual inputs (images) and textual instructions. It combines the capabilities of a visual encoder and a language model to process and respond to multimodal inputs.\nLet’s install the model:\nollama run llava-phi3:3.8b --verbose\nLet’s start with a text input:\n&gt;&gt;&gt; You are a helpful AI assistant. What is the capital of France?\n\nAs an AI language model, I can tell you that the capital of France \nis Paris. It's not only the largest city in the country but also \nserves as its political and administrative center. Paris is known \nfor its iconic landmarks such as the Eiffel Tower, Notre-Dame \nCathedral, and the Louvre Museum. The city has a rich history, \nbeautiful architecture, and is widely considered to be one of the \nmost romantic cities in the world.\nThe response took around 30s, with an eval rate of 3.93 tokens/s! Not bad!\nBut let us know to enter with an image as input. For that, let’s create a directory for working:\ncd Documents/\nmkdir OLLAMA\ncd OLLAMA\nLet’s download a 640x320 image from the internet, for example (Wikipedia: Paris, France):\n\nUsing FileZilla, for example, let’s upload the image to the OLLAMA folder at the Raspi-5 and name it image_test_1.jpg. We should have the whole image path (we can use pwd to get it).\n/home/mjrovai/Documents/OLLAMA/image_test_1.jpg\nIf you use a desktop, you can copy the image path by clicking the image with the mouse’s right button.\n\nLet’s enter with this prompt:\n&gt;&gt;&gt; Describe the image /home/mjrovai/Documents/OLLAMA/image_test_1.jpg\nThe result was great, but the overall latency was significant; almost 4 minutes to perform the inference.\n\n\n\nInspecting local resources\nUsing htop, we can monitor the resources running on our device.\nhtop\nDuring the time that the model is running, we can inspect the resources:\n\nAll four CPUs run at almost 100% of their capacity, and the memory used with the model loaded is 3.24GB. Exiting Ollama, the memory goes down to around 377MB (with no desktop).\nIt is also essential to monitor the temperature. When running the Raspberry with a desktop, you can have the temperature shown on the taskbar:\n\nIf you are “headless”, the temperature can be monitored with the command:\nvcgencmd measure_temp\nIf you are doing nothing, the temperature is around 50°C for CPUs running at 1%. During inference, with the CPUs at 100%, the temperature can rise to almost 70°C. This is OK and means the active cooler is working, keeping the temperature below 80°C / 85°C (its limit)."
  },
  {
    "objectID": "raspi/llm/llm.html#ollama-python-library",
    "href": "raspi/llm/llm.html#ollama-python-library",
    "title": "Small Language Models (SLM)",
    "section": "Ollama Python Library",
    "text": "Ollama Python Library\nSo far, we have explored SLMs’ chat capability using the command line on a terminal. However, we want to integrate those models into our projects, so Python seems to be the right path. The good news is that Ollama has such a library.\nThe Ollama Python library simplifies interaction with advanced LLM models, enabling more sophisticated responses and capabilities, besides providing the easiest way to integrate Python 3.8+ projects with Ollama.\nFor a better understanding of how to create apps using Ollama with Python, we can follow Matt Williams’s videos, as the one below:\n\nInstallation:\nIn the terminal, run the command:\npip install ollama\nWe will need a text editor or an IDE to create a Python script. If you run the Raspberry OS on a desktop, several options, such as Thonny and Geany, have already been installed by default (accessed by [Menu][Programming]). You can download other IDEs, such as Visual Studio Code, from [Menu][Recommended Software]. When the window pops up, go to [Programming], select the option of your choice, and press [Apply].\n\nIf you prefer using Jupyter Notebook for development:\npip install jupyter\njupyter notebook --generate-config\nTo run Jupyter Notebook, run the command (change the IP address for yours):\njupyter notebook --ip=192.168.4.209 --no-browser\nOn the terminal, you can see the local URL address to open the notebook:\n\nWe can access it from another computer by entering the Raspberry Pi’s IP address and the provided token in a web browser (we should copy it from the terminal).\nIn our working directory in the Raspi, we will create a new Python 3 notebook.\nLet’s enter with a very simple script to verify the installed models:\nimport ollama\nollama.list()\nAll the models will be printed as a dictionary, for example:\n  {'name': 'gemma2:2b',\n   'model': 'gemma2:2b',\n   'modified_at': '2024-09-24T19:30:40.053898094+01:00',\n   'size': 1629518495,\n   'digest': '8ccf136fdd5298f3ffe2d69862750ea7fb56555fa4d5b18c04e3fa4d82ee09d7',\n   'details': {'parent_model': '',\n    'format': 'gguf',\n    'family': 'gemma2',\n    'families': ['gemma2'],\n    'parameter_size': '2.6B',\n    'quantization_level': 'Q4_0'}}]}\nLet’s repeat one of the questions that we did before, but now using ollama.generate() from Ollama python library. This API will generate a response for the given prompt with the provided model. This is a streaming endpoint, so there will be a series of responses. The final response object will include statistics and additional data from the request.\nMODEL = 'gemma2:2b'\nPROMPT = 'What is the capital of France?'\n\nres = ollama.generate(model=MODEL, prompt=PROMPT)\nprint (res)\nIn case you are running the code as a Python script, you should save it, for example, test_ollama.py. You can use the IDE to run it or do it directly on the terminal. Also, remember that you should always call the model and define it when running a stand-alone script.\npython test_ollama.py\nAs a result, we will have the model response in a JSON format:\n{'model': 'gemma2:2b', 'created_at': '2024-09-25T14:43:31.869633807Z', \n'response': 'The capital of France is **Paris**. 🇫🇷 \\n', 'done': True, \n'done_reason': 'stop', 'context': [106, 1645, 108, 1841, 603, 573, 6037, 576,\n6081, 235336, 107, 108, 106, 2516, 108, 651, 6037, 576, 6081, 603, 5231, 29437, \n168428, 235248, 244304, 241035, 235248, 108], 'total_duration': 24259469458, \n'load_duration': 19830013859, 'prompt_eval_count': 16, 'prompt_eval_duration': \n1908757000, 'eval_count': 14, 'eval_duration': 2475410000}\nAs we can see, several pieces of information are generated, such as:\n\nresponse: the main output text generated by the model in response to our prompt.\n\nThe capital of France is **Paris**. 🇫🇷\n\ncontext: the token IDs representing the input and context used by the model. Tokens are numerical representations of text used for processing by the language model.\n\n[106, 1645, 108, 1841, 603, 573, 6037, 576, 6081, 235336, 107, 108, 106, 2516, 108, 651, 6037, 576, 6081, 603, 5231, 29437, 168428, 235248, 244304, 241035, 235248, 108]\n\n\nThe Performance Metrics:\n\ntotal_duration: The total time taken for the operation in nanoseconds. In this case, approximately 24.26 seconds.\nload_duration: The time taken to load the model or components in nanoseconds. About 19.38 seconds.\nprompt_eval_duration: The time taken to evaluate the prompt in nanoseconds. Around 1.9.0 seconds.\neval_count: The number of tokens evaluated during the generation. Here, 14 tokens.\neval_duration: The time taken for the model to generate the response in nanoseconds. Approximately 2.5 seconds.\n\nBut, what we want is the plain ‘response’ and, perhaps for analysis, the total duration of the inference, so let’s change the code to extract it from the dictionary:\nprint(f\"\\n{res['response']}\")\nprint(f\"\\n [INFO] Total Duration: {(res['total_duration']/1e9):.2f} seconds\")\nNow, we got:\nThe capital of France is **Paris**. 🇫🇷 \n\n [INFO] Total Duration: 24.26 seconds\nUsing Ollama.chat()\nAnother way to get our response is to use ollama.chat(), which generates the next message in a chat with a provided model. This is a streaming endpoint, so a series of responses will occur. Streaming can be disabled using \"stream\": false. The final response object will also include statistics and additional data from the request.\nPROMPT_1 = 'What is the capital of France?'\n\nresponse = ollama.chat(model=MODEL, messages=[\n{'role': 'user','content': PROMPT_1,},])\nresp_1 = response['message']['content']\nprint(f\"\\n{resp_1}\")\nprint(f\"\\n [INFO] Total Duration: {(res['total_duration']/1e9):.2f} seconds\")\nThe answer is the same as before.\nAn important consideration is that by using ollama.generate(), the response is “clear” from the model’s “memory” after the end of inference (only used once), but If we want to keep a conversation, we must use ollama.chat(). Let’s see it in action:\nPROMPT_1 = 'What is the capital of France?'\nresponse = ollama.chat(model=MODEL, messages=[\n{'role': 'user','content': PROMPT_1,},])\nresp_1 = response['message']['content']\nprint(f\"\\n{resp_1}\")\nprint(f\"\\n [INFO] Total Duration: {(response['total_duration']/1e9):.2f} seconds\")\n\nPROMPT_2 = 'and of Italy?'\nresponse = ollama.chat(model=MODEL, messages=[\n{'role': 'user','content': PROMPT_1,},\n{'role': 'assistant','content': resp_1,},\n{'role': 'user','content': PROMPT_2,},])\nresp_2 = response['message']['content']\nprint(f\"\\n{resp_2}\")\nprint(f\"\\n [INFO] Total Duration: {(response['total_duration']/1e9):.2f} seconds\")\nIn the above code, we are running two queries, and the second prompt considers the result of the first one.\nHere is how the model responded:\nThe capital of France is **Paris**. 🇫🇷 \n\n [INFO] Total Duration: 2.82 seconds\n\nThe capital of Italy is **Rome**. 🇮🇹 \n\n [INFO] Total Duration: 4.46 seconds\nGetting an image description:\nIn the same way that we have used the LlaVa-PHI-3 model with the command line to analyze an image, the same can be done here with Python. Let’s use the same image of Paris, but now with the ollama.generate():\nMODEL = 'llava-phi3:3.8b'\nPROMPT = \"Describe this picture\"\n\nwith open('image_test_1.jpg', 'rb') as image_file:\n    img = image_file.read()\n\nresponse = ollama.generate(\n    model=MODEL,\n    prompt=PROMPT,\n    images= [img]\n)\nprint(f\"\\n{response['response']}\")\nprint(f\"\\n [INFO] Total Duration: {(res['total_duration']/1e9):.2f} seconds\")\nHere is the result:\nThis image captures the iconic cityscape of Paris, France. The vantage point \nis high, providing a panoramic view of the Seine River that meanders through \nthe heart of the city. Several bridges arch gracefully over the river, \nconnecting different parts of the city. The Eiffel Tower, an iron lattice \nstructure with a pointed top and two antennas on its summit, stands tall in the \nbackground, piercing the sky. It is painted in a light gray color, contrasting \nagainst the blue sky speckled with white clouds.\n\nThe buildings that line the river are predominantly white or beige, their uniform\ncolor palette broken occasionally by red roofs peeking through. The Seine River \nitself appears calm and wide, reflecting the city's architectural beauty in its \nsurface. On either side of the river, trees add a touch of green to the urban \nlandscape.\n\nThe image is taken from an elevated perspective, looking down on the city. This \nviewpoint allows for a comprehensive view of Paris's beautiful architecture and \nlayout. The relative positions of the buildings, bridges, and other structures \ncreate a harmonious composition that showcases the city's charm.\n\nIn summary, this image presents a serene day in Paris, with its architectural \nmarvels - from the Eiffel Tower to the river-side buildings - all bathed in soft \ncolors under a clear sky.\n\n [INFO] Total Duration: 256.45 seconds\nThe model took about 4 minutes (256.45 s) to return with a detailed image description.\n\nIn the 10-Ollama_Python_Library notebook, it is possible to find the experiments with the Ollama Python library.\n\n\nFunction Calling\nSo far, we can see that, with the model’s (“response”) answer to a variable, we can efficiently work with it, integrating it into real-world projects. However, a big problem is that the model can respond differently to the same prompt. Let’s say that what we want, as the model’s response in the last examples, is only the name of a given country’s capital and its coordinates, nothing more, even with very verbose models such as the Microsoft Phi. We can use the Ollama function's calling to guarantee the same answers, which is perfectly compatible with OpenAI API.\n\nBut what exactly is “function calling”?\nIn modern artificial intelligence, function calling with Large Language Models (LLMs) allows these models to perform actions beyond generating text. By integrating with external functions or APIs, LLMs can access real-time data, automate tasks, and interact with various systems.\nFor instance, instead of merely responding to a query about the weather, an LLM can call a weather API to fetch the current conditions and provide accurate, up-to-date information. This capability enhances the relevance and accuracy of the model’s responses and makes it a powerful tool for driving workflows and automating processes, transforming it into an active participant in real-world applications.\nFor more details about Function Calling, please see this video made by Marvin Prison:\n\n\n\nLet’s create a project.\nWe want to create an app where the user enters a country’s name and gets, as an output, the distance in km from the capital city of such a country and the app’s location (for simplicity, We will use Santiago, Chile, as the app location).\n\nOnce the user enters a country name, the model will return the name of its capital city (as a string) and the latitude and longitude of such city (in float). Using those coordinates, we can use a simple Python library (haversine) to calculate the distance between those 2 points.\nThe idea of this project is to demonstrate a combination of language model interaction (IA), structured data handling with Pydantic, and geospatial calculations using the Haversine formula (traditional computing).\nFirst, let us install some libraries. Besides Haversine, the main one is the OpenAI Python library, which provides convenient access to the OpenAI REST API from any Python 3.7+ application. The other one is Pydantic (and instructor), a robust data validation and settings management library engineered by Python to enhance the robustness and reliability of our codebase. In short, Pydantic will help ensure that our model’s response will always be consistent.\npip install haversine\npip install openai \npip install pydantic \npip install instructor\nNow, we should create a Python script designed to interact with our model (LLM) to determine the coordinates of a country’s capital city and calculate the distance from Santiago de Chile to that capital.\nLet’s go over the code:\n\n\n\n1. Importing Libraries\nimport sys\nfrom haversine import haversine\nfrom openai import OpenAI\nfrom pydantic import BaseModel, Field\nimport instructor\n\nsys: Provides access to system-specific parameters and functions. It’s used to get command-line arguments.\nhaversine: A function from the haversine library that calculates the distance between two geographic points using the Haversine formula.\nopenAI: A module for interacting with the OpenAI API (although it’s used in conjunction with a local setup, Ollama). Everything is off-line here.\npydantic: Provides data validation and settings management using Python-type annotations. It’s used to define the structure of expected response data.\ninstructor: A module is used to patch the OpenAI client to work in a specific mode (likely related to structured data handling).\n\n\n\n2. Defining Input and Model\ncountry = sys.argv[1]       # Get the country from command-line arguments\nMODEL = 'phi3.5:3.8b'     # The name of the model to be used\nmylat = -33.33              # Latitude of Santiago de Chile\nmylon = -70.51              # Longitude of Santiago de Chile\n\ncountry: On a Python script, getting the country name from command-line arguments is possible. On a Jupyter notebook, we can enter its name, for example,\n\ncountry = \"France\"\n\nMODEL: Specifies the model being used, which is, in this example, the phi3.5.\nmylat and mylon: Coordinates of Santiago de Chile, used as the starting point for the distance calculation.\n\n\n\n3. Defining the Response Data Structure\nclass CityCoord(BaseModel):\n    city: str = Field(..., description=\"Name of the city\")\n    lat: float = Field(..., description=\"Decimal Latitude of the city\")\n    lon: float = Field(..., description=\"Decimal Longitude of the city\")\n\nCityCoord: A Pydantic model that defines the expected structure of the response from the LLM. It expects three fields: city (name of the city), lat (latitude), and lon (longitude).\n\n\n\n4. Setting Up the OpenAI Client\nclient = instructor.patch(\n    OpenAI(\n        base_url=\"http://localhost:11434/v1\",  # Local API base URL (Ollama)\n        api_key=\"ollama\",                      # API key (not used)\n    ),\n    mode=instructor.Mode.JSON,                 # Mode for structured JSON output\n)\n\nOpenAI: This setup initializes an OpenAI client with a local base URL and an API key (ollama). It uses a local server.\ninstructor.patch: Patches the OpenAI client to work in JSON mode, enabling structured output that matches the Pydantic model.\n\n\n\n5. Generating the Response\nresp = client.chat.completions.create(\n    model=MODEL,\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": f\"return the decimal latitude and decimal longitude \\\n            of the capital of the {country}.\"\n        }\n    ],\n    response_model=CityCoord,\n    max_retries=10\n)\n\nclient.chat.completions.create: Calls the LLM to generate a response.\nmodel: Specifies the model to use (llava-phi3).\nmessages: Contains the prompt for the LLM, asking for the latitude and longitude of the capital city of the specified country.\nresponse_model: Indicates that the response should conform to the CityCoord model.\nmax_retries: The maximum number of retry attempts if the request fails.\n\n\n\n6. Calculating the Distance\ndistance = haversine((mylat, mylon), (resp.lat, resp.lon), unit='km')\nprint(f\"Santiago de Chile is about {int(round(distance, -1)):,} \\\n        kilometers away from {resp.city}.\")\n\nhaversine: Calculates the distance between Santiago de Chile and the capital city returned by the LLM using their respective coordinates.\n(mylat, mylon): Coordinates of Santiago de Chile.\nresp.city: Name of the country’s capital\n(resp.lat, resp.lon): Coordinates of the capital city are provided by the LLM response.\nunit=‘km’: Specifies that the distance should be calculated in kilometers.\nprint: Outputs the distance, rounded to the nearest 10 kilometers, with thousands of separators for readability.\n\nRunning the code\nIf we enter different countries, for example, France, Colombia, and the United States, We can note that we always receive the same structured information:\nSantiago de Chile is about 8,060 kilometers away from Washington, D.C..\nSantiago de Chile is about 4,250 kilometers away from Bogotá.\nSantiago de Chile is about 11,630 kilometers away from Paris.\nIf you run the code as a script, the result will be printed on the terminal:\n\nAnd the calculations are pretty good!\n\n\nIn the 20-Ollama_Function_Calling notebook, it is possible to find experiments with all models installed.\n\n\n\nAdding images\nNow it is time to wrap up everything so far! Let’s modify the script so that instead of entering the country name (as a text), the user enters an image, and the application (based on SLM) returns the city in the image and its geographic location. With those data, we can calculate the distance as before.\n\nFor simplicity, we will implement this new code in two steps. First, the LLM will analyze the image and create a description (text). This text will be passed on to another instance, where the model will extract the information needed to pass along.\nWe will start importing the libraries\nimport sys\nimport time\nfrom haversine import haversine\nimport ollama\nfrom openai import OpenAI\nfrom pydantic import BaseModel, Field\nimport instructor\nWe can see the image if you run the code on the Jupyter Notebook. For that we need also import:\nimport matplotlib.pyplot as plt\nfrom PIL import Image\n\nThose libraries are unnecessary if we run the code as a script.\n\nNow, we define the model and the local coordinates:\nMODEL = 'llava-phi3:3.8b'\nmylat = -33.33\nmylon = -70.51\nWe can download a new image, for example, Machu Picchu from Wikipedia. On the Notebook we can see it:\n# Load the image\nimg_path = \"image_test_3.jpg\"\nimg = Image.open(img_path)\n\n# Display the image\nplt.figure(figsize=(8, 8))\nplt.imshow(img)\nplt.axis('off')\n#plt.title(\"Image\")\nplt.show()\n\nNow, let’s define a function that will receive the image and will return the decimal latitude and decimal longitude of the city in the image, its name, and what country it is located\ndef image_description(img_path):\n    with open(img_path, 'rb') as file:\n        response = ollama.chat(\n            model=MODEL,\n            messages=[\n              {\n                'role': 'user',\n                'content': '''return the decimal latitude and decimal longitude \n                              of the city in the image, its name, and \n                              what country it is located''',\n                'images': [file.read()],\n              },\n            ],\n            options = {\n              'temperature': 0,\n              }\n      )\n    #print(response['message']['content'])\n    return response['message']['content']\n\nWe can print the entire response for debug purposes.\n\nThe image description generated for the function will be passed as a prompt for the model again.\nstart_time = time.perf_counter()  # Start timing\n\nclass CityCoord(BaseModel):\n    city: str = Field(..., description=\"Name of the city in the image\")\n    country: str = Field(..., description=\"\"\"Name of the country where\"\n                                             the city in the image is located\n                                             \"\"\")\n    lat: float = Field(..., description=\"\"\"Decimal Latitude of the city in\"\n                                            the image\"\"\")\n    lon: float = Field(..., description=\"\"\"Decimal Longitude of the city in\"\n                                           the image\"\"\")\n\n# enables `response_model` in create call\nclient = instructor.patch(\n    OpenAI(\n        base_url=\"http://localhost:11434/v1\",\n        api_key=\"ollama\"\n    ),\n    mode=instructor.Mode.JSON,\n)\n\nimage_description = image_description(img_path)\n# Send this description to the model\nresp = client.chat.completions.create(\n    model=MODEL,\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": image_description,\n        }\n    ],\n    response_model=CityCoord,\n    max_retries=10,\n    temperature=0,\n)\nIf we print the image description , we will get:\nThe image shows the ancient city of Machu Picchu, located in Peru. The city is\nperched on a steep hillside and consists of various structures made of stone. It \nis surrounded by lush greenery and towering mountains. The sky above is blue with\nscattered clouds. \n\nMachu Picchu's latitude is approximately 13.5086° S, and its longitude is around\n72.5494° W.\nAnd the second response from the model (resp) will be:\nCityCoord(city='Machu Picchu', country='Peru', lat=-13.5086, lon=-72.5494)\nNow, we can do a “Post-Processing”, calculating the distance and preparing the final answer:\ndistance = haversine((mylat, mylon), (resp.lat, resp.lon), unit='km')\n\nprint(f\"\\n The image shows {resp.city}, with lat:{round(resp.lat, 2)} and \\\n      long: {round(resp.lon, 2)}, located in {resp.country} and about \\\n            {int(round(distance, -1)):,} kilometers away from \\\n            Santiago, Chile.\\n\")\n\nend_time = time.perf_counter()  # End timing\nelapsed_time = end_time - start_time  # Calculate elapsed time\nprint(f\" [INFO] ==&gt; The code (running {MODEL}), took {elapsed_time:.1f} \\\n      seconds to execute.\\n\")\nAnd we will get:\n The image shows Machu Picchu, with lat:-13.16 and long: -72.54, located in Peru\n and about 2,250 kilometers away from Santiago, Chile.\n\n [INFO] ==&gt; The code (running llava-phi3:3.8b), took 491.3 seconds to execute.\nIn the 30-Function_Calling_with_images notebook, it is possible to find the experiments with multiple images.\nLet’s now download the script calc_distance_image.py from the GitHub and run it on the terminal with the command:\npython calc_distance_image.py /home/mjrovai/Documents/OLLAMA/image_test_3.jpg\nEnter with the Machu Picchu image full patch as an argument. We will get the same previous result.\n\nHow about Paris?\n\nOf course, there are many ways to optimize the code used here. Still, the idea is to explore the considerable potential of function calling with SLMs at the edge, allowing those models to integrate with external functions or APIs. Going beyond text generation, SLMs can access real-time data, automate tasks, and interact with various systems."
  },
  {
    "objectID": "raspi/llm/llm.html#slms-optimization-techniques",
    "href": "raspi/llm/llm.html#slms-optimization-techniques",
    "title": "Small Language Models (SLM)",
    "section": "SLMs: Optimization Techniques",
    "text": "SLMs: Optimization Techniques\nLarge Language Models (LLMs) have revolutionized natural language processing, but their deployment and optimization come with unique challenges. One significant issue is the tendency for LLMs (and more, the SLMs) to generate plausible-sounding but factually incorrect information, a phenomenon known as hallucination. This occurs when models produce content that seems coherent but is not grounded in truth or real-world facts.\nOther challenges include the immense computational resources required for training and running these models, the difficulty in maintaining up-to-date knowledge within the model, and the need for domain-specific adaptations. Privacy concerns also arise when handling sensitive data during training or inference. Additionally, ensuring consistent performance across diverse tasks and maintaining ethical use of these powerful tools present ongoing challenges. Addressing these issues is crucial for the effective and responsible deployment of LLMs in real-world applications.\nThe fundamental techniques for enhancing LLM (and SLM) performance and efficiency are Fine-tuning, Prompt engineering, and Retrieval-Augmented Generation (RAG).\n\nFine-tuning, while more resource-intensive, offers a way to specialize LLMs for particular domains or tasks. This process involves further training the model on carefully curated datasets, allowing it to adapt its vast general knowledge to specific applications. Fine-tuning can lead to substantial improvements in performance, especially in specialized fields or for unique use cases.\nPrompt engineering is at the forefront of LLM optimization. By carefully crafting input prompts, we can guide models to produce more accurate and relevant outputs. This technique involves structuring queries that leverage the model’s pre-trained knowledge and capabilities, often incorporating examples or specific instructions to shape the desired response.\nRetrieval-Augmented Generation (RAG) represents another powerful approach to improving LLM performance. This method combines the vast knowledge embedded in pre-trained models with the ability to access and incorporate external, up-to-date information. By retrieving relevant data to supplement the model’s decision-making process, RAG can significantly enhance accuracy and reduce the likelihood of generating outdated or false information.\n\nFor edge applications, it is more beneficial to focus on techniques like RAG that can enhance model performance without needing on-device fine-tuning. Let’s explore it."
  },
  {
    "objectID": "raspi/llm/llm.html#rag-implementation",
    "href": "raspi/llm/llm.html#rag-implementation",
    "title": "Small Language Models (SLM)",
    "section": "RAG Implementation",
    "text": "RAG Implementation\nIn a basic interaction between a user and a language model, the user asks a question, which is sent as a prompt to the model. The model generates a response based solely on its pre-trained knowledge. In a RAG process, there’s an additional step between the user’s question and the model’s response. The user’s question triggers a retrieval process from a knowledge base.\n\n\nA simple RAG project\nHere are the steps to implement a basic Retrieval Augmented Generation (RAG):\n\nDetermine the type of documents you’ll be using: The best types are documents from which we can get clean and unobscured text. PDFs can be problematic because they are designed for printing, not for extracting sensible text. To work with PDFs, we should get the source document or use tools to handle it.\nChunk the text: We can’t store the text as one long stream because of context size limitations and the potential for confusion. Chunking involves splitting the text into smaller pieces. Chunk text has many ways, such as character count, tokens, words, paragraphs, or sections. It is also possible to overlap chunks.\nCreate embeddings: Embeddings are numerical representations of text that capture semantic meaning. We create embeddings by passing each chunk of text through a particular embedding model. The model outputs a vector, the length of which depends on the embedding model used. We should pull one (or more) embedding models from Ollama, to perform this task. Here are some examples of embedding models available at Ollama.\n\n\n\nModel\nParameter Size\nEmbedding Size\n\n\n\n\nmxbai-embed-large\n334M\n1024\n\n\nnomic-embed-text\n137M\n768\n\n\nall-minilm\n23M\n384\n\n\n\n\nGenerally, larger embedding sizes capture more nuanced information about the input. Still, they also require more computational resources to process, and a higher number of parameters should increase the latency (but also the quality of the response).\n\nStore the chunks and embeddings in a vector database: We will need a way to efficiently find the most relevant chunks of text for a given prompt, which is where a vector database comes in. We will use Chromadb, an AI-native open-source vector database, which simplifies building RAGs by creating knowledge, facts, and skills pluggable for LLMs. Both the embedding and the source text for each chunk are stored.\nBuild the prompt: When we have a question, we create an embedding and query the vector database for the most similar chunks. Then, we select the top few results and include their text in the prompt.\n\nThe goal of RAG is to provide the model with the most relevant information from our documents, allowing it to generate more accurate and informative responses. So, let’s implement a simple example of an SLM incorporating a particular set of facts about bees (“Bee Facts”).\nInside the ollama env, enter the command in the terminal for Chromadb instalation:\npip install ollama chromadb\nLet’s pull an intermediary embedding model, nomic-embed-text\nollama pull nomic-embed-text\nAnd create a working directory:\ncd Documents/OLLAMA/\nmkdir RAG-simple-bee\ncd RAG-simple-bee/\nLet’s create a new Jupyter notebook, 40-RAG-simple-bee for some exploration:\nImport the needed libraries:\nimport ollama\nimport chromadb\nimport time\nAnd define aor models:\nEMB_MODEL = \"nomic-embed-text\"\nMODEL = 'llama3.2:3B'\nInitially, a knowledge base about bee facts should be created. This involves collecting relevant documents and converting them into vector embeddings. These embeddings are then stored in a vector database, allowing for efficient similarity searches later. Enter with the “document,” a base of “bee facts” as a list:\ndocuments = [\n    \"Bee-keeping, also known as apiculture, involves the maintenance of bee \\\n    colonies, typically in hives, by humans.\",\n    \"The most commonly kept species of bees is the European honey bee (Apis \\\n    mellifera).\",\n    \n    ...\n    \n    \"There are another 20,000 different bee species in the world.\",  \n    \"Brazil alone has more than 300 different bee species, and the \\\n    vast majority, unlike western honey bees, don’t sting.\", \n    \"Reports written in 1577 by Hans Staden, mention three native bees \\\n    used by indigenous people in Brazil.\",\n    \"The indigenous people in Brazil used bees for medicine and food purposes\",\n    \"From Hans Staden report: probable species: mandaçaia (Melipona \\\n    quadrifasciata), mandaguari (Scaptotrigona postica) and jataí-amarela \\\n    (Tetragonisca angustula).\"\n]\n\nWe do not need to “chunk” the document here because we will use each element of the list and a chunk.\n\nNow, we will create our vector embedding database bee_facts and store the document in it:\nclient = chromadb.Client()\ncollection = client.create_collection(name=\"bee_facts\")\n\n# store each document in a vector embedding database\nfor i, d in enumerate(documents):\n  response = ollama.embeddings(model=EMB_MODEL, prompt=d)\n  embedding = response[\"embedding\"]\n  collection.add(\n    ids=[str(i)],\n    embeddings=[embedding],\n    documents=[d]\n  )\nNow that we have our “Knowledge Base” created, we can start making queries, retrieving data from it:\n\nUser Query: The process begins when a user asks a question, such as “How many bees are in a colony? Who lays eggs, and how much? How about common pests and diseases?”\nprompt = \"How many bees are in a colony? Who lays eggs and how much? How about\\\n          common pests and diseases?\"\nQuery Embedding: The user’s question is converted into a vector embedding using the same embedding model used for the knowledge base.\nresponse = ollama.embeddings(\n  prompt=prompt,\n  model=EMB_MODEL\n)\nRelevant Document Retrieval: The system searches the knowledge base using the query embedding to find the most relevant documents (in this case, the 5 more probable). This is done using a similarity search, which compares the query embedding to the document embeddings in the database.\nresults = collection.query(\n  query_embeddings=[response[\"embedding\"]],\n  n_results=5\n)\ndata = results['documents']\nPrompt Augmentation: The retrieved relevant information is combined with the original user query to create an augmented prompt. This prompt now contains the user’s question and pertinent facts from the knowledge base.\nprompt=f\"Using this data: {data}. Respond to this prompt: {prompt}\",\nAnswer Generation: The augmented prompt is then fed into a language model, in this case, the llama3.2:3b model. The model uses this enriched context to generate a comprehensive answer. Parameters like temperature, top_k, and top_p are set to control the randomness and quality of the generated response.\noutput = ollama.generate(\n  model=MODEL,\n  prompt=f\"Using this data: {data}. Respond to this prompt: {prompt}\",\n  options={\n    \"temperature\": 0.0,\n    \"top_k\":10,\n    \"top_p\":0.5                          }\n)\nResponse Delivery: Finally, the system returns the generated answer to the user.\nprint(output['response'])\nBased on the provided data, here are the answers to your questions:\n\n1. How many bees are in a colony?\nA typical bee colony can contain between 20,000 and 80,000 bees.\n\n2. Who lays eggs and how much?\nThe queen bee lays up to 2,000 eggs per day during peak seasons.\n\n3. What about common pests and diseases?\nCommon pests and diseases that affect bees include varroa mites, hive beetles,\nand foulbrood.\nLet’s create a function to help answer new questions:\ndef rag_bees(prompt, n_results=5, temp=0.0, top_k=10, top_p=0.5):\n    start_time = time.perf_counter()  # Start timing\n    \n    # generate an embedding for the prompt and retrieve the data \n    response = ollama.embeddings(\n      prompt=prompt,\n      model=EMB_MODEL\n    )\n    \n    results = collection.query(\n      query_embeddings=[response[\"embedding\"]],\n      n_results=n_results\n    )\n    data = results['documents']\n    \n    # generate a response combining the prompt and data retrieved\n    output = ollama.generate(\n      model=MODEL,\n      prompt=f\"Using this data: {data}. Respond to this prompt: {prompt}\",\n      options={\n        \"temperature\": temp,\n        \"top_k\": top_k,\n        \"top_p\": top_p                          }\n    )\n    \n    print(output['response'])\n    \n    end_time = time.perf_counter()  # End timing\n    elapsed_time = round((end_time - start_time), 1)  # Calculate elapsed time\n    \n    print(f\"\\n [INFO] ==&gt; The code for model: {MODEL}, took {elapsed_time}s \\\n          to generate the answer.\\n\")\nWe can now create queries and call the function:\nprompt = \"Are bees in Brazil?\"\nrag_bees(prompt)\nYes, bees are found in Brazil. According to the data, Brazil has more than 300\ndifferent bee species, and indigenous people in Brazil used bees for medicine and\nfood purposes. Additionally, reports from 1577 mention three native bees used by\nindigenous people in Brazil.\n\n [INFO] ==&gt; The code for model: llama3.2:3b, took 22.7s to generate the answer.\nBy the way, if the model used supports multiple languages, we can use it (for example, Portuguese), even if the dataset was created in English:\nprompt = \"Existem abelhas no Brazil?\"\nrag_bees(prompt)\nSim, existem abelhas no Brasil! De acordo com o relato de Hans Staden, há três \nespécies de abelhas nativas do Brasil que foram mencionadas: mandaçaia (Melipona\nquadrifasciata), mandaguari (Scaptotrigona postica) e jataí-amarela (Tetragonisca\nangustula). Além disso, o Brasil é conhecido por ter mais de 300 espécies \ndiferentes de abelhas, a maioria das quais não é agressiva e não põe veneno.\n\n [INFO] ==&gt; The code for model: llama3.2:3b, took 54.6s to generate the answer.\n\n\nGoing Further\nThe small LLM models tested worked well at the edge, both in text and with images, but of course, they had high latency regarding the last one. A combination of specific and dedicated models can lead to better results; for example, in real cases, an Object Detection model (such as YOLO) can get a general description and count of objects on an image that, once passed to an LLM, can help extract essential insights and actions.\nAccording to Avi Baum, CTO at Hailo,\n\nIn the vast landscape of artificial intelligence (AI), one of the most intriguing journeys has been the evolution of AI on the edge. This journey has taken us from classic machine vision to the realms of discriminative AI, enhancive AI, and now, the groundbreaking frontier of generative AI. Each step has brought us closer to a future where intelligent systems seamlessly integrate with our daily lives, offering an immersive experience of not just perception but also creation at the palm of our hand."
  },
  {
    "objectID": "raspi/llm/llm.html#conclusion",
    "href": "raspi/llm/llm.html#conclusion",
    "title": "Small Language Models (SLM)",
    "section": "Conclusion",
    "text": "Conclusion\nThis lab has demonstrated how a Raspberry Pi 5 can be transformed into a potent AI hub capable of running large language models (LLMs) for real-time, on-site data analysis and insights using Ollama and Python. The Raspberry Pi’s versatility and power, coupled with the capabilities of lightweight LLMs like Llama 3.2 and LLaVa-Phi-3-mini, make it an excellent platform for edge computing applications.\nThe potential of running LLMs on the edge extends far beyond simple data processing, as in this lab’s examples. Here are some innovative suggestions for using this project:\n1. Smart Home Automation:\n\nIntegrate SLMs to interpret voice commands or analyze sensor data for intelligent home automation. This could include real-time monitoring and control of home devices, security systems, and energy management, all processed locally without relying on cloud services.\n\n2. Field Data Collection and Analysis:\n\nDeploy SLMs on Raspberry Pi in remote or mobile setups for real-time data collection and analysis. This can be used in agriculture to monitor crop health, in environmental studies for wildlife tracking, or in disaster response for situational awareness and resource management.\n\n3. Educational Tools:\n\nCreate interactive educational tools that leverage SLMs to provide instant feedback, language translation, and tutoring. This can be particularly useful in developing regions with limited access to advanced technology and internet connectivity.\n\n4. Healthcare Applications:\n\nUse SLMs for medical diagnostics and patient monitoring. They can provide real-time analysis of symptoms and suggest potential treatments. This can be integrated into telemedicine platforms or portable health devices.\n\n5. Local Business Intelligence:\n\nImplement SLMs in retail or small business environments to analyze customer behavior, manage inventory, and optimize operations. The ability to process data locally ensures privacy and reduces dependency on external services.\n\n6. Industrial IoT:\n\nIntegrate SLMs into industrial IoT systems for predictive maintenance, quality control, and process optimization. The Raspberry Pi can serve as a localized data processing unit, reducing latency and improving the reliability of automated systems.\n\n7. Autonomous Vehicles:\n\nUse SLMs to process sensory data from autonomous vehicles, enabling real-time decision-making and navigation. This can be applied to drones, robots, and self-driving cars for enhanced autonomy and safety.\n\n8. Cultural Heritage and Tourism:\n\nImplement SLMs to provide interactive and informative cultural heritage sites and museum guides. Visitors can use these systems to get real-time information and insights, enhancing their experience without internet connectivity.\n\n9. Artistic and Creative Projects:\n\nUse SLMs to analyze and generate creative content, such as music, art, and literature. This can foster innovative projects in the creative industries and allow for unique interactive experiences in exhibitions and performances.\n\n10. Customized Assistive Technologies:\n\nDevelop assistive technologies for individuals with disabilities, providing personalized and adaptive support through real-time text-to-speech, language translation, and other accessible tools."
  },
  {
    "objectID": "raspi/llm/llm.html#resources",
    "href": "raspi/llm/llm.html#resources",
    "title": "Small Language Models (SLM)",
    "section": "Resources",
    "text": "Resources\n\n10-Ollama_Python_Library notebook\n20-Ollama_Function_Calling notebook\n30-Function_Calling_with_images notebook\n40-RAG-simple-bee notebook\ncalc_distance_image python script"
  },
  {
    "objectID": "raspi/vlm/vlm.html#why-florence-2-at-the-edge",
    "href": "raspi/vlm/vlm.html#why-florence-2-at-the-edge",
    "title": "Vision-Language Models at the Edge",
    "section": "Why Florence-2 at the Edge?",
    "text": "Why Florence-2 at the Edge?\nFlorence-2 is a vision-language model open-sourced by Microsoft under the MIT license, which significantly advances vision-language models by combining a lightweight architecture with robust capabilities. Thanks to its training on the massive FLD-5B dataset, which contains 126 million images and 5.4 billion visual annotations, it achieves performance comparable to larger models. This makes Florence-2 ideal for deployment at the edge, where power and computational resources are limited.\nIn this tutorial, we will explore how to use Florence-2 for real-time computer vision applications, such as:\n\nImage captioning\nObject detection\nSegmentation\nVisual grounding\n\n\nVisual grounding involves linking textual descriptions to specific regions within an image. This enables the model to understand where particular objects or entities described in a prompt are in the image. For example, if the prompt is “a red car,” the model will identify and highlight the region where the red car is found in the image. Visual grounding is helpful for applications where precise alignment between text and visual content is needed, such as human-computer interaction, image annotation, and interactive AI systems.\n\nIn the tutorial, we will walk through:\n\nSetting up Florence-2 on the Raspberry Pi\nRunning inference tasks such as object detection and captioning\nOptimizing the model to get the best performance from the edge device\nExploring practical, real-world applications with fine-tuning."
  },
  {
    "objectID": "raspi/vlm/vlm.html#florence-2-model-architecture",
    "href": "raspi/vlm/vlm.html#florence-2-model-architecture",
    "title": "Vision-Language Models at the Edge",
    "section": "Florence-2 Model Architecture",
    "text": "Florence-2 Model Architecture\nFlorence-2 utilizes a unified, prompt-based representation to handle various vision-language tasks. The model architecture consists of two main components: an image encoder and a multi-modal transformer encoder-decoder.\n\n\nImage Encoder: The image encoder is based on the DaViT (Dual Attention Vision Transformers) architecture. It converts input images into a series of visual token embeddings. These embeddings serve as the foundational representations of the visual content, capturing both spatial and contextual information about the image.\nMulti-Modal Transformer Encoder-Decoder: Florence-2’s core is the multi-modal transformer encoder-decoder, which combines visual token embeddings from the image encoder with textual embeddings generated by a BERT-like model. This combination allows the model to simultaneously process visual and textual inputs, enabling a unified approach to tasks such as image captioning, object detection, and segmentation.\n\nThe model’s training on the extensive FLD-5B dataset ensures it can effectively handle diverse vision tasks without requiring task-specific modifications. Florence-2 uses textual prompts to activate specific tasks, making it highly flexible and capable of zero-shot generalization. For tasks like object detection or visual grounding, the model incorporates additional location tokens to represent regions within the image, ensuring a precise understanding of spatial relationships.\n\nFlorence-2’s compact architecture and innovative training approach allow it to perform computer vision tasks accurately, even on resource-constrained devices like the Raspberry Pi."
  },
  {
    "objectID": "raspi/vlm/vlm.html#technical-overview",
    "href": "raspi/vlm/vlm.html#technical-overview",
    "title": "Vision-Language Models at the Edge",
    "section": "Technical Overview",
    "text": "Technical Overview\nFlorence-2 introduces several innovative features that set it apart:\n\nArchitecture\n\n\nLightweight Design: Two variants available\n\nFlorence-2-Base: 232 million parameters\nFlorence-2-Large: 771 million parameters\n\nUnified Representation: Handles multiple vision tasks through a single architecture\nDaViT Vision Encoder: Converts images into visual token embeddings\nTransformer-based Multi-modal Encoder-Decoder: Processes combined visual and text embeddings\n\n\n\nTraining Dataset (FLD-5B)\n\n\n126 million unique images\n5.4 billion comprehensive annotations, including:\n\n500M text annotations\n1.3B region-text annotations\n3.6B text-phrase-region annotations\n\nAutomated annotation pipeline using specialist models\nIterative refinement process for high-quality labels\n\n\n\nKey Capabilities\nFlorence-2 excels in multiple vision tasks:\n\nZero-shot Performance\n\nImage Captioning: Achieves 135.6 CIDEr score on COCO\nVisual Grounding: 84.4% recall@1 on Flickr30k\nObject Detection: 37.5 mAP on COCO val2017\nReferring Expression: 67.0% accuracy on RefCOCO\n\n\n\nFine-tuned Performance\n\nCompetitive with specialist models despite the smaller size\nOutperforms larger models in specific benchmarks\nEfficient adaptation to new tasks\n\n\n\n\nPractical Applications\nFlorence-2 can be applied across various domains:\n\nContent Understanding\n\nAutomated image captioning for accessibility\nVisual content moderation\nMedia asset management\n\nE-commerce\n\nProduct image analysis\nVisual search\nAutomated product tagging\n\nHealthcare\n\nMedical image analysis\nDiagnostic assistance\nResearch data processing\n\nSecurity & Surveillance\n\nObject detection and tracking\nAnomaly detection\nScene understanding\n\n\n\n\nComparing Florence-2 with other VLMs\nFlorence-2 stands out from other visual language models due to its impressive zero-shot capabilities. Unlike models like Google PaliGemma, which rely on extensive fine-tuning to adapt to various tasks, Florence-2 works right out of the box, as we will see in this lab. It can also compete with larger models like GPT-4V and Flamingo, which often have many more parameters but only sometimes match Florence-2’s performance. For example, Florence-2 achieves better zero-shot results than Kosmos-2 despite having over twice the parameters.\nIn benchmark tests, Florence-2 has shown remarkable performance in tasks like COCO captioning and referring expression comprehension. It outperformed models like PolyFormer and UNINEXT in object detection and segmentation tasks on the COCO dataset. It is a highly competitive choice for real-world applications where both performance and resource efficiency are crucial."
  },
  {
    "objectID": "raspi/vlm/vlm.html#setup-and-installation",
    "href": "raspi/vlm/vlm.html#setup-and-installation",
    "title": "Vision-Language Models at the Edge",
    "section": "Setup and Installation",
    "text": "Setup and Installation\nOur choice of edge device is the Raspberry Pi 5 (Raspi-5). Its robust platform is equipped with the Broadcom BCM2712, a 2.4GHz quad-core 64-bit Arm Cortex-A76 CPU featuring Cryptographic Extension and enhanced caching capabilities. It boasts a VideoCore VII GPU, dual 4Kp60 HDMI® outputs with HDR, and a 4Kp60 HEVC decoder. Memory options include 4GB and 8GB of high-speed LPDDR4X SDRAM, with 8GB being our choice to run Florence-2. It also features expandable storage via a microSD card slot and a PCIe 2.0 interface for fast peripherals such as M.2 SSDs (Solid State Drives).\n\nFor real applications, SSDs are a better option than SD cards.\n\nWe suggest installing an Active Cooler, a dedicated clip-on cooling solution for Raspberry Pi 5 (Raspi-5), for this lab. It combines an aluminum heatsink with a temperature-controlled blower fan to keep the Raspi-5 operating comfortably under heavy loads, such as running Florense-2.\n\n\nEnvironment configuration\nTo run Microsoft Florense-2 on the Raspberry Pi 5, we’ll need a few libraries:\n\nTransformers:\n\nFlorence-2 uses the transformers library from Hugging Face for model loading and inference. This library provides the architecture for working with pre-trained vision-language models, making it easy to perform tasks like image captioning, object detection, and more. Essentially, transformers helps in interacting with the model, processing input prompts, and obtaining outputs.\n\nPyTorch:\n\nPyTorch is a deep learning framework that provides the infrastructure needed to run the Florence-2 model, which includes tensor operations, GPU acceleration (if a GPU is available), and model training/inference functionalities. The Florence-2 model is trained in PyTorch, and we need it to leverage its functions, layers, and computation capabilities to perform inferences on the Raspberry Pi.\n\nTimm (PyTorch Image Models):\n\nFlorence-2 uses timm to access efficient implementations of vision models and pre-trained weights. Specifically, the timm library is utilized for the image encoder part of Florence-2, particularly for managing the DaViT architecture. It provides model definitions and optimized code for common vision tasks and allows the easy integration of different backbones that are lightweight and suitable for edge devices.\n\nEinops:\n\nEinops is a library for flexible and powerful tensor operations. It makes it easy to reshape and manipulate tensor dimensions, which is especially important for the multi-modal processing done in Florence-2. Vision-language models like Florence-2 often need to rearrange image data, text embeddings, and visual embeddings to align correctly for the transformer blocks, and einops simplifies these complex operations, making the code more readable and concise.\n\n\nIn short, these libraries enable different essential components of Florence-2:\n\nTransformers and PyTorch are needed to load the model and run the inference.\nTimm is used to access and efficiently implement the vision encoder.\nEinops helps reshape data, facilitating the integration of visual and text features.\n\nAll these components work together to help Florence-2 run seamlessly on our Raspberry Pi, allowing it to perform complex vision-language tasks relatively quickly.\nConsidering that the Raspberry Pi already has its OS installed, let’s use SSH to reach it from another computer:\nssh mjrovai@raspi-5.local\nAnd check the IP allocated to it:\nhostname -I\n192.168.4.209\n\nUpdating the Raspberry Pi\nFirst, ensure your Raspberry Pi is up to date:\nsudo apt update\nsudo apt upgrade -y\nInitial setup for using PIP:\nsudo apt install python3-pip\nsudo rm /usr/lib/python3.11/EXTERNALLY-MANAGED\npip3 install --upgrade pip\nInstall Dependencies\nsudo apt-get install libjpeg-dev libopenblas-dev libopenmpi-dev libomp-dev\nLet’s set up and activate a Virtual Environment for working with Florence-2:\npython3 -m venv ~/florence\nsource ~/florence/bin/activate\nInstall PyTorch\npip3 install setuptools numpy Cython\npip3 install requests\npip3 install torch torchvision --index-url https://download.pytorch.org/whl/cpu\npip3 install torchaudio --index-url https://download.pytorch.org/whl/cpu\nLet’s verify that PyTorch is correctly installed:\n\nInstall Transformers, Timm and Einops:\npip3 install transformers\npip3 install timm einops\nInstall the model:\npip3 install autodistill-florence-2\nJupyter Notebook and Python libraries\nInstalling a Jupyter Notebook to run and test our Python scripts is possible.\npip3 install jupyter\npip3 install numpy Pillow matplotlib\njupyter notebook --generate-config\n\n\nTesting the installation\nRunning the Jupyter Notebook on the remote computer\njupyter notebook --ip=192.168.4.209 --no-browser\nRunning the above command on the SSH terminal, we can see the local URL address to open the notebook:\n\nThe notebook with the code used on this initial test can be found on the Lab GitHub:\n\n10-florence2_test.ipynb\n\nWe can access it on the remote computer by entering the Raspberry Pi’s IP address and the provided token in a web browser ( copy the entire URL from the terminal).\nFrom the Home page, create a new notebook [Python 3 (ipykernel) ] and copy and paste the example code from Hugging Face Hub.\nThe code is designed to run Florence-2 on a given image to perform object detection. It loads the model, processes an image and a prompt, and then generates a response to identify and describe the objects in the image.\n\nThe processor helps prepare text and image inputs.\nThe model takes the processed inputs to generate a meaningful response.\nThe post-processing step refines the generated output into a more interpretable form, like bounding boxes for detected objects.\n\n\nThis workflow leverages the versatility of Florence-2 to handle vision-language tasks and is implemented efficiently using PyTorch, Transformers, and related image-processing tools.\n\nimport requests\nfrom PIL import Image\nimport torch\nfrom transformers import AutoProcessor, AutoModelForCausalLM \n\ndevice = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\ntorch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32\n\nmodel = AutoModelForCausalLM.from_pretrained(\"microsoft/Florence-2-base\",\n                                             torch_dtype=torch_dtype, \n                                             trust_remote_code=True).to(device)\nprocessor = AutoProcessor.from_pretrained(\"microsoft/Florence-2-base\", \n                                          trust_remote_code=True)\n\nprompt = \"&lt;OD&gt;\"\n\nurl = \"https://huggingface.co/datasets/huggingface/documentation-\nimages/resolve/main/transformers/tasks/car.jpg?download=true\"\nimage = Image.open(requests.get(url, stream=True).raw)\n\ninputs = processor(text=prompt, images=image, return_tensors=\"pt\").to(\n  device, torch_dtype)\n\ngenerated_ids = model.generate(\n    input_ids=inputs[\"input_ids\"],\n    pixel_values=inputs[\"pixel_values\"],\n    max_new_tokens=1024,\n    do_sample=False,\n    num_beams=3,\n)\ngenerated_text = processor.batch_decode(generated_ids, skip_special_tokens=False)[0]\n\nparsed_answer = processor.post_process_generation(generated_text, task=\"&lt;OD&gt;\", \n                                                  image_size=(image.width, \n                                                              image.height))\n\nprint(parsed_answer)\nLet’s break down the provided code step by step:\n\n1. Importing Required Libraries\nimport requests\nfrom PIL import Image\nimport torch\nfrom transformers import AutoProcessor, AutoModelForCausalLM \n\nrequests: Used to make HTTP requests. In this case, it downloads an image from a URL.\nPIL (Pillow): Provides tools for manipulating images. Here, it’s used to open the downloaded image.\ntorch: PyTorch is imported to handle tensor operations and determine the hardware availability (CPU or GPU).\ntransformers: This module provides easy access to Florence-2 by using AutoProcessor and AutoModelForCausalLM to load pre-trained models and process inputs.\n\n\n\n2. Determining the Device and Data Type\ndevice = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\ntorch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32\n\nDevice Setup: The code checks if a CUDA-enabled GPU is available (torch.cuda.is_available()). The device is set to “cuda:0” if a GPU is available. Otherwise, it defaults to \"cpu\" (our case here).\nData Type Setup: If a GPU is available, torch.float16 is chosen, which uses half-precision floats to speed up processing and reduce memory usage. On the CPU, it defaults to torch.float32 to maintain compatibility.\n\n\n\n3. Loading the Model and Processor\nmodel = AutoModelForCausalLM.from_pretrained(\"microsoft/Florence-2-base\", \n                                             torch_dtype=torch_dtype,\n                                             trust_remote_code=True).to(device)\nprocessor = AutoProcessor.from_pretrained(\"microsoft/Florence-2-base\",\n                                          trust_remote_code=True)\n\nModel Initialization:\n\nAutoModelForCausalLM.from_pretrained() loads the pre-trained Florence-2 model from Microsoft’s repository on Hugging Face. The torch_dtype is set according to the available hardware (GPU/CPU), and trust_remote_code=True allows the use of any custom code that might be provided with the model.\n.to(device) moves the model to the appropriate device (either CPU or GPU). In our case, it will be set to CPU.\n\nProcessor Initialization:\n\nAutoProcessor.from_pretrained() loads the processor for Florence-2. The processor is responsible for transforming text and image inputs into a format the model can work with (e.g., encoding text, normalizing images, etc.).\n\n\n\n\n\n4. Defining the Prompt\nprompt = \"&lt;OD&gt;\"\n\nPrompt Definition: The string \"&lt;OD&gt;\" is used as a prompt. This refers to “Object Detection”, instructing the model to detect objects on the image.\n\n\n5. Downloading and Loading the Image\nurl = \"https://huggingface.co/datasets/huggingface/documentation-\\\nimages/resolve/main/transformers/tasks/car.jpg?download=true\"\nimage = Image.open(requests.get(url, stream=True).raw)\n\nDownloading the Image: The requests.get() function fetches the image from the specified URL. The stream=True parameter ensures the image is streamed rather than downloaded completely at once.\nOpening the Image: Image.open() opens the image so the model can process it.\n\n\n\n6. Processing Inputs\ninputs = processor(text=prompt, images=image, return_tensors=\"pt\").to(device, \n                                                                      torch_dtype)\n\nProcessing Input Data: The processor() function processes the text (prompt) and the image (image). The return_tensors=\"pt\" argument converts the processed data into PyTorch tensors, which are necessary for inputting data into the model.\nMoving Inputs to Device: .to(device, torch_dtype) moves the inputs to the correct device (CPU or GPU) and assigns the appropriate data type.\n\n\n\n\n7. Generating the Output\ngenerated_ids = model.generate(\n    input_ids=inputs[\"input_ids\"],\n    pixel_values=inputs[\"pixel_values\"],\n    max_new_tokens=1024,\n    do_sample=False,\n    num_beams=3,\n)\n\nModel Generation: model.generate() is used to generate the output based on the input data.\n\ninput_ids: Represents the tokenized form of the prompt.\npixel_values: Contains the processed image data.\nmax_new_tokens=1024: Specifies the maximum number of new tokens to be generated in the response. This limits the response length.\ndo_sample=False: Disables sampling; instead, the generation uses deterministic methods (beam search).\nnum_beams=3: Enables beam search with three beams, which improves output quality by considering multiple possibilities during generation.\n\n\n\n8. Decoding the Generated Text\ngenerated_text = processor.batch_decode(generated_ids, skip_special_tokens=False)[0]\n\nBatch Decode: processor.batch_decode() decodes the generated IDs (tokens) into readable text. The skip_special_tokens=False parameter means that the output will include any special tokens that may be part of the response.\n\n\n\n9. Post-processing the Generation\nparsed_answer = processor.post_process_generation(generated_text, task=\"&lt;OD&gt;\", \n                                                  image_size=(image.width, \n                                                              image.height))\n\nPost-Processing: processor.post_process_generation() is called to process the generated text further, interpreting it based on the task (\"&lt;OD&gt;\" for object detection) and the size of the image.\nThis function extracts specific information from the generated text, such as bounding boxes for detected objects, making the output more useful for visual tasks.\n\n\n\n10. Printing the Output\nprint(parsed_answer)\n\nFinally, print(parsed_answer) displays the output, which could include object detection results, such as bounding box coordinates and labels for the detected objects in the image.\n\n\n\nResult\nRunning the code, we get as the Parsed Answer:\n{'&lt;OD&gt;': {'bboxes': [[34.23999786376953, 160.0800018310547, 597.4400024414062, \n371.7599792480469], [272.32000732421875, 241.67999267578125, 303.67999267578125, \n247.4399871826172], [454.0799865722656, 276.7200012207031, 553.9199829101562, \n370.79998779296875], [96.31999969482422, 280.55999755859375, 198.0800018310547, \n371.2799987792969]], 'labels': ['car', 'door handle', 'wheel', 'wheel']}}\nFirst, Let’s inspect the image:\nimport matplotlib.pyplot as plt\nplt.figure(figsize=(8, 8))\nplt.imshow(image)\nplt.axis('off')\nplt.show()\n\nBy the Object Detection result, we can see that:\n'labels': ['car', 'door handle', 'wheel', 'wheel']\nIt seems that at least a few objects were detected. we can also implement a code to draw the bounding boxes in the find objects:\ndef plot_bbox(image, data):\n   # Create a figure and axes\n    fig, ax = plt.subplots()\n\n    # Display the image\n    ax.imshow(image)\n\n    # Plot each bounding box\n    for bbox, label in zip(data['bboxes'], data['labels']):\n        # Unpack the bounding box coordinates\n        x1, y1, x2, y2 = bbox\n        # Create a Rectangle patch\n        rect = patches.Rectangle((x1, y1), x2-x1, y2-y1, linewidth=1, \n                                 edgecolor='r', facecolor='none')\n        # Add the rectangle to the Axes\n        ax.add_patch(rect)\n        # Annotate the label\n        plt.text(x1, y1, label, color='white', fontsize=8, \n                 bbox=dict(facecolor='red', alpha=0.5))\n\n    # Remove the axis ticks and labels\n    ax.axis('off')\n\n    # Show the plot\n    plt.show()\n\nBox (x0, y0, x1, y1): Location tokens correspond to the top-left and bottom-right corners of a box.\n\nAnd running\nplot_bbox(image, parsed_answer['&lt;OD&gt;'])\nWe get:"
  },
  {
    "objectID": "raspi/vlm/vlm.html#florence-2-tasks",
    "href": "raspi/vlm/vlm.html#florence-2-tasks",
    "title": "Vision-Language Models at the Edge",
    "section": "Florence-2 Tasks",
    "text": "Florence-2 Tasks\nFlorence-2 is designed to perform a variety of computer vision and vision-language tasks through prompts. These tasks can be activated by providing a specific textual prompt to the model, as we saw with &lt;OD&gt; (Object Detection).\nFlorence-2’s versatility comes from combining these prompts, allowing us to guide the model’s behavior to perform specific vision tasks. Changing the prompt allows us to adapt Florence-2 to different tasks without needing task-specific modifications in the architecture. This capability directly results from Florence-2’s unified model architecture and large-scale multi-task training on the FLD-5B dataset.\nHere are some of the key tasks that Florence-2 can perform, along with example prompts:\n\n1. Object Detection (OD)\n\nPrompt: \"&lt;OD&gt;\"\nDescription: Identifies objects in an image and provides bounding boxes for each detected object. This task is helpful for applications like visual inspection, surveillance, and general object recognition.\n\n\n\n2. Image Captioning\n\nPrompt: \"&lt;CAPTION&gt;\"\nDescription: Generates a textual description for an input image. This task helps the model describe what is happening in the image, providing a human-readable caption for content understanding.\n\n\n\n3. Detailed Captioning\n\nPrompt: \"&lt;DETAILED_CAPTION&gt;\"\nDescription: Generates a more detailed caption with more nuanced information about the scene, such as the objects present and their relationships.\n\n\n\n4. Visual Grounding\n\nPrompt: \"&lt;CAPTION_TO_PHRASE_GROUNDING&gt;\"\nDescription: Links a textual description to specific regions in an image. For example, given a prompt like “a green car,” the model highlights where the red car is in the image. This is useful for human-computer interaction, where you must find specific objects based on text.\n\n\n\n5. Segmentation\n\nPrompt: \"&lt;REFERRING_EXPRESSION_SEGMENTATION&gt;\"\nDescription: Performs segmentation based on a referring expression, such as “the blue cup.” The model identifies and segments the specific region containing the object mentioned in the prompt (all related pixels).\n\n\n\n6. Dense Region Captioning\n\nPrompt: \"&lt;DENSE_REGION_CAPTION&gt;\"\nDescription: Provides captions for multiple regions within an image, offering a detailed breakdown of all visible areas, including different objects and their relationships.\n\n\n\n7. OCR with Region\n\nPrompt: \"&lt;OCR_WITH_REGION&gt;\"\nDescription: Performs Optical Character Recognition (OCR) on an image and provides bounding boxes for the detected text. This is useful for extracting and locating textual information in images, such as reading signs, labels, or other forms of text in images.\n\n\n\n8. Phrase Grounding for Specific Expressions\n\nPrompt: \"&lt;CAPTION_TO_PHRASE_GROUNDING&gt;\" along with a specific expression, such as \"a wine glass\".\nDescription: Locates the area in the image that corresponds to a specific textual phrase. This task allows for identifying particular objects or elements when prompted with a word or keyword.\n\n\n\n9. Open Vocabulary Object Detection\n\nPrompt: \"&lt;OPEN_VOCABULARY_OD&gt;\"\nDescription: The model can detect objects without being restricted to a predefined list of classes, making it helpful in recognizing a broader range of items based on general visual understanding."
  },
  {
    "objectID": "raspi/vlm/vlm.html#exploring-computer-vision-and-vision-language-tasks",
    "href": "raspi/vlm/vlm.html#exploring-computer-vision-and-vision-language-tasks",
    "title": "Vision-Language Models at the Edge",
    "section": "Exploring computer vision and vision-language tasks",
    "text": "Exploring computer vision and vision-language tasks\nFor exploration, all codes can be found on the GitHub:\n\n20-florence_2.ipynb\n\nLet’s use a couple of images created by Dall-E and upload them to the Rasp-5 (FileZilla can be used for that). The images will be saved on a sub-folder named images :\ndogs_cats = Image.open('./images/dogs-cats.jpg')\ntable = Image.open('./images/table.jpg')\n\nLet’s create a function to facilitate our exploration and to keep track of the latency of the model for different tasks:\ndef run_example(task_prompt, text_input=None, image=None):\n    start_time = time.perf_counter()  # Start timing\n    if text_input is None:\n        prompt = task_prompt\n    else:\n        prompt = task_prompt + text_input\n    inputs = processor(text=prompt, images=image, \n                       return_tensors=\"pt\").to(device)\n    generated_ids = model.generate(\n      input_ids=inputs[\"input_ids\"],\n      pixel_values=inputs[\"pixel_values\"],\n      max_new_tokens=1024,\n      early_stopping=False,\n      do_sample=False,\n      num_beams=3,\n    )\n    generated_text = processor.batch_decode(generated_ids, \n                                            skip_special_tokens=False)[0]\n    parsed_answer = processor.post_process_generation(\n        generated_text,\n        task=task_prompt,\n        image_size=(image.width, image.height)\n    )\n    \n    end_time = time.perf_counter()  # End timing\n    elapsed_time = end_time - start_time  # Calculate elapsed time\n    print(f\" \\n[INFO] ==&gt; Florence-2-base ({task_prompt}), \n          took {elapsed_time:.1f} seconds to execute.\\n\")\n    \n    return parsed_answer\n\nCaption\n1. Dogs and Cats\nrun_example(task_prompt='&lt;CAPTION&gt;',image=dogs_cats)\n[INFO] ==&gt; Florence-2-base (&lt;CAPTION&gt;), took 16.1 seconds to execute.\n\n{'&lt;CAPTION&gt;': 'A group of dogs and cats sitting in a garden.'}\n2. Table\nrun_example(task_prompt='&lt;CAPTION&gt;',image=table)\n[INFO] ==&gt; Florence-2-base (&lt;CAPTION&gt;), took 16.5 seconds to execute.\n\n{'&lt;CAPTION&gt;': 'A wooden table topped with a plate of fruit and a glass of wine.'}\n\n\nDETAILED_CAPTION\n1. Dogs and Cats\nrun_example(task_prompt='&lt;DETAILED_CAPTION&gt;',image=dogs_cats)\n[INFO] ==&gt; Florence-2-base (&lt;DETAILED_CAPTION&gt;), took 25.5 seconds to execute.\n\n{'&lt;DETAILED_CAPTION&gt;': 'The image shows a group of cats and dogs sitting on top of a\nlush green field, surrounded by plants with flowers, trees, and a house in the \nbackground. The sky is visible above them, creating a peaceful atmosphere.'}\n2. Table\nrun_example(task_prompt='&lt;DETAILED_CAPTION&gt;',image=table)\n[INFO] ==&gt; Florence-2-base (&lt;DETAILED_CAPTION&gt;), took 26.8 seconds to execute.\n\n{'&lt;DETAILED_CAPTION&gt;': 'The image shows a wooden table with a bottle of wine and a \nglass of wine on it, surrounded by a variety of fruits such as apples, oranges, and \ngrapes. In the background, there are chairs, plants, trees, and a house, all slightly \nblurred.'}\n\n\nMORE_DETAILED_CAPTION\n1. Dogs and Cats\nrun_example(task_prompt='&lt;MORE_DETAILED_CAPTION&gt;',image=dogs_cats)\n[INFO] ==&gt; Florence-2-base (&lt;MORE_DETAILED_CAPTION&gt;), took 49.8 seconds to execute.\n\n{'&lt;MORE_DETAILED_CAPTION&gt;': 'The image shows a group of four cats and a dog in a garden. \nThe garden is filled with colorful flowers and plants, and there is a pathway leading up \nto a house in the background. The main focus of the image is a large German Shepherd dog \nstanding on the left side of the garden, with its tongue hanging out and its mouth open, \nas if it is panting or panting. On the right side, there are two smaller cats, one orange \nand one gray, sitting on the grass. In the background, there is another golden retriever \ndog sitting and looking at the camera. The sky is blue and the sun is shining, creating a \nwarm and inviting atmosphere.'}\n2. Table\nrun_example(task_prompt='&lt; MORE_DETAILED_CAPTION&gt;',image=table)\nINFO] ==&gt; Florence-2-base (&lt;MORE_DETAILED_CAPTION&gt;), took 32.4 seconds to execute.\n\n{'&lt;MORE_DETAILED_CAPTION&gt;': 'The image shows a wooden table with a wooden tray on it. On \nthe tray, there are various fruits such as grapes, oranges, apples, and grapes. There is \nalso a bottle of red wine on the table. The background shows a garden with trees and a \nhouse. The overall mood of the image is peaceful and serene.'}\n\nWe can note that the more detailed the caption task, the longer the latency and the possibility of mistakes (like “The image shows a group of four cats and a dog in a garden”, instead of two dogs and three cats).\n\n\n\nOD - Object Detection\nWe can run the same previous function for object detection using the prompt &lt;OD&gt;.\ntask_prompt = '&lt;OD&gt;'\nresults = run_example(task_prompt,image=dogs_cats)\nprint(results)\nLet’s see the result:\n[INFO] ==&gt; Florence-2-base (&lt;OD&gt;), took 20.9 seconds to execute.\n\n{'&lt;OD&gt;': {'bboxes': [[737.7920532226562, 571.904052734375, 1022.4640502929688, \n980.4800415039062], [0.5120000243186951, 593.4080200195312, 211.4560089111328, \n991.7440185546875], [445.9520263671875, 721.4080200195312, 680.4480590820312, \n850.4320678710938], [39.42400360107422, 91.64800262451172, 491.0080261230469, \n933.3760375976562], [570.8800048828125, 184.83201599121094, 974.3360595703125, \n782.8480224609375]], 'labels': ['cat', 'cat', 'cat', 'dog', 'dog']}}\nOnly by the labels ['cat,' 'cat,' 'cat,' 'dog,' 'dog'] is it possible to see that the main objects in the image were captured. Let’s apply the function used before to draw the bounding boxes:\nplot_bbox(dogs_cats, results['&lt;OD&gt;'])\n\nLet’s also do it with the Table image:\ntask_prompt = '&lt;OD&gt;'\nresults = run_example(task_prompt,image=table)\nplot_bbox(table, results['&lt;OD&gt;'])\n[INFO] ==&gt; Florence-2-base (&lt;OD&gt;), took 40.8 seconds to execute.\n\n\n\nDENSE_REGION_CAPTION\nIt is possible to mix the classic Object Detection with the Caption task in specific sub-regions of the image:\ntask_prompt = '&lt;DENSE_REGION_CAPTION&gt;'\n\nresults = run_example(task_prompt,image=dogs_cats)\nplot_bbox(dogs_cats, results['&lt;DENSE_REGION_CAPTION&gt;'])\n\nresults = run_example(task_prompt,image=table)\nplot_bbox(table, results['&lt;DENSE_REGION_CAPTION&gt;'])\n\n\n\nCAPTION_TO_PHRASE_GROUNDING\nWith this task, we can enter with a caption, such as “a wine glass”, “a wine bottle,” or “a half orange,” and Florence-2 will localize the object in the image:\ntask_prompt = '&lt;CAPTION_TO_PHRASE_GROUNDING&gt;'\n\nresults = run_example(task_prompt, text_input=\"a wine bottle\",image=table)\nplot_bbox(table, results['&lt;CAPTION_TO_PHRASE_GROUNDING&gt;'])\n\nresults = run_example(task_prompt, text_input=\"a wine glass\",image=table)\nplot_bbox(table, results['&lt;CAPTION_TO_PHRASE_GROUNDING&gt;'])\n\nresults = run_example(task_prompt, text_input=\"a half orange\",image=table)\nplot_bbox(table, results['&lt;CAPTION_TO_PHRASE_GROUNDING&gt;'])\n\n[INFO] ==&gt; Florence-2-base (&lt;CAPTION_TO_PHRASE_GROUNDING&gt;), took 15.7 seconds to execute\neach task.\n\n\nCascade Tasks\nWe can also enter the image caption as the input text to push Florence-2 to find more objects:\ntask_prompt = '&lt;CAPTION&gt;'\nresults = run_example(task_prompt,image=dogs_cats)\ntext_input = results[task_prompt]\ntask_prompt = '&lt;CAPTION_TO_PHRASE_GROUNDING&gt;'\nresults = run_example(task_prompt, text_input,image=dogs_cats)\nplot_bbox(dogs_cats, results['&lt;CAPTION_TO_PHRASE_GROUNDING&gt;'])\nChanging the task_prompt among &lt;CAPTION,&gt; &lt;DETAILED_CAPTION&gt; and &lt;MORE_DETAILED_CAPTION&gt;, we will get more objects in the image.\n\n\n\nOPEN_VOCABULARY_DETECTION\n&lt;OPEN_VOCABULARY_DETECTION&gt; allows Florence-2 to detect recognizable objects in an image without relying on a predefined list of categories, making it a versatile tool for identifying various items that may not have been explicitly labeled during training. Unlike &lt;CAPTION_TO_PHRASE_GROUNDING&gt;, which requires a specific text phrase to locate and highlight a particular object in an image, &lt;OPEN_VOCABULARY_DETECTION&gt; performs a broad scan to find and classify all objects present.\nThis makes &lt;OPEN_VOCABULARY_DETECTION&gt; particularly useful for applications where you need a comprehensive overview of everything in an image without prior knowledge of what to expect. Enter with a text describing specific objects not previously detected, resulting in their detection. For example:\ntask_prompt = '&lt;OPEN_VOCABULARY_DETECTION&gt;'\ntext = [\"a house\", \"a tree\", \"a standing cat at the left\", \n        \"a sleeping cat on the ground\", \"a standing cat at the right\", \n        \"a yellow cat\"]\nfor txt in text:\n    results = run_example(task_prompt, text_input=txt,image=dogs_cats)\n    bbox_results  = convert_to_od_format(results['&lt;OPEN_VOCABULARY_DETECTION&gt;'])\n    plot_bbox(dogs_cats, bbox_results)\n\n[INFO] ==&gt; Florence-2-base (&lt;OPEN_VOCABULARY_DETECTION&gt;), took 15.1 seconds \nto execute each task.\n\nNote: Trying to use Florence-2 to find objects that were not found can leads to mistakes (see exaamples on the Notebook).\n\n\n\nReferring expression segmentation\nWe can also segment a specific object in the image and give its description (caption), such as “a wine bottle” on the table image or “a German Sheppard” on the dogs_cats.\nReferring expression segmentation results format: {'&lt;REFERRING_EXPRESSION_SEGMENTATION&gt;': {'Polygons': [[[polygon]], ...], 'labels': ['', '', ...]}}, one object is represented by a list of polygons. each polygon is [x1, y1, x2, y2, ..., xn, yn].\n\nPolygon (x1, y1, …, xn, yn): Location tokens represent the vertices of a polygon in clockwise order.\n\nSo, let’s first create a function to plot the segmentation:\nfrom PIL import Image, ImageDraw, ImageFont\nimport copy\nimport random\nimport numpy as np\ncolormap = ['blue','orange','green','purple','brown','pink','gray','olive',\n    'cyan','red','lime','indigo','violet','aqua','magenta','coral','gold',\n    'tan','skyblue']\n\ndef draw_polygons(image, prediction, fill_mask=False):\n    \"\"\"\n    Draws segmentation masks with polygons on an image.\n\n    Parameters:\n    - image_path: Path to the image file.\n    - prediction: Dictionary containing 'polygons' and 'labels' keys.\n                  'polygons' is a list of lists, each containing vertices \n                  of a polygon.\n                  'labels' is a list of labels corresponding to each polygon.\n    - fill_mask: Boolean indicating whether to fill the polygons with color.\n    \"\"\"\n    # Load the image\n\n    draw = ImageDraw.Draw(image)\n\n\n    # Set up scale factor if needed (use 1 if not scaling)\n    scale = 1\n\n    # Iterate over polygons and labels\n    for polygons, label in zip(prediction['polygons'], prediction['labels']):\n        color = random.choice(colormap)\n        fill_color = random.choice(colormap) if fill_mask else None\n\n        for _polygon in polygons:\n            _polygon = np.array(_polygon).reshape(-1, 2)\n            if len(_polygon) &lt; 3:\n                print('Invalid polygon:', _polygon)\n                continue\n\n            _polygon = (_polygon * scale).reshape(-1).tolist()\n\n            # Draw the polygon\n            if fill_mask:\n                draw.polygon(_polygon, outline=color, fill=fill_color)\n            else:\n                draw.polygon(_polygon, outline=color)\n\n            # Draw the label text\n            draw.text((_polygon[0] + 8, _polygon[1] + 2), label, fill=color)\n    \n    # Save or display the image\n    #image.show()  # Display the image\n    display(image)\nNow we can run the functions:\ntask_prompt = '&lt;REFERRING_EXPRESSION_SEGMENTATION&gt;'\n\nresults = run_example(task_prompt, text_input=\"a wine bottle\",image=table)\noutput_image = copy.deepcopy(table)\ndraw_polygons(output_image, \n              results['&lt;REFERRING_EXPRESSION_SEGMENTATION&gt;'], \n              fill_mask=True)\n\nresults = run_example(task_prompt, text_input=\"a german sheppard\",image=dogs_cats)\noutput_image = copy.deepcopy(dogs_cats)\ndraw_polygons(output_image, \n              results['&lt;REFERRING_EXPRESSION_SEGMENTATION&gt;'], \n              fill_mask=True)\n\n[INFO] ==&gt; Florence-2-base (&lt;REFERRING_EXPRESSION_SEGMENTATION&gt;), \ntook 207.0 seconds to execute each task.\n\n\nRegion to Segmentation\nWith this task, it is also possible to give the object coordinates in the image to segment it. The input format is '&lt;loc_x1&gt;&lt;loc_y1&gt;&lt;loc_x2&gt;&lt;loc_y2&gt;', [x1, y1, x2, y2] , which is the quantized coordinates in [0, 999].\nFor example, when running the code:\ntask_prompt = '&lt;CAPTION_TO_PHRASE_GROUNDING&gt;'\nresults = run_example(task_prompt, text_input=\"a half orange\",image=table)\nresults\nThe results were:\n{'&lt;CAPTION_TO_PHRASE_GROUNDING&gt;': {'bboxes': [[343.552001953125,\n    689.6640625,\n    530.9440307617188,\n    873.9840698242188]],\n  'labels': ['a half']}}\nUsing the bboxes rounded coordinates:\ntask_prompt = '&lt;REGION_TO_SEGMENTATION&gt;'\nresults = run_example(task_prompt, \n                      text_input=\"&lt;loc_343&gt;&lt;loc_690&gt;&lt;loc_531&gt;&lt;loc_874&gt;\",\n                      image=table)\noutput_image = copy.deepcopy(table)\ndraw_polygons(output_image, results['&lt;REGION_TO_SEGMENTATION&gt;'], fill_mask=True)  \nWe got the segmentation of the object on those coordinates (Latency: 83 seconds):\n\n\n\nRegion to Texts\nWe can also give the region (coordinates and ask for a caption):\ntask_prompt = '&lt;REGION_TO_CATEGORY&gt;'\nresults = run_example(task_prompt, text_input=\"&lt;loc_343&gt;&lt;loc_690&gt;&lt;loc_531&gt;\n                      &lt;loc_874&gt;\",image=table)\nresults\n[INFO] ==&gt; Florence-2-base (&lt;REGION_TO_CATEGORY&gt;), took 14.3 seconds to execute.\n{'&lt;REGION_TO_CATEGORY&gt;': 'orange&lt;loc_343&gt;&lt;loc_690&gt;&lt;loc_531&gt;&lt;loc_874&gt;'}\nThe model identified an orange in that region. Let’s ask for a description:\ntask_prompt = '&lt;REGION_TO_DESCRIPTION&gt;'\nresults = run_example(task_prompt, text_input=\"&lt;loc_343&gt;&lt;loc_690&gt;&lt;loc_531&gt;\n                      &lt;loc_874&gt;\",image=table)\nresults\n[INFO] ==&gt; Florence-2-base (&lt;REGION_TO_CATEGORY&gt;), took 14.6 seconds to execute.\n\n{'&lt;REGION_TO_CATEGORY&gt;': 'orange&lt;loc_343&gt;&lt;loc_690&gt;&lt;loc_531&gt;&lt;loc_874&gt;'}\nIn this case, the description did not provide more details, but it could. Try another example.\n\n\nOCR\nWith Florence-2, we can perform Optical Character Recognition (OCR) on an image, getting what is written on it (task_prompt = '&lt;OCR&gt;' and also get the bounding boxes (location) for the detected text (ask_prompt = '&lt;OCR_WITH_REGION&gt;'). Those tasks can help extract and locate textual information in images, such as reading signs, labels, or other forms of text in images.\nLet’s upload a flyer from a talk in Brazil to Raspi. Let’s test works in another language, here Portuguese):\nflayer = Image.open('./images/embarcados.jpg')\n# Display the image\nplt.figure(figsize=(8, 8))\nplt.imshow(flayer)\nplt.axis('off')\n#plt.title(\"Image\")\nplt.show()\n\nLet’s examine the image with '&lt;MORE_DETAILED_CAPTION&gt;' :\n[INFO] ==&gt; Florence-2-base (&lt;MORE_DETAILED_CAPTION&gt;), took 85.2 seconds to execute.\n\n{'&lt;MORE_DETAILED_CAPTION&gt;': 'The image is a promotional poster for an event called \n\"Machine Learning Embarcados\" hosted by Marcelo Roval. The poster has a black \nbackground with white text. On the left side of the poster, there is a logo of a \ncoffee cup with the text \"Café Com Embarcados\" above it. Below the logo, it says \n\"25 de Setembro as 17th\" which translates to \"25th of September as 17\" in English. \n\\n\\nOn the right side, there aretwo smaller text boxes with the names of the \nparticipants and their names. The first text box reads \"Democratizando a Inteligência \nArtificial para Paises em Desenvolvimento\" and the second text box says \"Toda quarta-feira\" which is Portuguese for \"Transmissão via in Portuguese\".\\n\\nIn the center of\nthe image, there has a photo of Marcelo, a man with a beard and glasses, smiling at\nthe camera. He is wearing a white hard hat and a white shirt. The text boxes are \nin orange and yellow colors.'}\nThe description is very accurate. Let’s get to the more important words with the task OCR:\ntask_prompt = '&lt;OCR&gt;'\nrun_example(task_prompt,image=flayer)\n[INFO] ==&gt; Florence-2-base (&lt;OCR&gt;), took 37.7 seconds to execute.\n{'&lt;OCR&gt;': 'Machine LearningCafécomEmbarcadoEmbarcadosDemocratizando a \nInteligênciaArtificial para Paises em25 de Setembro ás 17hDesenvolvimentoToda quarta-\nfeiraMarcelo RovalProfessor na UNIFIEI eTransmissão viainCo-Director do TinyML4D'}\nLet’s locate the words in the flyer:\ntask_prompt = '&lt;OCR_WITH_REGION&gt;'\nresults = run_example(task_prompt,image=flayer)\nLet’s also create a function to draw bounding boxes around the detected words:\ndef draw_ocr_bboxes(image, prediction):\n    scale = 1\n    draw = ImageDraw.Draw(image)\n    bboxes, labels = prediction['quad_boxes'], prediction['labels']\n    for box, label in zip(bboxes, labels):\n        color = random.choice(colormap)\n        new_box = (np.array(box) * scale).tolist()\n        draw.polygon(new_box, width=3, outline=color)\n        draw.text((new_box[0]+8, new_box[1]+2),\n                    \"{}\".format(label),\n                    align=\"right\",\n\n                    fill=color)\n    display(image)\noutput_image = copy.deepcopy(flayer)\ndraw_ocr_bboxes(output_image, results['&lt;OCR_WITH_REGION&gt;'])\n\nWe can inspect the detected words:\nresults['&lt;OCR_WITH_REGION&gt;']['labels']\n'&lt;/s&gt;Machine Learning',\n 'Café',\n 'com',\n 'Embarcado',\n 'Embarcados',\n 'Democratizando a Inteligência',\n 'Artificial para Paises em',\n '25 de Setembro ás 17h',\n 'Desenvolvimento',\n 'Toda quarta-feira',\n 'Marcelo Roval',\n 'Professor na UNIFIEI e',\n 'Transmissão via',\n 'in',\n 'Co-Director do TinyML4D']"
  },
  {
    "objectID": "raspi/vlm/vlm.html#latency-summary",
    "href": "raspi/vlm/vlm.html#latency-summary",
    "title": "Vision-Language Models at the Edge",
    "section": "Latency Summary",
    "text": "Latency Summary\nThe latency observed for different tasks using Florence-2 on the Raspberry Pi (Raspi-5) varied depending on the complexity of the task:\n\nImage Captioning: It took approximately 16-17 seconds to generate a caption for an image.\nDetailed Captioning: Increased latency to around 25-27 seconds, requiring generating more nuanced scene descriptions.\nMore Detailed Captioning: It took about 32-50 seconds, and the latency increased as the description grew more complex.\nObject Detection: It took approximately 20-41 seconds, depending on the image’s complexity and the number of detected objects.\nVisual Grounding: Approximately 15-16 seconds to localize specific objects based on textual prompts.\nOCR (Optical Character Recognition): Extracting text from an image took around 37-38 seconds.\nSegmentation and Region to Segmentation: Segmentation tasks took considerably longer, with a latency of around 83-207 seconds, depending on the complexity and the number of regions to be segmented.\n\nThese latency times highlight the resource constraints of edge devices like the Raspberry Pi and emphasize the need to optimize the model and the environment to achieve real-time performance.\n\n\nRunning complex tasks can use all 8GB of the Raspi-5’s memory. For example, the above screenshot during the Florence OD task shows 4 CPUs at full speed and over 5GB of memory in use. Consider increasing the SWAP memory to 2 GB.\n\nChecking the CPU temperature with vcgencmd measure_temp , showed that temperature can go up to +80oC."
  },
  {
    "objectID": "raspi/vlm/vlm.html#fine-tunning",
    "href": "raspi/vlm/vlm.html#fine-tunning",
    "title": "Vision-Language Models at the Edge",
    "section": "Fine-Tunning",
    "text": "Fine-Tunning\nAs explored in this lab, Florence supports many tasks out of the box, including captioning, object detection, OCR, and more. However, like other pre-trained foundational models, Florence-2 may need domain-specific knowledge. For example, it may need to improve with medical or satellite imagery. In such cases, fine-tuning with a custom dataset is necessary. The Roboflow tutorial, How to Fine-tune Florence-2 for Object Detection Tasks, shows how to fine-tune Florence-2 on object detection datasets to improve model performance for our specific use case.\nBased on the above tutorial, it is possible to fine-tune the Florence-2 model to detect boxes and wheels used in previous labs:\n\nIt is important to note that after fine-tuning, the model can still detect classes that don’t belong to our custom dataset, like cats, dogs, grapes, etc, as seen before).\nThe complete fine-tunning project using a previously annotated dataset in Roboflow and executed on CoLab can be found in the notebook:\n\n30-Finetune_florence_2_on_detection_dataset_box_vs_wheel.ipynb\n\nIn another example, in the post, Fine-tuning Florence-2 - Microsoft’s Cutting-edge Vision Language Models, the authors show an example of fine-tuning Florence on DocVQA. The authors report that Florence 2 can perform visual question answering (VQA), but the released models don’t include VQA capability."
  },
  {
    "objectID": "raspi/vlm/vlm.html#conclusion",
    "href": "raspi/vlm/vlm.html#conclusion",
    "title": "Vision-Language Models at the Edge",
    "section": "Conclusion",
    "text": "Conclusion\nFlorence-2 offers a versatile and powerful approach to vision-language tasks at the edge, providing performance that rivals larger, task-specific models, such as YOLO for object detection, BERT/RoBERTa for text analysis, and specialized OCR models.\nThanks to its multi-modal transformer architecture, Florence-2 is more flexible than YOLO in terms of the tasks it can handle. These include object detection, image captioning, and visual grounding.\nUnlike BERT, which focuses purely on language, Florence-2 integrates vision and language, allowing it to excel in applications that require both modalities, such as image captioning and visual grounding.\nMoreover, while traditional OCR models such as Tesseract and EasyOCR are designed solely for recognizing and extracting text from images, Florence-2’s OCR capabilities are part of a broader framework that includes contextual understanding and visual-text alignment. This makes it particularly useful for scenarios that require both reading text and interpreting its context within images.\nOverall, Florence-2 stands out for its ability to seamlessly integrate various vision-language tasks into a unified model that is efficient enough to run on edge devices like the Raspberry Pi. This makes it a compelling choice for developers and researchers exploring AI applications at the edge.\n\nKey Advantages of Florence-2\n\nUnified Architecture\n\nSingle model handles multiple vision tasks vs. specialized models (YOLO, BERT, Tesseract)\nEliminates the need for multiple model deployments and integrations\nConsistent API and interface across tasks\n\nPerformance Comparison\n\nObject Detection: Comparable to YOLOv8 (~37.5 mAP on COCO vs. YOLOv8’s ~39.7 mAP) despite being general-purpose\nText Recognition: Handles multiple languages effectively like specialized OCR models (Tesseract, EasyOCR)\nLanguage Understanding: Integrates BERT-like capabilities for text processing while adding visual context\n\nResource Efficiency\n\nThe Base model (232M parameters) achieves strong results despite smaller size\nRuns effectively on edge devices (Raspberry Pi)\nSingle model deployment vs. multiple specialized models\n\n\n\n\nTrade-offs\n\nPerformance vs. Specialized Models\n\nYOLO series may offer faster inference for pure object detection\nSpecialized OCR models might handle complex document layouts better\nBERT/RoBERTa provide deeper language understanding for text-only tasks\n\nResource Requirements\n\nHigher latency on edge devices (15-200s depending on task)\nRequires careful memory management on Raspberry Pi\nIt may need optimization for real-time applications\n\nDeployment Considerations\n\nInitial setup is more complex than single-purpose models\nRequires understanding of multiple task types and prompts\nThe learning curve for optimal prompt engineering\n\n\n\n\nBest Use Cases\n\nResource-Constrained Environments\n\nEdge devices requiring multiple vision capabilities\nSystems with limited storage/deployment capacity\nApplications needing flexible vision processing\n\nMulti-modal Applications\n\nContent moderation systems\nAccessibility tools\nDocument analysis workflows\n\nRapid Prototyping\n\nQuick deployment of vision capabilities\nTesting multiple vision tasks without separate models\nProof-of-concept development"
  },
  {
    "objectID": "raspi/vlm/vlm.html#future-implications",
    "href": "raspi/vlm/vlm.html#future-implications",
    "title": "Vision-Language Models at the Edge",
    "section": "Future Implications",
    "text": "Future Implications\nFlorence-2 represents a shift toward unified vision models that could eventually replace task-specific architectures in many applications. While specialized models maintain advantages in specific scenarios, the convenience and efficiency of unified models like Florence-2 make them increasingly attractive for real-world deployments.\nThe lab demonstrates Florence-2’s viability on edge devices, suggesting future IoT, mobile computing, and embedded systems applications where deploying multiple specialized models would be impractical."
  },
  {
    "objectID": "raspi/vlm/vlm.html#resources",
    "href": "raspi/vlm/vlm.html#resources",
    "title": "Vision-Language Models at the Edge",
    "section": "Resources",
    "text": "Resources\n\n10-florence2_test.ipynb\n20-florence_2.ipynb\n30-Finetune_florence_2_on_detection_dataset_box_vs_wheel.ipynb"
  },
  {
    "objectID": "raspi/physical_comp/RPi_Physical_Computing.html#introduction",
    "href": "raspi/physical_comp/RPi_Physical_Computing.html#introduction",
    "title": "Physical Computing with Raspberry Pi",
    "section": "Introduction",
    "text": "Introduction\nPhysical computing creates interactive systems that sense and respond to the analog world. While this field has traditionally focused on direct sensor readings and programmed responses, we’re entering an exciting new era where Large Language Models (LLMs) can add sophisticated decision-making and natural language interaction to physical computing projects.\nIn the Small Language Models (SLM) chapter, we learned how it is possible to run an LLM (or, more precisely, an SLM) in a Single Board Computer (SBC) like the Raspberry Pi. This tutorial will guide us through setting up a Raspberry Pi for physical computing, with an eye toward future AI integration. We’ll cover:\n\nSetting up the Raspberry Pi for physical computing\nWorking with essential sensors and actuators\nUnderstanding GPIO (General Purpose Input/Output) programming\nEstablishing a foundation for integrating LLMs with physical devices\nCreating interactive systems that can respond to both sensor data and natural language commands\n\nWe will also use a Jupyter notebook (programmed in Python) to interact with sensors and actuators—an important and necessary first step toward the goal of integrating the Raspi with an SLM. The combination of Raspberry Pi’s versatility and the power of SLMs opens up exciting possibilities for creating more intelligent and responsive physical computing systems.\nThe diagram below gives us an overview of the project:"
  },
  {
    "objectID": "raspi/physical_comp/RPi_Physical_Computing.html#prerequisites",
    "href": "raspi/physical_comp/RPi_Physical_Computing.html#prerequisites",
    "title": "Physical Computing with Raspberry Pi",
    "section": "Prerequisites",
    "text": "Prerequisites\n\nRaspberry Pi (model 4 or 5)\nDHT22 Temperature and Relative Humidity Sensor\nBMP280 Barometric Pressure, Temperature and Altitude Sensor\nColored LEDs (3x)\nPush Button (1x)\nResistor 4K7 ohm (2x)\nResistor 220 or 330 ohm (3x)"
  },
  {
    "objectID": "raspi/physical_comp/RPi_Physical_Computing.html#install-the-raspi-operating-system",
    "href": "raspi/physical_comp/RPi_Physical_Computing.html#install-the-raspi-operating-system",
    "title": "Physical Computing with Raspberry Pi",
    "section": "Install the Raspi Operating System",
    "text": "Install the Raspi Operating System\nAs described in Setup, we will need an operating system to use the Raspberry Pi. By default, Raspberry Pis check for an operating system on any SD card inserted in the slot, so we should install an operating system using Raspberry Pi Imager.\nRaspberry Pi Imager is a tool for downloading and writing images on macOS, Windows, and Linux. It includes many popular operating system images for Raspberry Pi. We will also use the Imager to preconfigure credentials and remote access settings.\nAfter downloading the Imager and installing it on your computer, use a new and empty SD card. Select the device (RASPBERRY PI Zero, 4 or 5), the Operating System (RASPBERRY PI OS 32 or 64-BIT), and your Storage Device:\n\nWe should also define the options, such as the hostname, username, password, LAN configuration (on GENERAL TAB), and, more importantly, SSH Enable on the SERVICES tab.\n\nUsing the Secure Shell (SSH) protocol, you can access the terminal of a Raspberry Pi remotely from another computer on the same network.\n\n\nAfter burning the OS to the SD card, install it in the Raspi5’s SD slot and plug in the 5V power source.\n\nInteracting with the Raspi via SSH\nThe easiest way to interact with the Raspi is via SSH (“Headless”). We can use a Terminal (MAC/Linux) or PuTTy (Windows).\nOn terminal type &lt;username&gt;@&lt;hostname&gt;.local, for example:\nssh mjrovai@rpi-5.local\n\nYou should replace mjrovai with your username and rpi-5 with the hostname chosen during set-u\n\n\nWhen you see the prompt:\nmjrovai@rpi-5:~ $\nIt means that you are interacting remotely with your Raspi.\n\nNote: ssh &lt;username&gt;@&lt;hostname&gt;.local sometimes does not work. In those cases, try: ssh &lt;username&gt;@&lt;ip address&gt;\n\nIt is a good practice to update the system regularly. For that, you should run:\nsudo apt-get update\nPip is a tool for installing external Python modules on a Raspberry Pi. However, it has not been enabled in recent OS versions. To allow it, you should run the command (only once):\nsudo rm /usr/lib/python3.11/EXTERNALLY-MANAGED\nTo shut down the Rpi-Zero via terminal:\nDo not simply pull the power cord when you want to turn off your Raspberry Pi. The Raspi may still be writing data to the SD card, in which case, merely powering down may result in data loss or, even worse, a corrupted SD card.\nFor safety shut down, use the command line:\nsudo shutdown -h now\n\nTo avoid possible data loss and SD card corruption, wait a few seconds after shutting down for the Raspberry Pi’s LED to stop blinking and go dark before removing the power. Once the LED goes out, it’s safe to power down."
  },
  {
    "objectID": "raspi/physical_comp/RPi_Physical_Computing.html#accessing-the-gpios",
    "href": "raspi/physical_comp/RPi_Physical_Computing.html#accessing-the-gpios",
    "title": "Physical Computing with Raspberry Pi",
    "section": "Accessing the GPIOs",
    "text": "Accessing the GPIOs\nA simple way to reach the GPIO pins on a Raspberry Pi is from the GPIO Zero Library. With a few lines of code in Python, we can control actuators, read sensors, etc. It was created by Ben Nuttall of the Raspberry Pi Foundation, Dave Jones, and other contributors (GitHub).\n\nGPIO Zero is installed by default in the Raspberry Pi OS.\n\n\nPin Numbering\n\nIt is essential to mention that the GPIO Zero Library uses Broadcom (BCM) pin numbering for the GPIO pins, as opposed to physical (board) numbering. Any pin marked “GPIO” in the following diagrams can be used as a PIN. For example, if an LED were attached to “GPIO13,” you would specify the PIN as 18 rather than 33 (the physical one).\n\n\n“Hello World”: Blinking an LED\nTo connect our RPi to the world, let’s first connect:\n\nPhysical Pin 6 (GND) to GND Breadboard Power Grid (Blue -), using a black jumper\nPhysical Pin 1 (3.3V) to +VCC Breadboard Power Grid (Red +), using a red jumper\n\nNow, let’s connect an LED (red) using the physical pin 13 (GPIO13) connected to the LED cathode (longer LED leg). Connect the LED anode to the breadboard GND using a 330 ohms resistor to reduce the current drained from the Raspi, as shown below:\n\nOnce the HW is connected, let’s create a Python script to turn on the LED:\nfrom gpiozero import LED\nled = LED(13)\nled.on()\nWe can use any text editor (such as Nano) to create and run the script. Save the file, for example, as led_test.py, and then execute it using the terminal:\npython led_test.py\nAs we can see, it is elementary to code using the GPIO Zero Library.\nNow, let’s blink the LED (the actual “Hello world”) when talking about physical computing. To do that, we must also import another library, which is time. We need it to define how long the LED will be ON and OFF. In our case below, the LED will blink at a 1-second time.\nfrom gpiozero import LED\nfrom time import sleep\nled = LED(18)\nwhile True:\n    led.on()\n    sleep(1)\n    led.off()\n    sleep(1)\nAlternatively, we can reduce the blink code as below:\nfrom gpiozero import LED\nfrom signal import pause\nred = LED(17)\nred.blink()\npause()\n\n\nInstalling all LEDs (the “actuators”)\nThe LEDs can be used as “actuators”; depending on the condition of a code running on our Pi, we can command one of the LEDs to fire! We will install two more LEDs besides the red one already installed. Follow the diagram and install the yellow (on GPIO 19 ) and the green (on GPIO 26).\n\nFor testing we can run a similar code as the used with the single red led, changing the pin accordantly, for example.\nfrom gpiozero import LED\n\nledRed = LED(13)\nledYlw = LED(19)\nledGrn = LED(26)\n\nledRed.off()\nledYlw.off()\nledGrn.off()\n\nledRed.on()\nledYlw.on()\nledGrn.on()\n\nledRed.off()\nledYlw.off()\nledGrn.off()\n\nRemember that instead of LEDs, we could have relays, motors, etc."
  },
  {
    "objectID": "raspi/physical_comp/RPi_Physical_Computing.html#sensors-installation-and-setup",
    "href": "raspi/physical_comp/RPi_Physical_Computing.html#sensors-installation-and-setup",
    "title": "Physical Computing with Raspberry Pi",
    "section": "Sensors Installation and setup",
    "text": "Sensors Installation and setup\nIn this section, we will setup the Raspberry Pi to capture data from several different sensors:\nSensors and Communication type:\n\nButton (Command via a Push-Button) ==&gt; Digital direct connection\nDHT22 (Temperature and Humidity) ==&gt; Digital communication\nBMP280 (Temperature and Pressure) ==&gt; I2C Protocol\n\n\nButton\nThe simple way to read an external command is by using a push-button, and the GPIO Zero Library provides an easy way to include it in our project. We do not need to think about Pull-up or Pull-down resistors, etc. In terms of HW, the only thing to do is to connect one leg of our push-button to any one of RPi GPIOs and the other one to GND as shown in the diagram:\n\n\nPush-Button leg1 to GPIO 20\nPush-Button leg2 to GND\n\nA simple code for reading the button can be:\nfrom gpiozero import Button\nbutton = Button(20)\nwhile True: \n    if button.is_pressed: \n        print(\"Button is pressed\") \n    else:\n        print(\"Button is not pressed\")\n\n\nInstalling Adafruit CircuitPython\nThe GPIO Zero library is an excellent hardware interfacing library for Raspberry Pi. It’s great for digital in/out, analog inputs, servos, basic sensors, etc. However, it doesn’t cover SPI/I2C sensors or drivers, and by using CircuitPython via adafruit_blinka, we can unlock all of the drivers and example code developed by Adafruit!\n\nNote that we will keep using GPIO Zero for pins, buttons and LEDs.\n\nEnable Interfaces\nRun these commands to enable the various interfaces such as I2C and SPI:\nsudo raspi-config nonint do_i2c 0\nsudo raspi-config nonint do_spi 0\nsudo raspi-config nonint do_serial_hw 0\nsudo raspi-config nonint disable_raspi_config_at_boot 0\nInstall Blinka and Dependencies\nsudo apt-get install -y i2c-tools libgpiod-dev python-libgpiod\npip install --upgrade adafruit-blinka\nCheck I2C and SPI\nThe script will automatically enable I2C and SPI. You can run the following command to verify:\nls /dev/i2c* /dev/spi*\n\nBlinka Test\nCreate a new file called blinka_test.py with nano or your favorite text editor and put the following in:\nimport board\nimport digitalio\nimport busio\n\nprint(\"Hello, blinka!\")\n\n# Try to create a Digital input\npin = digitalio.DigitalInOut(board.D4)\nprint(\"Digital IO ok!\")\n\n# Try to create an I2C device\ni2c = busio.I2C(board.SCL, board.SDA)\nprint(\"I2C ok!\")\n\n# Try to create an SPI device\nspi = busio.SPI(board.SCLK, board.MOSI, board.MISO)\nprint(\"SPI ok!\")\n\nprint(\"done!\")\nSave it and run it at the command line:\npython blinka_test.py\n\n\n\nDHT22 - Temperature & Humidity Sensor\nThe first sensor to be installed will be the DHT22 for capturing air temperature and relative humidity data.\nOverview\nThe low-cost DHT temperature and humidity sensors are elementary and slow but great for logging basic data. They consist of a capacitive humidity sensor and a thermistor. A bare chip inside performs the analog-to-digital conversion and spits out a digital signal with the temperature and humidity. The digital signal is relatively easy to read using any microcontroller.\nDHT22 Main characteristics:\n\nSuitable for 0-100% humidity readings with 2-5% accuracy\nSuitable for -40 to 125°C temperature readings ±0.5°C accuracy\nNo more than 0.5 Hz sampling rate (once every 2 seconds)\nLow cost\n3 to 5V power and I/O\n2.5mA max current use during conversion (while requesting data)\nBody size 15.1mm x 25mm x 7.7mm\n4 pins with 0.1” spacing\n\nOnce we use the sensor at distances less than 20m, a 4K7 ohm resistor should be connected between the Data and VCC pins. The DHT22 output data pin will be connected to Raspberry GPIO 16. Check the electrical diagram, connecting the sensor to RPi pins as below:\n\nPin 1 - Vcc ==&gt; 3.3V\nPin 2 - Data ==&gt; GPIO 16\nPin 3 - Not Connect\nPin 4 - Gnd ==&gt; Gnd\n\n\nDo not forget to Install the 4K7 ohm resistor between the VCC and Data pins.\n\n\nOnce the sensor is connected, we must install its library on our Raspberry Pi. First, we should install the Adafruit CircuitPython library, which we have already done, and the Adafruit_CircuitPython_DHT.\npip install adafruit-circuitpython-dht\nOn your Raspberry, starting at home, go to Documents.\ncd Documents\nCreate a directory to install the library and move to there:\nmkdir sensors\ncd sensors\nCreate a new Python script as below and name it, for example, dht_test.py:\nimport time\nimport board\nimport adafruit_dht\ndhtDevice = adafruit_dht.DHT22(board.D16)\n\nwhile True:\n    try:\n        # Print the values to the serial port\n        temperature_c = dhtDevice.temperature\n        temperature_f = temperature_c * (9 / 5) + 32\n        humidity = dhtDevice.humidity\n        print(\n            \"Temp: {:.1f} F / {:.1f} C    Humidity: {}% \".format(\n                temperature_f, temperature_c, humidity\n            )\n        )\n\n    except RuntimeError as error:\n        # Errors happen fairly often, DHT's are hard to read, \n        # just keep going\n        print(error.args[0])\n        time.sleep(2.0)\n        continue\n    except Exception as error:\n        dhtDevice.exit()\n        raise error\n\n\n\n\nInstalling the BMP280: Barometric Pressure & Altitude Sensor\nSensor Overview:\nEnvironmental sensing has become increasingly important in various industries, from weather forecasting to indoor navigation and consumer electronics. At the forefront of this technological advancement are sensors like the BMP280 and BMP180 (deprected), which excel in measuring temperature and barometric pressure with exceptional precision and reliability.\nAs its predecessor, the BMP180, the BMP280 is an absolute barometric pressure sensor, which is especially feasible for mobile applications. Its diminutive dimensions and low power consumption allow for its implementation in battery-powered devices such as mobile phones, GPS modules, or watches. The BMP280 is based on Bosch’s proven piezo-resistive pressure sensor technology featuring high accuracy and linearity as well as long-term stability and high EMC robustness. Numerous device operation options guarantee the highest flexibility. The device is optimized for power consumption, resolution, and filter performance.\nTechnical data\n\n\n\n\n\n\n\nParameter\nTechnical data\n\n\n\n\nOperation range\nPressure: 300…1100 hPa Temp.: -40…85°C\n\n\nAbsolute accuracy (950…1050 hPa, 0…+40°C)\n~ ±1 hPa\n\n\nRelative accuracy p = 700…900hPa (Temp. @ 25°C)\n± 0.12 hPa (typical) equivalent to ±1 m\n\n\nAverage typical current consumption (1 Hz dt/rate)\n3.4 μA @ 1 Hz\n\n\nAverage current consumption (1 Hz dt refresh rate)\n2.74 μA, typical (ultra-low power mode)\n\n\nAverage current consumption in sleep mode\n0.1 μA\n\n\nAverage measurement time\n5.5 msec (ultra-low power preset)\n\n\nSupply voltage VDDIO\n1.2 … 3.6 V\n\n\nSupply voltage VDD\n1.71 … 3.6 V\n\n\nResolution of data\nPressure: 0.01 hPa ( &lt; 10 cm) Temp.: 0.01° C\n\n\nTemperature coefficient offset (+25°…+40°C @ 900hPa)\n1.5 Pa/K, equiv. to 12.6 cm/K\n\n\nInterface\nI²C and SPI\n\n\n\nBMP280 Sensor Installation\nFollow the diagram and make the connections:\n\nVin ==&gt; 3.3V\nGND ==&gt; GND\nSCL ==&gt; GPIO 3\nSDA ==&gt; GPIO 2\n\n\nEnabling I2C Interface\nGo to RPi Configuration and confirm that the I2C interface is enabled. If not, enable it.\nsudo raspi-config nonint do_i2c 0\nUsing the BMP280\nIf everything has been installed and connected correctly, you can turn on your Rapspi and start interpreting the BMP180’s information about the environment.\nThe first thing to do is to check if the Raspi sees your BMP280. Try the following in a terminal:\nsudo i2cdetect -y 1\nWe should confirm that the BMP280 is on channel 77 (default) or 76.\n\nIn my case, the bus address is 0x76, so we should define it during the library installation.\nInstalling the BMP 280 Library:\nOnce the sensor is connected, we must install its library on our Raspi. For that, we should install the Adafruit_CircuitPython_BMP280.\npip install adafruit-circuitpython-bmp280\nCreate a new Python script as below and name it, for example, bmp280_test.py:\nimport time\nimport board\n\nimport adafruit_bmp280\n\ni2c = board.I2C()\nbmp280 = adafruit_bmp280.Adafruit_BMP280_I2C(i2c, address = 0x76)\nbmp280.sea_level_pressure = 1013.25\n\nwhile True:\n    print(\"\\nTemperature: %0.1f C\" % bmp280.temperature)\n    print(\"Pressure: %0.1f hPa\" % bmp280.pressure)\n    print(\"Altitude = %0.2f meters\" % bmp280.altitude)\n    time.sleep(2)\nExecute the script:\npython bmp280Test.py\nThe Terminal shows the result.\n\n\nNote that that pressure is presented in hPa. See the next section to understand this unit better.\n\n\n\n\nMeasuring Weather and Altitude With BMP280\n\nLet’s take some time to understand more about what we will get with the BMP readings.\n\nYou can skip this part of the tutorial, or return later.\n\nThe BMP280 (and its predecessor, the BMP180) was designed to measure atmospheric pressure accurately. Atmospheric pressure varies with both weather and altitude.\nWhat is Atmospheric Pressure?\nAtmospheric pressure is a force that the air around you exerts on everything. The weight of the gasses in the atmosphere creates atmospheric pressure. A standard unit of pressure is “pounds per square inch” or psi. We will use the international notation, newtons per square meter, called pascals (Pa).\n\nIf you took 1 cm wide column of air would weigh about 1 kg\n\nThis weight, pressing down on the footprint of that column, creates the atmospheric pressure that we can measure with sensors like the BMP280. Because that cm-wide column of air weighs about 1 kg, the average sea level pressure is about 101,325 pascals, or better, 1013.25 hPa (1 hPa is also known as milibar - mbar). This will drop about 4% for every 300 meters you ascend. The higher you get, the less pressure you’ll see because the column to the top of the atmosphere is much shorter and weighs less. This is useful because you can determine your altitude by measuring the pressure and doing math.\n\nThe air pressure at 3,810 meters is only half that at sea level.\n\nThe BMP280 outputs absolute pressure in hPa (mbar). One pascal is a minimal amount of pressure, approximately the amount that a sheet of paper will exert resting on a table. You will often see measurements in hectopascals (1 hPa = 100 Pa). The library here provides outputs of floating-point values in hPa, equaling one millibar (mbar).\nHere are some conversions to other pressure units:\n\n1 hPa = 100 Pa = 1 mbar = 0.001 bar\n1 hPa = 0.75006168 Torr\n1 hPa = 0.01450377 psi (pounds per square inch)\n1 hPa = 0.02953337 inHg (inches of mercury)\n1 hPa = 0.00098692 atm (standard atmospheres)\n\nTemperature Effects\nBecause temperature affects the density of a gas, density affects the mass of a gas, and mass affects the pressure (whew), atmospheric pressure will change dramatically with temperature. Pilots know this as “density altitude”, which makes it easier to take off on a cold day than a hot one because the air is denser and has a more significant aerodynamic effect. To compensate for temperature, the BMP280 includes a rather good temperature sensor and a pressure sensor.\nTo perform a pressure reading, you first take a temperature reading, then combine that with a raw pressure reading to come up with a final temperature-compensated pressure measurement. (The library makes all of this very easy.)\nMeasuring Absolute Pressure\nIf your application requires measuring absolute pressure, all you have to do is get a temperature reading, then perform a pressure reading (see the test script for details). The final pressure reading will be in hPa = mbar. You can convert this to a different unit using the above conversion factors.\n\nNote that the absolute pressure of the atmosphere will vary with both your altitude and the current weather patterns, both of which are useful things to measure.\n\nWeather Observations\nThe atmospheric pressure at any given location on Earth (or anywhere with an atmosphere) isn’t constant. The complex interaction between the earth’s spin, axis tilt, and many other factors result in moving areas of higher and lower pressure, which in turn cause the variations in weather we see every day. By watching for changes in pressure, you can predict short-term changes in the weather. For example, dropping pressure usually means wet weather or a storm is approaching (a low-pressure system is moving in). Rising pressure usually means clear weather is coming (a high-pressure system is moving through). But remember that atmospheric pressure also varies with altitude. The absolute pressure in my home, Lo Barnechea, in Chile (altitude 960m), will always be lower than that in San Francisco (less than 2 meters, almost sea level). If weather stations just reported their absolute pressure, it would be challenging to compare pressure measurements from one location to another (and large-scale weather predictions depend on measurements from as many stations as possible).\nTo solve this problem, weather stations continuously remove the effects of altitude from their reported pressure readings by mathematically adding the equivalent fixed pressure to make it appear that the reading was taken at sea level. When you do this, a higher reading in San Francisco than in Lo Barnechea will always be because of weather patterns and not because of altitude.\nSea Level Pressure Calculation\nThe See Level Pressure can be calculated with the formula:\n\nWhere,\npo = SeaLevel Pressure \np = Atmospheric Pressure \nL = Temperature Lapse Rate \nh = Altitude \nTo = Sea Level Standard Temperature \ng = Earth Surface Gravitational Acceleration \nM = Molar Mass Of Dry Air \nR = Universal Gas Constant\n\nHaving the absolute pressure in Pa, you check the sea level pressure using the Calculator.\n\nOr calculating in Python, where the altitude is the real altitude in meters where the sensor is located.\npresSeaLevel = pres / pow(1.0 - altitude/44330.0, 5.255) \nDetermining Altitude\nSince pressure varies with altitude, you can use a pressure sensor to measure altitude (with a few caveats). The average pressure of the atmosphere at sea level is 1013.25 hPa (or mbar). This drops off to zero as you climb towards the vacuum of space. Because the curve of this drop-off is well understood, you can compute the altitude difference between two pressure measurements (p and p0) by using a specific equation. The BMP280 gives the measured altitude using bmp280Sensor.altitude.\n\nThe above explanation was based on the BMP 180 Sparkfun tutorial."
  },
  {
    "objectID": "raspi/physical_comp/RPi_Physical_Computing.html#playing-with-sensors-and-actuators",
    "href": "raspi/physical_comp/RPi_Physical_Computing.html#playing-with-sensors-and-actuators",
    "title": "Physical Computing with Raspberry Pi",
    "section": "Playing with Sensors and Actuators",
    "text": "Playing with Sensors and Actuators\n\nInstalling Jupyter Notebook\nWe all know that Jupyter Notebook is a fantastic tool—or, better yet, an open-source web application that allows you to create and share documents containing live code, equations, visualizations, and narrative text. Jupyter Notebook is used mainly in Data Science, cleaning and transforming data, numerical simulation, statistical modeling, data visualization, machine learning, and much more!\n\nHow about using Jupyter Notebooks to control Raspberry Pi GPIOs?\n\nIn this section, we will learn how to install Jupyter Notebook on a Raspberry Pi. Then, we will read sensors and act on actuators directly on the Pi.\nTo install Jupyter on your Raspberry (that will run with Python 3), open Terminal and enter the following commands:\npip install jupyter\nsudo reboot\njupyter notebook --generate-config\nEdit the config file:\nnano ~/.jupyter/jupyter_notebook_config.py\nAdd/modify these lines:\nc.NotebookApp.ip = '0.0.0.0'        # Listen on all interfaces\nc.NotebookApp.open_browser = False  # Disable browser auto-launch\nc.NotebookApp.port = 8888           # Default port (change if needed)\nNow, on the Raspi terminal, start the Jupyter notebook server with the command:\njupyter notebook --no-browser\n\n\nYou will need the Token; you can copy it from the terminal as shown above.\n\nOn your Desktop, set up SSH tunneling:\nssh -N -L 8888:localhost:8888 username@raspberry_pi_ip\nThe Jupyter Notebook will be running as a server on:\nhttp:localhost:8888\n\nThe first time you connect, you’ll need the token that appears in the Pi terminal when you start the notebook server.\n\n\n\nWhen you start your Pi and want to use Jupyter Notebook, type the “Jupyter Notebook” command on your terminal and keep it running. This is very important! If you need to use the terminal for another task, such as running a program, open a new Terminal window.\n\nTo stop the server and close the “kernels” (the Jupyter notebooks), press [Ctrl] + [C].\n\n\nTesting the Notebook setup\nLet’s create a new notebook (Kernel: Python 3). Open dht_test.py, copy the code, and paste it into the notebook. That’s it. We can see the temperature and humidity values appearing on the cell. To interrupt the execution, go to the [stop] button at the top menu.\n\nOK, this means we can access the physical world from our notebook! Let’s create a more structured code for dealing with sensors and actuators.\n\n\nInitialization\nImport libraries, instantiate and initialize sensors/actuators\n# time library \nimport time\nimport datetime\n\n# Adafruit DHT library (Temperature/Humidity)\nimport board\nimport adafruit_dht\nDHT22Sensor = adafruit_dht.DHT22(board.D16)\n\n# BMP library (Pressure/Temperature)\nimport adafruit_bmp280\ni2c = board.I2C()\nbmp280Sensor = adafruit_bmp280.Adafruit_BMP280_I2C(i2c, address = 0x76)\nbmp280Sensor.sea_level_pressure = 1013.25\n\n# LEDs\nfrom gpiozero import LED\n\nledRed = LED(13)\nledYlw = LED(19)\nledGrn = LED(26)\n\nledRed.off()\nledYlw.off()\nledGrn.off()\n\n# Push-Button\nfrom gpiozero import Button\nbutton = Button(20)\n\n\nGPIO Input and Output\nCreate a function to get GPIO status:\n# Get GPIO status data \ndef getGpioStatus():\n    global timeString\n    global buttonSts\n    global ledRedSts\n    global ledYlwSts\n    global ledGrnSts\n\n    # Get time of reading\n    now = datetime.datetime.now()\n    timeString = now.strftime(\"%Y-%m-%d %H:%M\")\n    \n    # Read GPIO Status\n    buttonSts = button.is_pressed\n    ledRedSts = ledRed.is_lit\n    ledYlwSts = ledYlw.is_lit\n    ledGrnSts = ledGrn.is_lit \nAnd another to print the status:\n# Print GPIO status data \ndef PrintGpioStatus():\n    print (\"Local Station Time: \", timeString)\n    print (\"Led Red Status:     \", ledRedSts)\n    print (\"Led Yellow Status:  \", ledYlwSts)\n    print (\"Led Green Status:   \", ledGrnSts)\n    print (\"Push-Button Status: \", buttonSts)\nNow, we can, for example, turn on the LEDs:\nledRed.on()\nledYlw.on()\nledGrn.on()\n\nAnd see their status:\n\nIf you press the push-button, its status will also be shown:\n\nAnd turning off the LEDS:\nledRed.off()\nledYlw.off()\nledGrn.off()\n\nWe can create a function to simplify turning LEDs on and off:\n# Acting on GPIOs and printing Status\ndef controlLeds(r, y, g):\n    if (r):\n        ledRed.on()\n    else:\n        ledRed.off()        \n    if (y):\n        ledYlw.on()\n    else:\n        ledYlw.off() \n    if (g):\n        ledGrn.on()\n    else:\n        ledGrn.off() \n    \n    getGpioStatus()\n    PrintGpioStatus()\nFor example, turning on the Yellow LED:\n\n\n\nGetting and displaying Sensor Data\nFirst, we should create a function to read the BMP280 and calculate the pressure value at sea level, once the sensor only gives us the absolute pressure based on the actual altitude:\n# Read data from BMP280\ndef bmp280GetData(real_altitude):\n    \n    temp = bmp280Sensor.temperature\n    pres = bmp280Sensor.pressure\n    alt =  bmp280Sensor.altitude\n    presSeaLevel = pres / pow(1.0 - real_altitude/44330.0, 5.255) \n    \n    temp = round (temp, 1)\n    pres = round (pres, 2) # absolute pressure in mbar\n    alt = round (alt)\n    presSeaLevel = round (presSeaLevel, 2) # absolute pressure in mbar\n    \n    return temp, pres, alt, presSeaLevel\nEntering the BMP280 real altitude where it is located, run the code:\nbmp280GetData(960)\nAs a result, we will get (26.9, 906.73, 927, 1017.29)which means:\n\nTemperature of 26.9 oC\nAbsolute Pressure of 906.73 hPa\nMeasured Altitude (from Pressure) of 927 m\nSea Level converted Pressure: 1,017.29 hPa\n\nNow, we will generate a unique function to get the BMP280 and the DHT data, including a timestamp:\n# Get data (from local sensors)\ndef getSensorData(altReal=0):\n    global timeString\n    global humExt\n    global tempLab\n    global tempExt\n    global presSL\n    global altLab\n    global presAbs\n    global buttonSts\n    \n    # Get time of reading\n    now = datetime.datetime.now()\n    timeString = now.strftime(\"%Y-%m-%d %H:%M\")\n    \n    tempLab, presAbs, altLab, presSL = bmp280GetData(altReal) \n    \n    tempDHT =  DHT22Sensor.temperature\n    humDHT =  DHT22Sensor.humidity\n    \n    if humDHT is not None and tempDHT is not None:\n        tempExt = round (tempDHT)\n        humExt = round (humDHT)\nAnd another function to print the values:\n# Display important data on-screen\ndef printData():\n    print (\"Local Station Time:             \", timeString)\n    print (\"External Air Temperature (DHT): \", tempExt, \"oC\")\n    print (\"External Air Humidity    (DHT): \", humExt, \"%\")\n    print (\"Station Air Temperature  (BMP): \", tempLab, \"oC\")\n    print (\"Sea Level Air Pressure:         \", presSL, \"mBar\")\n    print (\"Absolute Station Air Pressure:  \", presAbs, \"mBar\")\n    print (\"Station Measured Altitude:      \", altLab, \"m\")\nRuning them:\nreal_altitude = 960 # real altitude of where the BMP280 is installed\ngetSensorData(real_altitude)\nprintData()\nResults:\n\n\nUsing Python, we can command the actuators (LEDs) and read the sensors and GIPOs status at this stage. This is important, for example, to generate a data log to be read by an SLM in the future."
  },
  {
    "objectID": "raspi/physical_comp/RPi_Physical_Computing.html#widgets",
    "href": "raspi/physical_comp/RPi_Physical_Computing.html#widgets",
    "title": "Physical Computing with Raspberry Pi",
    "section": "Widgets",
    "text": "Widgets\npywidgets, or jupyter-widgets orwidgets, are interactive HTML widgets for Jupyter notebooks and the IPython kernel. Notebooks come alive when interactive widgets are used. We can gain control of our data and visualize changes in them.\nWidgets are eventful Python objects that have a representation in the browser, often as a control like a slider, text box, etc. We can use widgets to build interactive GUIs for our project.\nIn this lab, for example, we will use a slide bar to control the state of actuators in real time, such as by turning on or off the LEDs. Widgets are great for adding more dynamic behavior to Jupyter Notebooks.\nInstallation\nTo use Widgets, we must install the Ipywidgets library using the commands:\npip install ipywidgets\nAfter installation, we should call the library:\n# widget library\nfrom ipywidgets import interactive\nimport ipywidgets as widgetsfrom \nIPython.display import display\nAnd running the below line, we can control the LEDs in real-time:\nf = interactive(controlLeds, r=(0,1,1), y=(0,1,1), g=(0,1,1))\ndisplay(f)\n\n\nThis interactive widget is very easy to implement and very powerful. You can learn more about Interactive on this link: Interactive Widget."
  },
  {
    "objectID": "raspi/physical_comp/RPi_Physical_Computing.html#interacting-an-slm-with-the-physical-world",
    "href": "raspi/physical_comp/RPi_Physical_Computing.html#interacting-an-slm-with-the-physical-world",
    "title": "Physical Computing with Raspberry Pi",
    "section": "Interacting an SLM with the Physical world",
    "text": "Interacting an SLM with the Physical world\nThis section demonstrates in a simple way how to integrate a Small Language Model (SLM) with the sensors and LEDs we have set up. The diagram below shows how data flows from sensors through processing and AI analysis to control the actuators and ultimately provide user feedback.\n\nWe will use the Transformers library from Hugging Face for model loading and inference. This library provides the architecture for working with pre-trained language models, helping interact with the model, processing input prompts, and obtaining outputs.\nInstallation\npip install transformers torch\nLet’s create a simple SLM test in the Jupyter Notebook that checks if the model loads and measures inference time. The model used here is the TinyLLama 1.1B. We will ask a straightforward question:\n\"The weather today is\"\nAs a result, besides the SLM answer, we will also measure the latency.\nRun this script:\nimport time\nfrom transformers import pipeline\nimport torch\n\n# Check if CUDA is available (it won't be on our case, Raspberry Pi)\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(f\"Using device: {device}\")\n\n# Load the model and measure loading time\nstart_time = time.time()\n\nmodel='TinyLlama/TinyLlama-1.1B-intermediate-step-1431k-3T'\ngenerator = pipeline('text-generation', \n                    model=model,\n                    device=device)\nload_time = time.time() - start_time\nprint(f\"Model loading time: {load_time:.2f} seconds\")\n\n# Test prompt\ntest_prompt = \"The weather today is\"\n\n# Measure inference time\nstart_time = time.time()\nresponse = generator(test_prompt, \n                    max_length=50,\n                    num_return_sequences=1,\n                    temperature=0.7)\ninference_time = time.time() - start_time\n\nprint(f\"\\nTest prompt: {test_prompt}\")\nprint(f\"Generated response: {response[0]['generated_text']}\")\nprint(f\"Inference time: {inference_time:.2f} seconds\")\nAs we can see, the SLM works, but the latency is very high (+3 minutes). It is OK because this particular test is on a Raspberry Pi 4. With a Raspberry Pi 5, the result would be better.:\n\nThe Raspi uses around 1GB of memory (model + process) and all four cores to process the answer. The model alone needs around 800MB.\n\nNow, let us create a code showing a basic interaction pattern where the SLM can respond to sensor data and interact with the LEDs.\nInstall the Libraries:\nimport time\nimport datetime\nimport board\nimport adafruit_dht\nimport adafruit_bmp280\nfrom gpiozero import LED, Button\nfrom transformers import pipeline\nInitialize sensors\nDHT22Sensor = adafruit_dht.DHT22(board.D16)\ni2c = board.I2C()\nbmp280Sensor = adafruit_bmp280.Adafruit_BMP280_I2C(i2c, address=0x76)\nbmp280Sensor.sea_level_pressure = 1013.25\nInitialize LEDs and Button\nledRed = LED(13)\nledYlw = LED(19)\nledGrn = LED(26)\nbutton = Button(20)\nInitialize the SLM pipeline\n# We're using a small model suitable for Raspberry Pi\n\nmodel='TinyLlama/TinyLlama-1.1B-intermediate-step-1431k-3T'\ngenerator = pipeline('text-generation', \n                    model=model,\n                    device='cpu')\nSupport Functions\nNow, let’s create support functions for readings from all sensors and control the LEDs:\ndef get_sensor_data():\n    \"\"\"Get current readings from all sensors\"\"\"\n    try:\n        temp_dht = DHT22Sensor.temperature\n        humidity = DHT22Sensor.humidity\n        temp_bmp = bmp280Sensor.temperature\n        pressure = bmp280Sensor.pressure\n        \n        return {\n            'temperature_dht': round(temp_dht, 1) if temp_dht else None,\n            'humidity': round(humidity, 1) if humidity else None,\n            'temperature_bmp': round(temp_bmp, 1),\n            'pressure': round(pressure, 1)\n        }\n    except RuntimeError:\n        return None\n\n\ndef control_leds(red=False, yellow=False, green=False):\n    \"\"\"Control LED states\"\"\"\n    ledRed.value = red\n    ledYlw.value = yellow\n    ledGrn.value = green\n    \n\ndef process_conditions(sensor_data):\n    \"\"\"Process sensor data and control LEDs based on conditions\"\"\"\n    if not sensor_data:\n        control_leds(red=True)  # Error condition\n        return\n    \n    temp = sensor_data['temperature_dht']\n    humidity = sensor_data['humidity']\n    \n    # Example conditions for LED control\n    if temp &gt; 30:  # Hot\n        control_leds(red=True)\n    elif humidity &gt; 70:  # Humid\n        control_leds(yellow=True)\n    else:  # Normal conditions\n        control_leds(green=True)\nGenerating an SLM’s response\nSo far, the LEDs reaction is only based on logic, but let’s also use the SLM to “analyse” the sensors condition, generating a response based on that:\ndef generate_response(sensor_data):\n    \"\"\"Generate response based on sensor data using SLM\"\"\"\n    if not sensor_data:\n        return \"Unable to read sensor data\"\n    \n    prompt = f\"\"\"Based on these sensor readings:\n    Temperature: {sensor_data['temperature_dht']}°C\n    Humidity: {sensor_data['humidity']}%\n    Pressure: {sensor_data['pressure']} hPa\n    \n    Provide a brief status and recommendation in 2 sentences.\n    \"\"\"\n    \n    # Generate response from SLM\n    response = generator(prompt, \n                       max_length=100,\n                       num_return_sequences=1,\n                       temperature=0.7)[0]['generated_text']\n    \n    return response\nMain Function\nAnd now, let’s create a main() function to wait for the user to, for example, press a button and, capture the data generated by the sensors, delivering some observation or recommendation from the SLM:\ndef main_loop():\n    \"\"\"Main program loop\"\"\"\n    print(\"Starting Physical Computing with SLM Integration...\")\n    print(\"Press the button to get a reading and SLM response.\")\n    \n    try:\n        while True:\n            if button.is_pressed:\n                # Get sensor readings\n                sensor_data = get_sensor_data()\n                \n                # Process conditions and control LEDs\n                process_conditions(sensor_data)\n                \n                if sensor_data:\n                    # Get SLM response\n                    response = generate_response(sensor_data)\n                    \n                    # Print current status\n                    print(\"\\nCurrent Readings:\")\n                    print(f\"Temperature: {sensor_data['temperature_dht']}°C\")\n                    print(f\"Humidity: {sensor_data['humidity']}%\")\n                    print(f\"Pressure: {sensor_data['pressure']} hPa\")\n                    print(\"\\nSLM Response:\")\n                    print(response)\n                    \n                time.sleep(2)  # Debounce and allow time to read\n            \n            time.sleep(0.1)  # Reduce CPU usage\n            \n    except KeyboardInterrupt:\n        print(\"\\nShutting down...\")\n        control_leds(False, False, False)  # Turn off all LEDs\nTest Result\nThe sensors are read after the user presses the button to trigger a reading, and LEDs are controlled based on conditions. Sensor data is formatted into a prompt for the SLM to generate a response analyzing the current conditions. The results are displayed in the terminal, and the LED indicators are shown.\n\nRed: High temperature (&gt;30°C) or error condition\nYellow: High humidity (&gt;70%)\nGreen: Normal conditions\n\nThis simple code integrates a Small Language Model (TinyLlama model (1.1B parameters) with our physical computing setup, providing raw sensor data and intelligent responses from the SLM about the environmental conditions.\n\nWe can extend this first test to more sophisticated and valuable uses of the SLM integration, for example: adding:\n\nStarting the process from a User Prompt.\nReceive commands from the User to switch LEDs ON or OFF\nProvide the status of LEDS, Button, or specific sensor data from the user prompt\nLog data and responses to a file. Provide historical information by user request\nImplement different types of prompts for various use cases"
  },
  {
    "objectID": "raspi/physical_comp/RPi_Physical_Computing.html#other-models",
    "href": "raspi/physical_comp/RPi_Physical_Computing.html#other-models",
    "title": "Physical Computing with Raspberry Pi",
    "section": "Other Models",
    "text": "Other Models\nWe can use other SLMs in a Raspberry Pi that have distinct ways of handling them. For example, many modern models use GGUF formats, and to use them, we need to install llama-cpp-python, which is designed to work with GGUF models.\nAlso, as we saw in a previous lab, Ollama is a great way to download and test SLMs on the Raspberry Pi."
  },
  {
    "objectID": "raspi/physical_comp/RPi_Physical_Computing.html#conclusion",
    "href": "raspi/physical_comp/RPi_Physical_Computing.html#conclusion",
    "title": "Physical Computing with Raspberry Pi",
    "section": "Conclusion",
    "text": "Conclusion\n\nKey Achievements\nThroughout this tutorial, we’ve successfully: - Set up a complete physical computing environment using Raspberry Pi - Integrated multiple environmental sensors (DHT22 and BMP280) - Implemented visual feedback through LED actuators - Created interactive controls using push buttons - Integrated a Small Language Model (TinyLLama 1.1B) for intelligent analysis - Developed a foundation for AI-enhanced environmental monitoring\n\n\nTechnical Insights\n\nHardware Integration\nThe combination of digital (DHT22) and I2C (BMP280) sensors demonstrated different communication protocols and their implementations. This multi-sensor approach provides redundancy and comprehensive environmental monitoring capabilities. The LED actuators and push-button interface created a responsive and interactive system that bridges the digital and physical worlds.\n\n\nSoftware Architecture\nThe layered software architecture we developed supports: 1. Low-level sensor communication and actuator control 2. Data preprocessing and validation 3. SLM integration for intelligent analysis 4. Interactive user interfaces through both hardware and software\n\n\nAI Integration Learnings\nThe integration of TinyLLama 1.1B revealed several important insights: - Small Language Models can effectively run on edge devices like Raspberry Pi - Natural language processing can enhance sensor data interpretation - Real-time analysis is possible, though with some latency considerations - The system can provide human-readable insights from complex sensor data\n\n\n\nPractical Applications\nThis project serves as a foundation for numerous real-world applications: - Environmental monitoring systems - Smart home automation - Industrial sensor networks - Educational platforms for IoT and AI integration - Prototyping platforms for larger-scale deployments\n\n\nChallenges and Solutions\nThroughout the development, we encountered and addressed several challenges: 1. Resource Constraints: - Optimized SLM inference for Raspberry Pi capabilities - Implemented efficient sensor reading strategies - Managed memory usage for stable operation\n\nData Integration:\n\nDeveloped robust sensor data validation\nCreated effective data preprocessing pipelines\nImplemented error handling for sensor failures\n\nAI Integration:\n\nDesigned effective prompting strategies\nManaged inference latency\nBalanced accuracy with response time\n\n\n\n\nFuture Enhancements\nThe system can be extended in several directions: 1. Hardware Expansions: - Additional sensor types (air quality, light, motion) - Camera for IA applications - More complex actuators (displays, motors, relays) - Wireless connectivity options as WiFI, BLE, or LoRa 2. Software Improvements: - Advanced data logging and analysis - Web-based monitoring interface - Real-time visualization tools 3. AI Capabilities: 1. Models for detecting and counting objects 2. RAG or Fine-tuning SLM for specific applications 3. Multi-modal AI integration via sensor integration 4. Automated decision-making systems 5. Predictive maintenance capabilities\n\n\nFinal Thoughts\nThis tutorial demonstrates that integrating physical computing with AI is feasible and practical on accessible hardware like the Raspberry Pi. Combining sensors, actuators, and AI creates a powerful platform for developing intelligent environmental monitoring and control systems.\nWhile the current implementation focuses on environmental monitoring, the principles and techniques can be adapted to various applications. The modular nature of hardware and software components allows for customization and expansion based on specific needs.\nIntegrating Small Language Models in physical computing opens new possibilities for creating more intuitive and intelligent IoT devices. As edge AI capabilities evolve, projects like this will become increasingly important in developing the next generation of smart devices and systems.\nRemember that this is just the beginning. Our foundation can be extended in countless ways to create more sophisticated and capable systems. The key is building upon these basics while balancing functionality, reliability, and resource usage."
  },
  {
    "objectID": "raspi/physical_comp/RPi_Physical_Computing.html#resources",
    "href": "raspi/physical_comp/RPi_Physical_Computing.html#resources",
    "title": "Physical Computing with Raspberry Pi",
    "section": "Resources",
    "text": "Resources\n\nGPIOs - Scripts\nSensors - Scripts\nNotebooks"
  },
  {
    "objectID": "raspi/iot/slm_iot.html",
    "href": "raspi/iot/slm_iot.html",
    "title": "Experimenting with SLMs for IoT Control",
    "section": "",
    "text": "Conclusion\nThis lab has demonstrated the progressive evolution of an IoT system from basic sensor integration to an intelligent, interactive platform powered by Small Language Models. Through our journey, we’ve explored several key aspects of combining edge AI with physical computing:"
  },
  {
    "objectID": "raspi/iot/slm_iot.html#introduction",
    "href": "raspi/iot/slm_iot.html#introduction",
    "title": "Experimenting with SLMs for IoT Control",
    "section": "Introduction",
    "text": "Introduction\nThis lab explores the implementation of Small Language Models (SLMs) in IoT control systems, demonstrating the possibility of creating a monitoring and control system using edge AI. We’ll integrate these models with physical sensors and actuators, creating an intelligent IoT system capable of natural language interaction. While this implementation shows the potential of integrating AI with physical systems, it also highlights current limitations and areas for improvement.\n\nThis project builds upon the concepts introduced in “Small Language Models (SLMs)” and “Physical Computing with Raspberry Pi.”\n\nThe Physical Computing lab laid the groundwork for interfacing with hardware components using the Raspberry Pi’s GPIO pins. We’ll revisit these concepts, focusing on connecting and interacting with sensors (DHT22 for temperature and humidity, BMP280 for temperature and pressure, and a push-button for digital inputs), besides controlling actuators (LEDs) in a more sophisticated setup.\nWe will progress from a simple IoT system to a more advanced platform that combines real-time monitoring, historical data analysis, and natural language processing (NLP).\n\nThis lab demonstrates a progressive evolution through several key stages:\n\nBasic Sensor Integration\n\nHardware interface with DHT22 (temperature/humidity) and BMP280 (temperature/pressure) sensors\nDigital input through a push-button\nOutput control via RGB LEDs\nFoundational data collection and device control\n\nSLM Basic Analysis\n\nInitial integration with small language models\nSimple observation and reporting of system state\nDemonstration of SLM’s ability to interpret sensor data\n\nActive Control Implementation\n\nDirect LED control based on SLM decisions\nTemperature threshold monitoring\nEmergency state detection via button input\nReal-time system state analysis\n\nNatural Language Interaction\n\nFree-form command interpretation\nContext-aware responses\nMultiple SLM model support\nFlexible query handling\n\nData Logging and Analysis\n\nContinuous system state recording\nTrend analysis and pattern detection\nHistorical data querying\nPerformance monitoring\n\n\nLet’s begin by setting up our hardware and software environment, building upon the foundation established in our previous labs."
  },
  {
    "objectID": "raspi/iot/slm_iot.html#setup",
    "href": "raspi/iot/slm_iot.html#setup",
    "title": "Experimenting with SLMs for IoT Control",
    "section": "Setup",
    "text": "Setup\n\nHardware Setup\n\nConnection Diagram\n\n\n\nComponent\nGPIO Pin\n\n\n\n\nDHT22\nGPIO16\n\n\nBMP280 - SCL\nGPIO03\n\n\nBMP280 - SDA\nGPIO02\n\n\nRed LED\nGPIO13\n\n\nYellow LED\nGPIO19\n\n\nGreen LED\nGPIO26\n\n\nButton\nGPIO20\n\n\n\n\n\nRaspberry Pi 5 (with an OS installed, as detailed in previous labs)\nDHT22 temperature and humidity sensor\nBMP280 temperature and pressure sensor\n3 LEDs (red, yellow, green)\nPush button\n330Ω resistors (3)\nJumper wires and breadboard\n\n\n\n\nSoftware Prerequisites\n\nInstall required libraries:\n\npip install adafruit-circuitpython-dht \npip install adafruit-circuitpython-bmp280"
  },
  {
    "objectID": "raspi/iot/slm_iot.html#basic-sensor-integration",
    "href": "raspi/iot/slm_iot.html#basic-sensor-integration",
    "title": "Experimenting with SLMs for IoT Control",
    "section": "Basic Sensor Integration",
    "text": "Basic Sensor Integration\nLet’s create a Python script (monitor.py) to handle the sensors and actuators. This script will contain functions to be called from other scripts later:\nimport time\nimport board\nimport adafruit_dht\nimport adafruit_bmp280\nfrom gpiozero import LED, Button\n\nDHT22Sensor = adafruit_dht.DHT22(board.D16)\ni2c = board.I2C()\nbmp280Sensor = adafruit_bmp280.Adafruit_BMP280_I2C(i2c, address=0x76)\nbmp280Sensor.sea_level_pressure = 1013.25\n\nledRed = LED(13)\nledYlw = LED(19)\nledGrn = LED(26)\nbutton = Button(20)\n\ndef collect_data():\n    try:\n        temperature_dht = DHT22Sensor.temperature\n        humidity = DHT22Sensor.humidity\n        temperature_bmp = bmp280Sensor.temperature\n        pressure = bmp280Sensor.pressure\n        button_pressed = button.is_pressed\n        return temperature_dht, humidity, temperature_bmp, pressure, button_pressed\n    except RuntimeError:\n        return None, None, None, None, None\n\ndef led_status():\n    ledRedSts = ledRed.is_lit\n    ledYlwSts = ledYlw.is_lit\n    ledGrnSts = ledGrn.is_lit \n    return ledRedSts, ledYlwSts, ledGrnSts\n\n\ndef control_leds(red, yellow, green):\n    ledRed.on() if red else ledRed.off()\n    ledYlw.on() if yellow else ledYlw.off()\n    ledGrn.on() if green else ledGrn.off()\nWe can test the functions using:\nwhile True:\n    ledRedSts, ledYlwSts, ledGrnSts  = led_status()\n    temp_dht, hum, temp_bmp, press, button_state  = collect_data()\n\n    #control_leds(True, True, True)\n\n    if all(v is not None for v in [temp_dht, hum, temp_bmp, press]):\n        print(f\"DHT22 Temp: {temp_dht:.1f}°C, Humidity: {hum:.1f}%\")\n        print(f\"BMP280 Temp: {temp_bmp:.1f}°C, Pressure: {press:.2f}hPa\")\n        print(f\"Button {'pressed' if button_state else 'not pressed'}\")\n        print(f\"Red LED {'is on' if ledRedSts else 'is off'}\")\n        print(f\"Yellow LED {'is on' if ledYlwSts else 'is off'}\")\n        print(f\"Green LED {'is on' if ledGrnSts else 'is off'}\")\n\n\n    time.sleep(2)\n\nInstall Ollama on your Raspberry Pi (follow Ollama’s official documentation or the SLM lab)"
  },
  {
    "objectID": "raspi/iot/slm_iot.html#slm-basic-analysis",
    "href": "raspi/iot/slm_iot.html#slm-basic-analysis",
    "title": "Experimenting with SLMs for IoT Control",
    "section": "SLM Basic Analysis",
    "text": "SLM Basic Analysis\nNow, let’s create a new script, slm_basic_analysis.py, which will be responsible for analysing the hardware components’ status, according to the following diagram:\n\nThe diagram shows the basic analysis system, which consists of:\n\nHardware Layer:\n\nSensors: DHT22 (temperature/humidity), BMP280 (temperature/pressure)\nInput: Emergency button\nOutput: Three LEDs (Red, Yellow, Green)\n\nmonitor.py:\n\nHandles all hardware interactions\nProvides two main functions:\n\ncollect_data(): Reads all sensor values\nled_status(): Checks current LED states\n\n\nslm_basic_analysis.py:\n\nCreates a descriptive prompt using sensor data\nSends prompt to SLM (for example, the Llama 3.2 1B)\nDisplays analysis results\nIn this step we will not control the LEDs (observation only)\n\n\nOkay, let’s implement the code. First, if you haven’t already, install Ollama on your Raspberry Pi (follow Ollama’s official documentation or the SLM lab).\nLet’s import the Ollama library and the functions to monitor the HW (from the previous script):\nimport ollama\nfrom monitor import collect_data, led_status\nCalling the monitor functions, we will get all data:\nledRedSts, ledYlwSts, ledGrnSts  = led_status()\ntemp_dht, hum, temp_bmp, press, button_state  = collect_data()\nNow, the heart of out code, we will generate the Prompt, using the data captured on the previous variables:\nprompt = f\"\"\"\n        You are an experienced environmental scientist. \n        Analyze the information received from an IoT system:\n\n        DHT22 Temp: {temp_dht:.1f}°C and Humidity: {hum:.1f}%\n        BMP280 Temp: {temp_bmp:.1f}°C and Pressure: {press:.2f}hPa\n        Button {\"pressed\" if button_state else \"not pressed\"}\n        Red LED {\"is on\" if ledRedSts else \"is off\"}\n        Yellow LED {\"is on\" if ledYlwSts else \"is off\"}\n        Green LED {\"is on\" if ledGrnSts else \"is off\"}\n\n        Where,\n        - The button, not pressed, shows a normal operation\n        - The button, when pressed, shows an emergency\n        - Red LED when is on, indicates a problem/emergency.\n        - Yellow LED when is on indicates a warning situation.\n        - Green LED when is on, indicates system is OK.\n\n        If the temperature is over 20°C, mean a warning situation\n\n        You should answer only with: \"Activate Red LED\" or \n        \"Activate Yellow LED\" or \"Activate Green LED\"\n\n\"\"\"\nNow, the Prompt will be passed to the SLM, which will generate a response:\nMODEL = 'llama3.2:1b'\nPROMPT = prompt\nresponse = ollama.generate(\n    model=MODEL, \n    prompt=PROMPT\n    )\nThe last stage will be show the real monitored data and the SLM’s response:\nprint(f\"\\nSmart IoT Analyser using {MODEL} model\\n\")\n\nprint(f\"SYSTEM REAL DATA\")\nprint(f\" - DHT22 ==&gt; Temp: {temp_dht:.1f}°C, Humidity: {hum:.1f}%\")\nprint(f\" - BMP280 =&gt; Temp: {temp_bmp:.1f}°C, Pressure: {press:.2f}hPa\")\nprint(f\" - Button {'pressed' if button_state else 'not pressed'}\")\nprint(f\" - Red LED {'is on' if ledRedSts else 'is off'}\")\nprint(f\" - Yellow LED {'is on' if ledYlwSts else 'is off'}\")\nprint(f\" - Green LED {'is on' if ledGrnSts else 'is off'}\")\n\nprint(f\"\\n&gt;&gt; {MODEL} Response: {response['response']}\")\nRuning the Python script, we got:\n\nIn this initial experiment, the system successfully collected sensor data (temperatures of 26.3°C and 26.1°C from DHT22 and BMP280, respectively, 40.2% humidity, and 908.84hPa pressure) and processed this information through the SLM, which produced a coherent response recommending the activation of the yellow LED due to elevated temperature conditions.\nThe model’s ability to interpret sensor data and provide logical, rule-based decisions shows promise. Still, the simplistic nature of the current implementation (using basic thresholds and binary LED outputs) suggests room for significant enhancement through more sophisticated prompting strategies, historical data integration, and the implementation of safety mechanisms. Also, the result is probabilistic, meaning it should change after execution."
  },
  {
    "objectID": "raspi/iot/slm_iot.html#active-control-implementation",
    "href": "raspi/iot/slm_iot.html#active-control-implementation",
    "title": "Experimenting with SLMs for IoT Control",
    "section": "Active Control Implementation",
    "text": "Active Control Implementation\nOK, let’s get a usable output from the SLM by activating one of the LEDs. For that, we will create an action system flow diagram to understand the code implementation better:\n\nThe diagram shows the new action-based system, which adds a User Interface where a user will choose which Model to use based on the SLMs pulled by Ollama. The user will also select the temperature threshold for the test (For example, the actual temperature over this threshold should be configured as a “warning”).\nThe SLM will proceed with a decision-making process regarding what the active LED should be based on the data captured by the system.\nThe key differences for this new code are:\n\nThe basic analysis version only observes and reports\nThe action version actively controls the LEDs\nThe action version includes user configuration\nThe action version implements a continuous monitoring loop\n\nOk, let’s implement the code. Go to the GitHub and download the script slm_basic_analysis_action.py\nThe script implementation consists of several key components:\n\nModel Selection System:\n\nMODELS = {\n    1: ('deepseek-r1:1.5b', 'DeepSeek R1 1.5B'),\n    2: ('llama3.2:1b', 'Llama 3.2 1B'),\n    3: ('llama3.2:3b', 'Llama 3.2 3B'),\n    4: ('phi3:latest', 'Phi-3'),\n    5: ('gemma:2b', 'Gemma 2B'),\n}\n\nProvides multiple SLM options\nEach model offers different capabilities and performance characteristics\nUsers can select based on their needs (speed vs. accuracy)\n\n\nUser Interface Functions:\n\ndef get_user_input():\n    \"\"\"Get user input for model selection and temperature threshold\"\"\"\n    print(\"\\nAvailable Models:\")\n    for num, (_, name) in MODELS.items():\n        print(f\"{num}. {name}\")\n    \n    # Get model selection\n    while True:\n        try:\n            model_num = int(input(\"\\nSelect model (1-4): \"))\n            if model_num in MODELS:\n                break\n            print(\"Please select a number between 1 and 4.\")\n        except ValueError:\n            print(\"Please enter a valid number.\")\n    \n    # Get temperature threshold\n    while True:\n        try:\n            temp_threshold = float(input(\"Enter temperature threshold (°C): \"))\n            break\n        except ValueError:\n            print(\"Please enter a valid number for temperature threshold.\")\n    \n    return MODELS[model_num][0], MODELS[model_num][1], temp_threshold\n\nHandles model selection\nSets temperature threshold\nIncludes input validation\n\n\nResponse Parser:\n\ndef parse_llm_response(response_text):\n    \"\"\"Parse the LLM response to extract LED control instructions.\"\"\"\n    response_lower = response_text.lower()\n    red_led = 'activate red led' in response_lower\n    yellow_led = 'activate yellow led' in response_lower\n    green_led = 'activate green led' in response_lower\n    return (red_led, yellow_led, green_led)\n\nConverts text response to control signals\nSimple but effective parsing strategy\nReturns boolean tuple for LED states\n\n\nMonitoring System:\n\ndef monitor_system(model, model_name, temp_threshold):\n    \"\"\"Monitor system continuously\"\"\"\n    while True:\n        try:\n            # Collect sensor data\n            temp_dht, hum, temp_bmp, press, button_state = collect_data()\n            \n            # Generate prompt and get SLM response\n            response = ollama.generate(\n                model=model,\n                prompt=current_prompt\n            )\n            \n            # Control LEDs based on response\n            red, yellow, green = parse_llm_response(response['response'])\n            control_leds(red, yellow, green)\n            \n            # Print status\n            print_status(...)\n            \n            time.sleep(2)\n            \n        except KeyboardInterrupt:\n            print(\"\\nMonitoring stopped by user\")\n            control_leds(False, False, False)  # Turn off all LEDs\n            break\n\nContinuous monitoring loop\nError handling\nClean shutdown capability\nStatus reporting\n\n\nPrompt Engineering:\n\nprompt = f\"\"\"\n    You are monitoring an IoT system which is showing the \n    following sensor status: \n    - DHT22 Temp: {temp_dht:.1f}°C and Humidity: {hum:.1f}%\n    - BMP280 Temp: {temp_bmp:.1f}°C and Pressure: {press:.2f}hPa\n    - Button {\"pressed\" if button_state else \"not pressed\"}\n\n    Based on the Rules: \n    - If system is working in normal conditions → Activate Green LED \n    - If DHT22 Temp or BMP280 Temp are greater \n      than {temp_threshold}°C → Activate Yellow LED \n    - If Button pressed, it is an emergency → Activate Red LED\n\n    You should provide a brief answer only with: \"Activate Red LED\" \n    or \"Activate Yellow LED\" or \"Activate Green LED\"\n\"\"\"\n\nStructured prompt format\nClear rules and conditions\nConstrained response format\n\nIn the video, we can see how the system works with different models.\nAnd here one screen-shot of the SLM working on the Raspi:\n\nSo, at this point, what we have is something like:\n\n\nWhat we can realize is that the SLM-based system can read and react to the physical world, but with a simple prompt, we cannot guarantee that the result will be correct.\n\nLet’s see how it evolved from the previous code to a new approach, where the SLM should react to a user’s command."
  },
  {
    "objectID": "raspi/iot/slm_iot.html#natural-language-interaction-user-command",
    "href": "raspi/iot/slm_iot.html#natural-language-interaction-user-command",
    "title": "Experimenting with SLMs for IoT Control",
    "section": "Natural Language Interaction (User Command)",
    "text": "Natural Language Interaction (User Command)\nAfter implementing a basic monitoring and automated LED control with slm_basic_analysis_action.py, we can now create a more interactive system that responds to user commands in natural language. This represents an evolution where the SLM makes decisions based on sensor data and understands and responds to user queries and commands.\n\nKey Components and Features\n\nModel Selection\nMODELS = {\n    1: ('deepseek-r1:1.5b', 'DeepSeek R1 1.5B'),\n    2: ('llama3.2:1b', 'Llama 3.2 1B'),\n    3: ('llama3.2:3b', 'Llama 3.2 3B'),\n    4: ('phi3:latest', 'Phi-3'),\n    5: ('gemma:2b', 'Gemma 2B'),\n}\n\nMaintains the same model options as previous versions\nUsers can select their preferred SLM model for interaction\n\nCommand Processing\ndef process_command(model, temp_threshold, user_input):\n    prompt = f\"\"\"\n        You are monitoring an IoT system which is showing the \n        following sensor status: \n        - DHT22 Temp: {temp_dht:.1f}°C and Humidity: {hum:.1f}%\n        - BMP280 Temp: {temp_bmp:.1f}°C and Pressure: {press:.2f}hPa\n        - Button {\"pressed\" if button_state else \"not pressed\"}\n\n        The user command is: \"{user_input}\"\n    \"\"\"\n\nTakes natural language input from users\nCreates context-aware prompts by including current sensor data\nMaintains temperature threshold monitoring\n\nLED Control\ndef parse_llm_response(response_text):\n    \"\"\"Parse the LLM response to extract LED control instructions.\"\"\"\n    response_lower = response_text.lower()\n    red_led = 'activate red led' in response_lower\n    yellow_led = 'activate yellow led' in response_lower\n    green_led = 'activate green led' in response_lower\n    return (red_led, yellow_led, green_led)\n\nUses the same reliable parsing mechanism from previous versions\nMaintains consistency in LED control commands\n\nInteractive Loop\nwhile True:\n    user_input = input(\"Command: \").strip().lower()\n\n    if user_input == 'quit':\n        print(\"\\nShutting down...\")\n        control_leds(False, False, False)\n        break\n\n    process_command(model, temp_threshold, user_input)\n\nProvides continuous interaction through a command prompt\nProcesses one command at a time\nAllows clean system shutdown\n\n\n\n\nSystem Capabilities\nThe system can now: 1. Accept natural language commands and queries 2. Provide information about sensor readings 3. Control LEDs based on user commands 4. Monitor temperature thresholds 5. Display comprehensive system status after each command\n\n\nExample Usage\nSelect model (1-5): 2\nEnter temperature threshold (°C): 25\n\nStarting IoT control system with Llama 3.2 1B\nTemperature threshold: 25°C\nType 'quit' to exit\n\nCommand: what's the current temperature?\n==================================================\nTime: 14:30:45\nDHT22: 22.4°C, 44.8%\nBMP280: 23.2°C, 905.4hPa\nButton: not pressed\nSLM Response: The current temperature is 22.4°C from the DHT22 sensor \nand 23.2°C from the BMP280 sensor.\nLED Status: R=off, Y=off, G=off\n==================================================\n\nCommand: turn on the red led\n[System activates red LED and shows status]\n\nCommand: quit\nShutting down...\nThe previous diagram can be update as:\n\nLet’s see the system runing the above example using the model Llama 3.2 3B:\n\nOr, for example, asking for the SLM to turn on the red LED in case the push-button is activated:\n\nOr the green LED, in case the push-button is not activated:\n\nThe video shows several examples of how the system works.\nLet’s continue evolving the system, which now includes a log to record what happens with the IoT sensors and actuators every minute."
  },
  {
    "objectID": "raspi/iot/slm_iot.html#data-logging-and-analysis",
    "href": "raspi/iot/slm_iot.html#data-logging-and-analysis",
    "title": "Experimenting with SLMs for IoT Control",
    "section": "Data Logging and Analysis",
    "text": "Data Logging and Analysis\nIn this step, we enhance our IoT system by adding data logging, analysis capabilities, and more sophisticated interaction. We split the functionality into two files: monitor_log.py for logging and data analysis and slm_basic_interaction_log.py for user interaction.\n\nThe Logging System (monitor_log.py)\nThis module handles all data logging and analysis functions. Let’s break down its key components:\n# Core functionality\ndef setup_log_file():\n    \"\"\"Create or verify log file with headers\"\"\"\n    headers = ['timestamp', 'temp_dht', 'humidity', 'temp_bmp', 'pressure', \n              'button_state', 'led_red', 'led_yellow', 'led_green', 'command']\nThe system creates a CSV file with headers for all sensor data, LED states, and user commands.\ndef log_data(timestamp, sensors, leds, command=\"\"):\n    \"\"\"Log system data to CSV file\"\"\"\n    temp_dht, hum, temp_bmp, press, button = sensors\n    red, yellow, green = leds\n    \n    row = [\n        timestamp,\n        f\"{temp_dht:.1f}\" if temp_dht is not None else \"NA\",\n        f\"{hum:.1f}\" if hum is not None else \"NA\",\n        # ... other sensor and state data\n    ]\nThis function formats and logs each data point with proper error handling.\ndef automatic_logging():\n    \"\"\"Background thread for automatic logging every minute\"\"\"\n    while not stop_logging.is_set():\n        try:\n            sensors = collect_data()\n            leds = led_status()\n            # ... log data every minute\nA background thread that automatically logs system state every minute.\ndef count_state_changes(series):\n    \"\"\"Count actual state changes in a binary series\"\"\"\n    series = series.astype(int)\n    changes = 0\n    last_state = series.iloc[0]\n    \n    for state in series[1:]:\n        if state != last_state:\n            changes += 1\n            last_state = state\nAccurately counts state changes for LEDs and button presses.\ndef analyze_log_data():\n    \"\"\"Analyze log data and return statistics\"\"\"\n    # Calculates:\n    # - Temperature, humidity, and pressure trends\n    # - Averages for all sensor readings\n    # - LED and button state changes\ndef get_log_summary():\n    \"\"\"Get a formatted summary of log data for SLM prompts\"\"\"\n    # Formats all statistics into a readable summary\nHere is an example of the log summary generated, which will be sent to the SLM per request:\n\n\n\n2. The Interaction System (slm_basic_interaction_log.py)\nThis module handles user interaction and SLM integration:\nMODELS = {\n    1: ('deepseek-r1:1.5b', 'DeepSeek R1 1.5B'),\n    2: ('llama3.2:1b', 'Llama 3.2 1B'),\n    # ... other models\n}\nAvailable SLM models for interaction.\ndef process_command(model, temp_threshold, user_input):\n    \"\"\"Process a single user command\"\"\"\n    # Handles:\n    # 1. Log queries\n    # 2. LED control commands\n    # 3. Sensor data queries\ndef query_log(query, model):\n    \"\"\"Query the log data using SLM\"\"\"\n    # Gets log summary\n    # Creates context-aware prompt\n    # Returns SLM analysis\n\n\nKey Features and Improvements:\n\nData Logging\n\nAutomatic background logging every minute\nComprehensive data storage in CSV format\nCommand history tracking\n\nData Analysis\n\nTemperature and humidity trends\nLED and button state change tracking\nStatistical analysis of sensor data\n\nNatural Language Interaction\n\nLog querying using natural language\nTrend analysis and reporting\nHistorical data access\n\nImproved Error Handling\n\nRobust sensor reading protection\nData validation\nGraceful error recovery\n\n\nThe below flow diagram shows how, in a simplified way, the modules interact and their internal processes.\n\nAnd in this, with more details:\n\nNow, we can run the slm_basic_interaction_log.py, which will call the other two modules.\npython slm_basic_interaction_log.py\nWe can try queries like:\n\"what's the temperature trend?\"\n\"show me button press history\"\n\"turn on the red LED\"\n\"how many times was the button pressed?\"\nExamples:\n\n\nThis modular design separates concerns between data logging/analysis and user interaction, making the system more maintainable and extensible. The SLM integration allows for natural language interaction with current and historical data.\nBelow is an example of the log created."
  },
  {
    "objectID": "raspi/iot/slm_iot.html#evolution-to-structured-command-processing",
    "href": "raspi/iot/slm_iot.html#evolution-to-structured-command-processing",
    "title": "Experimenting with SLMs for IoT Control",
    "section": "Evolution to Structured Command Processing",
    "text": "Evolution to Structured Command Processing\nIn our journey to improve the IoT control system, we should explore a more robust approach to handling commands and responses using structured data models, as we saw in the “Calculating Distance project” section of the SLM Chapter, where the Pydantic python library was used for type checking. This evolution can significantly improve code reliability, maintainability, and extensibility.\nOur original implementation in slm_basic_interaction.py used simple string parsing and direct command processing. While functional, this approach had several limitations:\n\nInconsistent Responses: The SLM could return responses in varying formats, requiring complex parsing logic\nLimited Validation: No built-in validation for command structures or responses\nError-Prone: String parsing could break with slight variations in model outputs\nDifficult Maintenance: Adding new features or command types required modifying multiple code sections\n\n\nStructured Data Models\nAs we did in the SLM chapter, we can use Pydantic to create structured data models that define exactly what our commands and responses should look like. Here’s an example of how we can define our core data structures:\nfrom pydantic import BaseModel, Field\nfrom typing import Optional\n\nclass LEDCommand(BaseModel):\n    red: bool = Field(..., description=\"Whether to turn on red LED\")\n    yellow: bool = Field(..., description=\"Whether to turn on yellow LED\")\n    green: bool = Field(..., description=\"Whether to turn on green LED\")\n    reason: str = Field(..., description=\"Reasoning behind LED state changes\")\n\nclass SensorQuery(BaseModel):\n    temperature_dht: Optional[bool] = Field(False, \n        description=\"Whether to include DHT22 temperature\")\n    temperature_bmp: Optional[bool] = Field(False, \n        description=\"Whether to include BMP280 temperature\")\n    humidity: Optional[bool] = Field(False, \n        description=\"Whether to include humidity\")\n    pressure: Optional[bool] = Field(False, \n        description=\"Whether to include pressure\")\n    button: Optional[bool] = Field(False, \n        description=\"Whether to include button state\")\n\nclass CommandResponse(BaseModel):\n    command_type: str = Field(..., \n        description=\"Type of command (led_control, sensor_query, or system_status)\")\n    led_command: Optional[LEDCommand] = Field(None, \n        description=\"LED control instructions if applicable\")\n    sensor_query: Optional[SensorQuery] = Field(None, \n        description=\"Sensor query specifications if applicable\")\n    response_text: str = Field(..., \n        description=\"Human-readable response to the command\")\nThese models provide several benefits:\n\nType Safety: Automatic validation of data types and structures\nSelf-Documenting: Field descriptions provide built-in documentation\nClear Interface: Explicit definition of what data is expected and provided\nError Handling: Automatic validation with clear error messages\n\n\n\nImproved Command Processing\nThe command processing system should be changed to use these models, ensuring consistent handling:\ndef process_command(model: str, \n                    temp_threshold: float, \n                    user_input: str) -&gt; CommandResponse:\n    \"\"\"Process user command and return structured response\"\"\"\n    # Get current system state\n    temp_dht, hum, temp_bmp, press, button_state = collect_data()\n    \n    # Create structured prompt\n    prompt = f\"\"\"\n    You are an IoT system interface. Current system state:\n    - DHT22: Temperature {temp_dht:.1f}°C, Humidity {hum:.1f}%\n    - BMP280: Temperature {temp_bmp:.1f}°C, Pressure {press:.2f}hPa\n    - Button: {\"pressed\" if button_state else \"not pressed\"}\n    - Temperature threshold: {temp_threshold}°C\n\n    User command: \"{user_input}\"\n\n    Provide a response in this exact JSON format:\n    {{\n        \"command_type\": \"led_control/sensor_query/system_status\",\n        \"led_command\": {{\n            \"red\": boolean,\n            \"yellow\": boolean,\n            \"green\": boolean,\n            \"reason\": \"string\"\n        }},\n        \"sensor_query\": {{\n            \"temperature_dht\": boolean,\n            \"temperature_bmp\": boolean,\n            \"humidity\": boolean,\n            \"pressure\": boolean,\n            \"button\": boolean\n        }},\n        \"response_text\": \"human readable response\"\n    }}\n    \"\"\"\n\n    # Get and parse response\n    response = client.chat.completions.create(\n        model=model,\n        messages=[{\"role\": \"user\", \"content\": prompt}],\n        response_model=CommandResponse,\n        max_retries=3,\n        temperature=0,\n    )\n    \n    # Execute LED commands if present\n    if response.led_command:\n        control_leds(\n            response.led_command.red,\n            response.led_command.yellow,\n            response.led_command.green\n        )\n    \n    return response\n\n\nBenefits of the New Approach\n\nReliability:\n\nStructured responses ensure consistent data format\nAutomatic validation catches errors early\nClear error messages for debugging\n\nMaintainability:\n\nModels separate data structure from logic\nEasy to add new fields or command types\nSelf-documenting code with clear interfaces\n\nExtensibility:\n\nNew command types can be added by extending models\nEasy to add validation rules\nSimple to integrate with other systems\n\nBetter Error Handling:\ntry:\n    response = process_command(model, temp_threshold, user_input)\n    print_status(response, collect_data())\nexcept ValueError as e:\n    print(f\"Invalid command or response: {e}\")\nexcept Exception as e:\n    print(f\"Error processing command: {e}\")\n\n\n\nHandling Different Model Capabilities\nDifferent SLM models may have varying capabilities in generating structured responses. This new approach handles this through:\n\nClear Prompting: Explicit examples and format specifications\nFallback Mechanisms: Graceful degradation for simpler models\nError Recovery: Ability to extract partial information from responses"
  },
  {
    "objectID": "raspi/iot/slm_iot.html#example-usage-1",
    "href": "raspi/iot/slm_iot.html#example-usage-1",
    "title": "Experimenting with SLMs for IoT Control",
    "section": "Example Usage",
    "text": "Example Usage\n# Example command: \"What's the temperature?\"\nResponse:\n{\n    \"command_type\": \"sensor_query\",\n    \"sensor_query\": {\n        \"temperature_dht\": true,\n        \"temperature_bmp\": true,\n        \"humidity\": false,\n        \"pressure\": false,\n        \"button\": false\n    },\n    \"response_text\": \"Current temperature readings: DHT22: 22.4°C, BMP280: 22.8°C\"\n}\n\n# Example command: \"Turn on red LED\"\nResponse:\n{\n    \"command_type\": \"led_control\",\n    \"led_command\": {\n        \"red\": true,\n        \"yellow\": false,\n        \"green\": false,\n        \"reason\": \"User requested red LED activation\"\n    },\n    \"response_text\": \"Activating red LED as requested\"\n}\nThe evolution to structured command processing may significantly improve our IoT control system, providing a more robust and maintainable foundation for future enhancements.\n\nWhile the structured approach adds some overhead, the benefits in reliability and maintainability outweigh the minimal performance impact.\n\nI did not play extensively with this approach, but a first attempt can be found in the GitHub repo."
  },
  {
    "objectID": "raspi/iot/slm_iot.html#next-steps",
    "href": "raspi/iot/slm_iot.html#next-steps",
    "title": "Experimenting with SLMs for IoT Control",
    "section": "Next Steps",
    "text": "Next Steps\nThis lab involved experimenting with simple applications and verifying the feasibility of using an SLM to control IoT devices. The final result is far from something usable in the real world, but it can give the start point for more interesting applications. Below are some observations and suggestions for improvement:\n\nSLM responses can be probabilistic and inconsistent. To increase reliability, consider implementing a confidence threshold or voting system using multiple prompts/responses.\nTry to add data validation and sanity checks for sensor readings before passing them to the SLM.\nApply Structured Response Parsing as discussed early. Future improvements in this approuch could include:\n\nAdd more sophisticated validation rules\nImplement command history tracking\nAdd support for compound commands\nIntegrate with the logging system\nAdd user permission levels\nImplement command templates for common operations\n\nConsider implementing a fallback mechanism when SLM responses are ambiguous or inconsistent.\nStudy using RAG and fine-tuning to increase the system’s reliability when using very small models.\nConsider adding input validation for user commands to prevent potential issues.\nThe current implementation queries the SLM for every command. We did it to study how SLMs would behave. We should consider implementing a caching mechanism for common queries.\nSome simple commands could be handled without SLM intervention. We can do it programmatically.\nConsider implementing a proper state machine for LED control to ensure consistent behavior.\nImplement more sophisticated trend analysis using statistical methods.\nAdd support for more complex queries combining multiple data points."
  },
  {
    "objectID": "raspi/iot/slm_iot.html#resourses",
    "href": "raspi/iot/slm_iot.html#resourses",
    "title": "Experimenting with SLMs for IoT Control",
    "section": "Resourses",
    "text": "Resourses\n\nPython Scripts"
  },
  {
    "objectID": "raspi/advancing_adgeai/adv_edgeai.html#understanding-slm-limitations",
    "href": "raspi/advancing_adgeai/adv_edgeai.html#understanding-slm-limitations",
    "title": "Advancing EdgeAI: Beyond Basic SLMs",
    "section": "Understanding SLM Limitations",
    "text": "Understanding SLM Limitations\nSmall Language Models, while impressive in their ability to run on edge devices, face several key limitations:\n\n1. Knowledge Constraints\nSLMs have limited knowledge based on their training data, often outdated and incomplete. Unlike their larger counterparts, they cannot store the vast information needed for comprehensive expertise across all domains.\nLet’s run the below example to verify this limitation.\nimport ollama\n\nresponse = ollama.generate(\n    model=\"llama3.2:1b\",\n    prompt=\"Who won the 2024 Summer Olympics men's 100m sprint final?\"\n)\nprint(response['response'])\nThe output of the previous code will likely show hallucination or admission of not knowing, as in the case below:\n\nThis constraint could be solved simply by having an Agent search the Internet for the answer or using Retrieval-Augmented Generation (RAG), as we will see later.\n\n\n2. Reasoning Limitations\nComplex reasoning tasks often exceed the capabilities of SLMs, which struggle with multi-step logical deductions, mathematical computations, and a nuanced understanding of context. Agents can be used to mitigate such limitations.\nFor example, let’s reuse the previous code and ask to the SLM to multiply two numbers :\nimport ollama\n\nresponse = ollama.generate(\n    model=\"llama3.2:3b\",\n    prompt=\"Multiply 123456 by 123456\"\n)\nprint(response['response'])\n\nThe response is wrong; once the multiplication result should be 15,241,383,936. This is expected once the language models are not suitable for mathematical computations. Still, we can use an “agent” to determine whether a user asks for multiplication or a general question. We will learn how to create an agent later.\n\n\n3. Inconsistent Outputs\nSLMs may produce inconsistent responses to the same query, making them unreliable for critical applications requiring deterministic outputs. Several enhancements, such as Function Calling and Response Validation, can improve reliability.\n\n\n4. Domain Specialization\nSLMs perform worse than specialized models in domain-specific tasks like visual recognition or time-series analysis. Fine-tuning can adapt models to specific domains or tasks, improving performance for targeted applications."
  },
  {
    "objectID": "raspi/advancing_adgeai/adv_edgeai.html#techniques-for-enhancing-slm-at-the-edge",
    "href": "raspi/advancing_adgeai/adv_edgeai.html#techniques-for-enhancing-slm-at-the-edge",
    "title": "Advancing EdgeAI: Beyond Basic SLMs",
    "section": "Techniques for Enhancing SLM at the Edge",
    "text": "Techniques for Enhancing SLM at the Edge\nSmall Language Models (SLMs) offer remarkable capabilities for edge devices, but various techniques can significantly enhance their effectiveness. Here, we present a comprehensive framework for optimizing SLMs on resource-constrained devices like the Raspberry Pi, organized from fundamental to advanced approaches.\nWe will divide those technics into 3 segments:\n\nFundamentals: Optimizing Prompting Strategies\n\nChain-of-Thought Prompting\nFew-Shot Learning\nTask Decomposition\n\nIntermediate: Building Intelligence Systems\n\nBuilding Agents with SLMs\nGeneral Knowledge Router\nFunction Calling\nResponse Validation\n\nAdvanced: Extending Knowledge and Specialization\n\nRetrieval-Augmented Generation (RAG)\nFine-Tuning for Domain Specialization\n\nIntegration: Combining Techniques for Optimal Performance\n\nThe true power of these techniques emerges when they’re strategically combined:\n\nAgent Architecture with RAG: Create agents that can access both tools and knowledge bases\nValidation-Enhanced RAG: Apply response validation to ensure RAG outputs are accurate\nFine-Tuned Routers: Use specialized fine-tuned models to handle routing decisions\nChain-of-Thought with Function Calling: Combine reasoning traces with structured outputs\n\nFor example, a comprehensive weather monitoring system, as we introduced in the chapter “Experimenting with SLMs for IoT Control,” might use the following:\n\nRAG to access historical weather patterns and interpretation guides\nFunction calling to structure sensor data analysis\nResponse validation to verify recommendations\nTask decomposition to handle complex multi-part weather analysis"
  },
  {
    "objectID": "raspi/advancing_adgeai/adv_edgeai.html#optimizing-prompting-strategies",
    "href": "raspi/advancing_adgeai/adv_edgeai.html#optimizing-prompting-strategies",
    "title": "Advancing EdgeAI: Beyond Basic SLMs",
    "section": "Optimizing Prompting Strategies",
    "text": "Optimizing Prompting Strategies\n\nChain-of-Thought Prompting\nChain-of-thought prompting encourages SLMs to break down complex problems into step-by-step reasoning, leading to more accurate results:\ndef solve_math_problem(problem):\n    prompt = f\"\"\"\n    Problem: {problem}\n    Let's think about this step by step:\n    1. First, I'll identify what we're looking for\n    2. Then, I'll identify the relevant information\n    3. Next, I'll set up the appropriate equations\n    4. Finally, I'll solve the problem carefully\n    Solving:\n    \"\"\"\n    response = ollama.generate(model=\"llama3.2:3b\", prompt=prompt)\n    return response['response']\nThis technique significantly improves performance on reasoning tasks by emulating human problem-solving approaches.\n\n\nFew-Shot Learning\nFew-shot learning provides examples within the prompt, helping SLMs understand the expected response format and reasoning pattern:\ndef classify_sentiment(text):\n    prompt = f\"\"\"\n    Task: Classify the sentiment of the text as positive, negative, or neutral.\n    Examples:\n    Text: \"I love this product, it works perfectly!\"\n    Sentiment: positive\n    Text: \"This is the worst experience I've ever had.\"\n    Sentiment: negative\n    Text: \"The package arrived on time.\"\n    Sentiment: neutral\n    Text: \"{text}\"\n    Sentiment:\n    \"\"\"\n    response = ollama.generate(model=\"llama3.2:1b\", prompt=prompt)\n    return response['response'].strip()\nThis approach is particularly effective for classification tasks and standardized outputs.\n\n\nTask Decomposition\nFor complex tasks, breaking them into smaller subtasks helps SLMs manage complexity:\ndef analyze_product_review(review):\n    # Step 1: Extract main points\n    points_prompt = f\"Extract the main points from this product review: {review}\"\n    points_response = ollama.generate(model=\"llama3.2:1b\", prompt=points_prompt)\n    main_points = points_response['response']\n    \n    # Step 2: Determine sentiment\n    sentiment_prompt = f\"Determine the overall sentiment of this review: {review}\"\n    sentiment_response = ollama.generate(model=\"llama3.2:1b\", \n                                         prompt=sentiment_prompt)\n    sentiment = sentiment_response['response']\n    \n    # Step 3: Identify improvement suggestions\n    improvements_prompt = f\"What suggestions for improvement can be found in \\\n    this review? {review}\"\n    improvements_response = ollama.generate(model=\"llama3.2:1b\",\n                                            prompt=improvements_prompt)\n    improvements = improvements_response['response']\n    \n    # Final synthesis\n    final_prompt = f\"\"\"\n    Create a concise analysis of this product review based on:\n    Main points: {main_points}\n    Overall sentiment: {sentiment}\n    Improvement suggestions: {improvements}\n    \"\"\"\n    final_response = ollama.generate(model=\"llama3.2:1b\", \n                                     prompt=final_prompt)\n    return final_response['response']\nThis technique distributes cognitive load across multiple simpler prompts, enabling SLMs to handle tasks that might otherwise exceed their capabilities."
  },
  {
    "objectID": "raspi/advancing_adgeai/adv_edgeai.html#building-agents-with-slms",
    "href": "raspi/advancing_adgeai/adv_edgeai.html#building-agents-with-slms",
    "title": "Advancing EdgeAI: Beyond Basic SLMs",
    "section": "Building Agents with SLMs",
    "text": "Building Agents with SLMs\nTo address some of these limitations, we can develop agents that leverage SLMs as part of a more extensive system with additional capabilities.\n\nWhat is an Agent? It is an AI model capable of reasoning, planning, and interacting with its environment. It can be called an Agent because it has an agency that can interact with the environment.\n\nLet’s think about the multiplication problem that we faced before. An Agent can be used for that.\nAn agent is a system that uses an AI Model as its core reasoning engine to:\n\nUnderstand natural language: (1) Interpret and respond to human instructions meaningfully.\nReason and plan: (2) Analyze information, make decisions, and devise problem-solving strategies.\nInteract with its environment: (3 and 4) Gather information, take actions, and observe the results of those actions.\n\n\nFor example, if it is a multiplication, we can use a Python function as a “tool” to calculate it, as shown in the diagram:\n\nOur code works through the following steps:\n\nUser Input: The user types a query like “What is 7 times 8?” or “What is the capital of France?”\nProcess Query: The process_query() function handles the input and decides what to do with it.\nClassification: The ask_ollama_for_classification() function sends the user’s query to the SLM (using Ollama) with a prompt asking it to classify whether the query is requesting multiplication or asking a general question.\nDecision: Based on the SLM’s classification:\n\nIf it’s a multiplication request, the SLM also extracts the numbers, and we use our multiply() function.\nIf it’s a general question, we send the original query to the SLM for a direct answer.\n\nResponse: The system returns either the multiplication result or the SLM’s answer to the general question.\n\nHere’s a Python script that creates a simple agent (or router) between multiplication operations and general questions as described:\nimport requests\nimport json\n\n# Configuration\nOLLAMA_URL = \"http://localhost:11434/api\"\nMODEL = \"llama3.2:3b\"  # You can change this to any model you have installed\nVERBOSE = True\n\ndef ask_ollama_for_classification(user_input):\n    \"\"\"\n    Ask Ollama to classify whether the query is a multiplication request or a \\\n    general question.\n    \"\"\"\n    classification_prompt = f\"\"\"\n    Analyze the following query and determine if it's asking for multiplication \\\n    or if it's a general question.\n    \n    Query: \"{user_input}\"\n    \n    If it's asking for multiplication, respond with a JSON object in this format:\n    {{\n      \"type\": \"multiplication\",\n      \"numbers\": [number1, number2]\n    }}\n    \n    If it's a general question, respond with a JSON object in this format:\n    {{\n      \"type\": \"general_question\"\n    }}\n    \n    Respond ONLY with the JSON object, nothing else.\n    \"\"\"\n    \n    try:\n        if VERBOSE:\n            print(f\"Sending classification request to Ollama\")\n        \n        response = requests.post(\n            f\"{OLLAMA_URL}/generate\",\n            json={\n                \"model\": MODEL,\n                \"prompt\": classification_prompt,\n                \"stream\": False\n            }\n        )\n        \n        if response.status_code == 200:\n            response_text = response.json().get(\"response\", \"\").strip()\n            if VERBOSE:\n                print(f\"Classification response: {response_text}\")\n            \n            # Try to parse the JSON response\n            try:\n                # Find JSON content if there's any surrounding text\n                start_index = response_text.find('{')\n                end_index = response_text.rfind('}') + 1\n                if start_index &gt;= 0 and end_index &gt; start_index:\n                    json_str = response_text[start_index:end_index]\n                    return json.loads(json_str)\n                return {\"type\": \"general_question\"}\n            except json.JSONDecodeError:\n                if VERBOSE:\n                    print(f\"Failed to parse JSON: {response_text}\")\n                return {\"type\": \"general_question\"}\n        else:\n            if VERBOSE:\n                print(f\"Error: Received status code {response.status_code} \\\n                from Ollama.\")\n            return {\"type\": \"general_question\"}\n    \n    except Exception as e:\n        if VERBOSE:\n            print(f\"Error connecting to Ollama: {str(e)}\")\n        return {\"type\": \"general_question\"}\n\ndef multiply(a, b):\n    \"\"\"\n    Perform multiplication and return a formatted response.\n    \"\"\"\n    result = a * b\n    return f\"The product of {a} and {b} is {result}.\"\n\ndef ask_ollama(query):\n    \"\"\"\n    Send a query to Ollama for general question answering.\n    \"\"\"\n    try:\n        if VERBOSE:\n            print(f\"Sending query to Ollama\")\n        \n        response = requests.post(\n            f\"{OLLAMA_URL}/generate\",\n            json={\n                \"model\": MODEL,\n                \"prompt\": query,\n                \"stream\": False\n            }\n        )\n        \n        if response.status_code == 200:\n            return response.json().get(\"response\", \"\")\n        else:\n            return f\"Error: Received status code {response.status_code} \\\n            from Ollama.\"\n    \n    except Exception as e:\n        return f\"Error connecting to Ollama: {str(e)}\"\n\ndef process_query(user_input):\n    \"\"\"\n    Process the user input by first asking Ollama to classify it,\n    then either performing multiplication or sending it back as a \n    general question.\n    \"\"\"\n    # Let Ollama classify the query\n    classification = ask_ollama_for_classification(user_input)\n    \n    if VERBOSE:\n        print(\"Ollama classification:\", classification)\n    \n    if classification.get(\"type\") == \"multiplication\":\n        numbers = classification.get(\"numbers\", [0, 0])\n        if len(numbers) &gt;= 2:\n            return multiply(numbers[0], numbers[1])\n        else:\n            return \"I understood you wanted multiplication, but couldn't \\\n            extract the numbers properly.\"\n    else:\n        return ask_ollama(user_input)\n\ndef main():\n    \"\"\"\n    Main function to run the agent interactively.\n    \"\"\"\n    print(\"Ollama Agent (Type 'exit' to quit)\")\n    print(\"-----------------------------------\")\n    \n    while True:\n        user_input = input(\"\\nYou: \")\n        \n        if user_input.lower() in [\"exit\", \"quit\", \"bye\"]:\n            print(\"Goodbye!\")\n            break\n        \n        response = process_query(user_input)\n        print(f\"\\nAgent: {response}\")\n\n# Example usage\nif __name__ == \"__main__\":\n    # Set to True to see detailed logging\n    VERBOSE = True\n    main()\nWhen we run the script, we can see that, first, the SLM chooses multiplication, passing the numbers entered by the user to the “tool,” which, in this case, is the multiply() function. As a result, we got 15,241,383,936, which it is correct.\n\nLet’s now enter with another question that has no relation with arithmetic, for example: What is the capital of Brazil? In this case, the SLM will decide that the query is a general question and pass it on to the SLM to answer it.\n\nThis simple agent (or router) demonstrates the fundamental concept of using an SLM to make decisions about processing different types of user inputs. It shows both the power of SLMs for natural language understanding and their limitations in structured tasks.\n\nLimitations and Considerations\nThis agent seems to resolve our problem, but it has several limitations that are common when working with SLMs:\n\nJSON Parsing Issues: SLMs don’t always perfectly format JSON responses as requested. The code includes error handling for this.\nClassification Reliability: The SLM might not always correctly classify the query, especially with ambiguous questions.\nNumber Extraction: The SLM might extract numbers incorrectly or miss them entirely.\nError Handling: Robust error handling is essential when working with SLMs because their outputs can be unpredictable.\nLatency: Significant latency is involved in making multiple calls to the SLM. For example, for the above simple agent, the latency was about 50s when using the llama3.2:3B on a Raspberry Pi 5.\n\nHere, you can see the SLM latency (simple query) per device (in tokens/s):\n\n\n\n\n\n\n\n\n\nModel\nRaspi 5 (Cortex A-76)\nPC (i7)\nMac (M1 Pro)\n\n\n\n\nGemma3:4b\n3.8\n8.7\n39\n\n\nLlama3.2:3b\n5.5\n12\n63\n\n\nLlama3.2:1b\n7.5\n19.5\n111\n\n\nGemma3:1b\n12\n22.45\n91\n\n\n\n\nIn my simple tests, the 1B models struggled to classify the tasks correctly. The the 3B and 4B models worked fine\n\n\n\nImprovements\nTo create a more robust agent, we can, for example:\n\nExpand Capabilities: Add support for more operations (addition, subtraction, division).\nBetter Error Handling: Improve fallback mechanisms when the SLM fails to extract numbers or classify correctly.\nModel Preloading: Initialize the model at startup to reduce latency.\nAdding Regex Fallbacks: Use regular expressions as a fallback to extract numbers when the SLM fails.\nContext Preservation: Maintain conversation context for multi-turn interactions.\n\nA more robust script can be used with the above improvements. The diagram shows how it would work:\n\n\n\nimage-20250322113135882\n\n\nThe diagram illustrates the key components of the system:\n\nInitialization:\n\nThe system starts by initializing both models in parallel threads\nThis prevents cold starts and reduces latency\n\nQuery Processing Flow:\n\nUser input is first sent to a classification step\nA model (llama3.2:3B) determines if it’s a calculation or a general question (we can choose a different model here).\nIf it’s a calculation:\n\nThe system extracts the operation type and numbers\nNumbers are converted from strings to floats\nThe appropriate calculation is performed\nResults are formatted with comma separators (e.g., 1,234,567.89)\n\nIf it’s a general question:\n\nThe query is sent to the main model (llama3.2:3b) for answering (we can choose a different model here)\n\n\nOptimizations (highlighted in the subgraph):\n\nPersistent HTTP session for connection reuse\nKeep-alive parameter to prevent model unloading\nSimplified classification prompt for faster processing\nUsing a smaller model for the classification task\nRule-based fallback logic if the model classification fails\n\n\nThe main performance improvements come from:\n\nKeeping models loaded in memory\nUsing connection pooling\nSimplifying the classification task\nUsing a smaller model for classification\nInitializing models in parallel\n\nThis approach maintains the intelligent classification capability while significantly reducing execution time compared to the original implementation.\nRuning the script 3-ollama-calculator-agent.py, we get correct results with reduced latency of about 60%.\n\n\n\n\nGeneral Knowledge Router\nRemember when we asked our SLM: Who won the 2024 Summer Olympics men's 100m sprint final? We could not receive an answer because the modes were trained with information previously in late 2023.\nTo solve this issue, let’s build a more advanced agent to classify whether it should use its knowledge to answer a question or fetch updated information from the Internet. This addresses a key limitation of Small Language Models: their knowledge cutoff date.\nThe general architecture of our agent will be similar to the calculator, but now, we will use a web search API as a tool.\nThis agent addresses a critical limitation of SLMs - their knowledge cutoff date - by determining when to use the model’s built-in knowledge versus when to search for up-to-date information from the web.\n\nHow it works:\nUses SLM for Classification: Relies entirely on the SLM to determine whether a query needs web search or can be answered from the model’s knowledge.\nProvides Date Context: This section supplies the current date to help the SLM make informed decisions about whether information is outdated.\nIntegrates Tavily Search: Uses Tavily’s powerful search API to find relevant information for queries that need external data.\nHandles Timeouts: Includes fallback mechanisms when the model takes too long to respond.\nMaintains Source Attribution: Clearly indicates to the user whether the answer comes from the model’s knowledge or web search.\nLet’s run the script: 4-ollama-search-agent.py\nBut first, we should install the required libraries:\npip install requests\npip install tavily-python\nReplace \"tvly-YOUR_API_KEY\" with your actual Tavily API key.\n\nWhy Tavily is Superior for This Use Case\n\nBuilt for RAG: Tavily is specifically designed for retrieval-augmented generation, making it perfect for our knowledge router.\nHigh-Quality Results: It prioritizes reputable sources and provides context-relevant results.\nBuilt-in Summarization: The API can provide an AI-generated summary of search results, giving an additional layer of processing before your SLM.\nSimple Integration: Clean API with straightforward responses that are easy to parse.\nGenerous Free Tier: 1,000 free searches is plenty for testing and personal use.\n\n\nRuning the script and entering with the same questions that could not be answered before, we now have: Noah Lyles won the men's 100m sprint final at the 2024 Summer Olympics. He set a new personal best time of 9.79 seconds. This victory marked the United States' first win in the event since 2004.\n\nWhen the user enters a common-knowledge question, the agent will send it directly to the SLM. For example, if the user asks, \"Who is Albert Einstein?\", we get:"
  },
  {
    "objectID": "raspi/advancing_adgeai/adv_edgeai.html#improving-agent-reliability",
    "href": "raspi/advancing_adgeai/adv_edgeai.html#improving-agent-reliability",
    "title": "Advancing EdgeAI: Beyond Basic SLMs",
    "section": "Improving Agent Reliability",
    "text": "Improving Agent Reliability\nThere are several ways to enhance an agent’s reliability. One is to implement effective, approved, structured function calling, which makes agents’ responses more consistent and predictable.\n\n1. Function Calling with Pydantic\nIn the SLM chapter, we explored function calling when we created an app where the user enters a country’s name and gets, as an output, the distance in km from the capital city of such a country and the app’s location.\n\nOnce the user enters a country name, the model will return the name of its capital city (as a string) and the latitude and longitude of such city (in float). Using those coordinates, the app used a simple Python library (haversine) to calculate the distance between those 2 points.\nThe critical library used was Pydantic (and instructor), a robust data validation and settings management library engineered by Python to enhance the robustness and reliability of our codebase. In short, Pydantic helps ensure that the model’s response will always be consistent.\nFunction calling can improve an agent’s reliability by ensuring structured outputs and clear tool selection logic. Here’s a generic template about how we can implement it :\nfrom pydantic import BaseModel, Field\nfrom typing import Optional, List\nimport instructor\nfrom openai import OpenAI\n\nclass ToolCall(BaseModel):\n    tool_name: str = Field(..., description=\"Name of the tool to call\")\n    parameters: dict = Field(..., description=\"Parameters for the tool\")\n    reasoning: str = Field(..., description=\"Reasoning for using this tool\")\n\nclass AgentResponse(BaseModel):\n    needs_tool: bool = Field(..., description=\"Whether a tool is needed\")\n    tool_calls: Optional[List[ToolCall]] = Field(None, \n                                                 description=\"Tools to call\")\n    direct_response: Optional[str] = Field(None, \n                                           description=\"Direct response if no \\\n                                           tool needed\")\n\n# Set up the client with structured output\nclient = instructor.patch(\n    OpenAI(\n        base_url=\"http://localhost:11434/v1\",\n        api_key=\"ollama\"\n    ),\n    mode=instructor.Mode.JSON,\n)\n\ndef structured_think(query, available_tools, model):\n    tool_descriptions = \"\\n\".join([f\"- {name}: {desc}\" \n                                 for name, desc in available_tools.items()])\n    \n    prompt = f\"\"\"\n    Available tools:\n    {tool_descriptions}\n    \n    User query: {query}\n    \n    Determine if any tools are needed to answer this query accurately.\n    \"\"\"\n    \n    response = client.chat.completions.create(\n        model=model,\n        messages=[{\"role\": \"user\", \"content\": prompt}],\n        response_model=AgentResponse,\n        max_retries=3\n    )\n    \n    return response\n\n\n2. Response Validation\nResponse validation is crucial to developing and deploying AI agents powered by language models. Here are key points regarding LLM validation for agents: Types of Validation\n\nResponse Relevancy: Determines if the LLM output addresses the input informatively and concisely.\nPrompt Alignment: Check if the LLM output follows instructions from the prompt template.\nCorrectness: Assesses factual accuracy based on ground truth.\nHallucination Detection: Identifies fake or made-up information in LLM outputs.\n\nAdding validation prevents incorrect or harmful responses, and here, we can test it with a simple script:\nimport ollama\nimport json\n\ndef validate_response(query, response):\n    \"\"\"Validate that the response is appropriate for the query\"\"\"\n    validation_prompt = f\"\"\"\n    User query: {query}\n    Generated response: {response}\n    \n    Evaluate if this response:\n    1. Directly addresses the user's query\n    2. Is factually accurate to the best of your knowledge\n    3. Is helpful and complete\n    \n    Respond in the following JSON format:\n    {{\n        \"valid\": true/false,\n        \"reason\": \"Explanation if invalid\",\n        \"score\": 0-10\n    }}\n    \"\"\"\n    \n    try:\n        validation = ollama.generate(\n            model=\"llama3.2:3b\",  \n            prompt=validation_prompt\n        )\n        \n        result = json.loads(validation['response'])\n        return result\n    except Exception as e:\n        print(f\"Error during validation: {e}\")\n        return {\"valid\": False, \"reason\": \"Validation error\", \"score\": 0}\n\n# Test\nquery = \"What is the Raspberry Pi 5?\"\nresponse = \"It is a pie created with raspberry and cooked in an oven\"\nvalidation = validate_response(query, response)\nprint(validation)"
  },
  {
    "objectID": "raspi/advancing_adgeai/adv_edgeai.html#retrieval-augmented-generation-rag",
    "href": "raspi/advancing_adgeai/adv_edgeai.html#retrieval-augmented-generation-rag",
    "title": "Advancing EdgeAI: Beyond Basic SLMs",
    "section": "Retrieval-Augmented Generation (RAG)",
    "text": "Retrieval-Augmented Generation (RAG)\nRAG systems enhance Small Language Models (SLMs) by providing relevant information from external sources before generation. This is particularly valuable for edge devices with limited model sizes, as it allows them to access knowledge beyond their training data without increasing the model size.\n\nUnderstanding RAG\nIn a basic interaction between a user and a language model, the user asks a question, which is sent as a prompt to the model. The model generates a response based solely on its pre-trained knowledge. In a RAG process, there’s an additional step between the user’s question and the model’s response. The user’s question triggers a retrieval process from a knowledge base.\n\nThe RAG process consists of these key steps:\n\nQuery Processing: When a user asks a question, the system converts it into an embedding (a numerical representation).\nDocument Retrieval: The system searches a knowledge base for documents with similar embeddings.\nContext Enhancement: Relevant documents are retrieved and combined with the original query.\nGeneration: The SLM generates a response using both the query and the retrieved context.\n\n\n\nImplementing a Basic RAG System\nWe will develop two crucial components (scripts) of an RAG system:\n\nCreating the Vector Database (10-Create-Persistent-Vector-Database.py). This script builds a knowledge base by:\n\nLoading documents from PDFs and URLs\nSplitting them into manageable chunks\nCreating embeddings for each chunk\nStoring these embeddings in a vector database (Chroma)\n\nQuerying the Database (20-Query-the-Persistent-RAG-Database.py). This script:\n\nLoads the saved vector database\nAccepts user queries\nRetrieves relevant documents based on query similarity\nCombines documents with the query in a prompt\nGenerates a response using the SLM\n\n\nLet’s examine how these components work together to implement a RAG system on edge devices.\n\n\nKey Components of Our Edge RAG System\n\nDocument Processing\n\ndef create_vectorstore():\n    # Load documents from PDFs and URLs\n    docs_list = []\n    # [Document loading code]\n    \n    # Split documents into chunks\n    text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n        chunk_size=300, chunk_overlap=30\n    )\n    doc_splits = text_splitter.split_documents(docs_list)\n    \n    # Create embeddings and store in vector database\n    embedding_function = OllamaEmbeddings(model=\"nomic-embed-text\")\n    vectorstore = Chroma.from_documents(\n        documents=doc_splits,\n        collection_name=\"rag-edgeai-eng-chroma\",\n        embedding=embedding_function,\n        persist_directory=PERSIST_DIRECTORY\n    )\n    \n    # Persist to disk\n    vectorstore.persist()\nThis function processes our documents, creating a searchable knowledge base. Notice we’re using OllamaEmbeddingswith the nomic-embed-text model, which can run efficiently on edge devices like the Raspberry Pi.\n\nQuery Processing and Retrieval\n\ndef answer_question(question, retriever):\n    # Retrieve relevant documents\n    docs = retriever.invoke(question)\n    docs_content = \"\\n\\n\".join(doc.page_content for doc in docs)\n    \n    # Generate answer using RAG prompt\n    rag_prompt = hub.pull(\"rlm/rag-prompt\")\n    rag_chain = rag_prompt | llm | StrOutputParser()\n    \n    # Generate the answer\n    answer = rag_chain.invoke({\"context\": docs_content, \"question\": question})\n    \n    return answer\nThis function retrieves relevant documents based on the query and combines them with a specialized RAG prompt to generate a response. The RAG prompt is particularly important as it tells the model how to use the context documents to answer the question.\n\nSLM Integration\n\n# Initialize the LLM\nlocal_llm = \"llama3.2:3b\"\nllm = ChatOllama(model=local_llm, temperature=0)\nWe’re using Ollama to run the SLM locally on our edge device, in this case using the 3B parameter version of Llama 3.2.\n\n\nAdvantages of RAG for Edge AI\nUsing RAG on edge devices offers several significant advantages:\n\nKnowledge Extension: RAG allows small models to access knowledge beyond their training data, effectively extending their capabilities without increasing model size.\nReduced Hallucination: By providing factual context, RAG significantly reduces the likelihood of SLMs generating incorrect information.\nUp-to-date Information: Unlike the fixed knowledge in a model’s weights, RAG knowledge bases can be updated regularly with new information.\nDomain Specialization: RAG can make general SLMs perform like domain specialists by providing domain-specific knowledge bases.\nResource Efficiency: RAG allows smaller models (which require less memory and computation) to achieve performance comparable to much larger models.\n\n\n\nOptimizing RAG for Edge Devices\nWhen implementing RAG on resource-constrained edge devices like the Raspberry Pi, consider these optimizations:\n\nChunk Size: Smaller chunks (300-500 tokens) reduce memory usage during retrieval and generation.\nRetrieval Limits: Limit the number of retrieved documents (k=3 to 5) to reduce context size.\nEmbedding Model Selection: Choose lightweight embedding models like nomic-embed-text (137M parameters) or all-minilm (23M parameters).\nPersistent Storage: As shown in our examples, using persistent storage prevents recomputing embeddings every time that the RAG system is initiated.\nQuery Optimization: Implement query preprocessing to improve retrieval accuracy while reducing computational load.\n\ndef optimize_query(query):\n    \"\"\"Optimize the query for better retrieval results\"\"\"\n    # Remove filler words, focus on key terms\n    stop_words = {\"and\", \"or\", \"the\", \"a\", \"an\", \"in\", \"on\", \"at\", \"to\", \n                  \"for\", \"with\"}\n    terms = [term for term in query.lower().split() if term not in stop_words]\n    return \" \".join(terms)\n\n\nApplication: Enhanced Weather Station with RAG\nBuilding on our advanced weather station (see the chapter “Experimenting with SLMs for IoT Control”), we can, for example, integrate RAG to provide more contextual responses about weather conditions and historical patterns:\n\ndef weather_station_with_rag(retriever, model=\"llama3.2:3b\"):\n    # Get current sensor readings\n    temp_dht, humidity, temp_bmp, pressure, button_state = collect_data()\n    \n    # Formulate a query for the RAG system based on current readings\n    query = f\"Analysis of temperature {temp_dht}°C, humidity {humidity}%, \\\n    and pressure {pressure}hPa\"\n    \n    # Retrieve relevant context\n    docs = retriever.invoke(query)\n    context = \"\\n\\n\".join(doc.page_content for doc in docs)\n    \n    # Create a prompt that combines current readings with retrieved context\n    prompt = f\"\"\"\n    Current Weather Station Data:\n    - Temperature (DHT22): {temp_dht:.1f}°C\n    - Humidity: {humidity:.1f}%\n    - Pressure: {pressure:.2f}hPa\n    \n    Reference Information:\n    {context}\n    \n    Based on current readings and the reference information, provide:\n    1. An analysis of current weather conditions\n    2. What these conditions typically indicate\n    3. Recommendations for any actions needed\n    \"\"\"\n    \n    # Generate response using SLM\n    llm = ChatOllama(model=model, temperature=0)\n    response = llm.invoke(prompt)\n    \n    return response.content\nThis function enhances our weather station by providing context-aware responses incorporating current sensor readings and relevant information from our knowledge base. This is only an example. To use it, we should have “Weather Reference Data,” which we do not currently have. Instead, let’s create a general RAG system specializing in Edge AI Engineering.\n\n\nUsing the RAG System for Edge AI Engineering\nFor our RAG system, we will create a database with all chapters alheady written for the EdgeAI Engineering book (chapters as URLs) and a PDF Wevolver 2025 Edge AI Technology Report.\n# PDF documents to include\npdf_paths = [\"./data/2025_Edge_AI_Technology_Report.pdf\"]\n\n# Define URLs for document sources\nurls = [\n    \"https://mjrovai.github.io/EdgeML_Made_Ease_ebook/raspi\\\n    /object_detection/object_detection.html\",\n    \"https://mjrovai.github.io/EdgeML_Made_Ease_ebook/raspi/image_classification\\\n    /image_classification.html\",\n    \"https://mjrovai.github.io/EdgeML_Made_Ease_ebook/raspi/setup/setup.html\",\n    \"https://mjrovai.github.io/EdgeML_Made_Ease_ebook/raspi/counting_objects_yolo\\\n    /counting_objects_yolo.html\",\n    \"https://mjrovai.github.io/EdgeML_Made_Ease_ebook/raspi/llm/llm.html\",\n    \"https://mjrovai.github.io/EdgeML_Made_Ease_ebook/raspi/vlm/vlm.html\",\n    \"https://mjrovai.github.io/EdgeML_Made_Ease_ebook/raspi/physical_comp\\\n    /RPi_Physical_Computing.html\",\n    \"https://mjrovai.github.io/EdgeML_Made_Ease_ebook/raspi/iot/slm_iot.html\",\n]\nUsing the RAG system is straightforward. First, ensure you’ve created the vector database:\n# Run once to create the database\npython 10-Create-Persistent-Vector-Database.py\n\nThen, interact with the system through queries:\n# Start the interactive query interface\npython 20-Query-the-Persistent-RAG-Database.py\nExample interactions:\nYour question: What is edge AI?\n\nGenerating answer...\n\nQuestion: what is EdgeAI?\nRetrieving documents...\nRetrieved 4 document chunks\nGenerating answer...\nResponse latency: 165.72 seconds using model: llama3.2:3b\n\nANSWER:\n==================================================\nEdgeAI refers to the application of artificial intelligence (AI) at the edge \nof a network, typically in real-time applications such as IoT sensors, industrial \nrobots, and smart cameras. The Edge AI ecosystem includes edge devices, edge \nservers, and cloud platforms that work together to enable low-latency AI \ninferencing and processing of data on-site without relying on continuous cloud connectivity. Edge AI technologies aim to solve the world's biggest challenges \nin AI by leveraging energy-efficient, affordable, and scalable solutions for machine\nlearning and advanced edge computing.\n==================================================\n\nThose responses demonstrate how RAG enhances the SLM’s response with specific information from our knowledge base about Edge AI applications on Raspberry Pi. One issue that should be addressed is the latency.\nTo reduce latency, we can use for embedding, the all-minilm model which is much smaller (23M parameters vs. 137M for nomic-embed-text) and creates 384-dimensional embeddings instead of 768, significantly reducing computation time.\nAlso, smaller chunks can be helpful but have some disadvantages. For example, let’s say that we can use a small chunk size (100 tokens with 50 overlap). Here are some considerations:\n\nAdvantages\n\nMemory Efficiency: Smaller chunks require less memory during retrieval and processing, which is beneficial for resource-constrained devices like the Raspberry Pi.\nMore Granular Retrieval: Smaller chunks can potentially provide more precise matches to specific questions, especially for targeted queries about very specific details.\nReduced Context Window Usage: SLMs have limited context windows; smaller chunks allow you to include more distinct pieces of information while staying within these limits.\n\n\n\nDisadvantages\n\nLoss of Context: 100 tokens is approximately 75-80 words, which is often insufficient to capture complete concepts or explanations. Many paragraphs and technical descriptions require more space to convey their full meaning.\nIncreased Vector Store Size: More chunks mean more embeddings to store, potentially increasing the overall size of your vector database.\nFragmented Information: With such small chunks, related information will be split across multiple chunks, making it harder for the model to synthesize coherent answers.\n\n\n\n\nTesting Different Models and Chunk Sizes\nA good practice would be to experiment with different chunk sizes and embedding models and measure:\n\nRetrieval Quality: Are the retrieved chunks relevant to our queries?\nAnswer Accuracy: Does the SLM generate correct and comprehensive answers?\nMemory Usage: Is the system staying within the memory constraints of our device?\nResponse Time: How does chunk size affect latency?\n\nWe can create a simple benchmarking function to have one embedding model defined test the best chunk size:\ndef benchmark_chunk_sizes(document_list, \n                          query_list, \n                          sizes=[(100, 50), (300, 30), (500, 50), (1000, 100)]):\n    \"\"\"Test different chunk sizes and measure performance\"\"\"\n    results = {}\n    \n    for chunk_size, overlap in sizes:\n        print(f\"Testing chunk_size={chunk_size}, overlap={overlap}\")\n        \n        # Create splitter with current settings\n        text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n            chunk_size=chunk_size, chunk_overlap=overlap\n        )\n        \n        # Split documents\n        start_time = time.time()\n        doc_splits = text_splitter.split_documents(document_list)\n        split_time = time.time() - start_time\n        \n        # Create embeddings and store\n        embedding_function = OllamaEmbeddings(model=\"nomic-embed-text\")\n        temp_db_path = f\"temp_db_{chunk_size}_{overlap}\"\n        \n        start_time = time.time()\n        vectorstore = Chroma.from_documents(\n            documents=doc_splits,\n            collection_name=\"benchmark\",\n            embedding=embedding_function,\n            persist_directory=temp_db_path\n        )\n        db_time = time.time() - start_time\n        \n        # Create retriever\n        retriever = vectorstore.as_retriever(k=3)\n        \n        # Test queries\n        query_times = []\n        for query in query_list:\n            start_time = time.time()\n            docs = retriever.invoke(query)\n            query_time = time.time() - start_time\n            query_times.append(query_time)\n        \n        # Store results\n        results[(chunk_size, overlap)] = {\n            \"num_chunks\": len(doc_splits),\n            \"splitting_time\": split_time,\n            \"db_creation_time\": db_time,\n            \"avg_query_time\": sum(query_times) / len(query_times),\n            \"max_query_time\": max(query_times),\n            \"min_query_time\": min(query_times)\n        }\n        \n        # Clean up temporary DB\n        shutil.rmtree(temp_db_path)\n    \n    return results\nRegarding the query side, some optimations can also reduce the latency at the edge. Let’s modify the previous script, with:\n\nDirect Ollama API Calls: Bypasses the LangChain abstraction layer for embedding and LLM generation to reduce overhead.\nEmbedding Caching: Uses lru_cache to prevent recalculating embeddings for repeated queries.\nPreloading Models: Initializes models at startup to avoid cold-start latency.\nOptimized Retriever Settings: Uses minimal k-value (2) and adds a score threshold to filter out irrelevant matches.\nReduced Dependency Usage: Removes unnecessary imports and simplifies the pipeline.\nConcurrent Processing: Uses ThreadPoolExecutor for batch document embedding (when needed).\nEarly Termination: Checks for empty document results before running the LLM.\nSimplified Prompt: Uses a more concise prompt template focused on getting direct answers.\nFixed Seed: Uses a consistent seed for the LLM to reduce variability in response times.\n\nThis optimized version (25-optimized_RAG_query.py) significantly reduces the latency compared to our original implementation while maintaining compatibility with our existing nomic-embed-text vector database and chunk size (300/30).\n\nThe direct Ollama API approach removes several layers of abstraction in the LangChain implementations.\n\nWe can see latency improvements from 2 minutes down to approximately 50-110 seconds, depending on the complexity of the queries.\n\nIn the next section, we’ll explore how RAG can be combined with our agent architecture to create even more powerful edge AI systems."
  },
  {
    "objectID": "raspi/advancing_adgeai/adv_edgeai.html#advanced-agentic-rag-system",
    "href": "raspi/advancing_adgeai/adv_edgeai.html#advanced-agentic-rag-system",
    "title": "Advancing EdgeAI: Beyond Basic SLMs",
    "section": "Advanced Agentic RAG System",
    "text": "Advanced Agentic RAG System\nWe can significantly enhance traditional RAG implementations by incorporating some of the modules discussed earlier, such as intelligent routing, validation feedback loops, and explicit knowledge gap identification. This will provide more reliable and transparent answers for users querying document-based knowledge bases.\nFor example, let’s enhance the last RAG system created on the Edge AI Engineering dataset so that the agent can use tools, such as a calculator for arithmetic calculations.\n\nNote that any tool could be used here; the calculator is only a simple example to demonstrate the concept.\n\nWhen the user asks a question, the system first determines if it needs to use a tool or the RAG approach. For knowledge queries, the RAG system enhances the response with information from the database. The system then validates the answer quality, and if it’s not sufficient, tries again with an improved prompt. In cases where questions fall outside the database’s scope, the system will clearly inform the user rather than attempting to generate potentially misleading answers.\n\nSystem Architecture\n\n\n\nimage-20250322113245945\n\n\nThe system functions through several key components:\n\nQuery Router\n\nAnalyzes incoming queries to determine if they’re calculations or knowledge queries\nCan use the same model for the response generator or a lightweight model to reduce overhead\nImplements rule-based fallbacks for robust classification\n\nDocument Retriever\n\nConnects to a persistent vector database (Chroma)\nUses semantic embeddings to find relevant documents\nReturns contextually similar content for knowledge generation\n\nResponse Generator\n\nCreates answers based on retrieved documents\nImplements a two-stage approach with validation and improvement\nAdds appropriate disclaimers when information is insufficient\n\nValidation Engine\n\nEvaluates answer quality using structured criteria\nAssigns a numerical score to each generated response\nTriggers enhancement processes when quality is insufficient\n\nInteractive Interface\n\nProvides user-friendly interaction with clear quality indicators\nSupports model switching and verbosity control\nOffers guidance for improving query outcomes\n\n\n\n\nKey Workflow\nThe system follows this high-level workflow:\n\nUser submits a query\nRouter determines the query type (calculation vs. knowledge)\nFor calculations:\n\nExtract operation and numbers\nCompute and return result\n\nFor knowledge queries:\n\nRetrieve relevant documents\nGenerate initial answer with RAG\nValidate response quality\nIf quality is low, attempt enhancement with improved prompt\nIf still insufficient, add disclaimer about knowledge gaps\n\nReturn final answer with quality metrics\n\n\n\nImportant Code Sections\n\nQuery Routing\ndef route_query(query: str) -&gt; Dict[str, Any]:\n    \"\"\"Determine if the query is a calculation, otherwise use RAG\"\"\"\n    if VERBOSE:\n        print(f\"Routing query: {query}\")\n    \n    # Check for calculation keywords\n    calc_terms = [\"+\", \"add\", \"plus\", \"sum\", \"-\", \"subtract\", \"minus\", \n                  \"difference\", \"*\", \"×\", \"multiply\", \"times\", \"product\", \n                  \"/\", \"÷\", \"divide\",\n                  \"division\", \"quotient\"]\n    \n    # Simple rule-based detection for calculations\n    is_calc = any(term in query.lower() for term in calc_terms) and \\\n    re.search(r'\\d+', query)\n    \n    if is_calc:\n        # Use smaller, faster model for operation and number extraction\n        # ...extraction logic here...\n        return route_info\n    \n    # For everything else, use RAG\n    return {\"type\": \"rag\", \"reasoning\": \"Non-calculation query, using RAG\"}\n\n\nEnhanced RAG with Feedback Loop\n# First RAG attempt with standard prompt\nanswer = get_answer_with_rag(query, documents, llm)\nprocessing_type = \"rag_standard\"\n\n# Validate the response quality\nvalidation = validate_response(llm, query, answer)\nvalidation_score = validation.get(\"score\", 5)\n\n# If validation score is low, try again with enhanced prompt\nif validation_score &lt; 7:\n    if VERBOSE:\n        print(f\"First RAG attempt validation score: {validation_score}/10. \\\n        Trying enhanced prompt.\")\n    \n    # Second RAG attempt with enhanced prompt\n    enhanced_context = \"\\n\\n\".join(documents)\n    enhanced_prompt = f\"\"\"\n    I need a more detailed and accurate answer to the following question:\n    \n    {query}\n    \n    The previous answer wasn't satisfactory. Let me provide you with \\\n    relevant information:\n    \n    {enhanced_context}\n    \n    Based strictly on this information, provide a comprehensive answer.\n    Focus specifically on addressing the user's question with precise \\\n    information from the provided context.\n    If the information doesn't fully answer the question, clearly state \\\n    what you can determine\n    from the available information and what remains unknown.\n    \"\"\"\n    # ... process enhanced response ...\n\n\nKnowledge Gap Handling\n# If still low quality after enhancement, add a note\nif improved_score &lt; 6:\n    processing_type = \"rag_insufficient_info\"\n    information_gap_note = (\n        \"\\n\\nNote: The information in my knowledge base may be incomplete on this\"\n        \"topic. I've provided the best answer based on available information, but\" \n        \"there might be gaps or additional details that would provide a more \"\n        \"complete answer.\"\n    )\n    answer = answer + information_gap_note\n\n\n\nDetailed Workflow Diagram\n\n\n\n\n\nExamples\nRun the script 30-advanced_agentic_rag.py:\n\nSimple Calculation\n\n\n\nFirst Pass Rag\n\n\n\nMore Complex Queries\n\n\n\nQueries outside of the database scope:"
  },
  {
    "objectID": "raspi/advancing_adgeai/adv_edgeai.html#fine-tuning-slms-for-edge-deployment",
    "href": "raspi/advancing_adgeai/adv_edgeai.html#fine-tuning-slms-for-edge-deployment",
    "title": "Advancing EdgeAI: Beyond Basic SLMs",
    "section": "Fine-Tuning SLMs for Edge Deployment",
    "text": "Fine-Tuning SLMs for Edge Deployment\nFine-tuning can adapt models to specific domains or tasks, improving performance for targeted applications.\n\nPreparing for Fine-Tuning\n# Example dataset for fine-tuning a weather response model\ntraining_data = [\n    {\"input\": \"What's the weather in London?\", \n     \"output\": \"I need to check London's current weather. Please use the \\\n     weather tool.\"},\n    {\"input\": \"Is it going to rain tomorrow in Paris?\", \n     \"output\": \"To answer about tomorrow's weather in Paris, \\\n     I need to use the weather tool.\"},\n    {\"input\": \"Will it be sunny this weekend in Tokyo?\", \n     \"output\": \"To predict Tokyo's weekend weather, \\\n     I should use the weather tool.\"}\n]\n\n# Format data for fine-tuning\nformatted_data = []\nfor item in training_data:\n    formatted_data.append({\n        \"prompt\": item[\"input\"],\n        \"response\": item[\"output\"]\n    })\n\n# Save formatted data to a file\nimport json\nwith open(\"weather_finetune_data.json\", \"w\") as f:\n    json.dump(formatted_data, f)\n\n\nSetting Up a Fine-Tuning Process\nFine-tuning on edge devices is typically impractical due to resource constraints. Instead, fine-tune on a more powerful machine and deploy the result to the edge:\n\nThis is a conceptual example - actual implementation depends on the framework\n\ndef prepare_for_finetuning(data_path, output_path):\n    \"\"\"\n      Prepare a model for fine-tuning \n      (run this on a more powerful machine)\n    \"\"\"\n    # This is a conceptual example \n    print(f\"Fine-tuning model using data from {data_path}\")\n    print(f\"Fine-tuned model will be saved to {output_path}\")\n    \n    # The process would typically involve:\n    # 1. Loading the base model\n    # 2. Loading and preprocessing the training data\n    # 3. Setting up training parameters (learning rate, epochs, etc.)\n    # 4. Running the fine-tuning process\n    # 5. Evaluating the fine-tuned model\n    # 6. Saving and optimizing for edge deployment\n    \n    # For Ollama, we would create a custom model definition (Modelfile)\n    modelfile = f\"\"\"\n    FROM llama3.2:1b\n    \n    # Fine-tuning settings would go here\n    PARAMETER temperature 0.7\n    PARAMETER top_p 0.9\n    PARAMETER top_k 40\n    \n    # Custom system prompt for the specific domain\n    SYSTEM You are a specialized assistant for weather-related questions.\n    \"\"\"\n    \n    with open(output_path, \"w\") as f:\n        f.write(modelfile)\n    \n    print(\"Model preparation complete. Next steps:\")\n    print(\"1. Run fine-tuning on a powerful machine\")\n    print(\"2. Optimize the resulting model for edge deployment\")\n    print(\"3. Deploy to your Raspberry Pi\")\n\n\nReal implementation: Supervised Fine-Tuning (SFT)\nSupervised fine tuning (SFT) is a method to improve and customize pre-trained LLMs. It involves retraining base models on a smaller dataset of instructions and answers. The main goal is to transform a basic model that predicts text into an assistant that can follow instructions and answer questions. SFT can also enhance the model’s overall performance, add new knowledge, or adapt it to specific tasks and domains\nBefore considering SFT, it is recommended to try prompt engineering techniques like few-shot prompting or retrieval augmented generation (RAG), as discussed previously. In practice, these methods can solve many problems without fine-tuning. If this approach doesn’t meet our objectives (regarding quality, cost, latency, etc.), then SFT becomes a viable option when instruction data is available. SFT also offers benefits like additional control and customizability to create personalized LLMs.\nHowever, SFT has limitations. It works best when leveraging knowledge already present in the base model. Learning completely new information, like an unknown language, can be challenging and lead to more frequent hallucinations. For new domains unknown to the base model, it is recommended that it be continuously pre-trained on a raw dataset first.\nOn the opposite end of the spectrum, instruct models (i.e., already fine-tuned models) can be very close to our requirements. By providing chosen and rejected samples for a small set of instructions (between 100 and 1000 samples), we can force the LLM to behave as we need.\nThe easiest way to finetune an SLM is by using Unsloth. The three most popular SFT techniques are full fine-tuning, LoRA, and QLoRA.\n\nFor details, see: Fine-tune Llama 3.1 Ultra-Efficiently with Unsloth.\n\nFor example, using this link, it is possible to find several notebooks with the steps to finetune SLMs—for instance, the Gemma 3:1B.\nThe fine-tuned model can be saved on HF Hub or locally as GGUF, and to run a GGUF model locally, we can use Ollama, as shown below:\n\nDownload the finetuned GGUF model\nCreate a Modelfile in the home directory:\n\ncd ~\nnano Modelfile\n\nIn the Modelfile, specify the path to the GGUF file:\n\nFROM ~/Downloads/your-model-name.gguf\nPARAMETER temperature 1.0\nPARAMETER top_p 0.95\nPARAMETER top_k 64\n\nSave the Modelfile and exit the editor.\nCreate the loadable model in Ollama:\n\nollama create your-model-name -f Modelfile\n\nThe model name here can be anything.\n\n\nWe can now use the model through as we have done with the llama3.2:3B in this chapter.."
  },
  {
    "objectID": "raspi/advancing_adgeai/adv_edgeai.html#conclusion",
    "href": "raspi/advancing_adgeai/adv_edgeai.html#conclusion",
    "title": "Advancing EdgeAI: Beyond Basic SLMs",
    "section": "Conclusion",
    "text": "Conclusion\nThis chapter has explored comprehensive strategies for overcoming the inherent limitations of Small Language Models in edge computing environments. By implementing techniques ranging from optimized prompting strategies to sophisticated agent architectures and knowledge integration systems, we’ve demonstrated that it’s possible to significantly enhance the capabilities of edge AI systems without requiring more powerful hardware or cloud connectivity.\nThe techniques presented—chain-of-thought prompting, task decomposition, function calling, response validation, and RAG—form a toolkit that edge AI engineers can apply individually or in combination to address specific challenges. Each approach offers unique advantages: prompting techniques improve reasoning capabilities with minimal overhead, agent architectures enable SLMs to perform actions beyond text generation, and RAG systems dramatically expand an SLM’s knowledge without increasing model size.\nOur practical implementations on the Raspberry Pi showcase that these enhancements are not merely theoretical but can be deployed in real-world edge scenarios. From the simple calculator agent to the more sophisticated knowledge router and RAG-enabled question answering system, these examples provide templates that developers can adapt to their specific application requirements.\nThe true power of these techniques emerges when they’re strategically combined. An agent architecture with RAG capabilities, enhanced by chain-of-thought reasoning and validated with a feedback loop, creates an edge AI system that approaches the capabilities of much larger models while maintaining the advantages of edge deployment—privacy preservation, reduced latency, and operation without internet connectivity.\nAs edge AI continues to evolve, these techniques will become increasingly important in bridging the gap between the limited resources available on edge devices and the growing expectations for AI capabilities. By thoughtfully applying these approaches, developers can create intelligent systems that process data locally, respect user privacy, and operate reliably in diverse environments.\nThe future of edge AI lies not necessarily in deploying ever-larger models but in developing more innovative systems that combine efficient models with intelligent architectures, contextual knowledge integration, and robust validation mechanisms. By mastering these techniques, edge AI practitioners can create solutions that are not just technologically impressive but genuinely useful and trustworthy in addressing real-world challenges."
  },
  {
    "objectID": "raspi/advancing_adgeai/adv_edgeai.html#resources",
    "href": "raspi/advancing_adgeai/adv_edgeai.html#resources",
    "title": "Advancing EdgeAI: Beyond Basic SLMs",
    "section": "Resources",
    "text": "Resources\nThe scripts used in this chapter can be found here: Advancing EdgeAI Scripts"
  },
  {
    "objectID": "weekly_labs.html#week-1-introduction-and-setup",
    "href": "weekly_labs.html#week-1-introduction-and-setup",
    "title": "Edge AI Engineering - Weekly Labs",
    "section": "Week 1: Introduction and Setup",
    "text": "Week 1: Introduction and Setup\n\nLab 1: Raspberry Pi Configuration\nObjectives:\n\nInstall Raspberry Pi OS using Raspberry Pi Imager\nConfigure basic settings (hostname, SSH, WiFi)\nLearn essential Linux commands\nManage files between your computer and Raspberry Pi\n\nInstructions:\n\nDownload Raspberry Pi Imager on your computer\nConfigure OS settings (enable SSH, set hostname, WiFi credentials)\nBoot your Raspberry Pi and confirm connectivity\nLearn how to use SSH for remote access\nTransfer files using SCP or FileZilla\nUpdate your Raspberry Pi OS (sudo apt update && sudo apt upgrade)\nPractice basic Linux commands (ls, cd, mkdir, cp, mv)\n\nDeliverable: Screenshot showing successful SSH connection to your Raspberry Pi\n\n\nLab 2: Development Environment Setup\nObjectives:\n\nSet up Python environment for development\nConfigure remote development tools\nInstall essential libraries\nTest camera functionality\n\nInstructions:\n\nInstall Python essentials: pip install jupyter matplotlib numpy pillow\nConfigure Jupyter Notebook for remote access:\npip install jupyter\njupyter notebook --generate-config\njupyter notebook --ip=0.0.0.0 --no-browser\nConnect the camera module (USB or CSI) to your Raspberry Pi\nTest camera functionality using command-line tools\nWrite a simple Python script to capture an image\n\nDeliverable: A simple Python script that captures and displays an image from your camera and a Screenshot showing a successful image capture"
  },
  {
    "objectID": "weekly_labs.html#week-2-image-classification-fundamentals",
    "href": "weekly_labs.html#week-2-image-classification-fundamentals",
    "title": "Edge AI Engineering - Weekly Labs",
    "section": "Week 2: Image Classification Fundamentals",
    "text": "Week 2: Image Classification Fundamentals\n\nLab 3: Working with Pre-trained Models\nObjectives:\n\nInstall TensorFlow Lite runtime\nDownload and run MobileNet V2 model\nProcess and classify images\nUnderstand model inputs and outputs\n\nInstructions:\n\nInstall TensorFlow Lite runtime: pip install tflite_runtime\nDownload MobileNet V2 model:\nwget https://storage.googleapis.com/download.tensorflow.org/models/tflite_11_05_08/mobilenet_v2_1.0_224_quant.tgztar xzf mobilenet_v2_1.0_224_quant.tgz\nDownload labels file\nCreate a Python script that:\n\nLoads the TFLite model\nProcesses input images to 224x224 format\nRuns inference on test images\nDisplays top-5 predicted classes with confidence scores\n\n\nDeliverable: Python script that successfully classifies sample images with MobileNet V2 and a Screenshot showing a successful result\n\n\nLab 4: Custom Dataset Creation\nObjectives:\n\nCreate a simple custom dataset using Raspberry Pi camera\nOrganize images into classes\nPrepare dataset for model training\n\nInstructions:\n\nCreate a web interface for image capture:\n\nUse Flask to create a simple web server\nSet up camera preview and capture functionality\nSave captured images with appropriate filenames\n\nCapture at least 50 images per class for 3 classes\nOrganize the dataset into an appropriate directory structure\nDocument your dataset creation process\n\nDeliverable: Structured dataset with at least 3 classes and 50 images per class"
  },
  {
    "objectID": "weekly_labs.html#week-3-custom-image-classification",
    "href": "weekly_labs.html#week-3-custom-image-classification",
    "title": "Edge AI Engineering - Weekly Labs",
    "section": "Week 3: Custom Image Classification",
    "text": "Week 3: Custom Image Classification\n\nLab 5: Edge Impulse Model Training\nObjectives:\n\nCreate an Edge Impulse project\nUpload and process the dataset\nDesign and train a transfer learning model\nEvaluate model performance\n\nInstructions:\n\nCreate an Edge Impulse account and a new project\nUpload your custom dataset\nCreate an impulse design:\n\nSet image size to 160x160\nUse Transfer Learning for feature extraction\n\nGenerate features for all images\nTrain model using MobileNet V2\nAnalyze model performance (accuracy, confusion matrix)\nTest model on validation data\n\nDeliverable: Edge Impulse project link and screenshot of model performance metrics\n\n\nLab 6: Model Deployment to Raspberry Pi\nObjectives:\n\nExport trained model to TFLite format\nDeploy model to Raspberry Pi\nCreate a real-time inference application\nOptimize inference speed\n\nInstructions:\n\nExport model as TensorFlow Lite (.tflite)\nTransfer the model to Raspberry Pi\nCreate a Python application that:\n\nCaptures live images from the camera\nPreprocesses images for the model\nRuns inference and displays results\nShows confidence scores\n\nImplement a web interface for real-time classification\n\nDeliverable: Python script for real-time image classification with your custom model and a Screenshot showing a successful result"
  },
  {
    "objectID": "weekly_labs.html#week-4-object-detection-fundamentals",
    "href": "weekly_labs.html#week-4-object-detection-fundamentals",
    "title": "Edge AI Engineering - Weekly Labs",
    "section": "Week 4: Object Detection Fundamentals",
    "text": "Week 4: Object Detection Fundamentals\n\nLab 7: Pre-trained Object Detection\nObjectives:\n\nUnderstand object detection architecture\nRun pre-trained SSD-MobileNet model\nProcess detection outputs\nVisualize detected objects\n\nInstructions:\n\nDownload the pre-trained SSD-MobileNet V1 model\nCreate a Python script that:\n\nLoads the model and labels\nPreprocesses input images\nRuns inference\nExtracts bounding boxes, classes, and scores\nImplements Non-Maximum Suppression (NMS)\nVisualizes detections with bounding boxes\n\nTest on various images with multiple objects\n\nDeliverable: Python script that performs and visualizes object detection on test images and a Screenshot showing a successful result.\n\n\nLab 8: EfficientDet and FOMO Models\nObjectives:\n\nCompare different object detection architectures\nImplement EfficientDet and FOMO models\nAnalyze performance differences\nUnderstand trade-offs between models\n\nInstructions:\n\nDownload the EfficientDet Lite0 model\nImplement inference with EfficientDet\nCompare with SSD-MobileNet implementation\nLearn about FOMO (Faster Objects, More Objects)\nAnalyze trade-offs in accuracy vs. speed\nMeasure inference time on Raspberry Pi\n\nDeliverable: Comparison report of SSD-MobileNet vs. EfficientDet with performance metrics and visualized results"
  },
  {
    "objectID": "weekly_labs.html#week-5-custom-object-detection",
    "href": "weekly_labs.html#week-5-custom-object-detection",
    "title": "Edge AI Engineering - Weekly Labs",
    "section": "Week 5: Custom Object Detection",
    "text": "Week 5: Custom Object Detection\n\nLab 9: Dataset Creation and Annotation\nObjectives:\n\nCreate an object detection dataset\nLearn annotation techniques\nPrepare dataset for model training\n\nInstructions:\n\nCapture at least 100 images containing objects to detect\nUpload images to Roboflow or a similar annotation tool\nCreate bounding box annotations for each object\nApply data augmentation (rotation, brightness adjustment)\nExport dataset in YOLO format\nDocument the annotation process\n\nDeliverable: Annotated dataset with at least 2 object classes and 100 total images\n\n\nLab 10: Training Models in Edge Impulse\nObjectives:\n\nUpload annotated dataset to Edge Impulse\nTrain SSD MobileNet object detection model\nEvaluate model performance\nExport model for deployment\n\nInstructions:\n\nCreate a new Edge Impulse project for object detection\nUpload annotated dataset (train/test splits)\nCreate object detection impulse\nTrain SSD MobileNet model\nEvaluate model performance\nExport model as TensorFlow Lite\n\nDeliverable: Edge Impulse project link with trained object detection model and performance metrics"
  },
  {
    "objectID": "weekly_labs.html#week-6-advanced-object-detection",
    "href": "weekly_labs.html#week-6-advanced-object-detection",
    "title": "Edge AI Engineering - Weekly Labs",
    "section": "Week 6: Advanced Object Detection",
    "text": "Week 6: Advanced Object Detection\n\nLab 11: FOMO Model Training\nObjectives:\n\nUnderstanding FOMO architecture benefits\nTrain FOMO model on Edge Impulse\nCompare performance with SSD MobileNet\nDeploy optimized model to Raspberry Pi\n\nInstructions:\n\nCreate a new impulse in Edge Impulse using the same dataset\nTrain FOMO model instead of SSD MobileNet\nCompare inference speed and accuracy\nDeploy both models to Raspberry Pi\nCreate an application that can switch between models\nMeasure and document performance differences\n\nDeliverable: Python application that compares SSD MobileNet vs. FOMO performance in real-time\n\n\nLab 12: YOLO Implementation\nObjectives:\n\nInstall and configure Ultralytics YOLO\nConvert models to optimized NCNN format\nCreate real-time detection application\nImplement object counting\n\nInstructions:\n\nInstall Ultralytics: pip install ultralytics\nDownload and test YOLO (n) model\nExport model to NCNN format for optimization\nCreate Python script for real-time detection\nImplement object counting algorithm\nAdd visualization of counts over time\n\nDeliverable: Python application for real-time object detection and counting using YOLO"
  },
  {
    "objectID": "weekly_labs.html#week-7-object-counting-project",
    "href": "weekly_labs.html#week-7-object-counting-project",
    "title": "Edge AI Engineering - Weekly Labs",
    "section": "Week 7: Object Counting Project",
    "text": "Week 7: Object Counting Project\n\nLab 13: Custom YOLO Training\nObjectives:\n\nTrain YOLO on a custom dataset\nOptimize model for edge deployment\nCreate a complete application for object counting\n\nInstructions:\n\nTrain the YOLO model on your custom dataset\n\nUse Google Colab for training if needed\nSet appropriate hyperparameters\n\nExport optimized model for Raspberry Pi\nCreate a Python application that:\n\nCaptures video feed\nDetects objects using YOLO\nCounts objects over time\nLogs results to a database\n\n\nDeliverable: Complete object counting application with data logging\n\n\nLab 14: Fixed-Function AI Integration (Optional)\nObjectives:\n\nIntegrate multiple AI models into a single application\nCreate a dashboard for visualization\nOptimize application for long-term deployment\n\nInstructions:\n\nCreate an integration application that combines:\n\nObject detection capabilities\nClassification for detected objects\nCounting and tracking over time\n\nImplement a simple web dashboard for visualization\nAdd performance monitoring\nConfigure application for startup at boot\n\nDeliverable: Integrated application combining multiple AI capabilities with visualization dashboard"
  },
  {
    "objectID": "weekly_labs.html#week-8-introduction-to-generative-ai",
    "href": "weekly_labs.html#week-8-introduction-to-generative-ai",
    "title": "Edge AI Engineering - Weekly Labs",
    "section": "Week 8: Introduction to Generative AI",
    "text": "Week 8: Introduction to Generative AI\n\nLab 15: Raspberry Pi Configuration for SLMs\nObjectives:\n\nOptimize Raspberry Pi for running Small Language Models\nInstall an active cooling solution\nConfigure memory and swap\nInstall essential libraries\n\nInstructions:\n\nInstall active cooling solution on Raspberry Pi 5\nOptimize system configuration:\n\nIncrease swap memory: sudo dphys-swapfile swapoff, edit /etc/dphys-swapfile\nSet CONF_SWAPSIZE to 2048\nsudo dphys-swapfile setup && sudo dphys-swapfile swapon\n\nInstall dependencies:\nsudo apt update\nsudo apt install build-essential python3-dev\n\nDeliverable: Screenshot showing system configuration with increased swap and temperature monitor during stress test\n\n\nLab 16: Ollama Installation and Testing\nObjectives:\n\nInstall Ollama framework\nPull and test Small Language Models\nBenchmark model performance\nMonitor resource usage\n\nInstructions:\n\nInstall Ollama:\ncurl -fsSL https://ollama.com/install.sh | sh\nPull different models (such as):\nollama pull llama3.2:1b\nollama pull gemma:2b\nollama pull phi3:latest\nRun a basic inference test with each model\nMeasure and compare:\n\nLoad time\nInference speed (tokens/sec)\nMemory usage\nTemperature\n\n\nDeliverable: Benchmark report comparing performance metrics of different SLM models on your Raspberry Pi"
  },
  {
    "objectID": "weekly_labs.html#week-9-slm-python-integration",
    "href": "weekly_labs.html#week-9-slm-python-integration",
    "title": "Edge AI Engineering - Weekly Labs",
    "section": "Week 9: SLM Python Integration",
    "text": "Week 9: SLM Python Integration\n\nLab 17: Ollama Python Library\nObjectives:\n\nUse the Ollama Python library\nCreate interactive applications\nProcess SLM responses programmatically\nHandle multiple conversation turns\n\nInstructions:\n\nInstall Ollama Python library: pip install ollama\nCreate Python script to:\n\nConnect to Ollama API\nSend prompts to models\nProcess and format responses\nHandle conversation context\n\nImplement proper error handling\nCreate a simple interactive CLI application\n\nDeliverable: Python script demonstrating Ollama library usage with conversation handling\n\n\nLab 18: Function Calling and Structured Outputs\nObjectives:\n\nImplement function calling with SLMs\nCreate applications with structured outputs\nBuild validation mechanisms\nHandle image inputs\n\nInstructions:\n\nInstall required libraries:\npip install pydantic instructor openai\nCreate Pydantic models for structured outputs\nImplement function calling with an instructor:\nclient = instructor.patch(    OpenAI(base_url=\"http://localhost:11434/v1\", api_key=\"ollama\"),    mode=instructor.Mode.JSON,)\nBuild distance calculator application using SLM for city/country recognition\nAdd image input processing\n\nDeliverable: Python application that uses function calling for structured interaction with SLMs"
  },
  {
    "objectID": "weekly_labs.html#week-10-retrieval-augmented-generation",
    "href": "weekly_labs.html#week-10-retrieval-augmented-generation",
    "title": "Edge AI Engineering - Weekly Labs",
    "section": "Week 10: Retrieval-Augmented Generation",
    "text": "Week 10: Retrieval-Augmented Generation\n\nLab 19: RAG Fundamentals\nObjectives:\n\nUnderstand RAG architecture\nCreate vector database\nImplement embedding generation\nBuild simple RAG system\n\nInstructions:\n\nInstall required libraries:\npip install langchain chromadb\nCreate a simple dataset with text documents\nImplement document splitting and chunking\nGenerate embeddings using Ollama\nStore embeddings in ChromaDB\nCreate query system\n\nDeliverable: Python implementation of a basic RAG system with simple text documents\n\n\nLab 20: Advanced RAG\nObjectives:\n\nOptimize RAG for edge devices\nImplement more efficient retrieval\nCreate a specialized knowledge base\nBuild validation mechanisms\n\nInstructions:\n\nCreate a specialized knowledge base (e.g., technical documentation)\nImplement optimized embedding generation\nFine-tune retrieval parameters\nAdd response validation\nCreate a persistent vector store\nBenchmark performance\n\nDeliverable: Optimized RAG implementation with specialized knowledge base and performance analysis"
  },
  {
    "objectID": "weekly_labs.html#week-11-vision-language-models",
    "href": "weekly_labs.html#week-11-vision-language-models",
    "title": "Edge AI Engineering - Weekly Labs",
    "section": "Week 11: Vision-Language Models",
    "text": "Week 11: Vision-Language Models\n\nLab 21: Florence-2 Setup\nObjectives:\n\nInstall Florence-2 model\nConfigure environment\nRun basic inference tests\nUnderstand model capabilities\n\nInstructions:\n\nInstall required dependencies:\npip install transformers torch torchvision torchaudio\npip install timm einops\npip install autodistill-florence-2\nDownload model and test basic functionality\nRun image captioning test\nMeasure performance (memory usage, inference time)\n\nDeliverable: Python script demonstrating basic Florence-2 functionality with performance metrics\n\n\nLab 22: Vision Tasks with Florence-2\nObjectives:\n\nImplement various vision tasks\nCreate applications for captioning, detection, grounding\nOptimize performance\nCombine tasks\n\nInstructions:\n\nImplement image captioning:\n\nBasic caption generation\nDetailed caption generation\n\nImplement object detection:\n\nBounding box visualization\nMultiple object detection\n\nImplement visual grounding:\n\nHighlight specific objects based on text prompts\n\nCreate segmentation application\nMeasure the performance of each task\n\nDeliverable: Python application demonstrating multiple vision tasks with Florence-2 and performance analysis"
  },
  {
    "objectID": "weekly_labs.html#week-12-physical-computing-basics",
    "href": "weekly_labs.html#week-12-physical-computing-basics",
    "title": "Edge AI Engineering - Weekly Labs",
    "section": "Week 12: Physical Computing Basics",
    "text": "Week 12: Physical Computing Basics\n\nLab 23: Sensor and Actuator Integration\nObjectives:\n\nConnect digital sensors\nRead environmental data\nControl LEDs and actuators\nCreate a data collection system\n\nInstructions:\n\nConnect hardware components:\n\nDHT22 temperature/humidity sensor\nBMP280 pressure sensor\nLEDs (red, yellow, green)\nPush button\n\nInstall required libraries:\npip install adafruit-circuitpython-dht adafruit-circuitpython-bmp280\nCreate a Python script to read sensor data\nImplement LED control based on conditions\nCreate visualization of sensor data\n\nDeliverable: Python application for reading sensor data and controlling actuators with visualization\n\n\nLab 24: Jupyter Notebook Integration\nObjectives:\n\nUse Jupyter Notebook for physical computing\nCreate interactive widgets\nVisualize sensor data in real-time\nControl actuators from a notebook\n\nInstructions:\n\nInstall ipywidgets: pip install ipywidgets\nCreate a Jupyter Notebook for sensor data collection\nImplement interactive widgets for control\nCreate real-time visualization\nBuild a dashboard with multiple data views\n\nDeliverable: Jupyter Notebook with interactive widgets for sensor monitoring and actuator control"
  },
  {
    "objectID": "weekly_labs.html#week-13-slm-physical-computing-integration",
    "href": "weekly_labs.html#week-13-slm-physical-computing-integration",
    "title": "Edge AI Engineering - Weekly Labs",
    "section": "Week 13: SLM-Physical Computing Integration",
    "text": "Week 13: SLM-Physical Computing Integration\n\nLab 25: Basic SLM Analysis\nObjectives:\n\nIntegrate SLMs with sensor data\nCreate analysis application\nImplement decision-making logic\nControl actuators based on SLM responses\n\nInstructions:\n\nCreate a Python application that:\n\nCollects sensor data\nFormats data for SLM prompt\nSends prompt to model\nParses response\nControls actuators based on response\n\nImplement multiple analysis modes\nAdd error handling for SLM responses\n\nDeliverable: Python application integrating SLMs with physical sensors and actuators\n\n\nLab 26: SLM-IoT Control System\nObjectives:\n\nCreate a complete IoT monitoring system\nImplement natural language interaction\nAdd data logging and analysis\nCreate web interface\n\nInstructions:\n\nBuild a complete system with:\n\nSensor data collection\nSLM-based analysis\nNatural language command processing\nData logging to the database\nWeb interface for interaction\n\nImplement multiple SLM models\nAdd historical data analysis\nCreate visualization dashboard\n\nDeliverable: Complete IoT monitoring system with SLM integration and web interface"
  },
  {
    "objectID": "weekly_labs.html#week-14-advanced-edge-ai-techniques",
    "href": "weekly_labs.html#week-14-advanced-edge-ai-techniques",
    "title": "Edge AI Engineering - Weekly Labs",
    "section": "Week 14: Advanced Edge AI Techniques",
    "text": "Week 14: Advanced Edge AI Techniques\n\nLab 27: Building Agents\nObjectives:\n\nCreate agent architecture\nImplement tool usage\nBuild decision-making system\nHandle complex tasks\n\nInstructions:\n\nImplement calculator agent:\n\nCreate a query routing system\nImplement tool functions\nBuild decision-making logic\n\nCreate knowledge router:\n\nImplement web search integration\nBuild classification system\nHandle time-based queries\n\nMeasure and optimize performance\n\nDeliverable: Python implementation of agent architecture with tool usage and decision routing\n\n\nLab 28: Advanced Prompting and Validation\nObjectives:\n\nImplement chain-of-thought prompting\nCreate few-shot learning examples\nBuild task decomposition system\nImplement response validation\n\nInstructions:\n\nCreate examples for different prompting strategies\nImplement chain-of-thought framework\nBuild few-shot learning templates\nCreate a task decomposition system\nImplement validation mechanisms\nCompare the effectiveness of different strategies\n\nDeliverable: Python implementation demonstrating different prompting strategies with performance comparison"
  },
  {
    "objectID": "weekly_labs.html#week-15-final-project-integration",
    "href": "weekly_labs.html#week-15-final-project-integration",
    "title": "Edge AI Engineering - Weekly Labs",
    "section": "Week 15: Final Project Integration",
    "text": "Week 15: Final Project Integration\n\nLab 29: Agentic RAG System\nObjectives:\n\nCombine agent architecture with RAG\nCreate a complete knowledge system\nImplement advanced validation\nBuild query optimization\n\nInstructions:\n\nCreate a complete agentic RAG system:\n\nBuild knowledge database\nImplement agent architecture\nAdd tool functions\nCreate validation mechanisms\nOptimize retrieval\n\nTest with complex queries\nMeasure performance\nCreate visualization of system components\n\nDeliverable: Complete agentic RAG system with documentation and performance analysis\n\n\nLab 30: Final Project\nObjectives:\n\nDesign and implement a comprehensive Edge AI system\nCombine multiple techniques\nCreate complete documentation\nPresent project\n\nInstructions:\n\nDesign final project combining:\n\nComputer vision capabilities\nSLM integration\nPhysical computing\nAdvanced techniques (RAG, agents, etc.)\n\nImplement complete system\nCreate documentation\nMeasure performance\nPrepare presentation\n\nDeliverable: Complete final project with documentation, code, and presentation"
  },
  {
    "objectID": "weekly_labs.html#hardware-requirements",
    "href": "weekly_labs.html#hardware-requirements",
    "title": "Edge AI Engineering - Weekly Labs",
    "section": "Hardware Requirements",
    "text": "Hardware Requirements\n\nBasic Setup (Weeks 1-7)\n\nRaspberry Pi Zero 2W or Pi 5\nMicroSD card (32GB+)\nCamera module (USB webcam or Pi camera)\nPower supply\n\n\n\nGenerative AI (Weeks 8-15)\n\nRaspberry Pi 5 (8GB RAM recommended)\nActive cooling solution\nMicroSD card (64GB+ recommended)\n\n\n\nPhysical Computing (Weeks 12-15)\n\nDHT22 temperature/humidity sensor\nBMP280 pressure/temperature sensor\nLEDs (red, yellow, green)\nPush button\nResistors (4.7kΩ, 330Ω)\nJumper wires\nBreadboard"
  },
  {
    "objectID": "weekly_labs.html#software-requirements",
    "href": "weekly_labs.html#software-requirements",
    "title": "Edge AI Engineering - Weekly Labs",
    "section": "Software Requirements",
    "text": "Software Requirements\n\nDevelopment Environment\n\nRaspberry Pi OS (64-bit)\nPython 3.9+\nJupyter Notebook\nSSH client\n\n\n\nComputer Vision and DL\n\nTensorFlow Lite runtime\nOpenCV\nEdge Impulse\nUltralytics\n\n\n\nGenerative AI\n\nOllama\nTransformers\nPytorch\nChromaDB\nLangChain\nPydantic\nInstructor\n\n\n\nPhysical Computing\n\nGPIO Zero\nAdafruit CircuitPython libraries"
  },
  {
    "objectID": "weekly_labs.html#assessment-criteria",
    "href": "weekly_labs.html#assessment-criteria",
    "title": "Edge AI Engineering - Weekly Labs",
    "section": "Assessment Criteria",
    "text": "Assessment Criteria\nEach lab will be evaluated based on:\n\nFunctionality (40%): Does the implementation work as specified?\nCode Quality (20%): Is the code well-structured, documented, and efficient?\nDocumentation (20%): Are the process and results documented?\nAnalysis (20%): Is there a thoughtful analysis of results and performance?\n\nThe final project will be evaluated based on:\n\nIntegration (30%): How well different components are integrated\nInnovation (20%): Novel approaches or applications\nImplementation (30%): Overall quality and functionality\nPresentation (20%): Clear explanation and demonstration"
  },
  {
    "objectID": "weekly_labs.html#tips-for-success",
    "href": "weekly_labs.html#tips-for-success",
    "title": "Edge AI Engineering - Weekly Labs",
    "section": "Tips for Success",
    "text": "Tips for Success\n\nStart Early: These labs build on each other. Falling behind makes later labs more difficult.\nDocument As You Go: Take notes, screenshots, and document issues/solutions.\nOptimize Resources: SLMs and VLMs require careful resource management.\nCollaborate: Discuss approaches with classmates while ensuring individual work.\nBackup Regularly: Create backups of your SD card after significant progress.\nMeasure Performance: Always benchmark and optimize your implementations.\nAsk Questions: If you’re stuck, ask for help early rather than falling behind."
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "TinyML4D\nTinyML Made Easy, an eBook collection of a series of Hands-On tutorials, is part of the TinyML4D, an initiative to make Embedded Machine Learning (TinyML) education available to everyone, explicitly enabling innovative solutions for the unique challenges Developing Countries face."
  },
  {
    "objectID": "references.html#to-learn-more",
    "href": "references.html#to-learn-more",
    "title": "References",
    "section": "To learn more:",
    "text": "To learn more:\n\nOnline Courses\n\nHarvard School of Engineering and Applied Sciences - CS249r: Tiny Machine Learning\nProfessional Certificate in Tiny Machine Learning (TinyML) – edX/Harvard\nIntroduction to Embedded Machine Learning - Coursera/Edge Impulse\nComputer Vision with Embedded Machine Learning - Coursera/Edge Impulse\nUNIFEI-IESTI01 TinyML: “Machine Learning for Embedding Devices”\n\n\n\nBooks\n\n“Python for Data Analysis” by Wes McKinney\n“Deep Learning with Python” by François Chollet - GitHub Notebooks\n“TinyML” by Pete Warden and Daniel Situnayake\n“TinyML Cookbook 2nd Edition” by Gian Marco Iodice\n“Technical Strategy for AI Engineers, In the Era of Deep Learning” by Andrew Ng\n“AI at the Edge” book by Daniel Situnayake and Jenny Plunkett\n“XIAO: Big Power, Small Board” by Lei Feng and Marcelo Rovai\n“Machine Learning Systems” by Vijay Janapa Reddi\n\n\n\nProjects Repository\n\nEdge Impulse Expert Network"
  },
  {
    "objectID": "about_the_author.html",
    "href": "about_the_author.html",
    "title": "About the author",
    "section": "",
    "text": "Marcelo Rovai, a Brazilian living in Chile, is a recognized engineering and technology education figure. He holds the title of Professor Honoris Causa from the Federal University of Itajubá (UNIFEI), Brazil. His educational background includes an Engineering degree from UNIFEI and a specialization from the Polytechnic School of São Paulo University (POLI/USP). Further enhancing his expertise, he earned an MBA from IBMEC (INSPER) and a Master’s in Data Science from the Universidad del Desarrollo (UDD) in Chile.\nWith a career spanning several high-profile technology companies such as AVIBRAS Airspace, AT&T, NCR, and IGT, where he served as Vice President for Latin America, he brings industry experience to his academic endeavors. He is a prolific writer on electronics-related topics and shares his knowledge through open platforms like Hackster.io.\nIn addition to his professional pursuits, he is dedicated to educational outreach, serving as a volunteer professor at UNIFEI and engaging with the TinyML4D group and the EDGE AIP– the Academia-Industry Partnership of EDGEAI Foundation as a Co-Chair, promoting TinyML education in developing countries. His work underscores a commitment to leveraging technology for societal advancement.\nLinkedIn profile: https://www.linkedin.com/in/marcelo-jose-rovai-brazil-chile/\nLectures, books, papers, and tutorials: https://github.com/Mjrovai/TinyML4D"
  }
]